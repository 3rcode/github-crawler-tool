Commit Message,Score,Correspond Changelog Sentence,Commit Description,Label
docs: adjust base image to ubuntu20.04 (#17846),0.33005488,    # 2: Convert the image into a PIL Image to bytes and encode it with base64,,0
"Update rich requirement from <=13.0.1,>=12.3.0 to >=12.3.0,<=13.4.2 in /requirements (#17834)",0.56137323,Improved exception message if rich version is less than 10.2.2 (#10839),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
bump starlette & fastapi req. (#17615),0.4002626,"@awaelchli, @borda, @carmocca, @kaushikb11, @tchaton",,0
"Update redis requirement from <=4.2.4,>=4.0.1 to >=4.0.1,<=4.5.5 in /requirements (#17751)",0.4061278,Setup: added requirement freeze for the next major version (#14480),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Update websocket-client requirement from <1.5.2 to <1.5.3 in /requirements (#17752),0.38697737,"    version=""0.0.1"",",Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
"Update panel requirement from <=0.13.1,>=0.12.7 to >=0.12.7,<=1.0.2 in /requirements (#17675)",0.49098724,Setup: added requirement freeze for the next major version (#14480),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
Update urllib3 requirement from <=1.26.13 to <=2.0.2 in /requirements (#17753),0.5014738,Add missing python-multipart dependency (#17244),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Bump vite from 2.9.13 to 2.9.16 in /src/lightning/app/cli/react-ui-template/ui (#17760),0.5318257,- Removed the deprecated `LightningCLI` arguments ([#16380](https://github.com/Lightning-AI/lightning/pull/16380)),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Remove automatic sharding support with Fabric.run or fabric.launch(fn) (#17832),0.61099494,The Fabric.run() method is no longer abstract (#14992),Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
Document tensor shape requirements in all_gather (#17816),0.6693094,Auto convert tensors to contiguous format when gather_all (#4907),,0
ci: hotfix for doctests (#17841),0.5313719,The point of this release is more bug fixes ahead of v 1.0.0. We now have CI tests on TPU thanks to @zcain117 from Google! :slightly_smiling_face:,,0
Adding test for legacy checkpoint created with 2.0.3 (#17778),0.64640766,Removed deprecated checkpoint argument filepath (#5321),Co-authored-by: Borda Borda@users.noreply.github.com,0
Bump playwright from 1.32.1 to 1.35.0 in /requirements (#17835),0.41049296,Setup: added requirement freeze for the next major version (#14480),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz,0
Update onnxruntime requirement from <1.14.0 to <1.16.0 in /requirements (#17838),0.48160142,Setup: added requirement freeze for the next major version (#14480),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
docs: bump sphinx ver. (#17824),0.58542365,Pinned sphinx-autodoc-typehints with <v1.15 (#11400),  docs: bump sphinx ver.   myst-parser   myst-parser   myst-parser   6.2 ,0
"Update myst-parser requirement from <1.0.0,>=0.18.1 to >=0.18.1,<3.0.0 in /requirements (#17839)",0.5295448,Parsed local package versions (#13933),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Update ipython[all] requirement from <8.7.1 to <8.14.1 in /requirements (#17837),0.5499615,Add missing python-multipart dependency (#17244),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
"Update sphinx-copybutton requirement from <=0.5.0,>=0.3 to >=0.3,<=0.5.2 in /requirements (#17836)",0.6413437,Pinned sphinx-autodoc-typehints with <v1.15 (#11400),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
ci: extend the queue to 10 (#17823),0.45434192,@akihironitta @awaelchli @Borda @carmocca @kaushikb11 @krshrimali @mauvilsa @otaj @pre-commit-ci @rohitgr7 @semaphore-egg @tkonopka @wayi1,,0
ci: allow fail building NGC (#17815),0.3541253,"- Marked the `{Accelerator,Signal,Callback,Checkpoint,Data,Logger}Connector` classes as protected ([#17008](https://github.com/Lightning-AI/lightning/pull/17008))",,0
"Update fastapi requirement from <0.89.0,>=0.69.0 to >=0.69.0,<0.98.0 in /requirements (#17808)",0.4736863,"Removed deprecated properties DeepSpeedPlugin.cpu_offload* in favor of offload_optimizer, offload_parameters and pin_memory (#9244)",Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz,0
try to relax dependency on 3rd integrations (#17829),0.41982323,Many integrations made Lightning appear bloated,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Lightning Dataset (including optimized dataloading of s3 buckets) (#17743),0.52801734,- The top-level loops now own the data sources and combined dataloaders ([#16726](https://github.com/Lightning-AI/lightning/pull/16726)),"  Lightning DataLoader   lightning dataloader   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   init   example   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci   env var   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  Update src/lightning/pytorch/utilities/data/init.py  Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com   remove unused functions   extra reqs   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  Update src/lightning/pytorch/utilities/data/fileio.py  Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci   imports work now! yay   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   tests   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   imports   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   missing import   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   error handling   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update creds for local use case   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   codeowners   recursive get index   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   index   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   clean up get index   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update imagenet example   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   docstrings   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   docstrings   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   docstrings   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   example cleanup   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   changelog   reqs   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   codeowners   requirements   expose LightningDataset too   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   expost LightningDataset at top level   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   remove unused private methods from init   remove private imports   upper bound on extra requirements   review comments   loosen req   deps   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   test updating fabric base req   remove version pin on s3fs to test   recover missing function   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   tests   update   random   torchdata >= 0.3.0   update torchdata version   remove torchdata version to test   try rem torch version pin   req   update bucket in test   req   skips   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   import   update structure to lightning.data   base.txt for data reqs   fix imports   rename to LightningS3Dataset   new workflow   dont need to test warnings   reqs   req   revert data folder in pytorch   test import   tests   req   req   req   torch version   req   req   open dep   reformatted   pin strict   pin strict extra   req   modify workflow, no cache   try   patch   import   fix   dataset test   update getattr   pin everything to test   remove torch preinstall from workflow   workflow   req   Update .github/workflows/ci-tests-data.yml   Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com   workflow   workflow   req   Update .github/workflows/ci-tests-data.yml   Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com   workflow   print   skip test for now   update path join   revert app dep version bump   Update .github/workflows/ci-tests-data.yml   Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com   workflow updates   app base req   req   window test failure   add data req to assistant   try   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   add missing comma   updates   update   typo   requirements   try widening req   older torch version   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   update   update   cleanup tests   typo again   update   remove unnecessary line   Update .github/CODEOWNERS   Discard changes to requirements/pytorch/base.txt   Discard changes to requirements/fabric/base.txt   Discard changes to requirements/app/base.txt   requirements   requirements   one line   app workflow pick only app reqs   rename package   undo   don't use cache   examples CI   pytorch and fabric CI   try remove cache   Apply suggestions from code review   jirka playing   jirka playing   jirka playing   blah   flatten LightningDataset   cleans up dataset class   jirka playing   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   jirka playing   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   extra   fix dataset test   update checkgroups   Luca's review comments   val error fix   unskip test   min   fix precommit warning   cpu   docstrings   req   2.0.1   add return type   typing errors   req   return types with quotations   import for type-checking   no botocore in cloudagnostic code   exit args   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   backends typing   remove oldest from data tests   typing   typing   typing   types   type   typing   typing   typing   import fix   Changelog    Co-authored-by: Noha Alon nohaalon@Nohas-MacBook-Air.local Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Justus Schock justus.schock@posteo.de",0
Pass-through setattr for FabricModule (#17731),0.5525471,  * Changed the method signatrue of `Fabric.save` and `Fabric.load`,Co-authored-by: awaelchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
bump torch requirements to 2.0.1 (#17783),0.6777346,- Fixed an issue with comparing torch versions when using a version of torch built from source ([#17030](https://github.com/Lightning-AI/lightning/pull/17030)),,0
Fix deriving default map location when there is extra state (#17812),0.47041115,"    # previously, we were able to return state here",,0
Add rank_zero_first utility (#17784),0.561912,Set better defaults for rank_zero_only.rank when training is launched with SLURM and torchelastic (#6802),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Remove unnecessary check (#17788),0.46797353,"Simplified ""should run validation"" logic (#7682)",Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
update chlog with 2.0.2 & 2.0.3 (#17782),0.45715946,Removed collisions with logger versions by tying it to job id.,,0
Enable loading full optimizer checkpoints with FSDP (#17747),0.60889304,Updated model_checkpoint's to_yaml to use fsspec open (#3801),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Custom DeepSpeed config   format error with offload_optimizer (#17790),0.7369276,"Removed deprecated properties DeepSpeedPlugin.cpu_offload* in favor of offload_optimizer, offload_parameters and pin_memory (#9244)",Co-authored-by: francis xiao.wei@synyi.com,1
Document non-strict loading of checkpoints in Fabric (#17765),0.69034934,  * Changed the method signature of `Strategy.save_checkpoint` and `Fabric.load_checkpoint`,,0
Support empty weight initialization in Fabric.init_module() (#17627),0.5815096,"- Added `lightning.fabric.is_wrapped` to check whether a module, optimizer, or dataloader was already wrapped by Fabric ([#16953](https://github.com/Lightning-AI/lightning/pull/16953))",,0
"tests: clean too many , in mocks (#17776)",0.5038459,Cleaning up stale logger tests (#3490),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Fix to Parameters to MixedPrecisionPlugin are not validated and do not match doc string (#17687),0.62144405,NativeMixedPrecisionPlugin and its subclasses now take an optional GradScaler instance (#10055),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Remove warning on no_backward_sync with XLA strategy (#17761),0.5386838,  * Deprecated the internal `XLADeviceUtils.xla_available` staticmethod,,0
Added configurable strict loading for Fabric strategies (#17645),0.6331519,  * `Fabric.load` can now load state in-place onto models and optimizers,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: bas bas.krahmer@talentflyxpert.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
[FSDP] utility to apply optimizer during backward (#17710),0.62372595,Generalized Optimizer validation to accommodate both FSDP 1.x and 2.x (#16733),"  utility to apply optimizer during backward   start to address CI failures   address CI failures   address review comments and harden test   change union annotation syntax   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   try to debug CI   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  add skip_windows and standalone to fsdp test   Co-authored-by: Taylor Robie taylor.robie@lightning.ai Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
Add Fabric internal hooks (#17759),0.56693697,New Hooks,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
External callback registry through entry points for Fabric (#17756),0.5882219,Callback registration through entry points,,0
Expose public and private IP in LightningWork (#17742),0.4281871,The LoadBalancer now uses internal ip + port instead of URL exposed (#16119),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
ci: fix typo in skip if for TPU (#17757),0.4705552,- if trainer.tpu_cores:,  ci: fix typo in skip if for TPU      $   \   |   blablab   rew ,0
"Update tensorboardx requirement from <=2.5.1,>=2.2 to >=2.2,<=2.6 in /requirements (#17750)",0.7538764,Improved the error message for installing tensorboard or tensorboardx (#17053),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,1
Simplify step redirection in strategy (#17531),0.4542989,Renames model steps (#1051),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
Upgrade deepspeed version (#17748),0.7201317,"Updated DeepSpeed ZeRO (#6546, #6752, #6142, #6321)",Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,1
ci: drop NGC as required check (#17754),0.40307707,-  Converted validation loop config warnings to `PossibleUserWarning` ([#13377](https://github.com/Lightning-AI/lightning/pull/13377)),,0
Address feedback for Fabric.init_module() (4/4) (#17607),0.6320743,The Fabric.run() method is no longer abstract (#14992),,0
Fix race condition when downloading data (#17732),0.46991283,Resolved a bug about a race condition when sending the work state through the caller_queue (#14074),,0
docker: NGC prune git (#17740),0.53480947,Remove unnecessary intermediate layers in Dockerfiles (#5697),,0
ci: fix TPU skip if (#17672),0.5120176,Increased TPU check timeout from 20s to 100s (#5598),,0
Fix multithreading checkpoint loading (#17678),0.6440292,Refactor load in checkpoint connector (#4593),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: awaelchli aedu.waelchli@gmail.com,0
Enable loading full state dict checkpoints with FSDP (#17623),0.70924777,"def load_state_dict(self, checkpoint):",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
Address feedback for Fabric.init_module() (3/4) (#17723),0.6282094,The Fabric.run() method is no longer abstract (#14992),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Fix overlapping samples in DDP when no global seed is set (#17713),0.57526153,Allow setting replace_sampler_ddp=True with a distributed sampler already added (#4273),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Address feedback for Fabric.init_module() (2/4) (#17722),0.6348263,The Fabric.run() method is no longer abstract (#14992),,0
Address feedback for Fabric.init_module() (1/4) (#17721),0.63867617,The Fabric.run() method is no longer abstract (#14992),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Support kwargs input for LayerSummary (#17709),0.53165877,Allow passing model hyperparameters as complete kwarg list (#1896) ,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
avoid unnecessary workers with sequential CombinedLoader (#17639),0.7963279,CombinedLoader only starts DataLoader workers when necessary when operating in sequential mode (#17639),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
update group-check (#17719),0.49068066,group prepare data hook (#3212),,0
replace local adjustment script with external (#17582),0.4065453,"Prevent artefactual ""running from outside your current environment"" error (#15647)",,0
"Update deepdiff requirement from <6.2.4,>=5.7.0 to >=5.7.0,<6.3.1 in /requirements (#17631)",0.459218,"Updated DeepSpeed ZeRO (#6546, #6752, #6142, #6321)",Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
"Update docker requirement from <6.1.2,>=5.0.0 to >=5.0.0,<6.1.3 in /requirements (#17632)",0.4790631,Remove unnecessary intermediate layers in Dockerfiles (#5697),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Bump coverage from 6.5.0 to 7.2.5 in /requirements (#17629),0.5224456,Setup: added requirement freeze for the next major version (#14480),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
"Update torchvision requirement from <=0.15.1,>=0.12.0 to >=0.12.0,<=0.15.2 in /requirements (#17714)",0.7515992,Removed dependency on torchvision (#797),Update torchvision requirement in /requirements Updates the requirements on torchvision to permit the latest version. - Release notes - Commits  updated-dependencies: - dependency-name: torchvision   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,1
Fix doc formatting in batch_size_finder.py (#17696),0.5735732,Batch Size Finder (#11089),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
ci: fix runif ref (#17716),0.42647675,Updated fast_dev_run to accept integer representing num_batches (#4629),,0
Update README.md,0.5008326,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",,0
runif consistency (#17686),0.34648907,  * Deprecated the internal `inner_f` function,,0
ci: update gcheck name (#17690),0.48858523,Changed ModelCheckpoint version suffixes to start at 1 (#5008),"  ci: update gcheck name   name   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  name   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
ci: separate parity/benchmarks (#17502),0.429195,The slow and clunky data-parallel strategy (#16748),"  ci: separet benchmarks   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   measure   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   conf   isort   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci   ci   parity   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   taska   name   ...   var   ...   ...   ...   cd   reset_cudnn_benchmark   import   imports   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   models   xfail    Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
Fix Mix Precision settings for FSDP Plugins (#17670),0.60637265, * `pytorch_lightning.plugins.precision.fsdp_native_native_amp.FullyShardedNativeNativeMixedPrecisionPlugin` is now `pytorch_lightning.plugins.precision.fsdp.FSDPMixedPrecisionPlugin`,,0
"Update tensorboard requirement from <2.12.0,>=2.9.1 to >=2.9.1,<2.14.0 in /requirements (#17674)",0.77592176,Improved the error message for installing tensorboard or tensorboardx (#17053),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,1
Add Profiler table kwargs (#17662),0.58318293,Split profilers module (#6261),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Raise environment variable collision errors only when Fabric CLI is used (#17679),0.53759813,The Fabric.run() method is no longer abstract (#14992),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Refactor run-method-style Fabric tests (#17669),0.5793899,The Fabric.run() method is no longer abstract (#14992),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Update email address in SECURITY.md (#17664),0.33837602,Renamed xxx_AVAILABLE as protected (#5082),,0
Fixed error when using W&B project name from environment variables (#16222),0.4353805,"Prevent artefactual ""running from outside your current environment"" error (#15647)",Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
fix: get project (#17666),0.4656667,Application storage prefix moved from app_id to project_id/app_id (#14583),,0
ci: drop e2e as required check (#17658),0.43283242,"validation_step, val_dataloader are now optional.   ",,0
refactor Fabric tests to use launch method (#17648),0.6061438,fabric.launch(),Co-authored-by: bas bas.krahmer@talentflyxpert.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Log LearningRateMonitor values to Trainer.callback_metrics for EarlyStopping (#17626),0.76973057,    --trainer.callbacks.logging_interval=epoch,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
Bugfix: LR finder max val batches (#17636),0.5792265,LR Finder argument num_accumulation_steps,,0
Make configure_sharded_model implementation in test models idempotent (#17625),0.77623665,def configure_sharded_model(self):,,1
omitted mention of QuantizationAwareTraining callback (#17646),0.5928385,- Removed the `QuantizationAwareTraining` callback ([#16750](https://github.com/Lightning-AI/lightning/pull/16750)),Co-authored-by: bas bas.krahmer@talentflyxpert.com,0
"Update tqdm requirement from <4.65.0,>=4.57.0 to >=4.57.0,<4.66.0 in /requirements (#17630)",0.5265142,Setup: added requirement freeze for the next major version (#14480),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
"Update jsonargparse[signatures] requirement from <4.19.0,>=4.18.0 to >=4.18.0,<4.22.0 in /requirements (#17633)",0.66888905,- Changes in `LightningCLI` required for the new major release of jsonargparse v4.0.0 ([#10426](https://github.com/PyTorchLightning/pytorch-lightning/pull/10426)),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Support true 16-bit precision with deepspeed (#17576),0.7302919,Updated precision attributes in DeepSpeedPlugin (#10164),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
Fix ModelCheckpoint misformats filename (#17610),0.8290658,Fix saved filename in ModelCheckpoint if it already exists (#4861),Co-authored-by: awaelchli aedu.waelchli@gmail.com,1
fix: get project (#17617),0.46169004,Application storage prefix moved from app_id to project_id/app_id (#14583),"  fix: get project   test   fix tests   fix tests   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
Allow setting the SLURMEnvironment.main_address via an env variable (#17596),0.609536,    default_port = os.environ['SLURM_JOB_ID'],Co-authored-by: awaelchli aedu.waelchli@gmail.com,0
Set fixed seed for pytest execution order (#17614),0.556841,pl.seed_everything will now also set the seed on the DistributedSampler (#7024),,0
docs: fix links in fabric (#17611),0.53603536,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",,0
Add support for saving with full state-dict in Fabric's FSDP (#17526),0.6528589,  * `Fabric.save` accepts a state that can contain model and optimizer references,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Make StreamLit UI less flaky (#17598),0.3899661,- Fixed `RichProgressBar` progress when refresh rate does not evenly divide the total counter ([#11668](https://github.com/PyTorchLightning/pytorch-lightning/pull/11668)),Co-authored-by: thomas thomas@thomass-MacBook-Pro.local,0
Fix docs levels and broken links (without rename) (#17545),0.47724247,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ","  Fix docs levels and broken links   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Update advanced_level_17.rst   Delete oryx-build-commands.txt   Update BECOMING_A_CORE_CONTRIBUTOR.md   Update expert_level_24.rst    Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
"Update streamlit requirement from <1.16.1,>=1.13.0 to >=1.13.0,<1.22.1 in /requirements (#17589)",0.52167714,Setup: added requirement freeze for the next major version (#14480),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Update onnx requirement from <1.14.0 to <1.15.0 in /requirements (#17587),0.50479245,Setup: added requirement freeze for the next major version (#14480),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
ci: use randon seed (#17571),0.5222766,seed_everything now fails when an invalid seed value is passed instead of selecting a random seed (#8787),,0
drop IPU docker for runner (#17583),0.51110023,Remove unnecessary intermediate layers in Dockerfiles (#5697),,0
"Update docker requirement from <6.0.2,>=5.0.0 to >=5.0.0,<6.1.2 in /requirements (#17585)",0.48651516,Remove unnecessary intermediate layers in Dockerfiles (#5697),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Update uvicorn requirement from <0.21.2 to <0.22.1 in /requirements (#17586),0.49183357,Setup: added requirement freeze for the next major version (#14480),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Adding non-layer param count to summary (#17005),0.48972088,Generates a summary of all layers in a LightningModule. This currently works with the new RichProgressBar callback.,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: awaelchli aedu.waelchli@gmail.com,0
Fix property raised instead of returned (#17595),0.61237264,    # returning a value here is no longer supported,,0
Bump pytest from 7.2.2 to 7.3.1 in /requirements (#17588),0.5579387,Remove pytest as a requirement to run app by @manskx in https://github.com/Lightning-AI/lightning/pull/15449,Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Fixes a bug that causes CSVLogger to overwrite version_0 when root_dir is a relative path. (#17139),0.61998856,- Added support for writing logs remote file systems on `CSVLoggers`. ([#16880](https://github.com/Lightning-AI/lightning/pull/16880)),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
"[TPU] For XLA Strategy, added function arg to control broadcast_master_param() (#17522)",0.59554845,Moveed HPU broadcast override to the HPU strategy file (#17011),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Split init_module into init + sharded_model (#17488),0.6925709,def configure_sharded_model(self):,,0
Docs on sanity_checking property (#17579),0.56148934, - Sanity checking: `Trainer(num_sanity_val_steps>0)`,,0
Remove redundant install info in CLI (#17577),0.47532785,Resolved a bug where the install command was not installing the latest version of an app/component by default (#14181),,0
Make WandbLogger run initialization lazy for non-spawn distributed operation (#17573),0.6786584,WandbLogger does not force wandb reinit arg to True anymore and creates a run only when needed (#4648),,0
[TPU] Remove CPU conversion of metrics (#17572),0.5482136,Removed deprecated metrics (#8586),,0
Avoid flattening hyperparameters in WandbLogger (#17574),0.7248385,The WandbLogger no longer flattens dictionaries in the hyperparameters logged to the dashboard (#17574),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
Use PL is_overridden in BYOT example (#17070),0.6301382,modular is_overridden (#3290),,0
Check for mixed new and old style imports (#17548),0.48570904,Wrapped imports for traceability (#13924),,0
ruff: enable & fixing RET (#17540),0.46932125,Removed deprecated TrainResult (#5323),,0
Docs: Remove old link (#17514),0.45383477,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",docs,0
Verify Fabric.launch() was called (#17570),0.7635359,fabric.launch(),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
ruff: PT some more fixes (#17569),0.55724454,"In addition, we fixed:",,0
Some fixes in the CLI docs (#17575),0.57778215,Introducing CLI commands for apps (#13602)!,,0
Compose RunIf utilities (#17520),0.4299725,Introducing CLI commands for apps (#13602)!,,0
tests: randomized order for PT & Fabric (#17460),0.3817888,Fabric,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Adding test for legacy checkpoints (#17562),0.6385008,    # put all logic related to loading a checkpoint here,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
ci: drop secondary pkg for LAI (#17565),0.46754926,Release LAI docs as stable (#14250),,0
Added SaveConfigCallback.save_config (#17475),0.6310253,  * `save_config_overwrite`,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Remove duplicate import checks (#16648),0.531587,Module imports are now catching ModuleNotFoundError instead of ImportError (#9867),,0
update tags in bug issue (#17551),0.46066383,Deprecated tags_csv in favor of hparams_file (#1271),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
ruff: autofix PT (#17541),0.38939732,Renamed several Trainer atributes:  (#567),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Fix problems in trainer docs (#17561),0.73936033,Trainer code became harder to follow,,1
feat: add LargeFileManager configuration to Jupyter in Dockerfile (#17553),0.5154394,dataloader = fabric.setup_dataloaders(dataloader),,0
add note in LightningCLI docs that --optimizer must be given for --lr_scheduler to work (#17552),0.770854,  * Removed `optimizer_idx` argument from `LightningModule.lr_scheduler_step`,,1
Adding tests for legacy checkpoints - 1.8.x (#17374),0.6669413,all the checkpoint issues should be gone now (including backward support for old checkpoints),"  Adding tests for legacy checkpoints   2.0   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   fix simple   utils   import   pl   num_features=24   num_features=24   length=6000   other   rm   dru run   rm   prune   import    Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
App: change cache location (#17491),0.5327034,Application storage prefix moved from app_id to project_id/app_id (#14583),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: thomas thomas@thomass-MacBook-Pro.local Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
Add an option to overwrite the existing checkpoint file (#17320),0.71882576,    # put all logic related to saving a checkpoint here,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Support compiling a module after it was set up by Fabric (#17529),0.99999976,Support compiling a module after it was set up by Fabric (#17529),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
DDP Parity tests as standalone task (#17503),0.5192066,DDP Debugging Improvements,,0
Bump playwright from 1.30.0 to 1.32.1 in /requirements (#17537),0.40998173,Setup: added requirement freeze for the next major version (#14480),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
"Update numpy requirement from <1.24.2,>=1.17.2 to >=1.17.2,<1.24.4 in /requirements (#17538)",0.4884308,Dropped official support/testing for older PyTorch versions <1.3 (#1917),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
"Update pandas requirement from <1.5.4,>1.0 to >1.0,<2.0.2 in /requirements (#17534)",0.6178334,Removed dependency on pandas (#736),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
"Update fsspec[http] requirement from <2023.2.0,>2021.06.0 to >2021.06.0,<2023.5.0 in /requirements (#17536)",0.47244093,Setup: added requirement freeze for the next major version (#14480),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Bump mypy from 1.1.1 to 1.2.0 in /requirements (#17535),0.47745684,Add missing python-multipart dependency (#17244),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Add map location option to checkpoint upgrade utility (#17527),0.665467,We also added support for setting the checkpoint path statefully:,,0
Avoid creating CUDA stream if not running on CUDA (#17499),0.56192476,- Added an argument `include_cuda` in `pytorch_lightning.utilities.seed.isolate_rng` to disable managing `torch.cuda`'s rng ([#16423](https://github.com/Lightning-AI/lightning/pull/16423)),,0
[TPU] Rename classes to use XLA instead of TPU (#17383),0.4757616,  * Deprecated the internal `XLADeviceUtils.xla_available` staticmethod,,0
fabric: upstream runif to pkg (#17504),0.60748065,The Fabric.run() method is no longer abstract (#14992),,0
Update Fabric.init_module for FSDP (#17510),0.63263035,The Fabric.run() method is no longer abstract (#14992),,0
[TPU] Call auto_device_count for is_available (#17509),0.6084946,device_stats = DeviceStatsMonitor(),,0
Examples: expose learning rate (#17513),0.5367814,Learning Rate Finder (#13802),Co-authored-by: thomas thomas@thomass-MacBook-Pro.local,0
Support inspecting the signature of decorated hooks (#17507),0.54593813,Updated hooks arguments - breaking for setup and teardown (#2850),,0
ci: add current & total count to standalone test output (#17511),0.40854895,"nb_test_batches to num_test_batches,",,0
Resolve e2e CI (#17480),0.4767382,Resolve TPU miss rendezvous (#6781),  update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update    Co-authored-by: thomas thomas@thomass-MacBook-Pro.local,0
Fix setup_model typos in Fabric (#17498),0.6422766,"model, optimizer = fabric.setup(model, optimizer)",,0
Add timeout argument for FSDPStrategy (#17274),0.5552451,"- Added a `timeout` argument to `DDPStrategy` and `DDPSpawnStrategy`. ([#13244](https://github.com/Lightning-AI/lightning/pull/13244), [#13383](https://github.com/Lightning-AI/lightning/pull/13383))",Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: awaelchli aedu.waelchli@gmail.com,0
fix issue labeler (#17501),0.42291158,Renamed several Trainer atributes:  (#567),,0
adjust msg for external accelerators (#17465),0.63392335,Automatic accelerator selection (#16847),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
gh: fix duplicate id in bug issue (#17495),0.48235047,Resolved a bug where the work statuses will grow quickly and be duplicated (#13970),,0
upload checkpoint files to neptune from stream (#17430),0.6029725,- Removed duplicated file extension when uploading model checkpoints with `NeptuneLogger` ([#11015](https://github.com/PyTorchLightning/pytorch-lightning/pull/11015)),,0
Adding doc strings for exceptions raised in trainer.py (#16684),0.70860267,"    def on_exception(self, trainer, pl_module, exception):",Co-authored-by: Dingu Sagar dingu.sagar@engati.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka jirka.borovec@seznam.cz,1
Replace IPU with external implementation (#17075),0.4943202,Graphcore IPU devices,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Fix registry typing annotation (#17489),0.47090572,"The new version renders the registries and the auto_registry flag, introduced in 1.6.0, unnecessary, so we have deprecated them.",,0
ci: label issue with version (#17484),0.46865895,Truncated long version numbers in progress bar (#2594),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
True half-precision support in Fabric (#17287),0.5334986,Mixed precision overhaul (#16783),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
Lightning: make type hints public (#17100),0.74365354,Made type hints public (#17100),  Add missing MANIFESTs   move   one more   Ignore version.info properly   move   manifest    Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Fix conda badge in README (#17345),0.3937211,Deprecated training_tqdm_dict in favor of progress_bar_dict (#1450).,,0
[App] Add missing python-multipart dependency (#17244),0.8851693,Add missing python-multipart dependency (#17244),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz,1
adding check for bandit vulnerabilities 1/n (#17382),0.4013701, - Sanity checking: `Trainer(num_sanity_val_steps>0)`,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Shared Fabric._wrap_and_launch code (#17473),0.71423125,fabric.launch(),,1
simple examples instead of skips (#17482),0.386797,Simplify the PL examples structure (shallower and more readable) (#1247),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
[pre-commit.ci] pre-commit suggestions (#17271),0.62368876,@akihironitta @awaelchli @Borda @carmocca @kaushikb11 @krshrimali @mauvilsa @otaj @pre-commit-ci @rohitgr7 @semaphore-egg @tkonopka @wayi1," [pre-commit.ci] pre-commit suggestions  updates: - github.com/PyCQA/docformatter: v1.4 → v1.6.0 - github.com/psf/black: 22.12.0 → 23.3.0 - github.com/charliermarsh/ruff-pre-commit: v0.0.237 → v0.0.260  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci   update   apply   fixing   docs/lines    Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz",0
Improved model initialization API for Fabric (#17462),0.68390465,  * `Fabric.load` can now load state in-place onto models and optimizers,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
Update gradient clipping docs in Fabric (#17470),0.6210617,Gradient Clipping Customization,,0
Update the MockOptimizer set_to_none default to match PyTorch (#17463),0.6829351,Skipped testing with PyTorch 1.7 and Python 3.9 on Ubuntu (#11217),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Fallback to module available check for mlflow (#17467),0.9999998,Fallback to module available check for mlflow (#17467),,1
Update deepspeed requirement support window (#16813),0.6335482,"Updated DeepSpeed ZeRO (#6546, #6752, #6142, #6321)",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka jirka.borovec@seznam.cz,0
drop contribution badge (#17471),0.31397766,Removed deprecated: (#2760),,0
Remove devel.txt requirements file (#17466),0.43907642,Setup: added requirement freeze for the next major version (#14480),,0
Simplify strategy installation in CI (#17347),0.48529014,Renamed Strategy.reduce to Strategy.all_reduce in all strategies (#16370),,0
Patch for device placement (Reduce host and device syncs) (#17334),0.57533264,We cleaned up the properties related to device indices (#14829).,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Install project specific dependencies (#17376),0.5475395,Add missing python-multipart dependency (#17244),Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
Input validation for Fabric.launch (#17423),0.6941,fabric.launch(),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
add & apply flake8-simplify (#17386),0.42012218,Changed deprecated enable_pl_optimizer=True (#5244),,0
app/tests: skip instead of fail (#17461),0.5158684,Updated app testing (#16000),,0
"Update traitlets requirement from <5.9.0,>=5.3.0 to >=5.3.0,<5.10.0 in /requirements (#17398)",0.42862153,Changed ModelCheckpoint version suffixes to start at 1 (#5008),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
typing: fix App's core API - api (#16950),0.5787928,Removed deprecated API (#2073),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
"Update aiohttp requirement from <=3.8.3,>=3.8.0 to >=3.8.0,<=3.8.4 in /requirements (#17315)",0.4895702,Setup: added requirement freeze for the next major version (#14480),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
"Update inquirer requirement from <=3.1.2,>=2.10.0 to >=2.10.0,<=3.1.3 in /requirements (#17258)",0.4615848,"    version=""0.0.1"",",Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Update uvicorn requirement from <0.19.1 to <0.21.2 in /requirements (#17314),0.4865056,Setup: added requirement freeze for the next major version (#14480),Updates the requirements on uvicorn to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: uvicorn   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
ci: update OS for pkg release (#17455),0.44126788,Resolved a bug where the install command was not installing the latest version of an app/component by default (#14181),,0
[App] Fix resolution of latest version in CLI (#17351),0.5132929,"    version=""0.0.1"",",Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz,0
docs: fix past versions location (#17432),0.5169341,Set version as today (#13906),,0
set codecov as informational (#17453),0.3736134,Add code_dir argument to tracer run (#15771),,0
Enable precision autocast for LightningModule step methods in Fabric (#17439),1.0,Enable precision autocast for LightningModule step methods in Fabric (#17439),,1
"App: Fix AppState, streamlit example (#17452)",0.4360238,"Resolved LightningApp(..., debug=True) (#14464)",,0
hpu: add extra warning after removal (#17372),0.60475326,Don't raise a warning when nn.Module is not saved under hparams (#12669),Co-authored-by: Luca Antiga luca.antiga@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
feat: customize gradio components with lightning colors (#17054),0.49181366,Changed lightning_app.components.serve.gradio to  lightning_app.components.serve.gradio_server (#16201),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: Yurij Mikhalevich yurij@grid.ai,0
Add Fabric.launch to Fabric methods section (#17437),0.87475085,fabric.launch(),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
req: formatting (#17335),0.4034893,Syntax changes are: ,,0
Handle edge case in Fabric.setup() when model has no parameters (#17441),0.63902986,"model, optimizer = fabric.setup(model, optimizer)",,0
Fix num_nodes not set for FSDPStrategy (#17438),0.54226124,Native FSDP replaces Fairscale FSDP (#16400),,0
Resolve Lightning App with remote storage (#17426),0.96443963,Resolved Lightning App with remote storage (#17426),  update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update    Co-authored-by: thomas thomas@thomass-MacBook-Pro.local,1
Document access to Fabric attributes inside LightningModule (#17440),0.5658425,from lightning.fabric import Fabric,,0
Minor Fabric backward refactor (#17433),0.6445062,+         fabric.backward(loss),,0
Fix LightningModule step methods bypassing DDP wrapper in Fabric (#17424),0.7122885,Enable precision autocast for LightningModule step methods in Fabric (#17439),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
ruff: fixing flake8-comprehensions (#17385),0.4514798,Simplify the PL examples structure (shallower and more readable) (#1247),,0
[TPU] Fix PjRT tests (#17408),0.54860103,Resolve TPU miss rendezvous (#6781),,0
"[TPU] Do not delete jobs with ""keepalive"" in the name  (#17411)",0.49622655,Cleanup cluster waiting (#16054),"[TPU] Do not delete jobs with ""keepalive"" in the name",0
drop failing e2e quick app (#17409),0.36655787,Remove .item which causes sync issues (#1254),  drop failing e2e quick app   codeowners   Apply suggestions from code review ,0
Add dynamo RunIf skip condition (#17404),0.43297532,Run the flow only if the state has changed from the previous execution (#14076),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
[TPU] Add support for PJRT from PyTorch/XLA 2.0 (#17352),0.64133716,- Fixed an issue with the `TPUSpawnPlugin` handling the `XLA_USE_BF16` environment variable incorrectly ([#10990](https://github.com/PyTorchLightning/pytorch-lightning/pull/10990)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
[TPU] Fix workflow (#17406),0.60971004,moved TPU xxx_step to backend (#3118),  [TPU] Fix workflow   Whitespace ,0
[TPU] Refactor availability check (#17384),0.5756267,Updated logic for checking TPUs availability (#6767),,0
Bump default E2E tests image version (#17403),0.39649934,Set version as today (#13906),Bump default E2E image version Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
Fix PyTorch MPS test failure in master (#17405),0.63839597,Skipped testing with PyTorch 1.7 and Python 3.9 on Ubuntu (#11217),,0
Update Fabric CPU tests to work on GPU machines (#17391),0.56532955,"fabric = Fabric(accelerator=""cuda"", devices=8, strategy=""ddp"")",,0
[TPU] Fix workflow condition (#17379),0.5486243,moved TPU xxx_step to backend (#3118),,0
GPU suggestion does not require devices anymore (#17217),0.6165999,Support auto_select_gpus with the accelerator and devices API (#12608),,0
Replace range with RandomDataset in example (#17325),0.43190703,Disabled sampler replacement when using IterableDataset (#11507),,0
Fabric PPO example share_data flag (#17397),0.54796696,dataloader = fabric.setup_dataloaders(dataloader),,0
Add header_style argument to RichModelSummary callback (#16788),0.46916944,- Changed minimum supported version of `rich` from `10.14.0` to `12.13.0` ([#16798](https://github.com/Lightning-AI/lightning/pull/16798)),,0
skip some App tests (#17401),0.6661074,Updated app testing (#16000),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Update from_datasets to support arbitrary iterables (#17402),0.5780858,LightningModule.from_datasets() now accepts IterableDataset instances as training datasets. (#7503),,0
Wraps sharded model for proper access to it state_dict in FSDP strategy (#16558),0.5597693,def configure_sharded_model(self):,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Add test for compiling FSDP model in Fabric (#17394),0.5842622,Support compiling a module after it was set up by Fabric (#17529),,0
Remove reference to outdated Kaggle tutorial (#17390),0.501795,- Deprecated the `on_colab_kaggle` function ([#14247](https://github.com/Lightning-AI/lightning/pull/14247)),,0
Update pip upgrade command in CI (#17395),0.43791282,Use correct python version in lightning component template (#13790),,0
Cleanup the is_distributed property [TPU] (#17381),0.4919471,Updated logic for checking TPUs availability (#6767),,0
Save and load sharded checkpoints with FSDP in Fabric (#17323),0.65394586,  * Changed the method signature of `Strategy.save_checkpoint` and `Fabric.load_checkpoint`,Co-authored-by: Luca Antiga luca.antiga@gmail.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Support all CombinedLoader modes during evaluation (#17163),0.6391081,"        return CombinedLoader(iterables, mode=""min_size"")",,0
NumPy to Torch for lightning/fabric (#17291),0.57916903,Deprecated pytorch_lightning.lite.LightningLite in favor of lightning.fabric.Fabric (#16314),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Sync module states during non-fit (#17370),0.49017796,Avoided false positive warning about using sync_dist when using torchmetrics (#14143),Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
Remove numpy from src/lightning/pytorch and use torch only (#17278),0.7270006,- Removed the experimental `pytorch_lightning.utiltiies.meta` functions in favor of built-in https://github.com/pytorch/torchdistx support ([#13868](https://github.com/Lightning-AI/lightning/pull/13868)),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
Added return in convert_zero_checkpoint_to_fp32_state_dict (#17342),0.59989256,Called on_load_checkpoint before loading state_dict (#4057),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Add a warning to the detect_anomaly flag. (#17380),0.6954101, - Anomaly detection: `Trainer(detect_anomaly=True)`,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Fix load_from_checkpoint to return model on correct device (#17308),0.6774869,Enhanced load_from_checkpoint to also forward params to the model (#1307),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
change STEP_OUTPUT type from dict to mapping (#17387),0.59607095,Removed legacy code to include step dictionary returns in callback_metrics. Use self.log_dict instead. (#6682),,0
App: drop flaky doctest example (#17366),0.5118378,Updated app testing (#16000),"  skip app dcotest   Proposed change   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  delete   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
[TPU] Use pull_request_target event (#17377),0.39378667,Allow user to select individual TPU core to train on (#1729),,0
[TPU] Add testing matrix with PJRT (#17368),0.46889684,  * Removed the `Trainer(tpu_cores=...)` argument,  Replace GKE in CI with manual gcloud usage   Fix XRT test   Reduce timeout to 35 minutes   [TPU] Run tests with PJRT   runtime as part of the job name   CHANGELOG   Update for app too ,0
[TPU] Replace GKE in CI with manual gcloud usage (#17362),0.46118397,Removed implicit ui testing with testing.run_app_in_cloud in favor of headless login and app selection (#16741),,0
Fixes around Strategy.set_world_ranks (#16966),0.5668837,Renamed Strategy.reduce to Strategy.all_reduce in all strategies (#16370),  don't call set_world_ranks in xla strategy   update   fabric and other strategies   CHANGELOG   Typos   Reuse test    Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Update link to image in Fabric PPO example (#17360),0.49356973,  * Changed the method signatrue of `Fabric.save` and `Fabric.load`,,0
docker: fix building PL image (#17353),0.5181926,Remove unnecessary intermediate layers in Dockerfiles (#5697),,0
Update CHANGELOG after the 2.0.1 release (#17235),0.66383904,Full Changelog,,0
Update fastapi dependency pins (#17173),0.45669082,"Removed deprecated properties DeepSpeedPlugin.cpu_offload* in favor of offload_optimizer, offload_parameters and pin_memory (#9244)","  Update fastapi dependency pins   Apply suggestions from code review   Update test.txt   Update requirements/app/base.txt   Revert ""Update requirements/app/base.txt""   This reverts commit 59918ffc6c2cfd8b5509df48ca84b67ad264bc30.   cloud update   Bad merge   fastapi 0.69.0 which pins starlette 0.15.0   https://github.com/pydantic/pydantic/issues/1985   Avoid CVE: https://github.com/tiangolo/fastapi/pull/3213   Strict trio   Skip windows test    Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com",0
Replace codecov pip package with codecov uploader (#17349),0.4964381,Add missing python-multipart dependency (#17244),,0
Various Fabric documentation updates (#17236),0.5880008,:open_book: Go to Fabric documentation :open_book:,,0
[TPU] v4 support (#17227),0.60002786,TPU core selection,,0
[TPU] Remove error check for IterableDatasets (#17331),0.60501456,Disabled sampler replacement when using IterableDataset (#11507),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Simplified setup of optimizers in FSDP (#17309),0.7371188,Working with multiple optimizers (#16539),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
Combined setup of model and optimizer with FSDP (#17305),0.7479077,Generalized Optimizer validation to accommodate both FSDP 1.x and 2.x (#16733),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
fix missing tutorials (#17311),0.5162524,Removed the deprecated TrainerTrainingTricksMixin class (#8679),,0
Bump peter-evans/create-pull-request from 4 to 5 (#17313),0.40435973,Release LAI docs as stable (#14250),Bumps peter-evans/create-pull-request from 4 to 5. - Release notes - Commits  updated-dependencies: - dependency-name: peter-evans/create-pull-request   dependency-type: direct:production   update-type: version-update:semver-major ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
App: Fix frontends when using multiprocessing in the cloud (#17324),0.7605677,Fix frontend hosts when running with multi-process in the cloud (#17324),"  App: Fix frontends when using multiprocessing in the cloud   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  Update CHANGELOG.md   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",1
[TPU] Improve TPU workflow (#17237),0.6042464,TPU training (#2708),  Trigger TPU tests if [TPU] is in the PR title   Remove TODO   checkgroup   DEBUG   Update   1h timeout   Update   Update   Update   Update   Remove DEBUG ,0
checkgroup,0.37902558,Distributed group defaults to WORLD if None (#5125),,0
Remove TODO,0.49413955,Remove MetricsHolder (#7909),,0
Trigger TPU tests if [TPU] is in the PR title,0.4735087,- if trainer.tpu_cores:,,0
Update LightningDataModule docs (#17238),0.7533244,- Added profiling of `LightningDataModule` hooks ([#12971](https://github.com/Lightning-AI/lightning/pull/12971)),  Update LightningDataModule docs   Avoid torchvision ,1
Update CODEOWNERS (#17322),0.47551274,"Updated DeepSpeed ZeRO (#6546, #6752, #6142, #6321)",,0
readability deepspeed condition (#17232),0.45313174,Update the logic to check for accumulation steps with deepspeed (#9826),,0
Make the is_picklable function more robust (#17270),1.0000001,Make the is_picklable function more robust (#17270),,1
Replace NumPy with Torch in examples/fabric/ (#17279),0.62345576,Corrected call to torch.no_grad (#5124),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Minimal Transformer Example (#17282),0.3938042,"Under the hood, Colossal-AI implements different parallelism algorithms that are especially interesting for the development of SOTA transformer models:",,0
Fix broken links in README (#17292),0.614931,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",  update links   one more ,0
Update docs index (#17246),0.4858891,Docs improvements,"  update docs   update docs index   Delete version.info   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update glossary   typo   restructure   rm newline   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
Update usage hinting for upgrading checkpoints (#17276),0.6307339,"Versioning of ""last"" checkpoints",,0
NumPy to Torch: For LR Finder (#17264),0.61586744,Corrected call to torch.no_grad (#5124),Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Remove NumPy from Callback scripts (#17267),0.4988885,Removed deprecated callbacks (#3979),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
update bug template with version (#17226),0.49749148,- Removed the deprecated code in:,,0
Fix typo in combined_loader.py documentation (#17256),0.61290634,"combined_loader = CombinedLoader(iterables, mode=""min_size"")",Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
fixing master (#17268),0.44364306,Moved TrainsLogger to Bolts (#2384),"  fixing master   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
Update setuptools requirement from <65.7.0 to <67.7.0 in /requirements (#17050),0.6799585,Setup: added requirement freeze for the next major version (#14480),Updates the requirements on setuptools to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: setuptools   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
Bump pytest from 7.2.0 to 7.2.2 in /requirements,0.54734325,Remove pytest as a requirement to run app by @manskx in https://github.com/Lightning-AI/lightning/pull/15449,Bumps pytest from 7.2.0 to 7.2.2. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: pytest   dependency-type: direct:production   update-type: version-update:semver-patch ... Signed-off-by: dependabot[bot] support@github.com,0
Add link to benchmarks docs (#17239),0.5618835,"docs for all Metrics (#2184, #2209)",  Update benchmarks.rst   small fix ,0
revert docs,0.47221738,Backward Incompatible Changes,,0
Update 2,0.43759227,No changes,,0
Update docs structure,0.64339924,Docs improvements,,0
Simplify rank_zero_experiment for torch.compile support (#17216),0.6515752,Corrected call to torch.no_grad (#5124),,0
Drop pre-commit from /requirements (#17049),0.47511774,Setup: added requirement freeze for the next major version (#14480),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
Avoid inference_mode with torch.compile (#17215),0.8750415,Disable torch.inference_mode with torch.compile in PyTorch 2.0 (#17215),,1
Skip length checks for non-sized iterables (#17218),0.67352086,Support for arbitrary iterables (#16726),,0
Generalize Optimizer validation to accommodate both FSDP 1.x and 2.x (#16733),0.94402343,Generalized Optimizer validation to accommodate both FSDP 1.x and 2.x (#16733),Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Fix a typo in logger arg in trainer.py (#17206),0.74914384,"Deprecated passing flush_logs_every_n_steps as a Trainer argument, instead pass it to the logger init if supported (#9366)",,1
ci/docs: wheels from cache (#17201),0.37664008,"    'path/to/checkpoint.ckpt',",  ci: wheel from cache   ci: wheel from cache   rev ,0
ci: fix docs with caches (#17200),0.41324848,Release LAI docs as stable (#14250),,0
Document how to use TensorBoardLogger with fsspec (#16320),0.69008017,We have added own custom Tensorboard logger as default logger. ,,0
Set a fixed stage in the evaluation loops (#17094),0.540448,Set Loop.restarting=False at the end of the first iteration (#8362),,0
Update Fabric README (#17112),0.6896415,The Fabric.run() method is no longer abstract (#14992),  Add Fabric diff and resulting code to the README   Reduce horizontal space   sub   Contributor count   Feedback   Other snippets ,0
ci: update runner for IPU (#17183),0.44230223,Stopped optimizer_zero_grad from being called after IPU execution (#12913),,0
Fix *_batch_end typos (#17188),0.70310783,"Removed output argument from *_batch_end hooks (#3965, #3966)",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
fix typo in docs migration 1_6_regular (#17186),0.52968657,Syntax changes are: ,,0
Decoupled Reinforcement Learning example for Fabric (#17123),0.54508233,+         fabric.backward(loss),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: awaelchli aedu.waelchli@gmail.com Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
Update Custom Callback Docs (#17161),0.70201814,Removed deprecated callbacks (#3979),Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,1
Replace HPU with external implementation (#17067),0.5812239,Moveed HPU broadcast override to the HPU strategy file (#17011),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
test: adjust is_timing_close (#17178),0.5344634,Early stopping checks on_validation_end (#1458),,0
docs: unify PL ref in changelog (#17032),0.4843747,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",,0
Updated conda install commands in docs. (#17162),0.44414714,  * Deprecated the `pytorch_lightning.utilities.device_parser.is_cuda_available` in favor of `lightning_lite.accelerators.cuda.is_cuda_available`,,0
readme badges (#17114),0.43500155,[1.4.3] - 2021-08-17,,0
ci: separate integrations (#17170),0.3643892,Integrated TrainingEpochLoop.total_batch_idx (#8598),  integrations   Rename   Apply suggestions from code review   cut out   skip _   fix   setup   1e-2   uninstall   revert   note ,0
docs: update links to 1.6 1.5 1.4 (#17181),0.68614966,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",,0
Add check for bf16 in deepspeed inference (#16973),0.54403466,"Implemented DeepSpeedPlugin._setup_model_and_optimizers (#10009, #10064)",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Cole Hawkins  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: awaelchli aedu.waelchli@gmail.com,0
Support BaguaStrategy with external implementation (#17029),0.44954658,Direct support for compiled models (#15922),"  Support BaguaStrategy with external implementation   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   add an introduction of BaguaStrategy   update the introduction of BaguaStrategy   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Apply suggestions from code review   link   Fix document formatting issues   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update lightning-bagua version   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update lightning-bagua version   Apply suggestions from code review    Co-authored-by: Yafen Fang fangyaf@spaceml1.ethz.ch Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz",0
ci: unfreeze for master (#17116),0.43544367,Cleanup cluster waiting (#16054),"  ci unfreeze for master   accept str   crone   is in   str   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
update for neptune 1.0 (#16761),0.692219,"It now supports neptune-client 0.16.16 and neptune >=1.0, and we have replaced the log() method with append() and extend().",Co-authored-by: Sabine sabine.nyholm@neptune.ai Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Fix rich import error in Google Colab (#17156),0.51826674,- Changed minimum supported version of `rich` from `10.14.0` to `12.13.0` ([#16798](https://github.com/Lightning-AI/lightning/pull/16798)),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Ignore README in pre-commit rules (#17151),0.4318208,Avoid using the deprecated LooseVersion (#16162),,0
Assume the fetcher input will be a CombinedLoader (#17129),0.57800514,"        return [DataLoader(), DataLoader()]",,0
Update CI to pull torch 2.0 stable (#17107),0.657365,- Fixed an issue with comparing torch versions when using a version of torch built from source ([#17030](https://github.com/Lightning-AI/lightning/pull/17030)),,0
Drop the DataLoader iterator when pickling (#17130),0.60397995,Training Step With DataLoader Iterator,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Let TorchCollective works on the torch.distributed WORLD process group by default (#16995),1.0000002,Let TorchCollective works on the torch.distributed WORLD process group by default (#16995),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,1
Restructure Fabric docs (2/n) (#17126),0.6039845,Learn more about Fabric and what it can do in the new docs!,Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
fix(loggers): add best and latest aliases to wandb artifact in WandbLogger (#17121),0.7575811,Removed the deprecated sync_step argument from WandbLogger (#8763),Co-authored-by: awaelchli aedu.waelchli@gmail.com,1
Bump actions/setup-go from 3 to 4 (#17144),0.45601416,moved ___step_end hooks (#3130),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Never pickle the Trainer with the LightningModule (#17133),0.8608203,Pickling the LightningModule no longer pickles the Trainer (#17133),,1
Update CI config after #17122 (#17134),0.41598666,Configuration Validator (#9779),,0
Add a migration for the dataloader loops (#17125),0.60550165,"To use this new feature, return any iterable (or collection of iterables) from the dataloader hooks. ",Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
ci: hotfix name groupcheck (#17152),0.4593882,"Removed deprecated ModelCheckpoint arguments prefix, mode=""auto"" (#6162)",,0
docs: migration guide to the latest [2/n] (#17103),0.68553793,You can find a migration guide for this change in this PR's description.,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
docs order contributors alphabetically (#17102),0.42498854,Contributors,,0
docs: build docs for specific tags (#17055),0.5291602,Docs improvements,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
hotfix: readme formating (#17132),0.49078926,Pkg: fix parsing versions by @Borda in https://github.com/Lightning-AI/lightning/pull/15401,"  Update README.md   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Apply suggestions from code review   Apply suggestions from code review    Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
Reference the compatibility matrix (#17091),0.43364745,Here is a selection of notable changes that are not backward compatible with previous versions. The full list of changes and removals can be found in the CHANGELOG below.,,0
Reference trainer properties in docs (#16969),0.6980777,Trainer Device Attributes,,0
Typo fix in upgrade from 1.9.x to 2.0 docs: use_distributed_sample-> use_distributed_sampler (#17113),0.73712873,Allow setting replace_sampler_ddp=True with a distributed sampler already added (#4273),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Correct SaveConfigCallback error message (#17119),0.58182275,- Added support for custom parameters in subclasses of `SaveConfigCallback` ([#14998](https://github.com/Lightning-AI/lightning/pull/14998)),,0
Update docs for alternative dataset projects (#17096),0.5192963,"With the help of over 800 contributors, we have added many features and functionalities to make it the most complete research toolkit possible, but some of these changes also introduced issues:",,0
ci: switch to utils check (#17122),0.44556218,Removed no return warning from val/test step (#6139),,0
pkg: bump 2.1dev & chlog sections (#17109),0.47215807,Moved accelerators and plugins to its legacy pkg (#5645),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Typo fix: on_training_epoch_end -> on_train_epoch_end (#17110),0.73336875,"Changed the seq of on_train_batch_end, on_batch_end & on_train_epoch_end, on_epoch_end hooks (#5688)",Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
Restructure Fabric docs (#17111),0.62027955,Learn more about Fabric and what it can do in the new docs!,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
readme: logo 800px (#17108),0.38323882,Changed to the NeptuneLogger (#16761):,,0
Update CODEOWNERS,0.40926394,Key updates,,0
rtfd: building on PRs only (#17086),0.33678466,Native FSDP replaces Fairscale FSDP (#16400),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
readme: update logo (#17093),0.43760574,Changed to the NeptuneLogger (#16761):,,0
docs: update broken links & latest/stable (#16994),0.699903,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",  docs: update links to PL latest   also stable   last   Apply suggestions from code review   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   fixing   .   fabric   fixing   .    Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
readme typo (#17098),0.5229589,Pkg: fix parsing versions by @Borda in https://github.com/Lightning-AI/lightning/pull/15401,typo,0
docs: fix link in guide (#17090),0.687481,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",,0
Exclude some examples from docs navigation (#17081),0.381359,Docs improvements,Co-authored-by: Jirka jirka.borovec@seznam.cz,0
update links to Discord (#17087),0.45338243,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",  update links to Discord   link   Apply suggestions from code review   Co-authored-by: Luca Antiga luca.antiga@gmail.com   slack   Update docs/source-app/levels/expert/index.rst   Apply suggestions from code review    Co-authored-by: Luca Antiga luca.antiga@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
docs: migration guide to the latest [1/n] (#17034),0.68536514,You can find a migration guide for this change in this PR's description.,"  docs: migration guide - structure   update   try   ...   1.9   1.8   1.7   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   cleaning 1.9   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   crosslink   ...   placeholder   1.6   placeholders   1.5   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   1.4   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com",0
Prepare 2.0.0 release (#17056),0.49918514,"Since 2.0 is a major release, we took the opportunity to take our APIs to the next level and make considerable changes to reduce the backwards incompatible changes in the future. To alleviate this, we will commit to continue supporting the 1.9.x line of releases by doing bug-fix releases with any important fixes that are necessary.",,0
Update README (#17082),0.5399585,[1.6.2] - 2022-04-27,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: awaelchli aedu.waelchli@gmail.com Co-authored-by: Jirka jirka.borovec@seznam.cz,0
Improve the error message for installing tensorboardx (#17053),0.96576834,Improved the error message for installing tensorboard or tensorboardx (#17053),,1
Organize app examples (#17045),0.4479866,Introducing CLI commands for apps (#13602)!,  move   init files   reset change   missed   more   relative path   e2e tests   update checkgroup   reference   update app_* ,0
Rename PL installation to pip install lightning (#17074),0.50157243,Use correct python version in lightning component template (#13790),  rename PL installation   update   Apply suggestions from code review   Update docs/source-pytorch/starter/installation.rst   Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com  Update docs/source-pytorch/starter/installation.rst   Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com,0
Update data docs (#16839),0.48752677,Docs improvements,Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,0
Avoid inference_mode with FSDP (#17064),0.5706504,Inference mode support,,0
Update dynamo bug workaround condition (#17065),0.41047436,- Removed sanity check for multi-optimizer support with habana backends ([#13217](https://github.com/Lightning-AI/lightning/pull/13217)),,0
Update dataloader docstrings (#17061),0.6768342,"Accessing dataloaders (#16726, #16800)",,0
Add CombinedLoader to the API reference (#17062),0.6309253,- Renamed `CombinedLoader.loaders` to `CombinedLoader.iterables` ([#16743](https://github.com/Lightning-AI/lightning/pull/16743)),Co-authored-by: awaelchli aedu.waelchli@gmail.com,0
Remove outdated Trainer animation (#17060),0.6516406,Removed the deprecated TrainerTrainingTricksMixin class (#8679),,0
Use base version check before calling _register_load_state_dict_pre_hook (#17030),0.63573486,Called on_load_checkpoint before loading state_dict (#4057),,0
Rename ProgressBarBase to ProgressBar (#17058),0.72724223,- Renamed `ProgressBarBase` to `ProgressBar` ([#17058](https://github.com/Lightning-AI/lightning/pull/17058)),,1
Add is_wrapped utility function for Fabric (#16953),0.6062516,"- Added `lightning.fabric.is_wrapped` to check whether a module, optimizer, or dataloader was already wrapped by Fabric ([#16953](https://github.com/Lightning-AI/lightning/pull/16953))",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: awaelchli aedu.waelchli@gmail.com,0
Minor BYOT Follow-up (#17076),0.34602484,Minor changes,,0
Remove outdated trainer argument videos (#17071),0.65455973,Removed Warning from trainer loop (#1634),,0
Reroute LightningModule gradient clipping to Fabric (#16941),0.6170532,"By overriding the LightningModule.configure_gradient_clipping hook, you can customize gradient clipping to your needs:",Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
Remove duplicate CHANGELOG entry (#17069),0.51588416,Drop duplicate metrics (#5014),,0
Make BYOT imports forward compatible (#16997),0.4932938,Wrapped imports for traceability (#13924),,0
docs: adding hivemind (#17038),0.5612785,- Hivemind Strategy,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Add link to past versions in the docs header (#17063),0.5687568,A header with the version that generated the config is now included.,add past versions link,0
Remove outdated LightningModule animation (#17057),0.55373377,- Removed deprecated `get_progress_bar_dict` property from `LightningModule` ([#12839](https://github.com/Lightning-AI/lightning/pull/12839)),,0
Sort the arguments in the Trainer docs (#17047),0.6217464,Trainer Argument Defaults,,0
Update LightningModule.all_gather docs (#17046),0.6885543,- Removed the deprecated `summarize` method from the `LightningModule` ([#12559](https://github.com/Lightning-AI/lightning/pull/12559)),,0
"Revert ""import neptune instead of import neptune.new"" (#16898)",0.5989072,- Removed deprecated support for the old `neptune-client` API in the `NeptuneLogger` ([#14727](https://github.com/Lightning-AI/lightning/pull/14727)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Simplify the Trainer core logic (#17017),0.7126937,Trainer code became harder to follow,,1
Update introduction video (#17059),0.42724818,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",,0
Bump aws-actions/configure-aws-credentials from 1 to 2 (#17051),0.4648142,- Added authentication to HTTP queue ([#15202](https://github.com/Lightning-AI/lightning/pull/15202)),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Bump Lightning-AI/utilities from 0.7.1 to 0.8.0 (#17052),0.6299782,- Removed the deprecated ([#14471](https://github.com/Lightning-AI/lightning/pull/14471)),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
docs: update version matrix (#17041),0.5780051,Parsed local package versions (#13933),,0
Data connection mounts for jobs running from CloudSpaces (#17006),0.44104937,Add support to upload files to the Drive through an asynchronous upload_file endpoint (#14703),  Data connection mounts for jobs running from CloudSpaces   Tests   Tests fix   Bump Lightning Cloud ,0
Sort Trainer arguments based on importance (#17022),0.5803396,- Improved Trainer CLI arguments handling (generalization),,0
Add cute teaser animations to Fabric docs (#17021),0.53096735,Learn more about Fabric and what it can do in the new docs!,,0
Error checking for non-iterables (#17007),0.64367074,Support for arbitrary iterables (#16726),,0
Force keyword-only usage to init Fabric (#17023),0.53378767,fabric.launch(),,0
Allow frozen data classes in optimizer state dict (#16656),0.683452,- Added support for frozen dataclasses in the optimizer state ([#16656](https://github.com/Lightning-AI/lightning/pull/16656)),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Update links to latest PL docs (#17031),0.655004,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",,0
Update mypy job to 1.1.1 (#16974),0.4408033,$ PL_FAULT_TOLERANT_TRAINING=MANUAL python script.py,,0
rtfd: try to collapse docs (#17020),0.33739662,Docs improvements,,0
The psutil package is now required for CPU monitoring (#17010),1.0,The psutil package is now required for CPU monitoring (#17010),,1
Move HPU broadcast override to the HPU strategy file (#17011),0.9864663,Moveed HPU broadcast override to the HPU strategy file (#17011),,1
"Revert ""Use base version when comparing torch versions"" (#17019)",0.73673964,- Fixed an issue with comparing torch versions when using a version of torch built from source ([#17030](https://github.com/Lightning-AI/lightning/pull/17030)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Mark internal components as protected (#17009),0.51235354,Restricted public access to several internal functions (#8024),,0
Mark the connectors as protected (#17008),0.62517947,group connectors (#3472),,0
rtfd: try another redirect (#17018),0.35795388,Changed the default LightningClient(retry=False) to retry=True (#16382),,0
System customization syncing for jobs run (#16932),0.99999964,System customization syncing for jobs run (#16932),"  System customization syncing for jobs run   Constants moved   Moving files   Update src/lightning/app/runners/cloud.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   Update test_cloud.py   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  Update test_cloud.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",1
revert building docs with redirect (#17016),0.40921074,Updated app URLs to the latest format (#16568),,0
Inline functions called by _run_stage (#17015),0.58387464,remove obscure forward call in eval + CPU backend ___step (#3123),,0
RTFD: building docs for with redirect (#16993),0.4184429,Docs improvements,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
docs: listing past versions (#17014),0.6105561,Complete changelog,Co-authored-by: Luca Antiga luca.antiga@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Review APIs experimental status (#17012),0.52552223,Highlights of this release are adding Metric package and new hooks and flags to customize your workflow.,,0
Link to PyTorch in FSDP docs (#17013),0.74935246,"With the recent annoucement that FSDP becomes production ready in PyTorch 2.0, we are dropping the support for the experimental Fairscale version of FSDP and go all in on the native implementation instead.",,1
Update Fabric docs with installation instructions (#16996),0.5639472,The Fabric.run() method is no longer abstract (#14992),,0
Workarounds for log support with Torch 2.0 (#16986),0.6234579,- Fixed an issue with comparing torch versions when using a version of torch built from source ([#17030](https://github.com/Lightning-AI/lightning/pull/17030)),,0
Update Trainer property docstrings (#16989),0.7414018,Removed TrainerProperties mixin and moved property definitions directly into Trainer (#9495),,1
Remove unused code (#17001),0.54751456,Remove MetricsHolder (#7909),,0
Refactor loop.setup_data with utility functions (#16918),0.5145147,clean up hooks in run_evaluation (#3156),,0
Fix race condition in Fabric test (#17002),0.5897584,The Fabric.run() method is no longer abstract (#14992),,0
Typing fixes (#17000),0.70358276,Refactored setup for typing friendly (#6590),,1
Inline the ModelIO interface (#16999),0.5680427,Support shorthand notation to instantiate models (#9588),,0
Make examples runnable (#16981),0.46600506,- Full tests that test specific functionality in trainer.,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Add compatibility matrix (#16998),0.37142265,"device parser (#3400, #3405)",,0
[App] Use environment variables from args when loading apps for the CloudRuntime,0.67325264,Add --secret option to CLI to allow binding Secrets to app environment variables when running in the cloud (#14612),"What does this PR do? Passes environment variables to the CloudRuntime when loading an app from a file. Allows apps / works / flows to be properly configured when passing variables at runtime. Limitations  Only implemented for the CloudRuntime   Before submitting  - Was this **discussed/agreed** via a GitHub issue? (not for typos and docs) - [ ] Did you read the [contributor guideline](https://github.com/Lightning-AI/lightning/blob/master/.github/CONTRIBUTING.md), **Pull Request** section? - [ ] Did you make sure your **PR does only one thing**, instead of bundling different changes together? - Did you make sure to **update the documentation** with your changes? (if necessary) - Did you write any **new necessary tests**? (not for typos and docs) - [ ] Did you verify new and **existing tests pass** locally with your changes? - Did you list all the **breaking changes** introduced by this pull request? - Did you **update the CHANGELOG**? (not for typos, docs, test updates, or minor internal changes/refactors)   In the CHANGELOG, separate each item in the unreleased section by a blank line to reduce collisions   PR review Anyone in the community is welcome to review the PR. Before you start reviewing, make sure you have read the review guidelines. In short, see the following bullet-list:  Reviewer checklist  - [ ] Is this pull request ready for review? (if not, please submit in draft mode) - [ ] Check that all items from **Before submitting** are resolved - [ ] Make sure the title is self-explanatory and the description concisely explains the PR - [ ] Add labels and milestones (and optionally projects) to the PR so it can be classified     Did you have fun?  Make sure you had fun coding 🙃   cc @borda",0
Allow sys.argv and args in LightningCLI (#16808),1.0000002,Allow sys.argv and args in LightningCLI (#16808),,1
Use base version when comparing torch versions (#16657),0.64986885,- Fixed an issue with comparing torch versions when using a version of torch built from source ([#17030](https://github.com/Lightning-AI/lightning/pull/17030)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Refactor PL examples to examples/pytorch/ (#16925),0.6091238,Simplify the PL examples structure (shallower and more readable) (#1247),Co-authored-by: awaelchli aedu.waelchli@gmail.com,0
Add test for torch.compile() with Fabric.setup() (#16977),0.6340072,- Fixed an issue with comparing torch versions when using a version of torch built from source ([#17030](https://github.com/Lightning-AI/lightning/pull/17030)),,0
Miscellaneous updates in Fabric docs (#16980),0.55909145,The Fabric.run() method is no longer abstract (#14992),,0
ci/docs: clean toods (#16982),0.46864384,clean up data reset (#3161),,0
ci: update group-check (#16984),0.5027745,group prepare data hook (#3212),,0
typing: fix App's core API - App (#16949),0.52400875,Removed deprecated API (#2073),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Update documentation about configuration files structure (#16956),0.58584636,A header with the version that generated the config is now included.,,0
Document how to use multiple models and optimizers in Fabric (#16952),0.72028995,"model, optimizer = fabric.setup(model, optimizer)",,1
typing: fix App's core API - Queues (#16948),0.52634156,- Removed the deprecated `LightningModule.add_to_queue` and `LightningModule.get_from_queue` method ([#13600](https://github.com/Lightning-AI/lightning/pull/13600)),,0
Example of K-fold Cross Validation with Fabric (#16909),0.46100634,"Changed setup() and teardown() stage argument to take any of {fit,validate,test,predict} (#6386)",Co-authored-by: Nikhil Shenoy nikhilshenoy@dhcp-128-189-229-146.ubcsecure.wireless.ubc.ca Co-authored-by: Nikhil Shenoy nikhilshenoy@dhcp-128-189-231-235.ubcsecure.wireless.ubc.ca Co-authored-by: Nikhil Shenoy nikhilshenoy@dhcp-128-189-229-169.ubcsecure.wireless.ubc.ca Co-authored-by: Nikhil Shenoy nikhilshenoy@dhcp-128-189-230-40.ubcsecure.wireless.ubc.ca,0
add pkg info files (#16610),0.42373025,Moved accelerators and plugins to its legacy pkg (#5645),"  add pkg info files   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Update src/lightning_app/shell-folder_code-lives-lightning.info   Update src/lightning_app/shell-folder_code-lives-lightning.info   Update src/lightning_fabric/shell-folder_code-lives-lightning.info   Update src/lightning_fabric/shell-folder_code-lives-lightning.info   Update src/pytorch_lightning/shell-folder_code-lives-lightning.info   Update src/pytorch_lightning/shell-folder_code-lives-lightning.info    Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Luca Antiga luca.antiga@gmail.com",0
[App] Add --zip option to cp command (#16798),0.7732787,- Added `--zip` option to the `lightning cp` command to copy content from the Cloud Platform Filesystem as a zipfile,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
docs: building releases (#16967),0.6185977,This release includes:,  docs: building releases   codeowners   todo   rev   on *   push tag   name ,0
ci: limit keeping artifacts for PRs (#16976),0.4207605,Doing this minor release because correct validation metrics logging is critical.,,0
"Update pandoc requirement from <=2.2,>=1.0 to >=1.0,<=2.3 in /requirements (#16960)",0.4015018,Release LAI docs as stable (#14250),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Bump sphinx-toolbox from 3.2.0 to 3.4.0 in /requirements (#16959),0.60411596,Pinned sphinx-autodoc-typehints with <v1.15 (#11400),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Document SLURM interactive mode (#16955),0.58346975,Read more about our SLURM integration here.,,0
Bump default Lightning e2e image version for custom dependencies integration tests (#16975),0.42636436,Pkg: fix parsing versions by @Borda in https://github.com/Lightning-AI/lightning/pull/15401, [Do not merge] debugging custom dependencies integration tests,0
New fabric parity tests (#16899),0.43396756,+         fabric.backward(loss),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka jirka.borovec@seznam.cz,0
typing: fix App's core API - Flow (#16947),0.5451444,Refactored setup for typing friendly (#6590),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Mention that the Trainer has started in barebones mode (#16926),0.7957153,Barebones Trainer mode (#16854),Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,1
typing: fix App's core API - Work (#16946),0.5070434,Removed deprecated API (#2073),,0
ci: fix Azure trigger pattern (#16972),0.3878769,Refactor cloud dispatch and update to new API (#16456),,0
E2E image version using from environment variable instead of hardcoded one (#16968),0.40573812,Auto-upgrade / detect environment mis-match from the CLI (#15434), E2E image version using from environment variable instead of hardcoded one,0
Update mypy job to torch 2.0 (#16933),0.701543,Corrected call to torch.no_grad (#5124),,1
Call _cuda_clearCublasWorkspaces on teardown (#16907),0.5588504,Removed deprecated pytorch_lightning.utilities.memory.get_gpu_memory_map in favor of pytorch_lightning.accelerators.cuda.get_nvidia_gpu_stats (#15617),,0
Add max_size mode to CombinedLoader (#16939),0.72182405,"        return CombinedLoader(iterables, mode=""min_size"")",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Update Fabric docs navigation (#16957),0.49982774,:open_book: Go to Fabric documentation :open_book:,,0
Switch theme for Fabric (#16961),0.5508559,from lightning.fabric import Fabric,,0
The torchdynamo inline cache is fixed in 2.1 (#16934),0.58720493,- Fixed an issue with comparing torch versions when using a version of torch built from source ([#17030](https://github.com/Lightning-AI/lightning/pull/17030)),,0
Prepare for ShardedTensor deprecation (#16892),0.57617897,Deprecation warning (#3844),,0
BYOT example (#16938),0.4057986,Made type hints public (#17100),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Luca Antiga luca.antiga@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Document gradient clipping in Fabric (#16943),0.6154672,Gradient Clipping Customization,,0
docs: add link to lightning-colossalai (#16945),0.5976713,Learn how to install and use Colossal-AI effectively with Lightning here.,,0
docs: deploy all (#16951),0.44770688,Release LAI docs as stable (#14250),,0
Fabric: Test PyTorch 2.0 pre-release on CPU and CUDA (#16905),0.6686471,"fabric = Fabric(accelerator=""cuda"", devices=8, strategy=""ddp"")",Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
Maverick message fix (#16940),0.43724778,Silenced some warnings. verified ddp refactors (#3483),"  message fix for maverick   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  cleanup   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
update list of fist party packages (#16859),0.47082338,Parsed local package versions (#13933),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
torchmetrics: bump min version v0.10 in examples (#16936),0.5602533,"Set PyTorch 1.4 as min requirements, also for testing and examples torchvision>=0.5 and torchtext>=0.5 (#5418)",,0
refactor: move RunIf to PL pkg (#16923),0.48503965,Removed support for the deprecated on_load_checkpoint signature. The hook now takes a pl_module positional parameter (#8697),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Skip flaky ddp-spawn test on windows (#16942),0.7638276,Run ddp_spawn dataloader checks on Windows (#6930),,1
tests/hotfix: import pl (#16935),0.53913146,Removed support for the deprecated on_load_checkpoint signature. The hook now takes a pl_module positional parameter (#8697),,0
[App] Add healthz endpoint to plugin server (#16882),0.4789969,Initial plugin server (#16523),Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
Update CHANGELOG after the v1.9.4 release (#16906),0.6644743,Full Changelog,,0
Fix tests on single-GPU machine (#16911),0.5413803,"Deprecated the use of Trainer(gpus=""i"") (string) for selecting the i-th GPU; from v1.5 this will set the number of GPUs instead of the index (#6388)",,0
Optimize precision conversion in forward of Fabric module wrapper (#16903),0.61778617,Enable precision autocast for LightningModule step methods in Fabric (#17439),,0
Bump playwright from 1.29.1 to 1.30.0 in /requirements (#16735),0.4094469,Setup: added requirement freeze for the next major version (#14480),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz,0
Update requests requirement from <=2.28.1 to <2.28.3 in /requirements (#16547),0.5245141,Setup: added requirement freeze for the next major version (#14480),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
"Update torchmetrics requirement from <0.10.1,>=0.7.0 to >=0.7.0,<0.11.1 in /requirements (#15904)",0.7160087,    * Some configuration errors that were previously raised as `MisconfigurationException`s will now be raised as `ProcessRaisedException` (torch>=1.8) or as `Exception` (torch<1.8), Update torchmetrics requirement in /requirements  Updates the requirements on torchmetrics to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: torchmetrics   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com  task  Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz,1
Add support for fsspec paths for CSVLoggers (#16880),0.6199362,- Added support for writing logs remote file systems on `CSVLoggers`. ([#16880](https://github.com/Lightning-AI/lightning/pull/16880)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
update default websocket setting (#16446),0.38101247,The LoadBalancer now uses internal ip + port instead of URL exposed (#16119),,0
ci/gpu: fix install future & use local cache (#16929),0.46840936,Setup: added requirement freeze for the next major version (#14480),,0
docker: fix Torch url (#16927),0.593554,- Raised exception in `init_dist_connection()` when torch distributed is not available ([#10418](https://github.com/PyTorchLightning/pytorch-lightning/pull/10418)),,0
cleaning typing CI and config (#16860),0.5584061,Refactored setup for typing friendly (#6590),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
pkg: append PL to lightning (#16921),0.60029626,  * `pl.utilities.seed` ([#16422](https://github.com/Lightning-AI/lightning/pull/16422)),,0
add Any annotation (#16861),0.46503744,Made type hints public (#17100),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Ignore dead links due to fabric docs move (#16920),0.46541905,"Renamed the class LightningLite to Fabric (#15932, #15938)",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Maverick registration (#16913),0.4072336,Renamed several Trainer atributes:  (#567),Maverick registration (#16913),0
docs: move fabric on its own (#16742),0.5654324,Learn more about Fabric and what it can do in the new docs!,"  docs: move fabric to Lai   update imports   links   drop link to Trainer   own docs   ci   trigger   prune cross-links   cleaning   cleaning   template   imports   template   path   links   tensorboardX   plugins   label   drop fixme   drop copy nb + examples   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Apply suggestions from code review   try again   rev    Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
docs: typo in habana docs (#16910),0.6215875,- Removed sanity check for multi-optimizer support with habana backends ([#13217](https://github.com/Lightning-AI/lightning/pull/13217)),,0
PL: Test PyTorch 2.0 pre-release on CPU and CUDA (#16764),0.66597337,Enable PyTorch 1.7 compatibility (#3541),Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka jirka.borovec@seznam.cz,0
[App] Fix environment check with command redirection (#16883),0.54366213,"Prevent artefactual ""running from outside your current environment"" error (#15647)",,0
Require neptune 1.0 (#16888),0.6278608,"It now supports neptune-client 0.16.16 and neptune >=1.0, and we have replaced the log() method with append() and extend().",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
docs: rename source-app (#16863),0.5607606,Application storage prefix moved from app_id to project_id/app_id (#14583),"  docs: rename source-app   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   ci   group check   trigger   param   fix   cleaning    Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
ci: disable flagship apps as required (#16895),0.46834567,Improved support for running apps when dependencies aren't installed (#15711),,0
Log gradient norms of any magnitude (#16877),0.62428766,"def log_grad_norm(self, grad_norm_dict):",,0
Replace obsolete _FakeQueue in multiprocessing launcher (#16873),0.5345768,"Disabled strict loading in multiprocessing launcher (""ddp_spawn"", etc.) when loading weights back into the main process (#16365)",Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
ci: fix wheels' cashing (#16869),0.37860945,Cleanup cluster waiting (#16054),,0
Backwards compatibility for get_init_args (#16851),0.6675455,      init_args:,,0
Adds Gradient Clipping to Fabric (#16715),0.67024946,Gradient Clipping Customization,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
test/hotfix: DDPSpawnStrategy (#16889),0.8049603,Merged the implementation of DDPSpawnStrategy into DDPStrategy and removed DDPSpawnStrategy (#14952),,1
docs: update pytorch_lightning imports (#16864),0.8434895,Removed the deprecated pytorch_lightning.core.lightning module in favor of pytorch_lightning.core.module (#16318),  update docs imports   ci   fabric   trigger   links   .   docstring   chlog   cleaning ,1
Fix support for passing -1 to find_usable_cuda_devices function (#16866),0.65717304,  * Deprecated the `pytorch_lightning.utilities.device_parser.num_cuda_devices` in favor of `lightning_lite.accelerators.cuda.num_cuda_devices`,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Explain configure_sharded_model in ColossalAI docs (#16872),0.70116436,def configure_sharded_model(self):,,1
Merge DDPStrategy and DDPSpawnStrategy in PL (#16809),0.82608914,Merged the implementation of DDPSpawnStrategy into DDPStrategy and removed DDPSpawnStrategy (#14952),,1
Incorporate pytorch's fixes in device_count_nvml (#16795),0.6703572,- Changed `DeviceStatsMonitor` to group metrics based on the logger's `group_separator` ([#11254](https://github.com/PyTorchLightning/pytorch-lightning/pull/11254)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Disable the IPU check group (#16886),0.49639273,Stopped optimizer_zero_grad from being called after IPU execution (#12913),,0
Check for dataloader_idx presence in the hooks (#16837),0.69683814,"Related to this, we cleaned up which hooks need the dataloader_idx as an input argument. Now it's only required if you use multiple dataloaders. Don't worry, the Trainer will automatically check if it's required for you and tell you about it.",,0
Promote Fabric.launch() as the default experience in Fabric docs (#16878),0.72958004,fabric.launch(),,1
Bump Lightning-AI/utilities from 0.6.0 to 0.7.1 (#16879),0.6366331,- Removed the deprecated ([#14471](https://github.com/Lightning-AI/lightning/pull/14471)),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Remove redundant strategy property (#16811),0.55192626,Renamed Strategy.reduce to Strategy.all_reduce in all strategies (#16370),,0
Fix imports for lightning cli examples (#16871),0.7088116,- Deprecated LightningCLI's registries in favor of importing the respective package ([#13221](https://github.com/Lightning-AI/lightning/pull/13221)),,1
Update torch_xla installation instructions in tpu_basic.rst (#16865),0.6603168,    * Some configuration errors that were previously raised as `MisconfigurationException`s will now be raised as `ProcessRaisedException` (torch>=1.8) or as `Exception` (torch<1.8),,0
Introduce Trainer(barebones=True) (#16854),0.7908949,Barebones Trainer mode (#16854),Co-authored-by: Luca Antiga luca.antiga@gmail.com,1
Fix amp ddp test in Fabric (#16862),0.48731145,"- The selection `Fabric(strategy=""ddp_spawn"", ...)` no longer falls back to ""ddp"" when a cluster environment gets detected ([#16780](https://github.com/Lightning-AI/lightning/pull/16780))",,0
Trainer: auto default (#16847),0.7950346,Trainer Argument Defaults,,1
Fabric: auto default (#16842),0.68440664,"- Fabric now chooses `accelerator=""auto"", strategy=""auto"", devices=""auto""` as defaults ([#16842](https://github.com/Lightning-AI/lightning/pull/16842))",,0
Update changelog after 1.9.3 and bump version for RC (#16833),0.5100731,Full Changelog,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Fix for hanging issue on TPU Pod (#16844),0.5070762,Fix hanging in DDP HPC accelerators (#5157),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Fabric: do set_epoch for batch_sampler.sampler (#16841),0.69676507,The loops now call .set_epoch() also on batch samplers if the dataloader has one wrapped in a distributed sampler (#13396),,0
[App] Add support for plugins to return actions (#16832),0.46409467,- Added support for adding requirements to commands and installing them when missing when running an app command ([#15198](https://github.com/Lightning-AI/lightning/pull/15198),,0
Consume the prediction batch indices iteratively (#16826),0.63652706,"def training_step(self, batch, batch_idx, hiddens):",,0
Remove implicit frontend testing from testing.run_app_in_cloud (#16741),0.840894,Removed implicit ui testing with testing.run_app_in_cloud in favor of headless login and app selection (#16741),Co-authored-by: Ethan Harris ethanwharris@gmail.com,1
Rename replace_sampler_ddp|replace_sampler to use_distributed_sampler (#16829),0.869472,Allow setting replace_sampler_ddp=True with a distributed sampler already added (#4273),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Move max_batches definition to the Loops (#16820),0.66121477,"    batch_size=32,",,0
[App] Fix local app run with relative import (#16835),0.5220922,Running an app without a UI locally no longer opens the browser (#15875),,0
Add more pytree tests (#16825),0.45203537,python script.py test,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
fix make clean command (#16823),0.41141692,Renamed reset_on_epoch to reset_on_run (#9658),,0
Move the CombinedLoader to an utility file (#16819),0.5917407,"        return CombinedLoader(iterables, mode=""min_size"")",,0
Always run standalone tests (#16705),0.5144006,Removed implicit ui testing with testing.run_app_in_cloud in favor of headless login and app selection (#16741),,0
Avoid instantiating CombinedDataset unnecessarily (#16805),0.45699364,duplicate data interface definition up into DataHooks class (#3344),,0
Set accelerator through CLI only if set explicitly (#16818),0.74932075,Ensure accelerator is valid if running interactively (#5970),,1
Remove the *_step_end hooks (#16791),0.83249336,moved ___step_end hooks (#3130),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Move _TrainingEpochLoop (#16801),0.6861105,Deprecated @auto_move_data in favor of trainer.predict (#6993),,0
Resolve FitLoop setter TODOs (#16803),0.48757482,Marked FitLoop.should_accumulate as protected (#9515),,0
Remove Trainer(multiple_trainloader_mode) in favor of CombinedLoader(mode) (#16800),0.8414631,"To simplify the Trainer interface and with the goal of simpler iterable support inside the Trainer, we removed theTrainer(multiple_trainloader_mode=...) argument. The mode is now agnostic to the trainer stage (""train"" previously) and it's easier to debug and understand for the user as the logic is all encapsulated in the CombinedLoader",,1
Make DDP subprocess the default launcher for multi-device (#16780),0.5623095,"When using multiple devices, the strategy now defaults to ""ddp"" instead of ""ddp_spawn"" when none is set (#16388)",,0
Fix bug in lightning_cli_advanced_3.rst (#16792),0.6609479,- Removed the deprecated `LightningCLI` arguments ([#16380](https://github.com/Lightning-AI/lightning/pull/16380)),,0
Add missing docs quote (#16797),0.4176538,Syntax changes are: ,Co-authored-by: awaelchli aedu.waelchli@gmail.com,0
Fix XLAEnvironment detection on TPU pod (#16806),0.48405758,  * Deprecated the `XLADeviceUtils.tpu_device_exists` staticmethod in favor of `pytorch_lightning.accelerators.TPUAccelerator.is_available()`,,0
Add back external colossalai test (#16817),0.4138217,Enabled manual returns (#4089),,0
Bump Lightning-AI/utilities from 0.4.1 to 0.6.0 (#16812),0.6400897,- Removed the deprecated ([#14471](https://github.com/Lightning-AI/lightning/pull/14471)),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
[App] fix lightning open command & better redirects (#16794),0.6306948,Changed the command lightning connect to lightning connect app for consistency (#16670),"  fix(app): URLs, create run on app run   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   fix tests   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
Introduce new precision layout in PL (#16783),0.5589161,Mixed precision overhaul (#16783),,0
Sequential CombinedLoader to flatten the eval and predict loops (#16726),0.5999851,"        return CombinedLoader(iterables, mode=""min_size"")",,0
Update changelog after 1.9.2 release (#16777),0.6449122,Full Changelog,changelog Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
[App] Refactor plugins to be a standalone LightningPlugin (#16765),0.4915943,Update the Lightning App docs (#13537),,0
Introduce new precision layout in fabric (#16767),0.4791293,Learn more about Fabric and what it can do in the new docs!,,0
[App] Reserve APP_SERVER_PORT in cloud port allocation (#16782),0.5183264,    default_port = 12910,Co-authored-by: thomas chaton thomas@grid.ai,0
fix warning so the user has a clear next step (#16751),0.5854991,Removed Warning from trainer loop (#1634),,0
Rename the TPUSpawnStrategy to XLAStrategy (#16781),0.7529906,- Renamed `TPUSpawnStrategy` to `XLAStrategy` ([#16781](https://github.com/Lightning-AI/lightning/pull/16781)),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
[App] Fix idle timeout e2e (#16786),0.47140163,Removed the SingleProcessRuntime (#15933),,0
Trigger colossalai integration test in CI (#16789),0.40786916,We have upgrade Continues Integration to speed up the automatic testing. ,,0
Fix set_epoch not getting called for prediction dataloaders (#16785),0.69435346,The dataloader wrapper returned from .setup_dataloaders() now calls .set_epoch() on the distributed sampler if one is used (#16101),,0
SequentialMode and dataloader_iter improvements (#16784),0.73373544,"Redesigned multi-dataloader support (#16743, #16784, #16939)",,1
Update Colossal AI docs and integration (#16778),0.642959,Colossal-AI,,0
Remove duplicate no_grad context managers (#16773),0.5372075,The default value of accumulate_grad_batches has changed from 1 to None (#9652).,,0
Prefetch if it's not a sized iterable (#16776),0.57675177,    def prefetch_batches(self):,,0
Run XLA's dataloader validation per dataloader (#16775),0.65752494,enabled multiple dataloaders for validation.    ,,0
Fix RunningStage properties for sanity checking (#16774),0.655429,| Enum RunningStage.TUNING                                    | 1.10             | No longer supported         |,,0
Use the local batch_idx to update the progress bar (#16760),0.6684392,Changed the default progress bar to print to stdout instead of stderr (#531),,0
Remove DP (#16748),0.67100215,Remove MetricsHolder (#7909),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Make the trainer a required loop argument (#16771),0.7798795,Removed Warning from trainer loop (#1634),,1
Remove the QuantizationAwareTraining callback (#16750),0.70320964,- Removed the `QuantizationAwareTraining` callback ([#16750](https://github.com/Lightning-AI/lightning/pull/16750)),,1
Update colossalai version in Dockerfile (#16766),0.45155573,"- Removed the `ColossalAIStrategy` and `ColossalAIPrecisionPlugin` in favor of the new [lightning-colossalai](https://github.com/Lightning-AI/lightning-colossalai) package ([#16757](https://github.com/Lightning-AI/lightning/pull/16757), [#16778](https://github.com/Lightning-AI/lightning/pull/16778))",update docker,0
ci/hotfix: if cache/wheels missing (#16769),0.37725723,Update lr_finder to check for attribute if not running fast_dev_run (#5990),,0
Remove dead code in the loops (#16754),0.60838366,Refactored Loops,,0
Remove the Trainer.prediction_writer_callbacks property (#16759),0.8576648,- Removed the `Trainer.prediction_writer_callbacks` property ([#16759](https://github.com/Lightning-AI/lightning/pull/16759)),,1
Replace ColossalAIStrategy with external implementation (#16757),0.48071122,from lightning.pytorch.strategies import ColossalAIStrategy,,0
CI: Update colossalai version (#16747),0.4963673,"- Removed the `ColossalAIStrategy` and `ColossalAIPrecisionPlugin` in favor of the new [lightning-colossalai](https://github.com/Lightning-AI/lightning-colossalai) package ([#16757](https://github.com/Lightning-AI/lightning/pull/16757), [#16778](https://github.com/Lightning-AI/lightning/pull/16778))",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Make dataloader_idx optional for batch start/end hooks (#16753),0.70469695,refactored dataloader process hook (#3139),,1
ci: cleaning caches (#16752),0.533263,clean up data reset (#3161),,0
[App] Add rm one level below project level (#16740),0.5659095,Application storage prefix moved from app_id to project_id/app_id (#14583),Co-authored-by: Ethan Harris ethanwharris@gmail.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: thomas thomas@thomass-MacBook-Pro.local,0
Remove the BaguaStrategy (#16746),0.51139426,Remove MetricsHolder (#7909),  remove bagua   remove   remove docker file entry ,0
Remove Trainer's track_grad_norm argument (#16745),0.76861453,trainer = L.Trainer(track_grad_norm=2),,1
[App] Add support for private data (#16738),0.59627116,"Introducing encrypted secrets (#14612), a feature requested by Lightning App users :tada:!",Co-authored-by: thomas thomas@thomass-MacBook-Pro.local,0
ci: fix dependabot (#16749),0.47225633,"Fixing missing packaging package in dependencies, which was affecting the only installation to a very blank system.",,0
Remove to-device functionality from fetchers (#16731),0.54532933,We cleaned up the properties related to device indices (#14829).,,0
"""sequential"" mode for CombinedLoader (#16743)",0.6940277,CombinedLoader only starts DataLoader workers when necessary when operating in sequential mode (#17639),,0
Remove the unused utilities.parsing.flatten_dict (#16744),0.57301754,- Removed the unused `lightning.pytorch.utilities.parsing.flatten_dict` function ([#16744](https://github.com/Lightning-AI/lightning/pull/16744)),,0
Group trainer call methods as functions (#16702),0.6300708,Changed callbacks argument in Trainer to allow Callback input (#5446),,0
Remove argparse utils (#16708),0.79868037,argparse_utils >> argparse,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,1
Separate the Gradient Accumulation Scheduler from Trainer (#16729),1.0000002,Separate the Gradient Accumulation Scheduler from Trainer (#16729),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
ci/hotfix: legacy workflow (#16721),0.44816202,Doing this minor release because correct validation metrics logging is critical.,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
"Update hydra-core requirement from <1.3.0,>=1.0.5 to >=1.0.5,<1.4.0 in /requirements (#16736)",0.6548997,Temporarily removed support for Hydra multi-run (#15737),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
rename docs/source-app & adjust docs links for lightning (#16676),0.58240724,Update the Lightning App docs (#13537),  update CI   config / import   lightning_app imports   source/ dir   html   ci: dirs   pr   req dir   on push   rename   drop   cleaning ,0
Refactor CombinedLoader using pytrees (#16714),0.62985444,"combined_loader = CombinedLoader(iterables, mode=""min_size"")",,0
[App] Revert raising error from LightningClient on 500 (#16723),0.634168,Changed the default LightningClient(retry=False) to retry=True (#16382),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Fix min-epochs and early-stopping triggering too many validation runs (#16719),0.7222055,Separate epoch validation from step validation (#5208),Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,1
update chlog after 1.9.1 release (#16696),0.51617134,Here are two major changes that apply when using multiple loggers in 1.8:,,0
ci: freeze pypi releasing (#16724),0.4331699,"Resolved schedule step bug for PyTorch Profiler (#6674, #6681)",,0
Move inference_mode logic to the loops (#16704),0.5776016,Refactored training loop (#2336),,0
Remove custom log events (#16707),0.58328676,Removed LoggerStages (#5673),,0
Group torch.compile utilities together (#16711),0.671556,PyTorch 2.0 and torch.compile,,0
Fix strategy type validation in connectors (#16693),0.5076259,"All TrainingTypePlugins have been renamed to Strategy (#11120). Strategy is a more appropriate name because it encompasses more than simply training communcation. This change is now aligned with the changes we implemented in 1.5, which introduced the new strategy and devices flags to the Trainer.",,0
Update pytorch_lightning imports in examples (#16615),0.75773764,from pytorch_lightning import Trainer,Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,1
Update examples with multiple optimizers (#16710),0.7683408,Working with multiple optimizers (#16539),,1
Add .git-blame-ignore-revs (#16709),0.47126383,Pkg: fix parsing versions by @Borda in https://github.com/Lightning-AI/lightning/pull/15401,Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
Move Trainer._log_hyperparams to an utility (#16712),0.6630296,"Deprecated passing flush_logs_every_n_steps as a Trainer argument, instead pass it to the logger init if supported (#9366)",,0
move _HPU_AVAILABLE (#16713),0.50693053,move lr_finder (#3434),,0
Remove the unused utilities.metrics.metrics_to_scalars (#16681),0.8175005,Changed metrics_to_scalars to work with any collection or value (#7888),,1
Remove the unused utilities.finite_checks (#16682),0.51712996,Removed deprecated property Trainer.running_sanity_check in favor of Trainer.sanity_checking (#9209),,0
Remove the trainer.data_parallel property (#16703),0.76090324,Deprecated the Trainer.train_loop property in favor of Trainer.fit_loop (#8025),,1
Split train- and val progress into separate bars (#16695),0.7325702,now the progress bar has a full bar for the full train + val epochs and a second bar visible only during val.,,1
Update Fabric introduction (#16672),0.73262787,Learn more about Fabric and what it can do in the new docs!,Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com,1
[App] Maverick (#16701),0.4891584,App,introducing maverick,0
[App] Connect and Disconnect node (#16700),0.86456835,Connect and Disconnect node (#16700),Connect and Disconnect node,1
ci: wheels with continue-on-error (#16675),0.41299325,"The default value of the max_steps Trainer argument has changed from None to -1 (#9460). You can no longer specify Trainer(max_steps=None) and if you did, you need to change the code to Trainer(max_steps=-1).",,0
ci: parameterize GPU testing (#16697),0.5732615,refactored GPU backend __step (#3120),,0
[App] Hot fix. Resolve platform CI (#16699),0.5463717,"Resolved LightningApp(..., debug=True) (#14464)",Co-authored-by: thomas thomas@thomass-MacBook-Pro.local,0
[App] Enable to register data connections (#16670),0.46367067,Introduce lightning connect (#14452),Co-authored-by: thomas thomas@thomass-MacBook-Pro.local,0
tests: drop slow flag (#16692),0.5111089,Cleaning up stale logger tests (#3490),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
ci: merge pytest with slow (#16689),0.42934585,Deprecated self.log(sync_dist_op) in favor of self.log(reduce_fx). (#7891),,0
Move fetchers classes with the loops (#16678),0.4922642,"train loop refactor - moving train loop to own object (#3310, #3312, #3313, #3314)",,0
Move logger utilities with the loggers (#16680),0.7114731,Change default logger to a dedicated one (#1064),,1
Add Strategy.on_exception (#16646),0.6721213,- Added a new method `Strategy.on_exception` to the strategy base interface ([#16646](https://github.com/Lightning-AI/lightning/pull/16646)),,0
[App] Initial plugin server (#16523),0.84562826,Initial plugin server (#16523),,1
Typo in printing app.py (#16643),0.45358634,Removed support for self.log()ing a dictionary (#16389),,0
"[App] Fix e2e CI, use display name in show logs command (#16679)",0.48270017,- Improved the show logs command to be standalone and re-usable ([#15343](https://github.com/Lightning-AI/lightning/pull/15343),,0
Flatten fetching abstract interface (#16664),0.41035303,Simpler interface,,0
Refactor supporters (#16662),0.41754815,Renamed several Trainer atributes:  (#567),,0
Remove outputs from on_predict_epoch_end (#16655),0.6980087,"def training_epoch_end(self, outputs):",,0
ci: fix torch +cpu in wheels (#16674),0.5384559,Corrected call to torch.no_grad (#5124),,0
Remove metric reset after checkpoint load (#16661),0.6812887,Change Metrics persistent default mode to False (#4685),,0
Clarify default epoch and step values in ModelCheckpoint (#16651),0.65165377,ModelCheckpoint now runs at the end of the training epoch by default (#8389),,0
ci: replace pip cache with wheels (#16668),0.40529835,Removed deprecated auto_move_data decorator (#9231),,0
ci: flagship flow - clean inputs (#16666),0.48847914,Changed the flow.flows to be recursive wont to align the behavior with the flow.works (#15466),,0
Fix import from torch.distributed when distributed not available (#16658),0.7435545,- Raised exception in `init_dist_connection()` when torch distributed is not available ([#10418](https://github.com/PyTorchLightning/pytorch-lightning/pull/10418)),,1
Refactor deterministic and benchmark logic (#16653),0.5448652,Support for warn-level determinism,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Fix a typo in transfer_batch_to_device function's comment (#16659),0.5972258,Don't convert namedtuple to tuple when transferring the batch to target device (#1589),,0
[App] Improve Storage Commands (#16645),0.4909985,Introducing CLI commands for apps (#13602)!,  update   update   update   update    Co-authored-by: thomas thomas@thomass-MacBook-Pro.local,0
Fix TPU tests (#16628),0.644506,This means we fixed many TPU bugs we hadn’t caught before because we had no tests.,,0
Remove memory-retaining epoch-end hooks (#16520),0.7782612,The *_epoch_end hooks were removed (#16520),,1
"Update jinja2 requirement from <3.1.0,>=3.0.0 to >=3.0.0,<3.2.0 in /requirements (#16546)",0.43149704,Setup: added requirement freeze for the next major version (#14480),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Properly terminate MultiProcessing Runtime (#16623),0.53521705,  * Deprecated the internal `pl_multi_process` function,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Update websocket-client requirement from <=1.3.3 to <1.5.2 in /requirements (#16640),0.40316045,"    version=""0.0.1"",",Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
"Update fsspec[http] requirement from <2022.8.0,>2021.06.0 to >2021.06.0,<2023.2.0 in /requirements (#16544)",0.47243607,Setup: added requirement freeze for the next major version (#14480),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
[App] Fix AutoScaler example (#16557),0.65128016,Porting fixes to autoscaler component (#16249),,0
Set find_unused_parameters=False as the default (#16611),0.90273166,            find_unused_parameters=True,,1
Move _POPTORCH_AVAILABLE and _IPU_AVAILABLE (#16509),0.4894723,Removed deprecated auto_move_data decorator (#9231),Co-authored-by: Nikhil Shenoy nikhilshenoy@dhcp-128-189-225-225.ubcsecure.wireless.ubc.ca Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
copyright Lightning AI team (#16647),0.52461696,  [#14620](https://github.com/Lightning-AI/lightning/pull/14620)),  copyright Lightning AI team   more... ,0
Bump docker/build-push-action from 3 to 4 (#16641),0.5160162,"Show a message when BuildConfig(dockerfile=""..."") is passed but a Dockerfile file is already present in the Work (#15799)",Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Move fsdp_native to fine-tuning recommendation (#16630),0.644125," * `strategy=""fsdp_native""` is now `strategy=""fsdp""`",,0
[App] Enable uploading files to a project. (#16631),0.6728482,Enabled cp (upload) at project level (#16631),  update   update   update   update   update   update   update   update   update   update   update   update   update   update    Co-authored-by: thomas thomas@thomass-MacBook-Pro.local,0
Fabric docs typo correction (#16635),0.5628562,  * Changed the method signatrue of `Fabric.save` and `Fabric.load`,,0
update PR template (#16620),0.424653,Syntax changes are: ,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
fix(tests): use display_name in app/e2e (#16632),0.49400204,Removed implicit ui testing with testing.run_app_in_cloud in favor of headless login and app selection (#16741),,0
[App] Add support for running with multiprocessing in the cloud (#16624),0.56065756,- Added support to explicitly specify the process group backend for parallel strategies ([#11745](https://github.com/PyTorchLightning/pytorch-lightning/pull/11745)),,0
[App] Enable listing at project level 2/n (#16622),0.6284965,Add support for listing Lightning AI apps (#13987),Co-authored-by: Ethan Harris ethanwharris@gmail.com Co-authored-by: thomas thomas@thomass-MacBook-Pro.local Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
[App] Add Missing Copyright (#16625),0.47525012,Updated app URLs to the latest format (#16568),Co-authored-by: thomas thomas@thomass-MacBook-Pro.local,0
Add MPI cluster environment (#16570),0.62603384,"adding compute environments (#3837, [#3842)",Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Avoid wrapping prediction dataloader twice on TPU (#16571),0.58007574,Ensured process_dataloader is called when tpu_cores > 1 to use Parallel DataLoader (#6015),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Upgrade to HPU release 1.8.0 (#16621),0.5265486,Moveed HPU broadcast override to the HPU strategy file (#17011),,0
[App] Introduce Lightning Storage Commands (#16606),0.5737784,- Added support for adding requirements to commands and installing them when missing when running an app command ([#15198](https://github.com/Lightning-AI/lightning/pull/15198),"  update   update   update   update   update   update   update   update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   update   update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   update    Co-authored-by: thomas thomas@thomass-MacBook-Pro.local Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
ci: update coverage scope (#16619),0.48448953,- Code coverage (99%),,0
Remove the obsolete Strategy.dispatch method (#16618),0.6494995,- Removed `Strategy.dispatch` ([#16618](https://github.com/Lightning-AI/lightning/pull/16618)),,0
Update import in bug report template (#16616),0.4961028,Module imports are now catching ModuleNotFoundError instead of ImportError (#9867),,0
Show tf32 info only on rank 0 (#16152),0.33753103,Tracks all outputs including TBPTT and multiple optimizers (#2890),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
ci: allow dispatch for flagship integrations (#16612),0.5506506,Refactor cloud dispatch and update to new API (#16456),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Rename optimization loops (#16598),0.7355014,        # 4. Perform the optimization in a loop,Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,1
Fix TPU CI (#16613),0.6255919,Resolve TPU miss rendezvous (#6781),,0
cleaning as standalone==mirror (#16601),0.50166285,"Cleaning (#5948, #5949, #5950)",,0
opt_idx cleanup after optimizer loop changes (#16597),0.6555319,Refactored the logic around manual and automatic optimization inside the optimizer loop (#7526),,0
add secondary label to templates (#16577),0.3214859,Included app templates to the lightning and app packages (#13731),,0
ci: simplify cashing (#16599),0.42954844,reduced all simplified forward (#3126),,0
tests: switch imports for pytorch (#16595),0.732884,Skipped testing with PyTorch 1.7 and Python 3.9 on Ubuntu (#11217),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
Ignore generated package files (#16605),0.4505421,Parsed local package versions (#13933),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz,0
Add FileSystem (#16581),0.38861465,"adding compute environments (#3837, [#3842)",Co-authored-by: Luca Antiga luca.antiga@gmail.com Co-authored-by: thomas thomas@thomass-MacBook-Pro.local Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
tests: switch imports for fabric (#16592),0.6099374,The Fabric.run() method is no longer abstract (#14992),,0
Fix broken links after reverse mirror changes (#16600),0.46180457,Updated app URLs to the latest format (#16568),,0
adjust codeowners for PL (#16602),0.38324922,Renamed xxx_AVAILABLE as protected (#5082),,0
Replace custom AllGather implementation (#11531),0.44465932,- Removed deprecated passthrough methods and properties from `Accelerator` base class:,,0
Drop support for PyTorch 1.10 (#16492),0.7861314,Drop PyTorch 1.9 support (#15347),"  Drop support for PyTorch 1.10   CHANGELOG   READMEs   mypy   ls   New poplar version   Fixed tests   links   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   skip azure badges   Table   Matching dockerfiles   Drop unnecessary channels and packages   Push nightly   Undo unrelated changes   Revert ""Push nightly""   This reverts commit 9618f737c4dc65331fef4bb11fe46a61513d220a.  Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",1
Sized iterable typing improvements (#16585),0.64837,Support for arbitrary iterables (#16726),,0
move pytorch_lightning >> lightning/pytorch (#16594),0.80433524,Removed the deprecated pytorch_lightning.core.lightning module in favor of pytorch_lightning.core.module (#16318),,1
Update docs for multiple optimizers in 2.0 (#16588),0.73392165,Working with multiple optimizers (#16539),,1
move lightning_fabric >> lightning/fabric (#16589),0.7444458,"Renamed the class LightningLite to Fabric (#15932, #15938)",Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
Make manual optimization mandatory for multiple optimizers (#16539),0.69712925,"Training with multiple optimizers is now restricted to the ""manual optimization mode"":",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Run on_train_epoch_end after the LM for callbacks that monitor (#16567),0.7439649,def on_train_epoch_end(self):,,1
clean loading version (#16591),0.5442575,loading,,0
update codeowners for lit (#16596),0.48940015,Release LAI docs as stable (#14250),,0
Update CI workflow docs (#16578),0.50556266,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",,0
Only run TPU standalone tests (#16586),0.49430022,Removed implicit ui testing with testing.run_app_in_cloud in favor of headless login and app selection (#16741),,0
tests: switch imports for apps (#16554),0.50990087,Updated app testing (#16000),,0
align lit-utils version to post0 (#16593),0.5033695,There were two different ways of importing Lite in <= 1.9.0,,0
Bump max deepspeed version to 0.8.0 (#16469),0.65861976,"Implemented DeepSpeedPlugin._setup_model_and_optimizers (#10009, #10064)",Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
move lightning_app >> lightning/app (#16553),0.6960291,Unification of app template: moved app.py to root dir for lightning init app <app_name> template (#13853),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Install colossalai==0.1.12 in CI (#16587),0.5843297,Learn how to install and use Colossal-AI effectively with Lightning here.,,0
Drop support for Python 3.7 (#16579),0.7422221,Drop Python 3.6 support,Co-authored-by: Jirka jirka.borovec@seznam.cz,1
Ignore leaked XLA environment variables (#16582),0.5636231,  * Deprecated the internal `XLADeviceUtils.xla_available` staticmethod,,0
ci/hotfix: replace SD with Flashy (#16584),0.42046392,We cleaned up the properties related to device indices (#14829).,ci: replace SD with Flashy,0
Cascade SIGTERM to subprocesses (#16525),0.47555447,Multiple CPU processes,,0
[App] Fix app name in URL (#16575),0.7774029,Updated app URLs to the latest format (#16568),,1
ci: update doctest for installed packages (#16574),0.4478532,Resolved a bug where the install command was not installing the latest version of an app/component by default (#14181),,0
[App] Update app URLs to latest format (#16568),0.950134,Updated app URLs to the latest format (#16568),,1
"Update omegaconf requirement from <2.3.0,>=2.0.5 to >=2.0.5,<2.4.0 in /requirements (#16545)",0.5025831,Setup: added requirement freeze for the next major version (#14480),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Remove optimizer_idx from toggle/untoggle_optimizer methods (#16560),0.826513,Changed calling of untoggle_optimizer(opt_idx) out of the closure function (#7563),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Minor formatting fix on model_parallel docs (#16565),0.47063357,Support shorthand notation to instantiate models (#9588),,0
Switch multi-optimizer tests to manual optimization (#16559),0.7069502,Refactored the logic around manual and automatic optimization inside the optimizer loop (#7526),,1
update version for Fabric CLI (#16556),0.61291194,The Fabric.run() method is no longer abstract (#14992),,0
bump version Dev (#16562),0.48072168,[1.4.9] - 2021-09-30,,0
Remove DataLoader serialization (under fault tolerance) (#16533),0.6367556,"Did not always create a DataLoader during reinstantiation, but the same type as before (if a subclass of DataLoader) (#1346)",,0
Fabric RL example: add torchmetrics and minor fixes (#16543),0.5355362,- Removed the experimental `pytorch_lightning.utiltiies.meta` functions in favor of built-in https://github.com/pytorch/torchdistx support ([#13868](https://github.com/Lightning-AI/lightning/pull/13868)),Co-authored-by: awaelchli aedu.waelchli@gmail.com,0
Remove using_lbfgs argument from optimizer_step module hook (#16538),0.73731303,Executing the optimizer_closure is now required when overriding the optimizer_step hook (#9360),,1
Remove on_tpu argument from optimizer_step module hook (#16537),0.7529377,Executing the optimizer_closure is now required when overriding the optimizer_step hook (#9360),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
store: update API in messages (#16535),0.4499921,This release has breaking API changes. See #124 for all details. ,,0
store: mock/fixture home (#16536),0.42976892,Updated app testing (#16000),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
ci: hotfix precommit/poetry/isort (#16549),0.44398242,@akihironitta @awaelchli @Borda @carmocca @kaushikb11 @krshrimali @mauvilsa @otaj @pre-commit-ci @rohitgr7 @semaphore-egg @tkonopka @wayi1,,0
Ensure SIGTERM handlers other than ours can be added (#16534),0.48583195,We removed some Callback hooks that were ambiguous to use Removed deprecated callback hooks (#14834):,,0
Fix docstring for LightningWork.has_stopped (#16532),0.61830556,Improved the error message when the LightningWork is missing the run method (#14759),,0
Fabric docs feedback 2/n (#16480),0.7468191,Learn more about Fabric and what it can do in the new docs!,,1
Move progress file (#16524),0.46470392,Better progress bar (#16695),,0
PyTorch 2.0 switched the set_to_none default (#16531),0.67849696,Drop PyTorch 1.9 support (#15347),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Remove result serialization (under fault tolerance) (#16516),0.5446809,"Removed experimental fault-tolerance support (#16516, #16533)",,0
Decouple Tuner from Trainer (#16462),0.693106,The Tuner and Trainer broke up (#16462),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
store: adding group-check (#16528),0.5064415,group prepare data hook (#3212),,0
store: drop requirements_file_path (#16527),0.5179819,Deprecated filepath in ModelCheckpoint (#4213),,0
Make the FaultToleranceCheckpoint callback opt-in (#16512),0.6007268,Replace MisconfigurationException with warning in ModelCheckpoint Callback (#4560),,0
Add reinforcement learning example for Fabric (#16506),0.5621603,"model, optimizer = fabric.setup(model, optimizer)",Co-authored-by: awaelchli aedu.waelchli@gmail.com Co-authored-by: Luca Antiga luca@lightning.ai,0
Init: Models store API (#15811),0.5164194,Support shorthand notation to instantiate models (#9588),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
Allow MLFlowLogger to work with mlflow-skinny (#16513),0.79651755,- Added support for running the `MLFlowLogger` with the `mlflow-skinny` package ([16513](https://github.com/Lightning-AI/lightning/pull/16513)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Fixes https://github.com/Lightning-AI/lightning/issues/16486,1
SIGTERM handling is now unrelated to fault tolerance (#16501),0.72512317,"Removed experimental fault-tolerance support (#16516, #16533)",,1
Fix torch.compile tests (#16503),0.7193006,Corrected call to torch.no_grad (#5124),,1
Remove docs about automatic fault tolerance (#16500),0.69636124,"Removed experimental fault-tolerance support (#16516, #16533)",Remove docs about the experimental automatic fault tolerance,0
ci: trigger on action edit (#16514),0.36919212,Run the flow only if the state has changed from the previous execution (#14076),,0
Fabric checkpointing 3/n: Implement missing get_module_state_dict for strategies (#16487),0.7870377,  * Changed the method signature of `Strategy.save_checkpoint` and `Fabric.load_checkpoint`,,1
fabric: test with tbX (#16511),0.51564527,The Fabric.run() method is no longer abstract (#14992),,0
app: hotfix import pytest (#16510),0.6834136,Remove pytest as a requirement to run app by @manskx in https://github.com/Lightning-AI/lightning/pull/15449,,0
relax packaging versions (#16508),0.69526875,Parsed local package versions (#13933),,0
Rename LiteMultiNode to FabricMultiNode (#16505),0.6534374,Renamed lightning.app.components.LiteMultiNode to lightning.app.components.FabricMultiNode (#16505),,0
ci: move Flagships to GH (#16420),0.41152215,moves sync bn to each backend (#3925),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Multi-node documentation for Fabric (#16495),0.638486,:open_book: Go to Fabric documentation :open_book:,Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
Fix strict torch_xla availability check (#16476),0.6804149,    * Some configuration errors that were previously raised as `MisconfigurationException`s will now be raised as `ProcessRaisedException` (torch>=1.8) or as `Exception` (torch<1.8),Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz,0
Docs: Update BYOC IAM Policy Permissions (#16474),0.45496234,Add support for Lightning AI BYOC cluster management (#13835),Added codebuild permissions to IAM policy JSON,0
Update starlette requirement from <=0.22.0 to <0.24.0 in /requirements (#16471),0.48320013,Pinning starsessions to 1.x (#14333),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Remove old platform docs (#16499),0.4886799,| API                                                                                                                      | Removal version | Alternative                                     |,  Remove old platform docs   More   More ,0
Fix minor typos in sharing_components.rst (#16468),0.4109845,Pkg: fix parsing versions by @Borda in https://github.com/Lightning-AI/lightning/pull/15401,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Docs for Trainer init method need double quotes added (#16498),0.698603,"  * Removed class methods from Trainer: `default_attributes()`, `from_argparse_args()`, `parse_argparser()`, `match_env_arguments()`, `add_argparse_args()`",,0
"Remove the ""native"" suffix from the codebase (#16490)",0.65883815,"- ""Native"" suffix removal ([#16490](https://github.com/Lightning-AI/lightning/pull/16490))",,0
[App] Add interruptible work (#16399),0.48824692,Resolved a bug where the work statuses will grow quickly and be duplicated (#13970),Co-authored-by: thomas thomas@thomass-MacBook-Pro.local Co-authored-by: Your Name you@example.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
update copyright in PL & Fabric (#16481),0.4404487,  * Changed the method signatrue of `Fabric.save` and `Fabric.load`,update copy write in PL & Fabric,0
adding license,0.24703963,2. Create the Tuner,,0
Update labeleler config (#16491),0.36402455,Porting fixes to autoscaler component (#16249),,0
"Update traitlets requirement from <=5.4.0,>=5.3.0 to >=5.3.0,<5.9.0 in /requirements (#16470)",0.4289688,Changed ModelCheckpoint version suffixes to start at 1 (#5008),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Grammar corrections for Fabric docs (#16494),0.55910474,Syntax changes are: ,,0
Enable more shorthand strategy names in the Fabric CLI (#16485),0.734797,- Enabled all shorthand strategy names that can be supported in the CLI ([#16485](https://github.com/Lightning-AI/lightning/pull/16485)),,1
docs: move assets_lightning to pl-public-data (#16419),0.5044093,  * `pl.utilities.data` ([#16440](https://github.com/Lightning-AI/lightning/pull/16440)),,0
Add Fabric.all_reduce (#16459),0.79854727,- Added `Fabric.all_reduce` ([#16459](https://github.com/Lightning-AI/lightning/pull/16459)),,1
Fabric checkpointing 2/n: DeepSpeed implementation (#16452),0.6509464,- Added support for saving and loading DeepSpeed checkpoints through `Fabric.save/load()` ([#16452](https://github.com/Lightning-AI/lightning/pull/16452)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
[App] Add lightning open command (#16482),0.6215396,CLI Commands for Lightning Apps,,0
[App] Wrap LightningClient methods with retry wrapper by default (#16382),0.7471932,Changed the default LightningClient(retry=False) to retry=True (#16382),Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: thomas thomas@thomass-MacBook-Pro.local,1
Mark the loop classes as protected (#16445),0.6703403,  * The loop classes are now marked as protected ([#16445](https://github.com/Lightning-AI/lightning/pull/16445)),,0
Add support for async method and remove context PythonServer (#16453),0.70199543,Add support for async predict method in PythonServer and remove torch context (#16453),Co-authored-by: thomas thomas@thomass-MacBook-Pro.local,1
[App] Resolved root_folder not parsed properly (#16454),0.5426769,"Resolved LightningApp(..., debug=True) (#14464)",Co-authored-by: thomas thomas@thomass-MacBook-Pro.local,0
Remove the FairScale integration (#16400),0.5367166,  * Removed the `pytorch_lightning.overrides.fairscale.LightningShardedDataParallel` class,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Two fixes for handling edge cases in MLflow logging (#16451),0.5994247,Un-balanced logging properly supported (#5119),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Update Fabric docs based on user feedback (#16460),0.5983613,Learn more about Fabric and what it can do in the new docs!,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
[App] Refactor cloud dispatch and update to new API (#16456),0.9495796,Refactor cloud dispatch and update to new API (#16456),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Test DeepSpeed in Fabric CI (#16458),0.6401931,- Added support for saving and loading DeepSpeed checkpoints through `Fabric.save/load()` ([#16452](https://github.com/Lightning-AI/lightning/pull/16452)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Test that connector defaults match the ones in Trainer/Fabric (#16463),0.6496724,Changed Trainer connectors to be protected attributes:,,0
"Update numpy requirement from <1.24.1,>=1.17.2 to >=1.17.2,<1.24.2 in /requirements (#16473)",0.50393313,Dropped official support/testing for older PyTorch versions <1.3 (#1917),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
"Update pandas requirement from <1.5.2,>1.0 to >1.0,<1.5.4 in /requirements (#16472)",0.628488,Removed dependency on pandas (#736),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
"LightningFabric: Error handling for accelerator=""mps"" and ddp strategy pairing (#16455)",0.6155155,- Added a friendly error message when attempting to use `DeepSpeedStrategy` on unsupported accelerators ([#12699](https://github.com/Lightning-AI/lightning/pull/12699)),Co-authored-by: awaelchli aedu.waelchli@gmail.com,0
fixing typos reported by community user (#16457),0.5183412,"In addition, we fixed:",,0
Update link to new forum (#16449),0.45188725,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",,0
Solved minor bug with MLFlow logger (#16418),0.7315855,MLFlowLogger now accepts run_name as an constructor argument (#7622),Resolves https://github.com/Lightning-AI/lightning/issues/16411,1
Remove the deprecated code in pl.utilities.data (#16440),0.62506485,Changed deprecated enable_pl_optimizer=True (#5244),,0
Fabric checkpointing 1/n: base implementation (#16434),0.6654601,  * Changed the method signature of `Strategy.save_checkpoint` and `Fabric.load_checkpoint`,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Loop flattening: reduce base interface (#16429),0.47368535,Refactored Loops,  Loop flattening: remove the default .run() implementation   None return   mypy   Loop flattening: reduce base interface   Fix   DOcs   Bad merge   Fix   Fix ,0
Remove the deprecated code in pl.utilities.optimizer (#16439),0.82506967,Changed deprecated enable_pl_optimizer=True (#5244),,1
Remove the deprecated code in pl.utilities.cloud_io (#16438),0.6711879,  * `pl.utilities.cloud_io` ([#16438](https://github.com/Lightning-AI/lightning/pull/16438)),,0
Remove the deprecated Accelerator.setup_environment method (#16436),0.71404046,- Removed the deprecated `Accelerator.setup_environment` method ([#16436](https://github.com/Lightning-AI/lightning/pull/16436)),,1
Remove the deprecated pl.strategies.utils.on_colab_kaggle function (#16437),0.8275976,- Removed the deprecated `pl.strategies.utils.on_colab_kaggle` function ([#16437](https://github.com/Lightning-AI/lightning/pull/16437)),,1
Remove the deprecated code in pl.utilities.seed (#16422),0.6560794,Changed deprecated enable_pl_optimizer=True (#5244),,0
Remove the deprecated code in pl.core.mixins (#16424),0.6645042,  * `pl.core.mixins` ([#16424](https://github.com/Lightning-AI/lightning/pull/16424)),,0
Loop flattening: remove the default .run() implementation (#16427),0.5840392,  * Removed the default `Loop.run()` implementation ([#16384](https://github.com/Lightning-AI/lightning/pull/16384)),,0
Remove the HivemindStrategy (#16407),0.52296954,Remove MetricsHolder (#7909),Remove the collaborative strategy,0
Remove the deprecated code in pl.utilities.apply_func (#16413),0.64078736,Changed deprecated enable_pl_optimizer=True (#5244),,0
Remove deadlock detection / process reconciliation logic (#16204),0.47204176,Silenced some warnings. verified ddp refactors (#3483),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Remove the deprecated code in pl.utilities.device_parser (#16412),0.71430856,"device parser (#3400, #3405)",,1
Remove the deprecated code in pl.utilities.xla_device (#16404),0.75301564,xla_device_utils >> xla_device,,1
Remove support for logging multiple metrics together (#16389),0.67590296,Allow metrics logged together with hparams (#1630),,0
Remove deprecated code in pl.utilities.distributed (#16390),0.68658936,  * Removed the deprecated `pl_module` argument from the distributed module wrappers,,0
Mark forward_module as required (#16386),0.7517276,- Mark the `forward_module` argument as required ([#16386](https://github.com/Lightning-AI/lightning/pull/16386)),,1
Loop flattening: remove .connect() (#16384),0.53683895,        hiddens = hiddens.detach(),,0
Remove the deprecated tuning property and enums (#16379),0.7442633,| Enum RunningStage.TUNING                                    | 1.10             | No longer supported         |,,1
Remove the deprecated LightningCLI arguments (#16380),0.81355405,- Removed the deprecated `LightningCLI` arguments ([#16380](https://github.com/Lightning-AI/lightning/pull/16380)),,1
Remove support for PL_INTER_BATCH_PARALLELISM (#16355),0.6006568,  * Deprecated the internal `pl_multi_process` function,,0
Loop flattening: remove .replace() (#16361),0.43397725,  * Removed `Loop.replace()` ([#16361](https://github.com/Lightning-AI/lightning/pull/16361)),,0
Remove Trainer(move_metrics_to_cpu=True) (#16358),0.9200852,Removed the Trainer(move_metrics_to_cpu=True) argument (#16358),,1
Remove the deprecated pytorch_lightning.profiler module (#16359),0.8954569,Deprecated pytorch_lightning.profiler in favor of pytorch_lightning.profilers (#16059),,1
Remove the deprecated AllGatherGrad class (#16360),0.59734184,- Removed the deprecated code in:,,0
Remove special handling of loss in progress bar (#16192),0.7718341,Logging the loss to the progress bar (#16192),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Remove the redundant precision attribute from LightningModule (#16203),0.77939713,- Removed the `LightningModule.precision` attribute ([#16203](https://github.com/Lightning-AI/lightning/pull/16203)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Remove truncated backpropagation from loops (#16337),0.6351381,Truncated backpropagation through time (TBPTT) (#16172),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Remove the graveyard (#16138),0.5319931,Remove MetricsHolder (#7909),,0
Add a trainer.ckpt_path setter for stateful loading (#16187),0.82284343,trainer.ckpt_path = None,,1
Remove the deprecated auto_select_gpus Trainer argument (#16184),0.8570346,  * Removed the `Trainer(auto_select_gpus=...)` argument,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Remove the deprecated resume_from_checkpoint Trainer argument (#16167),0.84224045,1. Remove resume_from_checkpoint from the Trainer,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Remove the deprecated Trainer device arguments (#16171),0.8489784,Removed obsolete self._device in Trainer (#1849),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Remove horovod (#16150),0.6229158,Remove MetricsHolder (#7909),,0
Add amp_scaling_state (apex) migration (#16161),0.6102493,- Added migration logic to warn about checkpoints with apex AMP state ([#16161](https://github.com/Lightning-AI/lightning/pull/16161)),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Fixes https://github.com/Lightning-AI/lightning/pull/16149#discussion_r1054271661,0
Remove nivida/apex (#16149),0.6424712,- `nvidia/apex` removal ([#16149](https://github.com/Lightning-AI/lightning/pull/16149)),,0
Lite: Remove legacy code (#15953),0.58739,- Removed the deprecated code in:,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Fix protobuf incompatibility blocking CI (#16441),0.41047674,Setup: added requirement freeze for the next major version (#14480),"  [WIP] Fix protobuf incompatibility blocking CI   1.0.0 bad, try 1.9.0   1.9.0 bad, try 1.16.0   1.16.0 good, try 1.13.0 ",0
ci: replace flake8 by ruff (#16433),0.3877861,Moved TrainsLogger to Bolts (#2384),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Fix broken link (#16442),0.47276515,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",,0
feat: update pyproject.toml (#16430),0.51541567,Updated references to self.forward() to instead use the __call__ interface. (#1211),,0
Option to skip cuda rng in isolate_rng utility function (#16423),0.6519852,- Added an argument `include_cuda` in `pytorch_lightning.utilities.seed.isolate_rng` to disable managing `torch.cuda`'s rng ([#16423](https://github.com/Lightning-AI/lightning/pull/16423)),Co-authored-by: awaelchli aedu.waelchli@gmail.com,0
Disable strict loading in multiprocessing launcher (#16365),0.77882946,"Disabled strict loading in multiprocessing launcher (""ddp_spawn"", etc.) when loading weights back into the main process (#16365)",Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz,1
Distributed communication docs for Lite (#16373),0.48604208,  * `pl.utilities.distributed` ([#16390](https://github.com/Lightning-AI/lightning/pull/16390)),Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
Fix save_hyperparameters for multiple inheritance and mixins (#16369),0.6914673,Moved save_hyperparameters to its own function (#7119),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
[docs][App] Include components in the API reference (#16414),0.5966353,"In the previous release, we added shorthand notation support for registered components. In this release, we added a flag to automatically register all available components:",,0
changelog: fix links & section (#16396),0.63987285,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",,0
Revert #16401 and user proper CSVLogger (#16405),0.47708106,- Added support for writing logs remote file systems on `CSVLoggers`. ([#16880](https://github.com/Lightning-AI/lightning/pull/16880)),Co-authored-by: Jirka jirka.borovec@seznam.cz,0
adding  @lantiga as code-owner (#16394),0.47662154,"If we forgot someone due to not matching the commit email with the GitHub account, let us know :]",,0
pypi releasing fix lower (#16406),0.44722867,- Fixed security vulnerabilities CVE-2020-1747 and CVE-2020-14343 caused by the `PyYAML` dependency ([#11099](https://github.com/PyTorchLightning/pytorch-lightning/pull/11099)),,0
fix dirpath in log_dir for CSVLogger (#16401),0.65597975,- all the file path errors with loggers (txs @awaelchli),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Clarify cluster advanced docs (#16403),0.63856053,Support running on multiple clusters (#16016),,0
rename app _exit 2/2 (#16398),0.45195797,Application storage prefix moved from app_id to project_id/app_id (#14583),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
dispatch cache clean (#16393),0.47865447,Cleanup cluster waiting (#16054),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
ci: adding Muse app (#16392),0.38433018,"Changed the root directory of the app (which gets uploaded) to be the folder containing the app file, rather than any parent folder containing a .lightning file (#15654)",,0
rename flow _exit (#16378),0.52389,Renames model steps (#1051),Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
Make subprocess launcher the default in Lite (#16388),0.41871384,"Lite(accelerator=""gpu"", devices=""auto"").run()",,0
bump version 1.9.0 (#16356),0.476996,[1.7.6] - 2022-09-13,  bump version 1.9.0   17th   drop lite ,0
ci: adding jupyter component (#16391),0.4305721,Distributed support in Jupyter Notebooks,,0
"Update lightning-utilities requirement from <0.5.0,>=0.4.2 to >=0.4.2,<0.6.0 in /requirements (#16371)",0.6859393,- Integrate the `lightning_utilities` package (,Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Deprecate the FairScale integration (#16353),0.6291579,Avoided requiring the FairScale package to use precision with the fsdp native strategy (#14092),,0
Bump playwright from 1.28.0 to 1.29.1 in /requirements (#16294),0.41020247,Setup: added requirement freeze for the next major version (#14480), Bump playwright from 1.28.0 to 1.29.1 in /requirements  Bumps playwright from 1.28.0 to 1.29.1. - Release notes - Commits  updated-dependencies: - dependency-name: playwright   dependency-type: direct:production   update-type: version-update:semver-minor ... Signed-off-by: dependabot[bot] support@github.com   docker   note   Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz,0
[App] Fix e2e test race condition in waiting for openapi (#16381),0.4430879,- Removed deadlock detection / process reconciliation (`PL_RECONCILE_PROCESS=1`) ([#16204](https://github.com/Lightning-AI/lightning/pull/16204)),,0
Bump loader-utils from 1.4.0 to 1.4.2 in /src/lightning_app/cli/pl-app-template/ui (#16375),0.5813958,"- Deprecated the `pl_module` argument in `LightningParallelModule`, `LightningDistributedModule`, `LightningShardedDataParallel`, `LightningBaguaModule` and `LightningDeepSpeedModule` wrapper classes ([#13738](https://github.com/Lightning-AI/lightning/pull/13738))",Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Bump json5 from 2.2.1 to 2.2.3 in /src/lightning_app/cli/react-ui-template/ui (#16237),0.5306362,- Changes in `LightningCLI` required for the new major release of jsonargparse v4.0.0 ([#10426](https://github.com/PyTorchLightning/pytorch-lightning/pull/10426)),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Bump pytest-rerunfailures from 10.2 to 10.3 in /requirements (#15841),0.5485747,Remove pytest as a requirement to run app by @manskx in https://github.com/Lightning-AI/lightning/pull/15449,Bumps pytest-rerunfailures from 10.2 to 10.3. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: pytest-rerunfailures   dependency-type: direct:production   update-type: version-update:semver-minor ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Rename Strategy.reduce to Strategy.all_reduce in Lite (#16370),0.9049144,Renamed Strategy.reduce to Strategy.all_reduce in all strategies (#16370),,1
Update psutil requirement from <5.9.4 to <5.9.5 in /requirements (#16372),0.60477436,The psutil package is now required for CPU monitoring (#17010),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
move tensorboardX to extra (#16349),0.77639973,Switch from tensorboard to tensorboardx in TensorBoardLogger (#15728),  tensorboardX > extra   default   chlog   doctest_skip   mypy   Update docs   plus   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Luca Antiga luca.antiga@gmail.com   fix   mypy   docs   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   .   ll   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Luca Antiga luca.antiga@gmail.com,1
[App] Rename gradio.py gradio_server.py (#16201),0.624068,Changed lightning_app.components.serve.gradio to  lightning_app.components.serve.gradio_server (#16201),,0
"Update deepdiff requirement from <6.2.3,>=5.7.0 to >=5.7.0,<6.2.4 in /requirements (#16292)",0.47213054,Increased DeepDiff's verbose level to properly handle dict changes (#13960),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
hotfix build docs + docker (#16351),0.5718044,"Show a message when BuildConfig(dockerfile=""..."") is passed but a Dockerfile file is already present in the Work (#15799)",,0
ci: flagship disable pr (#16354),0.50113994,Enable None model checkpoint default (#3669),,0
update documentation with upcoming supported regions (#16331),0.5464206,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",,0
Add CSVLogger for Lightning Lite (#16346),0.7365825,- Added support for writing logs remote file systems on `CSVLoggers`. ([#16880](https://github.com/Lightning-AI/lightning/pull/16880)),Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,1
update changelog (#16348),0.64044917,Full Changelog,,0
[App] Fix env variable login (#16339),0.48170695,Changed the command lightning connect to lightning connect app for consistency (#16670),,0
ci: flagship apps 1/n (#16304),0.47453433,App,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
make cluster creation/deletion async by default (#16185),0.96515787,Made cluster creation/deletion async by default (#16185),,1
Document how to run apps with a local version of Lightning on the cloud (#16163),0.7623387,- Added support to start lightning app on cloud without needing to install dependencies locally ([#15019](https://github.com/Lightning-AI/lightning/pull/15019),Co-authored-by: Akihiro Nitta akihiro@lightning.ai,1
"CI: e2e adding queue_type: ""http"" (#16175)",0.47797945,- Added authentication to HTTP queue ([#15202](https://github.com/Lightning-AI/lightning/pull/15202)),,0
Improve requirements parser (#13912),0.46343467,The brittle argument parsing utilities (#16708),,0
rename integrations_app for accommodating flagships (#16345),0.4798293,Updated app URLs to the latest format (#16568),,0
Fix configuration validation error message in Lite CLI (#16334),0.57307494,Configuration Validator (#9779),,0
"Fix ""finished"" status code in MLFlowLogger (#16340)",0.6428112,"- The `MLFlowLogger.finalize()` now sets the status to `FAILED` when an exception occurred in `Trainer`, and sets the status to `FINISHED` on successful completion ([#12292](https://github.com/Lightning-AI/lightning/pull/12292))",Co-authored-by: awaelchli aedu.waelchli@gmail.com,0
ci: parametrize publish (#16343),0.46022615,@akihironitta @awaelchli @Borda @carmocca @kaushikb11 @krshrimali @mauvilsa @otaj @pre-commit-ci @rohitgr7 @semaphore-egg @tkonopka @wayi1,,0
Support logging for MetricCollection with compute groups (#15580),0.6258623,Metric reduction with Logging (#5150),,0
Lite Example: Model Agnostic Meta Learning (MAML) (#16333),0.42312795,Extended support for purely iteration-based training (#5726),Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
[App] Check for NOT_STARTED app status in cloud tests (#16344),0.639102,Removed implicit ui testing with testing.run_app_in_cloud in favor of headless login and app selection (#16741),,0
Update Lightning Lite docs (6/n) (#16342),0.7825477,Update the Lightning App docs (#13537),,1
Do not warn about the scheduler's interval key during manual optim (#16308),0.5709988,Disabled lr_scheduler.step() in manual optimization  (#6825),,0
"Error handling for accelerator=""mps"" and ddp strategy pairing (#16153)",0.5636476,"Raise MisconfigurationException when the accelerator is available but the user passes invalid ([]/0/""0"") values to the devices flag (#12708)",Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Nikhil Shenoy nikhilshenoy@dhcp-128-189-225-81.ubcsecure.wireless.ubc.ca Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: awaelchli aedu.waelchli@gmail.com Fixes https://github.com/Lightning-AI/lightning/issues/16148,0
Address feedback for new Lite docs (#16330),0.5484481,Docs improvements,,0
fix import removed DDPShardedStrategy (#16341),0.7032111,Merged the implementation of DDPSpawnStrategy into DDPStrategy and removed DDPSpawnStrategy (#14952),,1
ci: fix env vars (#16323),0.45942986,  * `env_prefix`,,0
Fabric: drop FairScale's sharded implementation (#16329),0.5821011,Removed support in LightningLite for FairScale's sharded training (strategy='ddp_sharded'|'ddp_sharded_spawn'). Use Fully-Sharded Data Parallel instead (strategy='fsdp') (#16329),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Fabric: Remove _Connector.is_distributed (#16327),0.57594943,The Fabric.run() method is no longer abstract (#14992),,0
Remove lightning_transformers from our docs (#16335),0.6433514,- Removed the deprecated ([#14471](https://github.com/Lightning-AI/lightning/pull/14471)),,0
Remove _StrategyType (#16328),0.5992884,Renamed Strategy.reduce to Strategy.all_reduce in all strategies (#16370),,0
Annotate Fabric.log_dict with mapping input (#16325),0.57178175,"Our logging mechanism previously supported log(""key"", {""something"": 123}) (not using log_dict). However, this added significant complexity to the implementation with little benefit, as these keys could not be monitored by our Callbacks and most logger implementations do not support this notation. If you were using this feature with a compatible logger, you can still publish data directly to the Logger using self.logger.log_metrics().",,0
Update Lightning Lite docs (5/n) (#16291),0.7898228,Update the Lightning App docs (#13537),"  organize   organize   organize   organize   Fix   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   accelerator   distributed launch   notebooks   code structure   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   lightning_module   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   x   update   conflicts   fix duplicates   links.rst   api folder   add todo for build errors   resolve duplicate reference warnings   address review by eden   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",1
Fix typo in comet.py (#16326),0.5669532,Using .comet.config file for CometLogger (#1913),,0
Add a introduction documents for using Intel Neural Compressor to conduct post-training quantization (#16085),0.4939693,"- Accelerator.{call_configure_sharded_model_hook, connect_training_type_plugin, connect_precision_plugin, on_reset_*_dataloader, on_train_epoch_end, on_save, post_optimizer_step, update_global_step}",Co-authored-by: awaelchli aedu.waelchli@gmail.com,0
Lite: Support self.log from a LightningModule (#16311),0.7852947,The coverage of self.log-ing in any LightningModule or Callback hook has been improved (#8498),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Switch v1.10 deprecation references (#16321),0.5703408,Deprecation warning (#3844),,0
Remove the deprecated pytorch_lightning.callbacks.base module (#16319),0.93296564,Removed the deprecated pytorch_lightning.callbacks.base module in favor of pytorch_lightning.callbacks.callback (#16319),,1
Rename leftover definitions in Lite tests (#16309),0.40181696,Cleaning up stale logger tests (#3490),,0
Remove the deprecated pytorch_lightning.core.lightning module (#16318),0.89800996,Removed the deprecated pytorch_lightning.core.lightning module in favor of pytorch_lightning.core.module (#16318),,1
Deprecate LightningLite (#16314),0.6531862,"Over time, LightningLite has evolved from a simple onboarding tool to a powerful toolbox enabling users to build performant and hackable trainers. In 1.9.0, we renamed it to Lightning Fabric and gave users early access to its new features. In 2.0, we are dropping LightningLite from the package completely.",,0
Remove untested NVIDIA Dali example (#16306),0.5686554,Updated DALIClassificationLoader to not use deprecated arguments (#4925),,0
rename integrations (#16312),0.43758214,Renames model steps (#1051),"  rename integrations   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   name   Artifact   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
Fixes in sphinx docs links (#16255),0.6319834,Pinned sphinx-autodoc-typehints with <v1.15 (#11400),Co-authored-by: Nikhil Shenoy nikhilshenoy@dhcp-128-189-229-137.ubcsecure.wireless.ubc.ca Co-authored-by: Nikhil Shenoy nikhilshenoy@dhcp-128-189-227-69.ubcsecure.wireless.ubc.ca Co-authored-by: Nikhil Shenoy nikhilshenoy@dhcp-128-189-228-93.ubcsecure.wireless.ubc.ca Fixes https://github.com/Lightning-AI/lightning/issues/8107,0
Logger support in Lite (#16121),0.6166994,support for latest test-tube logger optimized for PT 1.2.0.   ,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Fix-multiple-loggers-typo (#16305),0.6231987,Here are two major changes that apply when using multiple loggers in 1.8:,,0
added support for logging in different trainer stages (#16002),0.7313565,"All trainers now have a default logger, early stopping and checkpoint object. To modify the behavior, pass in your own versions of those. ",Co-authored-by: thinkin-machine you@example.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Bump hivemind from 1.1.2 to 1.1.5 in /requirements (#16293),0.5054847,"    strategy=HivemindStrategy(target_batch_size=8192), ",Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
fix package and validate folders (#16299),0.46443284,- all the file path errors with loggers (txs @awaelchli),,0
Handle set_to_none when using DeepSpeed optimizer in Lite (#16275),0.6247463,Support for manual optimization with DeepSpeed (#7970),,0
app: adding silent dependencies (#16302),0.70105135,Improved support for running apps when dependencies aren't installed (#15711),  adding silent dependencies   versions   strict   freeze   client ,1
fix legacy creation (#16282),0.50754404,Removed legacy references for magic keys in the Result object (#6016),,0
[App] Fixing race condition while setting servers to be free for next batch in the Loadbalancer (#16279),0.54908115,Resolved a bug about a race condition when sending the work state through the caller_queue (#14074),rece condition fix when setting server to be free for next request,0
Fixes app CLI tests by checking for the dynamically assigned port (#16283),0.5066792,Removed implicit ui testing with testing.run_app_in_cloud in favor of headless login and app selection (#16741),"  Fixes app CLI tests by checking for the dynamically assigned port.   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: Noha Alon nohaalon@Nohas-MacBook-Air.local Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
Update precision input type annotations (#14857),0.6623821,Precision Plugins (#5718),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Update Lightning Lite docs (4/n) (#16246),0.7878279,Update the Lightning App docs (#13537),Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com,1
Fix import error in test_pl_examples.py (#16268),0.56004417,$ PL_FAULT_TOLERANT_TRAINING=MANUAL python script.py,Fixes https://github.com/Lightning-AI/lightning/issues/16265,0
Skip horovod tests with cuda errors (#16276),0.53838176,Moved torch.cuda.set_device() to enable collective calls earlier in setup (#8312),,0
ci: pypi / fabtic (#16272),0.41466987,@pritamsoni-hsr,,0
[App] Porting fixes to autoscaler component (#16249),0.91061914,Porting fixes to autoscaler component (#16249),  autoscaler update   cold-start-proxy-updates   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Ethan Harris ethanwharris@gmail.com,1
Update Lightning Lite docs (3/n) (#16245),0.7871104,Update the Lightning App docs (#13537),,1
Reuse code in demos.BoringModel (#16242),0.42809767,import my_code.models,,0
Check broken links (#16168),0.48738903,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",Co-authored-by: Nikhil Shenoy nikhilshenoy@dhcp-128-189-225-81.ubcsecure.wireless.ubc.ca Co-authored-by: Nikhil Shenoy nikhilshenoy@dhcp-128-189-227-53.ubcsecure.wireless.ubc.ca Co-authored-by: Nikhil Shenoy nikhilshenoy@dhcp-128-189-226-199.ubcsecure.wireless.ubc.ca Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: Nikhil Shenoy nikhilshenoy@dhcp-128-189-227-171.ubcsecure.wireless.ubc.ca Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Nikhil Shenoy nikhilshenoy@dhcp-128-189-230-32.ubcsecure.wireless.ubc.ca Co-authored-by: Nikhil Shenoy nikhilshenoy@dhcp-128-189-224-226.ubcsecure.wireless.ubc.ca Co-authored-by: Nikhil Shenoy nikhilshenoy@dhcp-128-189-229-137.ubcsecure.wireless.ubc.ca Fixes https://github.com/Lightning-AI/lightning/issues/8107,0
Selects random port for lightning app when running multiple apps locally (#15819),0.5431242,Changed the command lightning connect to lightning connect app for consistency (#16670),"  Selects random port for lightning app when running multiple apps locally   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Uses already created utility function instead of creating a new one   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   changelog update   reformatting   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   fix cli tests   fixing silly imports   remove port assignement from constants file and into the CLI dispatch process   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   lint errors   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   resolves lint error: unused import   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update tests   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   tests   tests   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   tests   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  update changelog  Co-authored-by: Noha Alon nohaalon@Nohas-MacBook-Air.local Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Jirka jirka.borovec@seznam.cz",0
Lightning 1.9.0 Release Candidate 0 (#16264),0.6315078,0.4.0 is the first public release after a short period testing with public users. Thanks for all the help ironing out bugs to get Lightning to run on everything from notebooks to local to server machines.,  release   Apply suggestions from code review   cleaning   Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz,0
Skip sphinx linkcheck on CHANGELOG files (#16259),0.5317188,Pinned sphinx-autodoc-typehints with <v1.15 (#11400),Co-authored-by: Nikhil Shenoy nikhilshenoy@dhcp-128-189-229-137.ubcsecure.wireless.ubc.ca,0
Restructure Lite examples and add GAN (#16240),0.54609454,We have fixed GAN training - supporting multiple optimizers.,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
update tutorials (#16263),0.44918466,"Updated DeepSpeed ZeRO (#6546, #6752, #6142, #6321)",,0
"Update streamlit requirement from <=1.15.2,>=1.0.0 to >=1.0.0,<1.16.1 in /requirements (#16221)",0.5229167,Setup: added requirement freeze for the next major version (#14480),Update streamlit requirement in /requirements Updates the requirements on streamlit to permit the latest version. - Release notes - Commits  updated-dependencies: - dependency-name: streamlit   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
[pre-commit.ci] pre-commit suggestions (#16224),0.61237895,@akihironitta @awaelchli @Borda @carmocca @kaushikb11 @krshrimali @mauvilsa @otaj @pre-commit-ci @rohitgr7 @semaphore-egg @tkonopka @wayi1," [pre-commit.ci] pre-commit suggestions  updates: - github.com/pre-commit/pre-commit-hooks: v4.3.0 → v4.4.0 - github.com/asottile/pyupgrade: v2.34.0 → v3.3.1 - https://github.com/myint/docformatter → https://github.com/PyCQA/docformatter - github.com/PyCQA/docformatter: v1.4 → v1.5.1 - github.com/asottile/yesqa: v1.3.0 → v1.4.0 - github.com/PyCQA/isort: 5.10.1 → 5.11.4 - github.com/psf/black: 22.6.0 → 22.12.0 - github.com/executablebooks/mdformat: 0.7.14 → 0.7.16  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
Update CI to CUDA 11.7.1 (#16123),0.5552052,  * Deprecated the `pytorch_lightning.utilities.device_parser.is_cuda_available` in favor of `lightning_lite.accelerators.cuda.is_cuda_available`,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Relax on_train_batch_* hook check with dataloader_iter to a warning (#16062),0.8315166,"Overriding the on_train_batch_{start,end} hooks in conjunction with taking a dataloader_iter in the training_step no longer errors out and instead shows a warning (#16062)",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Fix app e2e race condition. Add more logs verbosity (#16251),0.4800861,Un-balanced logging properly supported (#5119),,0
Added Optional sphinx docs linkcheck (#16234),0.6336222,Pinned sphinx-autodoc-typehints with <v1.15 (#11400),Fixes https://github.com/Lightning-AI/lightning/issues/8107,0
Update Lightning Lite docs (2/n) (#16239),0.78565687,Update the Lightning App docs (#13537),Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com,1
Fix Fabric CHANGELOG (#16247),0.651713,  * Changed the method signatrue of `Fabric.save` and `Fabric.load`,,0
Update Lightning Lite docs (1/n) (#16250),0.79543877,Update the Lightning App docs (#13537),  fabric docs   fix reference   fabric ,1
Rename LightningLite to Fabric (#16244),0.79400504,"Renamed the class LightningLite to Fabric (#15932, #15938)",  Rename LightningLite to Fabric   Fix introspection test   Fix deprecated Lite tests   Undo accidental Horovod removal   Fixes ,1
Support arbitrary Optimizables as optimizers (#16189),0.7331727,Refactored optimizer (#4658),,1
Improvements to checkpoint migration (#16233),0.6692226,all the checkpoint issues should be gone now (including backward support for old checkpoints),Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,0
Update ipython[all] requirement from <8.6.1 to <8.7.1 in /requirements (#16220),0.5444515,Drop Python 3.6 support,Updates the requirements on ipython[all] to permit the latest version. - Release notes - Commits  updated-dependencies: - dependency-name: ipython[all]   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Added support and test for custom artifact names in WandbLogger (#16173),0.66819483,- Added `WandbLogger.download_artifact` and `WandbLogger.use_artifact` for managing artifacts with Weights and Biases ([#14551](https://github.com/Lightning-AI/lightning/pull/14551)),,0
Fix DDP on XLA (#16020),0.5474246,decoupled DDP2 (#3816),Co-authored-by: awaelchli aedu.waelchli@gmail.com,0
Fix type error when dividing chunk size in colossalai strategy (#16212),0.4073914,"    strategy=HivemindStrategy(target_batch_size=8192), ",Co-authored-by: awaelchli aedu.waelchli@gmail.com,0
Skip a failing Bagua test for manual optimization (#16225),0.47723782,Refactored the logic around manual and automatic optimization inside the optimizer loop (#7526),,0
Update endpoint access examples: added info about accessing auth-protected apps (#16145),0.48122075,- Added authentication to HTTP queue ([#15202](https://github.com/Lightning-AI/lightning/pull/15202)),,0
ci: upload only with release (#16194),0.45746297,Only check versions / env when not in the cloud (#15504),,0
"Update s3fs requirement from <2022.8.3,>=2022.5.0 to >=2022.5.0,<2022.11.1 in /requirements (#16198)",0.4746858,Setup: added requirement freeze for the next major version (#14480),Update s3fs requirement in /requirements Updates the requirements on s3fs to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: s3fs   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
"Update numpy requirement from <1.23.1,>=1.17.2 to >=1.17.2,<1.24.1 in /requirements (#16199)",0.5154016,Dropped official support/testing for older PyTorch versions <1.3 (#1917),Update numpy requirement in /requirements Updates the requirements on numpy to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: numpy   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Fix inclusion of model_parallel document (#16197),0.5295665,Implemented DataParallelPlugin._setup_model (#10010),fix link to gpu/advanced section,0
simplify torch.Tensor (#16190),0.62486184,"def serve_step(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:",,0
Remove CUDA_LAUNCH_BLOCKING from Lite tests (#16177),0.6291864,  * Deprecated the `pytorch_lightning.utilities.device_parser.is_cuda_available` in favor of `lightning_lite.accelerators.cuda.is_cuda_available`,,0
Using internal ip + port in a load balancer instead of URL exposed (#16119),0.77009135,The LoadBalancer now uses internal ip + port instead of URL exposed (#16119),Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
update BYOC documentation with AWS details (#16044),0.5614666,Add support for Lightning AI BYOC cluster management (#13835),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Always use the local rank zero imports (#16178),0.50209314,Wrapped imports for traceability (#13924),,0
docs: updated broken links (#16191),0.7203877,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",Co-authored-by: Shashwat  Fixes https://github.com/Lightning-AI/lightning/issues/16186,1
docs: fix order of on_fit_start() hook (#16180),0.64876974,"def on_fit_start(self, *args, **kwargs):",Fixes https://github.com/Lightning-AI/lightning/issues/16170,0
[App] Introduce basic auth to Lightning CLI (#16105),0.60272515,- Added authentication to HTTP queue ([#15202](https://github.com/Lightning-AI/lightning/pull/15202)),  Introduce basic auth to Lightning CLI for app creation   Parsing creds added   Adding auth field to app instance body   Adding tests   Adding changelog entry   Adding more tests   Update runtime.py   Setting auth on update   Fix test   Update lightning-cloud dep   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   Update runtime.py   Fix for release   Update base.txt   Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Deprecate auto_select_gpus (#16147),0.75194955,Support auto_select_gpus with the accelerator and devices API (#12608),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
update chlog after 1.8.6 (#16174),0.5510143,Here are two major changes that apply when using multiple loggers in 1.8:,,0
unify extras & minor CI cleaning: move env. var (#15942),0.3916329,"Unify sLURM/TorchElastic under backend plugin (#4578, #4580, #4581, #4582, #4583)",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Fix incorrect tuner error message (#16104),0.72917604,Changed fsspec to tuner (#4458),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz Fixes https://github.com/Lightning-AI/lightning/issues/15855,1
Update fault_tolerant_training_basic.rst (#16012),0.6494435,PL_FAULT_TOLERANT_TRAINING=1 python train.py,,0
ci: adjust version in all requirements (#16100),0.54336834,Set version as today (#13906),,0
Remove the deprecated pl.loops.base module (#16142),0.6307082,Removed the deprecated pytorch_lightning.loops.base module in favor of pytorch_lightning.loops.loop (#16142),,0
Remove deprecated items from api reference (#16151),0.7377065,Removed deprecated API (#2073),,1
Avoid using the deprecated LooseVersion (#16162),1.0,Avoid using the deprecated LooseVersion (#16162),,1
Avoid relpath bug on Windows (#16164),1.0000002,Avoid relpath bug on Windows (#16164),,1
Small fix in test_cli.py to avoid failure with future version of jsonargparse (#16156),0.6130963,- Changes in `LightningCLI` required for the new major release of jsonargparse v4.0.0 ([#10426](https://github.com/PyTorchLightning/pytorch-lightning/pull/10426)),,0
[App] Add annotations endpoint (#16159),0.4382365,Add support for listing Lightning AI apps (#13987),,0
[App] Simplify messaging in cloud dispatch (#16160),0.863708,Simplified messaging in cloud dispatch (#16160),,1
drop colossalai from testing as no stable release yet (#16122),0.4888182,"- Removed the `ColossalAIStrategy` and `ColossalAIPrecisionPlugin` in favor of the new [lightning-colossalai](https://github.com/Lightning-AI/lightning-colossalai) package ([#16757](https://github.com/Lightning-AI/lightning/pull/16757), [#16778](https://github.com/Lightning-AI/lightning/pull/16778))",,0
"Fix broken link in ""Build a Model"" section of docs (#16025)",0.5866555,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",fix build model link,0
fixing install from src package & CI release (#16027),0.58417225,"Fixing missing packaging package in dependencies, which was affecting the only installation to a very blank system.", debug install hotfix local refactor lit swap pruning,0
[App] Fix support for streamlit > 1.14 (#16139),0.48051372,This release has breaking API changes. See #124 for all details. ,,0
[App] Remove outdated warning from cloud requirements (#16140),0.52565277,Removed deprecation warnings being called for on_{task}_dataloader (#9279),,0
[App] Fix e2e tests (#16146),0.6871078,Updated app testing (#16000),,0
Deprecate the HorovodStrategy (#16141),0.7893989,horovod deprecation (#16141),,1
Remove support for LightningCLI(seed_everything_default=None) (#16137),0.9266206,Removed support for LightningCLI(seed_everything_default=None) (#16131),,1
[App] Implement ready for components (#16129),0.6452291,Implemented ready for components (#16129),,0
Remove the deprecated Trainer.reset_train_val_dataloaders (#16131),0.96241426,"Removed the deprecated Trainer.reset_train_val_dataloaders() in favor of Trainer.reset_{train,val}_dataloader (#16131)",,1
Fix endpoint information tab not showing up in AutoScaler UI (#16128),0.5141693,Porting fixes to autoscaler component (#16249),"  .   why   Revert ""why""   This reverts commit 375d3e85f442226c6990ecf0e812fff94bed2de9.   tried api access with fixed values   Revert ""tried api access with fixed values""   This reverts commit f1720f6b1ad64b0734b5a3ea7b010124cf39b5b7.   Fix typo :tada:   update chglog   revert removing lines in chlog   update chglog   Co-authored-by: Akihiro Nitta akihiro@lightning.ai",0
Deprecate nvidia/apex (#16039),0.93540406,nvidia/apex deprecation (#16039),,1
[App] Cold start proxy in autoscaler (#16094),0.4357983,Removed mode='auto' from EarlyStopping (#6167),  cold start proxy   Update src/lightning_app/components/serve/auto_scaler.py   changelog   better-doc   Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
update chlog after 1.8.5 (#16125),0.56328344,Here are two major changes that apply when using multiple loggers in 1.8:,,0
loggers: revert useless exception (#15789),0.67353374,Removed logger_connector legacy code (#6733),,0
Remove the deprecated pl.loggers.base module (#16120),0.71314144,Removed logger_connector legacy code (#6733),,1
[App] Improve PythonServer info message on startup (#15989),0.61372864,Do not override PYTHONWARNINGS (#4700), change msg update chgl show the user's class name,0
[App] Change overwrite to True (#16009),0.772624,Changed overwrite to True (#16009),Co-authored-by: thomas thomas@thomass-MacBook-Pro.local,1
[App] update app testing (#16000),0.80177635,Updated app testing (#16000),Co-authored-by: thomas thomas@thomass-MacBook-Pro.local,1
"Update scikit-learn requirement from <1.1.3,>0.22.1 to >0.22.1,<1.2.1 in /requirements (#16107)",0.6499721,Removed dependency on scikit-learn (#801),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Add support for colossalai 0.1.11 (#15888),0.5762643,"- Removed the `ColossalAIStrategy` and `ColossalAIPrecisionPlugin` in favor of the new [lightning-colossalai](https://github.com/Lightning-AI/lightning-colossalai) package ([#16757](https://github.com/Lightning-AI/lightning/pull/16757), [#16778](https://github.com/Lightning-AI/lightning/pull/16778))",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
[App] Resolve some bugs from the Training Studio scaling (#16114),0.60949445,  * Removed `Trainer(auto_scale_batch_size=...)` in favor of `Tuner(trainer).scale_batch_size()` ([#16462](https://github.com/Lightning-AI/lightning/pull/16462)),Co-authored-by: thomas thomas@thomass-MacBook-Pro.local,0
Remove the deprecated pl.utilities.cli module (#16116),0.5774369,  * Removed the deprecated `pl_module` argument from the distributed module wrappers,,0
Distributed sampling parity between Lite and PyTorch (#16101),0.6521963,Restored sampling parity between PyTorch and Fabric dataloaders when using the DistributedSampler (#16101),,0
Fix test failing on master due to bad auto-merge (#16118),0.4470318,Removed no return warning from val/test step (#6139),,0
CI: settle file names (#16098),0.5223228,Fix saved filename in ModelCheckpoint if it already exists (#4861),"  CI: settle file names   rename   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
"Update torchvision requirement from <=0.14.0,>=0.11.1 to >=0.11.1,<0.15.0 in /requirements (#16108)",0.7521764,Removed dependency on torchvision (#797),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz,1
Clarify work.stop() limitation (#16073),0.45515648,"    num_workers=10,",,0
"[App] Add status endpoint, enable ready (#16075)",0.4698709,This release has breaking API changes. See #124 for all details. ,Co-authored-by: thomas chaton thomas@grid.ai,0
Set the default work start method to spawn on MacOS (#16089),0.79192543,The default start_method for creating Work processes locally on macOS is now 'spawn' (previously 'fork') (#16089),,1
[App] Scale out/in interval for autoscaler (#16093),0.67899364,Enabled users to have more control over scaling out/in intervals (#16093),  Adding arguments for scale out/in interval   Tests ,0
[App] Min replica=0 would break autoscaler component (#16092),0.5945811,Porting fixes to autoscaler component (#16249),  fixing the bug where num_replica=0 would fail   changelog ,0
Re-enable Lite CLI on Windows + PyTorch 1.13 (#15645),0.64014477,"| Objects pytorch_lightning.utilities.cli.{OPTIMIZER,LR_SCHEDULER,MODEL,DATAMODULE,CALLBACK,LOGGER}_REGISTRY | 1.9             | Not necessary anymore                           |",Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,0
[App] Improve the autoscaler UI (#16063),0.63298756,Porting fixes to autoscaler component (#16249),[App] Improve the autoscaler UI (#16063),0
[App] Add work.delete (#16103),0.4321938,Remove MetricsHolder (#7909),Co-authored-by: thomas thomas@thomass-MacBook-Pro.local,0
Fix detection of whether app is running in cloud (#16045),0.72952706,The utility lightning.app.utilities.cloud.is_running_in_cloud now returns True during the loading of the app locally when running with --cloud (#16045),,1
[App] Add display name property to the work (#16095),0.46016058,"The WandbLogger.name property no longer returns the name of the experiment, and instead returns the project's name (#14145)",Co-authored-by: thomas thomas@thomass-MacBook-Pro.local,0
Remove redundant find_unused_parameters=False in Lite (#16026),0.7887979,            find_unused_parameters=True,,1
add 1.13.1 to adjust versions (#16099),0.6455543,Set version as today (#13906),,0
docs: add PT version (#16010),0.53192735,Set version as today (#13906),  docs: add PT version   stable   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
[App] Serve datatypes with better client code (#16018),0.42080694,LightningCloud client calls to use keyword arguments instead of positional arguments (#14685),,0
Update Multinode Warning (#16091),0.928052,Updated Multinode Warning (#16091),,1
Have checkgroup pull the latest runs (#16033),0.44619215,"Automatically reload the ""last"" checkpoint",,0
[App] PoC: Add support for Request (#16047),0.46454853,Removed deprecated API (#2073),,0
[App] Removing single quote (#16079),0.4056108,  * Removed `Loop.replace()` ([#16361](https://github.com/Lightning-AI/lightning/pull/16361)),,0
Better check for programmatic lightningignore (#16080),0.53938794,- Fixed ``LightningCLI`` signature parameter resolving for some lightning classes ([#13283](https://github.com/Lightning-AI/lightning/pull/13283)),Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
[App] Fix bug where previously deleted apps cannot be re-run from the CLI (#16082),0.5097614,Resolved a bug where the install command was not installing the latest version of an app/component by default (#14181),,0
ci: print existing candidates (#16077),0.42074817,    # use the last 4 numbers in the job id as the id,,0
minor fix: indent spaces in comment-out (#16076),0.42481607,"In addition, we fixed:",,0
Drop FairScale sharded parity tests (#16069),0.49327093,Disabled val and test shuffling (#1600),,0
Add function to remove checkpoint to allow override for extended classes (#16067),1.0,Add function to remove checkpoint to allow override for extended classes (#16067),,1
fix(cloud): detect and ignore venv (#16056),0.4961236,- Fixed a bug with a default CloudCompute for Lightning flows ([#15371](https://github.com/Lightning-AI/lightning/pull/15371)),Co-authored-by: Ethan Harris ethanwharris@gmail.com,0
Load the app before setting LIGHTNING_DISPATCHED (#16071),0.53051925,"Resolved LightningApp(..., debug=True) (#14464)",,0
[App] Hot fix: Resolve detection of python debugger (#16068),0.6011678,"Resolved LightningApp(..., debug=True) (#14464)",Co-authored-by: thomas thomas@thomass-MacBook-Pro.local Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
"Revert ""Load app before setting LIGHTNING_DISPATCHED"" (#16064)",0.6137723,"Resolved LightningApp(..., debug=True) (#14464)","Revert ""Load app before setting LIGHTNING_DISPATCHED (#16057)"" This reverts commit 8d3339a0e99ed91871e0c4f7214aca1146e95a89.",0
Remove the deprecated profiler imports (#16059),0.6721633,Moved profilers to their own file (#7822),,0
Avoid deprecated Lite import (#16058),0.7435325,There were two different ways of importing Lite in <= 1.9.0,,1
Remove the deprecated GPUAccelerator (#16050),0.54269236,- Removed deprecated `GPUStatsMonitor` callback ([#12554](https://github.com/Lightning-AI/lightning/pull/12554)),,0
Load app before setting LIGHTNING_DISPATCHED (#16057),0.49961233,"Resolved LightningApp(..., debug=True) (#14464)",,0
Add guards to cluster deletion from cli (#16053),0.7044279,Made cluster creation/deletion async by default (#16185),Adds guards to cluster deletion. - If cluster has running apps -> throw an error - If cluster has stopped apps -> confirm w/ user that apps and logs will be deleted,1
feature(cli): login flow fixes and improvements (#16052),0.45429996,Enabled self.log in callbacks (#5094),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Cleanup cluster waiting (#16054),1.0000002,Cleanup cluster waiting (#16054),,1
[App] Improve lightning connect experience (#16035),0.7525506,Introduce lightning connect (#14452),,1
[App] Support running on multiple clusters (#16016),0.84015286,Support running on multiple clusters (#16016),,1
Remove the deprecated LightningDeepSpeedModule (#16041),0.97104114,Removed the deprecated LightningDeepSpeedModule (#16041),,1
Adding hint to the logger's error messages (#16034),0.59076744,"Finally, loggers are also now configurable with shorthand:",Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Fixes https://github.com/Lightning-AI/lightning/issues/15143,0
Nightly PyTorch version is now 2.0 (#16017),0.80147797,"Whenever there is a new PyTorch version, or a new Python version, it is time to say good bye to the oldest one we support. With the introduction of PyTorch 2.0, we are dropping support for PyTorch 1.10 to continue our support window of the four latest versions: 1.11, 1.12, 1.13 and 2.0. Similarly, with Lightning 2.0 we support the latest three versions of Python: 3.8, 3.9, and 3.10 (3.11 is coming soon).",,1
Document Gradient Clipping during Manual Optimization (#16023),0.64646965,    gradient_clip_algorithm,Co-authored-by: Nikhil Shenoy nikhilshenoy@dhcp-128-189-224-163.ubcsecure.wireless.ubc.ca Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Add info message for Ampere GPUs to enable tf32 matmuls (#16037),0.49845767,"Deprecated the use of Trainer(gpus=""i"") (string) for selecting the i-th GPU; from v1.5 this will set the number of GPUs instead of the index (#6388)",,0
Remove deperecated code in pl.utilities.meta (#16038),0.54639566,Removed deprecated code in pytorch_lightning.utilities.meta (#16038),,0
Merge setup_tools into assistant (#15847),0.62347126,from setuptools import setup, Merge setup_tools and assistant Project root Fix PROJECT_ROOT  Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
"Introduce {Work,Flow}.lightningignore (#15818)",0.5215403,class Flow(L.LightningFlow):,,0
Integrate lightning_utilities==0.4.2 (#15817),0.8658361,- Integrate the `lightning_utilities` package (,Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,1
Fix DDPStrategy import in app framework after #14952  (#16029),0.7280236,Merged the implementation of DDPSpawnStrategy into DDPStrategy and removed DDPSpawnStrategy (#14952),,1
Merge DDPStrategy and DDPSpawnStrategy in Lite (#14952),0.8319756,Merged the implementation of DDPSpawnStrategy into DDPStrategy and removed DDPSpawnStrategy (#14952),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
CI: clean install & share pkg build (#15986),0.40956384,"Fixing missing packaging package in dependencies, which was affecting the only installation to a very blank system.", abstract pkg build share ci syntax Checkgroup folders whl 1st doctest  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
LightningCLI support for optimizers and schedulers via dependency injection (#15869),0.6949343,"LightningCLI now supports registries for callbacks, optimizers, learning rate schedulers, LightningModules and LightningDataModules. This greatly improves the command line experience as only the class names and arguments are required as follows:",Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Batch MLFlowLogger requests (#15915),0.7286606,MLFlowLogger now logs hyperparameters and metrics in batched API calls (#15915),Co-authored-by: Jake Schmidt jake.schmidt@utexas.edu Co-authored-by: awaelchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Use LRScheduler for torch >= 1.14 otherwise use _LRScheduler (#15768),0.73347723,        scheduler = torch.optim.lr_scheduler.OneCycleLR(,Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
app: update doctest_skip (#15997),0.5861927,Updated app testing (#16000),simple Co-authored-by: hhsecond sherin@grid.ai,0
"Update deepdiff requirement from <=5.8.1,>=5.7.0 to >=5.7.0,<6.2.3 in /requirements (#16006)",0.44935077,Increased DeepDiff's verbose level to properly handle dict changes (#13960),Update deepdiff requirement in /requirements Updates the requirements on deepdiff to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: deepdiff   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
"Update docker requirement from <=5.0.3,>=5.0.0 to >=5.0.0,<6.0.2 in /requirements (#16007)",0.48774126,Remove unnecessary intermediate layers in Dockerfiles (#5697),Update docker requirement in /requirements Updates the requirements on docker to permit the latest version. - Release notes - Commits  updated-dependencies: - dependency-name: docker   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
update changelog after 1.8.4.post (#16008),0.6092647,Full Changelog,,0
Fix pip install with no PACKAGE_NAME or editable mode (#15853),0.44858783,pip install rich,Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com,0
Fix typo in PR titles generated by github-actions bot (#16003),0.44822216,"In addition, we fixed:",,0
[App] Fix AutoScaler trying to replicate multiple works in a single machine (#15991),0.53333044,Porting fixes to autoscaler component (#16249),  dont try to replicate new works in the existing machine   update chglog   Update comment   Update src/lightning_app/components/auto_scaler.py   add test ,0
Set the logger explicitly in tests (#15815),0.7194491,Change default logger to a dedicated one (#1064),,1
CI: Add remote fetch (#16001),0.39576066,"Removed RPCPlugin and RPCSequentialPlugin. If you were successfully using these plugins, please open a GitHub discussion about your use case (#8101)",Co-authored-by: thomas thomas@thomass-MacBook-Pro.local,0
Lite: Flatten XLAStrategy (#15838),0.40412405,fix result obj DP auto reduce (#3013),,0
Dont try to update an instance that isnt running yet (#15998),0.41929814,Changed Trainer arg and functionality from reload_dataloaders_every_epoch to reload_dataloaders_every_n_epochs (#5043),,0
[App] Fix import errors for the packages installed by shebang (#15996),0.53993225,Parse all lines in app file looking for shebangs to run commands (#15714),"  import fix   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   import fix   move req   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Update requirements/app/base.txt   revert   skip doctest   import fix   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz",0
CI: hotfix signal if (#15995),0.44242096,"Removed deprecated ModelCheckpoint arguments prefix, mode=""auto"" (#6162)",signal if,0
[App] Install exact version whn upgrading and not when testing (#15984),0.6031493,Resolved a bug where the install command was not installing the latest version of an app/component by default (#14181), [App] Install exact version whn upgrading and not when testing Update CHANGELOG.md  Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
Document running dev lightning on the cloud (#15962),0.5696002,Resolved Lightning App with remote storage (#17426),  document running dev lightning on the cloud   document running dev lightning on the cloud   Update .github/CONTRIBUTING.md   Co-authored-by: Noha Alon nohalon@gmail.com   document running dev lightning on the cloud   git clone & pip install -e   Update .github/CONTRIBUTING.md   Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: Noha Alon nohalon@gmail.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
Fix cloudcomputes registration for structures (#15964),0.5644033,"adding compute environments (#3837, [#3842)", fix cloudcomputes updates cloudcompute registration changelog,0
ci: update signaling (#15981),0.49850285,@akihironitta @awaelchli @Borda @carmocca @kaushikb11 @krshrimali @mauvilsa @otaj @pre-commit-ci @rohitgr7 @semaphore-egg @tkonopka @wayi1, ci: update signaling config,0
Fix multinode cloud component (#15965),0.6200402,Updated Multinode Warning (#16091), fix multinode cloud component add tests,0
Fix action_name usage in XLAProfiler (#15886),0.589121,  * Deprecated the internal `XLADeviceUtils.xla_available` staticmethod, Fix action_name usage in XLAProfiler add changelog Update src/pytorch_ligh Update xla.py  Co-authored-by: awaelchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
Avoid using the same port number for autoscaler works (#15966),0.5321084,Porting fixes to autoscaler component (#16249), dont hardcode port in python server add another chglog,0
App: Move AutoScaler dependency to extra requirements (#15971),0.6503315,Porting fixes to autoscaler component (#16249), Make autoscaler dependency optional update chglog dont directly import aiohttp,0
[App] Resolve run installation (#15974),0.53943086,Resolved a bug where the install command was not installing the latest version of an app/component by default (#14181),,0
update changelog (#15975),0.6266197,Full Changelog,,0
"Apply dynamo to training_step, validation_step, test_step, predict_step (#15957)",0.58364004,"Changed setup() and teardown() stage argument to take any of {fit,validate,test,predict} (#6386)","  Apply dynamo to training_step, validation_step, test_step, predict_step   Add entry to CHANGELOG.md ",0
[App] Enable running an app from the Gallery (#15941),0.50598437,This is how you enable it:,Co-authored-by: thomas thomas@thomass-MacBook-Pro.local Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Ethan Harris ethanwharris@gmail.com Co-authored-by: Jirka jirka.borovec@seznam.cz,0
Fix typo in definition of world size in docs (#15954),0.39970672,"In addition, we fixed:",,0
Fix ImportErrors on Multinode if package not present (#15963),0.57562125,Add missing python-multipart dependency (#17244),,0
refactor: simplify Tensor import (#15959),0.5997779,Auto convert tensors to contiguous format when gather_all (#4907),,0
Make LightningModule torch.jit.script-able again (#15947),0.7777062,- Fixed torchscript error with containers of LightningModules ([#14904](https://github.com/Lightning-AI/lightning/pull/14904)), Make LightningModule torch.jit.script-able again remove skip  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
[App] Add automatic conversion to structures (#15961),0.40085772,"In the previous release, we added shorthand notation support for registered components. In this release, we added a flag to automatically register all available components:",,0
[App] Improve debug triggering (#15951),0.5674107,Updated app testing (#16000),,0
[App] Improve pdb for multiprocessing (#15950),0.6551368,  * Deprecated the internal `pl_multi_process` function,Co-authored-by: thomas thomas@thomass-MacBook-Pro.local Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Fix restarting attribute for lr finder (#15620),0.5533982,move lr_finder (#3434),,0
[App] Temporarily disable ready (#15958),0.5020602,Removed mode='auto' from EarlyStopping (#6167),,0
Lite: Fix DataLoader shuffling when using DistributedSampler (#15931),0.6426828,Restored sampling parity between PyTorch and Fabric dataloaders when using the DistributedSampler (#16101),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
[App] Resolve PythonServer on M1 (#15949),0.47980314,- Removed support for Python 3.7 ([#16579](https://github.com/Lightning-AI/lightning/pull/16579)),Co-authored-by: thomas thomas@thomass-MacBook-Pro.local Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Multinode on MPS (#15748),0.6533013,Updated Multinode Warning (#16091), Fix restarting attribute for lr finder update lite executor update trainer executor update spawn executor add multinode component tests add testing helpers add lite tests add trainer tests update changelog update trainer update workflow update tests debug add reason for skipif Apply suggestions from code review switch skipif  Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
Upgrade to HPU release 1.7.1 (#15956),0.50961053,Moveed HPU broadcast override to the HPU strategy file (#17011), Upgrade to HPU release 1.7.1 Update torch version check for hpu  Signed-off-by: Jerome janand@habana.ai Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
[App] Multiprocessing-safe work pickling (#15836),0.7424417,Utility for pickling work object safely even from a child process (#15836),,1
Don't try to aggregate requirements/__pycache__/base.txt in setuptools (#15775),0.49348092,from setuptools import setup,Exlucde pycache in setuptools,0
Make gradients available for all_gather on TPU (#15003),0.5122628,Auto convert tensors to contiguous format when gather_all (#4907), Make gradients available for all_gather on TPU Modify switch and tests Apply suggestions from code review Modify tests Fix test Drop test  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
[App] Add configure_layout method for works (#15926),0.42084098,"From now on, the backend has to be set in the code (#14693):", Add configure_layout method for works Check for api access availability Updates from review Update CHANGELOG.md Apply suggestions from code review  Co-authored-by: Sherin Thomas sherin@lightning.ai,0
Bump playwright from 1.27.1 to 1.28.0 in /requirements (#15903),0.40446192,Setup: added requirement freeze for the next major version (#14480), Bump playwright from 1.27.1 to 1.28.0 in /requirements  Bumps playwright from 1.27.1 to 1.28.0. - Release notes - Commits  updated-dependencies: - dependency-name: playwright   dependency-type: direct:production   update-type: version-update:semver-minor ... Signed-off-by: dependabot[bot] support@github.com  1.28  Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz,0
[docs] Include all components in the API reference (#15805),0.56910574,"In the previous release, we added shorthand notation support for registered components. In this release, we added a flag to automatically register all available components:", Update docs  Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
[App] Wait for full file to be transferred in Path / Payload (#15934),0.8767876,Wait for full file to be transferred in Path / Payload (#15934), Wait for full file to be transferred in Path / Payload Fixes,1
[App] Fix bug when using structures with works (#15911),0.4106529,Resolved a bug about a race condition when sending the work state through the caller_queue (#14074), Fix bug when using structures with works Add test Update CHANGELOG.md,0
[App] Remove SingleProcessRuntime (#15933),0.85990256,Removed the SingleProcessRuntime (#15933), Remove SingleProcessRuntime Remove unused queues Docs,1
CI: fix pypi flow (#15944),0.45371345,Renamed and moved core/step_result.py to trainer/connectors/logger_connector/result.py (#7736), CI: fixing pypi syntax (#15943) connect input,0
Fix LRScheduler import for PyTorch 2.0 (#15940),0.7008085,- Deprecated `Trainer.lr_schedulers` in favor of `Trainer.lr_scheduler_configs` which returns a list of dataclasses instead of dictionaries ([#11443](https://github.com/PyTorchLightning/pytorch-lightning/pull/11443)), Fix LRScheduler import for PyTorch 2.0 Add comment for posterity,1
ENG-627: Docs for CloudCompute Mount Argument (#15182),0.48389512,ls: List files from your Cloud Platform Filesystem,fixed conflicts,0
[App] Introduce auto scaler (#15769),0.57844275,Enabled users to have more control over scaling out/in intervals (#16093),"  Exlucde pycache in setuptools   Add load balancer example   wip   Update example   rename   remove prints   _LoadBalancer -> LoadBalancer   AutoScaler(work)   change var name   remove locust   Update docs   include autoscaler in api ref   docs typo   docs typo   docs typo   docs typo   remove unused loadtest   remove unused device_type   clean up   clean up   clean up   Add docstring   type   env vars to args   expose an API for users to override to customise autoscaling logic   update example   comment   udpate var name   fix scale mechanism and clean up   Update exampl   ignore mypy   Add test file   .   update impl and update tests   Update changlog   .   revert docs   update test   update state to keep calling 'flow.run()'   Co-authored-by: Aniket Maurya theaniketmaurya@gmail.com   Add aiohttp to base requirements   Update docs   Co-authored-by: Luca Antiga luca.antiga@gmail.com   Use deserializer utility   fake trigger   wip: protect /system/* with basic auth   read password at runtime   Change env var name   import torch as optional   Don't overcreate works   simplify imports   Update example   aiohttp   Add work_args work_kwargs   More docs   remove FIXME   Apply Jirka's suggestions   Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci   clean example device   add comment on init threshold value   bad merge   nit: logging format   {in,out}put_schema -> {in,out}put_type   lowercase   docs on seconds   process_time -> processing_time   Dont modify work state from flow   Update tests   worker_url -> endpoint   fix exampl   Fix default scale logic   Fix default scale logic   Fix num_pending_works   Update num_pending_works   Fix bug creating too many works   Remove up/downscale_threshold args   Update example   Add typing   Fix example in docstring   Fix default scale logic   Update src/lightning_app/components/auto_scaler.py   Co-authored-by: Noha Alon nohalon@gmail.com  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci   rename method   rename locvar   Add todo   docs ci   docs ci   asdfafsdasdf pls docs   Apply suggestions from code review   Co-authored-by: Ethan Harris ethanwharris@gmail.com  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci   .   doc   Update src/lightning_app/components/auto_scaler.py   Co-authored-by: Noha Alon nohalon@gmail.com  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci  Revert ""[pre-commit.ci] auto fixes from pre-commit.com hooks""  This reverts commit 24983a0a5ab915fad3456690499a8ea4157e58f0.  Revert ""Update src/lightning_app/components/auto_scaler.py""  This reverts commit 56ea78b45f3a2d5e28b622cfc2240d95a906ac6d.   Remove redefinition   Remove load balancer run blocker   raise RuntimeError   remove has_sent   lower the default timeout_batching from 10 to 1   remove debug   update the default timeout_batching   .   tighten condition   fix endpoint   typo in runtimeerror cond   async lock update severs   add a test   {in,out}put_type typing   Update examples/app_server_with_auto_scaler/app.py   Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com  Update .actions/setup_tools.py  Co-authored-by: Aniket Maurya theaniketmaurya@gmail.com Co-authored-by: Luca Antiga luca.antiga@gmail.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Noha Alon nohalon@gmail.com Co-authored-by: Ethan Harris ethanwharris@gmail.com Co-authored-by: Akihiro Nitta aki@pop-os.localdomain Co-authored-by: thomas chaton thomas@grid.ai",0
Enable back inference mode support with hpu & update links (#15918),0.5291147,Moveed HPU broadcast override to the HPU strategy file (#17011), Enable back inference mode support with hpu Remove unused Update document link and address comment  Signed-off-by: Jerome janand@habana.ai Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Fix compiler support test (#15927),0.5350507,"Removed experimental fault-tolerance support (#16516, #16533)",,0
[App] Enable running with spawn context (#15923),0.54531336,The default start_method for creating Work processes locally on macOS is now 'spawn' (previously 'fork') (#16089),,0
Simplify enabling CPU offload in FSDP (#15832),0.70153946," * `strategy=""fsdp_native_full_shard_offload""` is now `strategy=""fsdp_cpu_offload""`",Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,1
[App] Add ready property to the flow (#15921),0.5612088,Improved the error message when the root LightningFlow passed to LightningApp is missing the run method (#14760),,0
CI: parameterize TPU tests (#15876),0.46756387,Increased TPU check timeout from 20s to 100s (#5598), update param Apply suggestions from code review,0
Activation checkpointing in FSDP without boilerplate (#15826),0.59365195,Native FSDP implementation, initial input type checkpointing fsdp in pl all_close  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Direct support for compiled models (#15922),1.0,Direct support for compiled models (#15922),  Direct support for compiled models   Update test   Update src/pytorch_lightning/core/module.py   Co-authored-by: Ethan Harris ethanwharris@gmail.com Co-authored-by: Ethan Harris ethanwharris@gmail.com,1
replace MD template with Yaml - docs (#15909),0.5146437,Replace mata_tags.csv with hparams.yaml (#1271),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
replace MD template with Yaml - feature (#15908),0.48482886,Replace mata_tags.csv with hparams.yaml (#1271),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
replace MD template with Yaml - refactor (#15907),0.4885146,Replace mata_tags.csv with hparams.yaml (#1271),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Update setuptools requirement from <65.6.0 to <65.7.0 in /requirements (#15902),0.6818011,Setup: added requirement freeze for the next major version (#14480),Updates the requirements on setuptools to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: setuptools   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
CI: merge two install steps (#15776),0.43490806,Resolved a bug where the install command was not installing the latest version of an app/component by default (#14181),,0
[App] Fix hanging CI (#15913),0.4836168,Cleanup cluster waiting (#16054),,0
[App] Support for headless apps (#15875),0.51999307,Improved support for running apps when dependencies aren't installed (#15711),"  Add is_headless when dispatching in the cloud   Bump cloud version   Add tests   Dont open app page for headless apps locally   Refactor   Update CHANGELOG.md   Support dynamic UIs at runtime   Comments   Fix   Updates   Fixes and cleanup   Fix tests   Dont open view page for headless apps   Fix test, resolve URL the right way   Remove launch   Clean   Cleanup tests   Fixes   Updates   Add test   Increase app cloud tests timeout   Increase timeout   Wait for running   Revert timeouts   Clean   Dont update if it hasnt changed   Increase timeout ",0
Add CLI Command to Delete Lightning App (#15783),0.70279527,CLI Commands for Lightning Apps,"  initial work on deleting apps   after PR review   delete CLI working   restructred to make tests easier   revert manifest changes   added changelog, fix mypy issue   updates   Update src/lightning_app/cli/cmd_apps.py   Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com  Update src/lightning_app/cli/lightning_cli_delete.py  Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com  Update src/lightning_app/cli/lightning_cli_delete.py  Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com  Update src/lightning_app/cli/lightning_cli_delete.py  Co-authored-by: Sherin Thomas sherin@lightning.ai  Update src/lightning_app/cli/lightning_cli_delete.py  Co-authored-by: Sherin Thomas sherin@lightning.ai   import typing   adding tests   finished adding tests   addressed code review comments   fix mypy error   make mypy happy   make mypy happy   make mypy happy   make mypy happy   fix windows cli   Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: Sherin Thomas sherin@lightning.ai",1
update bug report template (#15905),0.4993605,Fixing critical bugs in newly added hooks and hparams assignment.,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
waiting on feedback (#15893),0.43271828,Updated app testing (#16000), waiting builds,0
[CLI] drop name column from cluster list (#15721),0.8235938,Dropped name column from cluster list (#15721),  drop name column from cluster list   change create cluster to accept id as well   rename validator   remove cluster name from logs   fix merge with master   more merge with master issues ,1
CI: update signalling (#15887),0.48529404,Doing this minor release because correct validation metrics logging is critical.,,0
unblock legacy checkpoints (#15798),0.7225952,all the checkpoint issues should be gone now (including backward support for old checkpoints), fixing legacy checkpoints Apply suggestions from code review  Co-authored-by: Akihiro Nitta nitta@akihironitta.com,1
CI: prune dependency for benchmarks (#15879),0.48320806,Pruned requirements duplicity (#13739), prune dependency for benchmarks drop,0
Remove checks for torch greater than 1.10 (#15846),0.6262814,    * Some configuration errors that were previously raised as `MisconfigurationException`s will now be raised as `ProcessRaisedException` (torch>=1.8) or as `Exception` (torch<1.8),,0
CI: signal lai build (#15871),0.44120434,Release LAI docs as stable (#14250),,0
[App] Fixing Sigterm Handler causing thread lock which caused KeyboardInterrupt to hang (#15881),0.50408614,Deprecated on_keyboard_interrupt callback hook in favor of new on_exception hook (#9260),  terminating only once   changelog ,0
Moving lightning_api_access out of base requirements (#15844),0.53304964,Why is Lightning deprecating APIs in every release?,"  moving the requirements to components extras   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   component requirements to devel   importing torch in local scope   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  skipping doctest  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com",0
[App] Raise error when launching app on multiple clusters (#15484),0.77100015,- Fixed a bug when launching an app on multiple clusters ([#15226](https://github.com/Lightning-AI/lightning/pull/15226)),"  Error when running on multiple clusters   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Revert this in separate PR: keep this focused   Improve testing   fixup! Improve testing   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   pass flake8   Update changelog   Address PR feedback   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   remove unused import   Reword error message   Error if running on cluster that doesn't exist   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   fixup! Error if running on cluster that doesn't exist   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  Remove unsued import  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com",1
CI: freeze docs requirements [hotfix] (#15865),0.56665045,Setup: added requirement freeze for the next major version (#14480),freeze ipy,0
[App] Improve cluster creation / deletion experience (#15458),0.7255737,Made cluster creation/deletion async by default (#16185),"Cluster creation and deletion can take a long time. Instead of having these long running operations happen in the background, they should happen in the foreground. The advantage is that failures are brought to the users attention immediately, instead of the next time they decide to run lightning list clusters. While the CLI waits for the cluster to run / delete, it will display cluster status changes to the user. This PR also hides the --enable-performance and --edit-before-creation creation flags, as well as the --force deletion flag. They are either not frequently used (performance mode is expensive), or prone to misuse. Co-authored-by: Neven Miculinic neven.miculinic@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Raphael Randschau nicolai86@users.noreply.github.com",1
update changelog 1.8.3 (#15845),0.5986318,This section outlines notable changes that are not backward compatible with previous versions. The full list of changes and removals can be found in the Full Changelog below.,changelog,0
hotfix import torch (#15849),0.8252387,import torch,"  fix import torch   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   plugin   fix   skip   patch require   seed   warn   .   ..   skip True   0.0.3   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",1
"Update cloudpickle requirement from <=2.1.0,>=1.3 to >=1.3,<2.3.0 in /requirements (#15840)",0.48112503,Pkg: fix parsing versions by @Borda in https://github.com/Lightning-AI/lightning/pull/15401,Update cloudpickle requirement in /requirements Updates the requirements on cloudpickle to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: cloudpickle   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Bump hivemind from 1.0.1 to 1.1.2 in /requirements (#15839),0.52255905,"    strategy=HivemindStrategy(target_batch_size=8192), ",Bumps hivemind from 1.0.1 to 1.1.2. - Release notes - Commits  updated-dependencies: - dependency-name: hivemind   dependency-type: direct:production   update-type: version-update:semver-minor ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Bump google-github-actions/get-gke-credentials from 0 to 1 (#15843),0.4369453,lightning run app app.py --cloud --secret API_TOKEN=github_api_token,Bumps google-github-actions/get-gke-credentials from 0 to 1. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: google-github-actions/get-gke-credentials   dependency-type: direct:production   update-type: version-update:semver-major ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
"Update fairscale requirement from <=0.4.6,>=0.4.5 to >=0.4.5,<0.4.13 in /requirements (#15842)",0.55398387,"    strategy=""fsdp_native"",  # or strategy=""fsdp"" for fairscale",Update fairscale requirement in /requirements Updates the requirements on fairscale to permit the latest version.  updated-dependencies: - dependency-name: fairscale   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
[CLI] fix ssh listing stopped components (#15810),0.43169266,lightning add ssh-key CLI command has been transitioned to lightning create ssh-key, [CLI] fix ssh listing stopped components update CHANGELOG,0
Fix device placement when setting up FSDP model in Lite (#15822),0.6129906,Native FSDP replaces Fairscale FSDP (#16400), fix debug test simplify  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Deprecate description and env parameters in LightningCLI.init (#15651),0.86128265,"Deprecated description, env_prefix and env_parse parameters in LightningCLI.__init__ in favour of giving them through parser_kwargs (#15651)",Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: awaelchli aedu.waelchli@gmail.com,1
Add warning comment to cloud requirements (#15790),0.46790323,Show a message when BuildConfig(requirements=[...]) is passed but a requirements.txt file is already present in the Work (#15799),,0
Do not modify MANIFEST.in on install (#15646),0.46139818,Resolved a bug where the install command was not installing the latest version of an app/component by default (#14181),,0
Added note about custom base images (#14125),0.38923043,    # 2: Convert the image into a PIL Image to bytes and encode it with base64,Co-authored-by: awaelchli aedu.waelchli@gmail.com,0
Print the e2e app ID as early as possible (#15821),0.43947852,... or the device IDs,,0
[App] Resolve a condition bug with spawning (#15812),0.5261448,- Renamed `strategy='tpu_spawn'` to `strategy='xla'` and `strategy='tpu_spawn_debug'` to `strategy='xla_debug'` ([#16781](https://github.com/Lightning-AI/lightning/pull/16781)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Checkgroup config fixes (#15787),0.5138788,Configuration Validator (#9779),,0
Fix typo in 1_bug_report.yaml (#15764),0.533043,"In addition, we fixed:",Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Update flake8 version (#15816),0.42837638,Pkg: fix parsing versions by @Borda in https://github.com/Lightning-AI/lightning/pull/15401,,0
"Warn when self.log(..., logger=True) is called without a logger (#15814)",0.68003947,    logger.do_something(),,0
[App] Enable Python Server and Gradio Serve to run on accelerated device such as GPU CUDA / MPS (#15813),0.52745336,Changed lightning_app.components.serve.gradio to  lightning_app.components.serve.gradio_server (#16201),,0
Cleaner datadir management for some tests (#15791),0.44830868,Cleaning up stale logger tests (#3490),,0
[App] Add utility to get install command for package extras (#15809),0.48076338,Resolved a bug where the install command was not installing the latest version of an app/component by default (#14181),,0
lit extras (#15793),0.38976806,Lite,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Ignore num_nodes when running MultiNode components locally (#15806),0.8303236,The MultiNode components now warn the user when running with num_nodes > 1 locally (#15806),,1
Update lightning-utilities requirement from ==0.3. to ==0.4. in /requirements (#15420),0.66349524,Pkg: fix parsing versions by @Borda in https://github.com/Lightning-AI/lightning/pull/15401,Update lightning-utilities requirement in /requirements Updates the requirements on lightning-utilities to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: lightning-utilities   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Minor LightningLite clean up (#15780),0.6604979,LightningLite:,,0
[App] Add CloudMultiProcessBackend to run an children App within the Flow in the cloud (#15800),0.63148916,- Added support for configuring flow cloud compute ([#14831](https://github.com/Lightning-AI/lightning/pull/14831)),  update   update   update   update   update   update   update   update   update   update   update   update   update   update   updte   update   update   update   update   update   update   update   update   update   update   update   Update src/lightning_app/CHANGELOG.md   Co-authored-by: Ethan Harris ethanwharris@gmail.com  Update src/lightning_app/utilities/port.py  Co-authored-by: Ethan Harris ethanwharris@gmail.com  Update src/lightning_app/utilities/port.py  Co-authored-by: Ethan Harris ethanwharris@gmail.com  Update src/lightning_app/utilities/port.py  Co-authored-by: Ethan Harris ethanwharris@gmail.com  Update src/lightning_app/utilities/port.py  Co-authored-by: Ethan Harris ethanwharris@gmail.com  Update src/lightning_app/utilities/port.py  Co-authored-by: Ethan Harris ethanwharris@gmail.com  Update src/lightning_app/utilities/port.py  Co-authored-by: Ethan Harris ethanwharris@gmail.com Co-authored-by: Ethan Harris ethanwharris@gmail.com,0
Add code_dir argument to tracer run (#15771),1.0000002,Add code_dir argument to tracer run (#15771),,1
Notify the user of ignored requirements (#15799),0.53033185,Show a message when BuildConfig(requirements=[...]) is passed but a requirements.txt file is already present in the Work (#15799),,0
[App] Stop App when it has succeeded (#15801),0.45490938,App,,0
"Remove pytorch_lightning.profiler.{AbstractProfiler,BaseProfiler} deprecated since v1.6 (#15637)",0.94257945,Removed deprecated pytorch_lightning.profiler.base.AbstractProfiler in favor of pytorch_lightning.profilers.profiler.Profiler (#15637),"  remove deprecated base profilers   Update changelog   remove import statement   rip   correct deprecation version   update changelog   Mark buried classes as private   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci  Add typing  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",1
Lazy import tensorboard (#15762),0.702142,Move tensorboardX to extra dependencies. Use the CSVLogger by default (#16349),,1
tests: split examples and pytests (#15774),0.45422462,python script.py test,split examples and pytests,0
feature: add _generate_works_json method (#15767),0.5268156,This release has breaking API changes. See #124 for all details. ,,0
Fix App Docs for lightning ssh-keys command (#15773),0.7338426,lightning add ssh-key CLI command has been transitioned to lightning create ssh-key,fixed ssh-keys docs,1
Tests/App: refactor examples - structure (#15770),0.5233881,Updated app testing (#16000),  rename _examples dir   refactor   clean   path   add inits   skip   e2e   azure   e2e   rev   unify single depth for ignore docs req.   group ,0
Deduplicate top level lighting CLI command groups (#15761),0.9951945,Deduplicate top-level lighting CLI command groups (#15761), unify remove and delete command groups & the add and delete command groups added changelog fix tests Apply suggestions from code review  Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,1
lightning delete cluster CLI command help text update (#15760),0.5798923,Made cluster creation/deletion async by default (#16185), updated the lighting delete cluster CLI command help text output updated changelog typo fix Apply suggestions from code review  Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
mergify: drop ready for draft (#15766),0.3650968,Changed to the NeptuneLogger (#16761):,,0
Fix the examples/app_dag App (#14359),0.5172068,"Resolved LightningApp(..., debug=True) (#14464)", Fix app dag example Add test Update doc Update tests/tests_app_examples/test_app_dag.py  Co-authored-by: Sherin Thomas sherin@grid.ai,0
Bump pytest from 7.1.3 to 7.2.0 in /requirements (#15677),0.56081086,Remove pytest as a requirement to run app by @manskx in https://github.com/Lightning-AI/lightning/pull/15449,Bumps pytest from 7.1.3 to 7.2.0. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: pytest   dependency-type: direct:production   update-type: version-update:semver-minor ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Enable Probot CheckGroup v5.1 (#15763),0.42753506,Enable None model checkpoint default (#3669),,0
Disable XSRF protection in StreamlitFrontend to support upload in localhost (#15684),0.9999999,Disable XSRF protection in StreamlitFrontend to support upload in localhost (#15684), Enable CORS in StreamlitFrontend to support upload Only disable XSRF when running on localhost Update test Use utility fn to detect if localhost  Co-authored-by: Luca Antiga luca@lightning.ai,1
Fix azure path excludes (#15756),0.41738638,Changed Checkpoint path parameter from filepath to dirpath (#1016),Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
update chlog after 1.8.2 (#15754),0.56454676,Here are two major changes that apply when using multiple loggers in 1.8:, update chlog after 1.8.2 Apply suggestions from code review,0
Switch from tensorboard to tensorboardx in logger (#15728),0.9414176,Switch from tensorboard to tensorboardx in TensorBoardLogger (#15728), Switch from tensorboard to tensorboardx in logger Warn if log_graph is set to True but tensorboard is not installed Fix warning message formatting Apply suggestions from code review simplify for TBX as required pkg docs example chlog tbx 2.2  Co-authored-by: Luca Antiga luca@lightning.ai Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz,1
FCCV Docs (#15598),0.42953148,model reference provided:,  add custom data iter docs   add custom data iter docs   Update docs/source-pytorch/data/custom_data_iterables.rst   remove ToDevice   nit   Update docs/source-pytorch/data/custom_data_iterables.rst   Co-authored-by: Luca Antiga luca.antiga@gmail.com   clarification for @lantiga   typo   Update docs/source-pytorch/data/custom_data_iterables.rst   Update docs/source-pytorch/data/custom_data_iterables.rst   Update docs/source-pytorch/data/custom_data_iterables.rst   Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Luca Antiga luca.antiga@gmail.com,0
Revert new Hydra launch behavior (#15737),0.5990901,Temporarily removed support for Hydra multi-run (#15737), revert new hydra cwd behavior remove debug statements changelog  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
Move s3fs to cloud extras (#15729),0.4279247,Add support to upload files to the Drive through an asynchronous upload_file endpoint (#14703),Co-authored-by: Luca Antiga luca@lightning.ai,0
test for Enable setting property (#15755),0.47871804,Enabled no returns from eval (#2446),Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Ethan Harris ethanwharris@gmail.com,0
[App] Enable properties for the Lightning flow (#15750),0.6197,- Added support for configuring flow cloud compute ([#14831](https://github.com/Lightning-AI/lightning/pull/14831)),,0
Enable Probot CheckGroup v5 (#15670),0.43507177,Enable None model checkpoint default (#3669),,0
[App] Improve LightningTrainerScript start-up time (#15751),0.94977224,Improved LightningTrainerScript start-up time (#15751),,1
[App] Fix multi-node pytorch example CI (#15753),0.70114934,- Removed `AcceleratorConnector.num_nodes` ([#12107](https://github.com/PyTorchLightning/pytorch-lightning/pull/12107)),,1
Update beautifulsoup4 requirement from <=4.8.2 to <4.11.2 in /requirements (#15745),0.39651993,This release has breaking API changes. See #124 for all details. , Update beautifulsoup4 requirement in /requirements  Updates the requirements on beautifulsoup4 to permit the latest version.  updated-dependencies: - dependency-name: beautifulsoup4   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com  Apply suggestions from code review  Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
"Update tensorboard requirement from <2.11.0,>=2.9.1 to >=2.9.1,<2.12.0 in /requirements (#15746)",0.7722815,Improved the error message for installing tensorboard or tensorboardx (#17053),Update tensorboard requirement in /requirements Updates the requirements on tensorboard to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: tensorboard   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,1
FSDP (native) support for LightningLite (#14967),0.65516776,Native FSDP implementation,Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Add support for logging the model checkpoints to MLFlowLogger (#15246),0.67657673,MLFlowLogger now logs hyperparameters and metrics in batched API calls (#15915),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
[App] Fix VSCode IDE debugger (#15747),0.576622,"Resolved LightningApp(..., debug=True) (#14464)",,0
feature(docs/app/lit_tabs): add works (#15731),0.512112,Add support for listing Lightning AI apps (#13987),,0
Speed up subprocess launch (#15738),0.45577696,"Setting Trainer(accelerator=""ddp_cpu"") now does not spawn a subprocess if num_processes is kept 1 along with num_nodes > 1 (#9603)",,0
[App] Update multi-node examples (#15700),0.4976859,Slightly safer multi node (#15538),Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Torch inference mode for prediction (#15719),0.8914533,Set Torch inference mode for prediction (#15719),torch inference mode for prediction,1
Fix broken link to CLI docs (#15723),0.56087124,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",,0
Fix typo in script name (#15724),0.5237235,python script.py \,,0
Fix typo 'wether' (#15710),0.4513194,"In addition, we fixed:",,0
remove unused random_split import from tutorial (#15716),0.45706892,  * Removed the `FitLoop.split_idx` property,,0
Bump coverage from 6.4.2 to 6.5.0 in /requirements (#15674),0.53970104,Setup: added requirement freeze for the next major version (#14480),Bumps coverage from 6.4.2 to 6.5.0. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: coverage   dependency-type: direct:production   update-type: version-update:semver-minor ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Parse all lines in app file looking for shebangs to run commands. (#15714),0.9951057,Parse all lines in app file looking for shebangs to run commands (#15714),fixed command parsing so that all lines in the file are parsed,1
Fix catimage import (#15712),0.4110112,Module imports are now catching ModuleNotFoundError instead of ImportError (#9867),,0
[App] Mock missing package imports when launching in the cloud (#15711),0.5927923,Removed implicit ui testing with testing.run_app_in_cloud in favor of headless login and app selection (#16741),Co-authored-by: manskx ahmed.mansy156@gmail.com,0
"fix(docs/app/lit_tabs): remove unused app_id, enable run instead (#15702)",0.5215893,- Fixed a bug when launching an app on multiple clusters ([#15226](https://github.com/Lightning-AI/lightning/pull/15226)),,0
docs 5/n (#15669),0.6291452,Docs,"  examples   fix few examples   Update pl_multinode.py   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
Enable Probot CheckGroup v4 (#15649),0.4453528,Enable None model checkpoint default (#3669),,0
Docs: Fix import for scikit in XGBoost template (#15693),0.5358962,Removed dependency on scikit-learn (#801),,0
Bump google-github-actions/auth from 0 to 1 (#15675),0.41075814,lightning run app app.py --cloud --secret API_TOKEN=github_api_token,Bumps google-github-actions/auth from 0 to 1. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: google-github-actions/auth   dependency-type: direct:production   update-type: version-update:semver-major ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Update onnxruntime requirement from <1.13.0 to <1.14.0 in /requirements (#15672),0.4881066,Setup: added requirement freeze for the next major version (#14480),Updates the requirements on onnxruntime to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: onnxruntime   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Bump google-github-actions/setup-gcloud from 0 to 1 (#15671),0.43963683,lightning run app app.py --cloud --secret API_TOKEN=github_api_token,Bumps google-github-actions/setup-gcloud from 0 to 1. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: google-github-actions/setup-gcloud   dependency-type: direct:production   update-type: version-update:semver-major ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
fix(docs/app): broken links in the intermediate/web-ui section (#15691),0.5775783,Updated app URLs to the latest format (#16568),,0
Add Python 3.10 badge (#15681),0.5410938,Compatibility for Python 3.10,,0
add contributing guide to readme,0.40555158,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",,0
Validate the combination of CloudCompute and BuildConfig (#14929),0.5113401,"Show a message when BuildConfig(dockerfile=""..."") is passed but a Dockerfile file is already present in the Work (#15799)",Co-authored-by: otaj 6065855+otaj@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz,0
Fixed Import in Docs For Multinode Trainer Name Which does Not Exist (#15663),0.5657115,"Removed the deprecated Trainer.reset_train_val_dataloaders() in favor of Trainer.reset_{train,val}_dataloader (#16131)",,0
Upgrade CI to PyTorch 1.13 (#15403),0.7263779,Enable PyTorch 1.7 compatibility (#3541),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
add title and description to ServeGradio (#15639),0.40928996,Renamed several Trainer atributes:  (#567), add title and description update test apply suggestions  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Improves the PanelFrontend docs (#14493),0.6195755,Adds PanelFrontend to easily create complex UI in Python (#13531),Co-authored-by: Marc Skov Madsen masma@orsted.dk Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Felonious-Spellfire felonious.spellfire@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Mansy ahmed.mansy156@gmail.com,0
[App] Rename failed -> error in tables (#15608),0.853771,Rename failed -> error in tables (#15608),Co-authored-by: Raphael Randschau nicolai86@users.noreply.github.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Include images with the mirror package (#15659),0.31558213,Avoid using the deprecated LooseVersion (#16162),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Add axes argument to lr finder plot (#15652),0.75307447,Add an axes argument ax to the .lr_find().plot() to enable writing to a user-defined axes in a matplotlib figure (#15652),Co-authored-by: awaelchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Fix ddp_spawn -> ddp fallback logic when on LSF cluster (#15657),0.6332974,"- The selection `Fabric(strategy=""ddp_spawn"", ...)` no longer falls back to ""ddp"" when a cluster environment gets detected ([#16780](https://github.com/Lightning-AI/lightning/pull/16780))",Co-authored-by: awaelchli aedu.waelchli@gmail.com,0
PKG: distribute single semver (#15374),0.41107425,Parsed local package versions (#13933), global distrib ver codeowners Apply suggestions from code review  Co-authored-by: otaj 6065855+otaj@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
"Prevent artifactual ""running from outside your current environment"" error (#15647)",0.84154785,"Prevent artefactual ""running from outside your current environment"" error (#15647)",Prevent warning when shutil.executable returns a symlink Co-authored-by: Luca Antiga luca@lightning.ai,1
update chlogs after 1.8.1 (#15632),0.6007177,Here are two major changes that apply when using multiple loggers in 1.8:,"  update chlogs   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  Apply suggestions from code review  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Fix  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com",0
Upgrade GPU CI to PyTorch 1.13 (#15583),0.6788774,Enable PyTorch 1.7 compatibility (#3541),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka jirka.borovec@seznam.cz,0
Add LightningLite to top level imports (#15502),0.8060946,from lightning.lite import LightningLite,Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
[App] Change app root / config path to be the app.py parent directory (#15654),0.6072077,Unification of app template: moved app.py to root dir for lightning init app <app_name> template (#13853),  Change app root / config path to be the app.py parent directory   Update CHANGELOG.md   mypy   Fix   Mypy ,0
[App] Accelerate Multi Node Startup Time (#15650),0.53390676,Improved LightningTrainerScript start-up time (#15751),,0
Update run_ptl_script.py,0.49929413,$ PL_FAULT_TOLERANT_TRAINING=MANUAL python script.py,,0
Delete unused TPU CI files (#15611),0.44588965,moved TPU xxx_step to backend (#3118),Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
Update docs with new Lightning Lite usage 1/n (#15600),0.6887529,Update the Lightning App docs (#13537),,0
Refactor checkgroup to avoid duplicated checks (#15633),0.39693034,"Simplified ""should run validation"" logic (#7682)",Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
[App] Resolve bi-directional queue bug (#15642),0.6214025,Resolved a bug about a race condition when sending the work state through the caller_queue (#14074),,0
Support individual setup of model and optimizer in Lite (#15185),0.6910758,    # Let Lite setup your model and optimizer,,0
"Revert ""Do not modify MANIFEST.in on install (#15549)"" (#15644)",0.5668802,Resolved a bug where the install command was not installing the latest version of an app/component by default (#14181),This reverts commit 4ea44dd276c9ffa350553a05d33d2050cffec242.,0
Checkpoint migration for ModelCheckpoint state-key changes (#15606),0.7072804,Forced ModelCheckpoint callbacks to run after all others to guarantee all states are saved to the checkpoint (#5731),Co-authored-by: rohitgr7 rohitgr1998@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Make sure save_dir can be empty str (#15638),0.6718213,Make sure save_dir can be empty str (#15638](https://github.com/PyTorchLightning/pytorch-lightning/issues/15638)),,0
Remove the docs for passing strategy args to accelerator (#15636),0.67958647,"The Accelerator and PrecisionPlugin have moved into Strategy. All strategies now take an optional parameter accelerator and precision_plugin (#11022, #10570).",,0
[App] Resolve race condition to move ui files (#15398),0.4551134,- Removed deadlock detection / process reconciliation (`PL_RECONCILE_PROCESS=1`) ([#16204](https://github.com/Lightning-AI/lightning/pull/16204)),,0
Validate that state-key is unique when using multiple callbacks of the same type (#15634),0.5833498,"    # previously, only the state for this callback was passed in as argument",,0
[App] Enable state broadcast with MultiNode (#15607),0.8687192,Enabled MultiNode Components to support state broadcasting (#15607),,1
Do not modify MANIFEST.in on install (#15549),0.4714846,Resolved a bug where the install command was not installing the latest version of an app/component by default (#14181),Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
Enable Probot CheckGroup v3 (#15622),0.43449944,Enable None model checkpoint default (#3669),,0
Remove get_gpu_memory_map deprecated since v1.5 (#15617),0.7080928,"Deprecated log_gpu_memory, gpu_metrics, and util funcs in favor of DeviceStatsMonitor callback (#9921)","  Remove function deprecated in v1.5   Update changelog   rip   Update graveyard   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Add death test  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   Update test case name   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  Address pre-commit failures  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",1
CI: fix pypi pkg (#15630),0.46142167,Add missing python-multipart dependency (#17244),,0
mypy: ignore mypy serve (#15631),0.4150015,            find_unused_parameters=True,,0
Docs 4/n (#15628),0.6483138,Docs,"  remove source-lit   docs   docs   docs   docs   ic   deploy   deploy   deploy   deploy   deploy   deploy   Apply suggestions from code review   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  make build run  Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Rick Izzo rick@grid.ai",0
Sample datatype for Serve Component (#15623),0.4158061,Implemented ready for components (#16129),"  introducing serve component   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   clean up tests   clean up tests   doctest   mypy   structure-fix   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   cleanup   cleanup   test fix   addition   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   test fix   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   requirements   getting future url   url for local   sample data typeg   changes   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   prediction   updates   updates   manifest   fix type error   fixed test   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Rick Izzo rick@grid.ai Co-authored-by: Jirka jirka.borovec@seznam.cz",0
Add Ethan as Lightning App CodeOwners (#15626),0.5221739,Update the Lightning App docs (#13537),update,0
[App] Rename to new convention (#15621),0.5700151,Updated app URLs to the latest format (#16568),  update   update ,0
Drop PyTorch 1.9 support (#15347),1.0000001,Drop PyTorch 1.9 support (#15347),  Drop 1.9   Everything else   READMEs   Missed some   IPU skips   Remove exception type   Add back ,1
Fix: App comment command execution sequencing  (#15615),0.4672594,Resolved a bug where the install command was not installing the latest version of an app/component by default (#14181),  fixed condition in which app command comments would not execute before the flow is dispatched   remove debug print statment   updated example code   fix test   updates to test   Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
[App] Resolve multi-node cloud bug (#15619),0.6793083,- Fixed a bug when launching an app on multiple clusters ([#15226](https://github.com/Lightning-AI/lightning/pull/15226)),update,0
Optimize Payload Time locally (#15601),0.47040606,Wait for full file to be transferred in Path / Payload (#15934),,0
[App] Introduce Bi-directional queues (#15582),0.47015962,Simplified messaging in cloud dispatch (#16160),  update   update   update   update   update   udpate   update   update   update   update   update   updatre   update   update   update   updar   update   update   update   remove print   update   update   update   update   update   update   update   update ,0
Use monkeypatch.chdir instead of os.chdir in tests (#15579),0.47604954,Fix for load_from_checkpoint() not working with absolute path on Windows (#2294),,0
Avoid torchelastic warning message when importing lightning (#15610),0.64587563,  * `pytorch_lightning.utilities.warnings.LightningDeprecationWarning` in favor of `pytorch_lightning.utilities.rank_zero.LightningDeprecationWarning`,,0
Serve component (#15609),0.5527284,Implemented ready for components (#16129),Serve component (#15609),0
Upgrade to HPU release 1.7.0 (#15616),0.5055008,Moveed HPU broadcast override to the HPU strategy file (#17011),Signed-off-by: Jerome janand@habana.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Update Lightning Lite examples (#15599),0.66346306,Update the Lightning App docs (#13537),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Enable Probot CheckGroup V2 (#15612),0.43556315,Enabled manual returns (#4089),"  Enable v2   Run on draft   DEBUG   maintainers and owner   Revert ""DEBUG""   This reverts commit 1414e8590285d62962a44590ae70cf9635c0d4e7.",0
Integrate Lite launcher into Lightning CLI (#15506),0.6679605,Lightning CLI, add click wrapped argparse lite -> model notebook update  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Expose sharding context manager in Lite (#15169),0.42632532,- Enabled using any Sampler in distributed environment in Lite ([#13646](https://github.com/Lightning-AI/lightning/pull/13646)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
ENG-1524: Change retry mechansim default in LightningClient (#15412),0.71879256,Changed the default LightningClient(retry=False) to retry=True (#16382)," Change retry mechansim default in LightningClient add changelog fix weird patching mechanism remove unused import hack the system try again, maybe something has changed in the package  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com",1
[App] Enable debugger with LightningApp (#15590),0.7459607,"Resolved LightningApp(..., debug=True) (#14464)",,1
Enhance reduce_boolean_decision to accommodate any-analogous semantics expected by EarlyStopping Callback (#15253),0.9631486,Enhanced reduce_boolean_decision to accommodate any-analogous semantics expected by the EarlyStopping callback (#15253),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
"Update myst-parser requirement from <0.17,>=0.15 to ==0.18.1 in /requirements (#14417)",0.52896464,Pkg: fix parsing versions by @Borda in https://github.com/Lightning-AI/lightning/pull/15401,Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
[App] Resolve Multi Node examples (#15605),0.4426712,root_node = self.trainer.resolve_root_node_address(root_node),  update   update   update   update ,0
Add .actions CODEOWNERS (#15602),0.40209395,apex plugin (#3502),,0
Fix LightningCLI docs after overhaul of the documentation (#14976),0.6898076,LightningCLI changes:,   Fix diverse issues introduced when the documentation was restructured.    Change the docs to be focused on configure hyperparameters instead of reduction of bolierplate.   Update docs/source-pytorch/cli/lightning_cli.rst   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com    Fix diverse issues introduced when the documentation was restructured.    Change the docs to be focused on configure hyperparameters instead of reduction of bolierplate.   Fixes based on review.   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   Fix wrap width.   Move save_hyperparameters and load_from_checkpoint to lightning_module page.   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
Update outdated deadlock detection test (#15594),0.5286186,"Removed experimental fault-tolerance support (#16516, #16533)", improve deadlock detection test check if failure gets caught remove,0
Run Command from App Comments (#15577),0.55289984,Introducing CLI commands for apps (#13602)!,  initial work   this seems to work well   added example test   updated docs & logging   fixed errors   fix typing error   now using the --setup flag to decide if we should execute app comment commands or not   updated tests   added tests   added test to ci   fixed failing tests   code review   updates ,0
Make checkpointing on train epoch end condition dynamic (#15300),0.6800523,Made training_epoch_end behave like validation_epoch_end (#1357),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
CI: Use new syntax for setting github output (#15415),0.39650702,@akihironitta @awaelchli @Borda @carmocca @kaushikb11 @krshrimali @mauvilsa @otaj @pre-commit-ci @rohitgr7 @semaphore-egg @tkonopka @wayi1,Use new syntax for setting github output Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Undo marking tests that don't need it as standalone (#15355),0.5296083,Refactored setup_training and remove test_mode (#5388),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
[App] Add start_with_flow flag to works (#15591),0.5969066,Improved the error message when the root LightningFlow passed to LightningApp is missing the run method (#14760),"  Initial commit   Update cloud runner   Add start_with_flow flag   Update CHANGELOG.md   Update src/lightning_app/core/work.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   Update cloud runner   Revert, not needed   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com",0
Update fastapi requirement from <0.83.0 to <0.87.0 in /requirements (#15564),0.51214236,Setup: added requirement freeze for the next major version (#14480),Updates the requirements on fastapi to permit the latest version. - Release notes - Commits  updated-dependencies: - dependency-name: fastapi   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Docs: Update tutorial to match PyTorchProfiler changes (#15440),0.68088174,- Renamed the `TrainingTypePlugin` to `Strategy` ([#11120](https://github.com/PyTorchLightning/pytorch-lightning/pull/11120)),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Fix wandb test writing artifacts to cwd (#15551),0.5673037,Removed wandb logger's finalize method (#1193),,0
Reuse assistant code to create the mirror package (#15365),0.3525921,"    name=""my-package"",",,0
Fixed typo Havana -> Habana for HPUs (#15589),0.56687814,- Removed sanity check for multi-optimizer support with habana backends ([#13217](https://github.com/Lightning-AI/lightning/pull/13217)),Fixed typo Havana -> Habana HPUs are accelerators built by Habana Labs.,0
Support DDP with LRFinder (#15304),0.61953837,DDP custom implementation support (override these hooks):, Support DDP for LRFinder Apply suggestions from code review rank 0 is the decision maker  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Fix bagua strategy raising AttributeError during manual optimization (#13137),0.6250785,"  * Removed `optimizer_idx` argument from `Strategy.{optimizer_step,backward}` and all of its overrides in subclasses", fix: fix bagua manual backward Update bagua module Simplify test case Fix type annotations  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Akihiro Nitta akihiro@pytorchlightning.ai,0
Enable quick start e2e test again and run on cloud without installing dependencies 😎  (#15546),0.49173558,Removed implicit ui testing with testing.run_app_in_cloud in favor of headless login and app selection (#16741), Enable quick start e2e test to run without installing dependencies yaml formatting clone the repo,0
Checkpoint migration for loop's internal state (#15500),0.6920568,The Loop's state is now included as part of the checkpoints saved by the library. This enables finer restoration of custom loops.,,0
Merge the slow and regular test workflows (#15331),0.50838035,Cleaning up stale logger tests (#3490),  extend matrix   drop   group-check   groups   cat   typo   cat2type   cat2type   env vars   ''   Rename to slow. Fix timeout   Examples are GPU only   str   Extra step   ''   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
prune installation artifact (#15558),0.5162325,Setup: added requirement freeze for the next major version (#14480), prune installation artifact  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Support fused Adam with mixed precision (#15555),0.6029281,Mixed precision overhaul (#16783),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Simplify codeowners (#15585),0.4702314,reduced all simplified forward (#3126),  Simplify PL CODEOWNERS   Add William   Update apps too ,0
Remove MANIFEST reference in docs job (#15584),0.43899512,Removed LoggerStages (#5673),,0
Update docs for seed_everything in LightningCLI (#15308),0.7292131,Default seed_everything(workers=True) in the LightningCLI (#7504),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Add test to verify that lowering gpus on restart works with sharded spawn (#15317),0.50504637,Pass init args to ShardedDataParallel (#9483),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
[App] Expose Run Work Executor (#15561),0.5973796,Expose RunWorkExecutor to the work and provides default ones for the MultiNode Component (#15561),,0
Fix which groups require docs builds (#15581),0.42979038,Release LAI docs as stable (#14250),,0
Fix result transfer in multiprocessing launcher on multi-node (#15567),0.63103724,"Disabled strict loading in multiprocessing launcher (""ddp_spawn"", etc.) when loading weights back into the main process (#16365)",  Fix result transfer in multiprocessing launcher on multi-node   add simple test   add comment   update test   changelog   use tempfile   fix   assert None   unused import   add comment ,0
Added a check to validate that wrapped FSDP models are used while initializing optimizers (#15301),0.68231624,Generalized Optimizer validation to accommodate both FSDP 1.x and 2.x (#16733),Co-authored-by: awaelchli aedu.waelchli@gmail.com,0
Use _PATH in annotations and convert to str if Path (#15560),0.4818133,"trainer.test(ckpt_path=""my_path"") # load path (NEW BEHAVIOR!)",Co-authored-by: Bryn Lloyd lloyd@itis.swiss,0
Reuse existing commands when running connect more than once (#15471),1.0000002,Reuse existing commands when running connect more than once (#15471), Reuse connection if it matches a connection from an active terminal Remove unused import Include both name and id in the check Fix messages and tests Add test Handle monkeypatching more cleanly Remove unused imports  Co-authored-by: Luca Antiga luca@lightning.ai Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
[App] Re-wording build config warning in the docs (#15570),0.49981853,Deprecation warning (#3844), build config commands Apply suggestions from code review,0
Fix: Revert  lightning_lite.utilities.rank_zero_only to preserve backward compatibility (#15536),0.72805977,- Removed the deprecated ([#14471](https://github.com/Lightning-AI/lightning/pull/14471)), Fix: Revert  to preserve backward compatibility  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
Update governance and codeowners,0.6085073,Updated governance docs,,0
Move krshrimali to Alumni (#15568),0.39946133,move lr_finder (#3434),Move myself to Alumni,0
"Revert ""Fix PL docs build on readthedocs.org (#15511)"" (#15565)",0.5370497,Release LAI docs as stable (#14250),This reverts commit e818e823e315625d06db9316ec62e7f58fc047fd.,0
Fix tests_pytorch import error in legacy checkpoint CI (#15566),0.7035813,Skipped testing with PyTorch 1.7 and Python 3.9 on Ubuntu (#11217),Fix tests_pytorch import error,1
[App] Fixed Multi Node and add examples (#15557),0.5050765,Slightly safer multi node (#15538),,0
Update path type annotation for load_from_checkpoint (#15540),0.7422309,Removed deprecated checkpoint argument filepath (#5321),,1
Bump pytest-cov from 3.0.0 to 4.0.0 in /requirements (#15563),0.59894973,Remove pytest as a requirement to run app by @manskx in https://github.com/Lightning-AI/lightning/pull/15449,Bumps pytest-cov from 3.0.0 to 4.0.0. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: pytest-cov   dependency-type: direct:production   update-type: version-update:semver-major ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Update uvicorn requirement from <=0.18.2 to <0.19.1 in /requirements (#15562),0.49433684,Setup: added requirement freeze for the next major version (#14480),Updates the requirements on uvicorn to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: uvicorn   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Add basic SSH documentation for CLI (#15316),0.5395714,lightning add ssh-key CLI command has been transitioned to lightning create ssh-key,  add basic ssh documentation   rename workflow ssh debugging   Update docs/source-app/workflows/ssh/index.rst   Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com  Update docs/source-app/workflows/ssh/index.rst  Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com  Update docs/source-app/workflows/ssh/index.rst  Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com   add more details about ssh command   Update docs/source-app/workflows/ssh/index.rst   Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com   add more motivation to the audience section   fix sphinx errors   Update docs/source-app/workflows/index.rst   Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com   add details how to get app id   add docs about component name   add more context to the audience section   Update docs/source-app/workflows/ssh/index.rst   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update docs/source-app/workflows/ssh/index.rst  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   address adrians comment about order   add one-time notice   fix headers   wording   update to match ssh params   Update docs/source-app/workflows/ssh/index.rst   Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com  Update docs/source-app/workflows/ssh/index.rst  Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com  Update docs/source-app/workflows/ssh/index.rst  Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com   drop verification   fix merge conflict error   remove symlink   fix doctree   Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Docs 3/n (#15554),0.6335608,Docs,"  remove source-lit   remove source-lit   remove source-lit   remove source-lit   remove source-lit   remove source-lit   remove source-lit   remove source-lit   remove source-lit   remove source-lit   remove source-lit   remove source-lit   remove source-lit   remove source-lit   remove source-lit   remove source-lit   remove source-lit   remove source-lit   remove source-lit   remove source-lit   remove source-lit   remove source-lit   remove source-lit   remove source-lit   remove source-lit   remove source-lit   remove source-lit   remove source-lit   remove source-lit   remove source-lit   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
Let metadata score be serializable by wand (#15544),0.49132916,def configure_serialization(self):,Co-authored-by: awaelchli aedu.waelchli@gmail.com,0
Grep for potential errors in standalone tests (#15341),0.4858262,Cleaning up stale logger tests (#3490),Co-authored-by: awaelchli aedu.waelchli@gmail.com,0
Print the logs when TPU tests fail (#15533),0.53890896,Cleaning up stale logger tests (#3490),,0
Slightly safer multi node (#15538),0.9999998,Slightly safer multi node (#15538),update Co-authored-by: Luca Antiga luca.antiga@gmail.com,1
Improve the checkpoint upgrade utility script (#15333),0.60405695,Removed deprecated checkpoint argument filepath (#5321),,0
load_from_checkpoint returns the expected type (#15496),0.75334203,"def load_checkpoint(self, path):",Co-authored-by: Bryn Lloyd lloyd@itis.swiss,1
Fix usage of fs.listdir in CheckpointConnector (#15413),0.6985325,Removed deprecated checkpoint argument filepath (#5321),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: otaj 6065855+otaj@users.noreply.github.com,0
Fix ReduceOp type hint in ColossalAI strategy (#15535),0.5447403,Renamed Strategy.reduce to Strategy.all_reduce in all strategies (#16370),,0
[App] Introduce Multi Node Component (#15524),0.5639275,Implemented ready for components (#16129),,0
remove source-lit docs 2 (#15527),0.4722798,- `nvidia/apex` removal ([#16149](https://github.com/Lightning-AI/lightning/pull/16149)),,0
Only load global step when fitting (#15532),0.5055839,"def on_fit_start(self, *args, **kwargs):",Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
Do not modify PACKAGE_NAME on install (#15493),0.56213665,"    name=""my-package"",","  Do not modify PACKAGE_NAME on install   Fix ci pkg action   Required   Typos   Apply suggestions from code review   Undo defaults   Cleanup   Implement idea   Fuck   Apps mock fix   Fix app-pytest with PKG_NAME=app   Justus suggestion   Debug Windows   Update setup.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Revert ""Debug Windows""  This reverts commit 9fe3ba366545ba9e308b0757876279b1fff8a553.   SSH action   Crazy bug   Revert ""SSH action""   This reverts commit 5061e8e7d6fdf47b38c676623b02387cd451d403.   Package import step   Avoid env conflict   Debug   Whitespace   Try removing existing lite build   This should be redundant now   Add back env now that source-lit is gone   Remove download artifact   checkgroup   TODOs suggested by Jirka   _   Revert ""_"". These are local variables, do not need protected   This reverts commit 8340b85991bb6927d851d86861b7efe2c845377b. Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com",0
Remove dead ModelCheckpoint code (#15534),0.73020566,Changed ModelCheckpoint version suffixes to start at 1 (#5008),,1
CI: skip jobs in draft (#15529),0.3733014,"We've added the ability to turn the automatic resubmission on or off when a job gets interrupted by the SLURM controller (via signal handling). Users who prefer to let their code handle the resubmission (for example, when submitit is used) can now pass:", skip jobs in draft types,0
Update governance docs (#15479),0.9214225,Updated governance docs,,1
Replace pull_request event from docs workflow (#15530),0.45107955,Refactor cloud dispatch and update to new API (#16456),"Revert ""Replace pull_request_target event from docs workflow (#15526)"" This reverts commit 4750e221d1e7622bbf2d0e5b09e7136032e9c410. Co-authored-by: Luca Antiga luca.antiga@gmail.com",0
Move to alumni (#15522),0.3792693,| Old name                     | New name                       |,Update governance.rst,0
Fix handling of script arguments in tracer (#15518),0.71636176,The params argument in TracerPythonScript.run no longer prepends -- automatically to parameters (#15518),  Don't assume script args start with double dash   add changelog   Co-authored-by: Luca Antiga luca@lightning.ai Co-authored-by: awaelchli aedu.waelchli@gmail.com,1
[App] Hot Fix: Missing root flow in app.flows (#15531),0.6630123,Improved the error message when the root LightningFlow passed to LightningApp is missing the run method (#14760),,0
Docs 2/n (#15521),0.6340986,Docs,"  merge master   merge master   merge master   merge master   install colors   install colors   install colors   install colors   install colors   install colors   install colors   install colors   docs   docs   docs   docs   docs   docs   docs   docs   Revert ""docs""   This reverts commit c83d9854fb4bb4248b4e478430907cb4aa808b66.  Revert ""docs""  This reverts commit a2bb66d2f06d4e995cefdd079bcdf25f927e92ef.   docs   docs   remove source-lit   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   precommit   files   folder   Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
Replace pull_request_target event from docs workflow (#15526),0.453412,Refactor cloud dispatch and update to new API (#16456),,0
[App] Add start method to the LightningWork  (#15523),0.6336075,- Added support to pass a `LightningWork` to the `LightningApp` ([#15215](https://github.com/Lightning-AI/lightning/pull/15215),,0
remove source-lit docs (#15525),0.4465869,- `nvidia/apex` removal ([#16149](https://github.com/Lightning-AI/lightning/pull/16149)),  remove source-lit   remove source-lit   remove source-lit ,0
[App] Resolve inconsistency where the flow.flows property isn't recursive leading to flow overrides (#15466),0.69084764,Changed the flow.flows to be recursive wont to align the behavior with the flow.works (#15466),  update   update   update   update   update   resolve attachment   update   update   update   update   update   update   update   update   update   update   update   update   update ,0
fix(docs/app): setup muse card (#15513),0.44070673,- Improved the error message for installing tensorboard or tensorboardx ([#17053](https://github.com/Lightning-AI/lightning/pull/17053)),,0
CI: exclude docs requirements changes (#15488),0.4636364,Show a message when BuildConfig(requirements=[...]) is passed but a requirements.txt file is already present in the Work (#15799),,0
Periodically sync database to the drive (#15441),1.0,Periodically sync database to the drive (#15441),,1
Fix PL docs build on readthedocs.org (#15511),0.5670899,Release LAI docs as stable (#14250),Set env var,0
CI: resolving Docs (#15508),0.44962418,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ","  placeholder   pytorch   fix CI   fix package name   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   use os theme for pytorch docs   switch source-app to lai_sphinx_theme   pull_request   doc error fix   another build error fix   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   removed unused glossary.rst   lit   doc fixes   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   fix last warning   try   lit   flake8   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: Yurij Mikhalevich yurij@grid.ai Co-authored-by: rohitgr7 rohitgr1998@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
Checkout the HEAD SHA for docs builds (#15510),0.5022353,"For more information, check out the docs.",,0
Add SSH command to CLI (#15310),0.5996112,lightning add ssh-key CLI command has been transitioned to lightning create ssh-key,  implement ssh command   add tests that ssh command is available   most SSH command tests   update changelog   Update src/lightning_app/cli/lightning_cli.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   adrian feedback pt1   Update src/lightning_app/cli/lightning_cli.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   more feedback   rework to support app name   update tests based on different interface   Update src/lightning_app/cli/lightning_cli.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update src/lightning_app/cli/lightning_cli.py  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update src/lightning_app/cli/lightning_cli.py  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update src/lightning_app/cli/lightning_cli.py  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   update tests with changed expectation   fix tests that broke with introduction of shutils   fix too long line   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
docs updates 1/n (#15473),0.58687425,Docs improvements,"  docs   docs updates   docs updates   docs updates   docs updates   d   d   d   d   d   d   ??   d1   d1   d1   d1   d1   d1   d1   d1   d1   d1   d1   d1   d1   d1   d1   d1   d1   d1   d1   d1   d1   d1   d1   d1   d1   d1   d1   d1   d   d   d   d   d   d   d   d   d   d   new title   new title   new title   new title   new title   new title   new title   new title   new title   new title   new title   new title   new title   new title   new title   new title   new title   new title   new title   new title   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   only select from parent   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   use OSS template   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  Update docs/README.md  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: William Falcon williamfalcon@Williams-MacBook-Pro-2.local Co-authored-by: William Falcon williamfalcon@Williams-MBP-2.lan Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",0
CI: drop nightly (#15480),0.39186856,Cleanup cluster waiting (#16054),,0
CI: lower timeout for e2e (#15483),0.54467964,Increased TPU check timeout from 20s to 100s (#5598),,0
Change probot workflow source (#15492),0.41502407,Renames model steps (#1051),,0
[App] Only check versions / env when not in the cloud (#15504),0.829229,Only check versions / env when not in the cloud (#15504),Only check versions / env when not in the cloud,1
Ensure first entry of connect.txt is the app name (#15443),0.59407747,Changed the command lightning connect to lightning connect app for consistency (#16670),Co-authored-by: Luca Antiga luca@lightning.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: awaelchli aedu.waelchli@gmail.com,0
Set development version to 1.9.0dev (#15472),0.671597,"    version=""0.0.1"",",Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Add check groups for specific workflow changes (#15503),0.43912047,Run the flow only if the state has changed from the previous execution (#14076),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
using lai docs template (#15486),0.5772,Release LAI docs as stable (#14250),  using lai docs tempalte   sync   prune   lai_sphinx_theme   ls   aws   ext   list   cleaning   pr ,0
CI: filter mypy triggers (#15481),0.4002652,"def on_fit_start(self, *args, **kwargs):",  CI: filter mypy triggers   Fixes to checkgroup   Remove setup.cfg   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Luca Antiga luca.antiga@gmail.com,0
Fix srun detection causing permission error on non-SLURM platforms (#15485),0.9077794,Fix an issue with the SLURM srun detection causing permission errors (#15485), improve srun detection changelog try catch is obsolete  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
Update governance (#15478),0.7559844,Updated governance docs,,1
[App] Auto-upgrade / detect environment mis-match from the CLI (#15434),0.887725,Auto-upgrade / detect environment mis-match from the CLI (#15434),  Add auto-upgrade from the CLI and check for current env   No longer require python -m in docs   Tabs -> spaces   Ignore pre-releases   Test + docs ,1
Fix TensorBoardLogger's validation of example input when logging graph (#15323),0.6928883,Changed the default logger to TensorBoardLogger (#609),,0
releasing 1.8.0.post1 (#15476),0.5162151,The point of this release is more bug fixes ahead of v 1.0.0. We now have CI tests on TPU thanks to @zcain117 from Google! :slightly_smiling_face:,  releasing 1.8.1   post1 ,0
Remove pytest as a requirement to run app (#15449),0.82105017,Remove pytest as a requirement to run app by @manskx in https://github.com/Lightning-AI/lightning/pull/15449,"  Remove pytest as a requirment to run app   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",1
Introduce checkpoint migration (#15237),0.6451726,all the checkpoint issues should be gone now (including backward support for old checkpoints),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Launch options for Lightning Lite (#14992),0.5422568,Lightning CLI,Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
ci: fix & unify pkg name (#15470),0.51257175,Pkg: fix parsing versions by @Borda in https://github.com/Lightning-AI/lightning/pull/15401,,0
CI: fix typo in workflow & trigger (#15465),0.42532215,"In addition, we fixed:", fix typo in workflow pr trigger,0
CI: adjust doctests for lightning.pytorch (#15469),0.6801293,- Marked the `lightning.pytorch.trainer.configuration_validator.verify_loop_configurations` function as protected ([#17009](https://github.com/Lightning-AI/lightning/pull/17009)),,0
update docs deploy: stable/latest (#15460),0.5948153,Release LAI docs as stable (#14250), update docs deploy gcp - bucket,0
Pkg: fix parsing versions (#15401),0.7867216,Pkg: fix parsing versions by @Borda in https://github.com/Lightning-AI/lightning/pull/15401, testing fix regex,1
Update psutil requirement from <5.9.3 to <5.9.4 in /requirements (#15423),0.5923965,The psutil package is now required for CPU monitoring (#17010),Updates the requirements on psutil to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: psutil   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
CI: adjust examples for lightning.pytorch (#15457),0.72724724,"- Deprecated the functions in `pytorch_lightning.utilities.apply_func` in favor of `lightning_utilities.core.apply_func` ([#14516](https://github.com/Lightning-AI/lightning/pull/14516), [#14537](https://github.com/Lightning-AI/lightning/pull/14537))", adjust examples Apply suggestions from code review  Co-authored-by: Akihiro Nitta nitta@akihironitta.com,1
Use PL 1.8.0 instead of 1.8.0rc in app example requirements (#15454),0.53461426,"    version=""0.0.1"",",,0
Adding test for legacy checkpiont created with 1.8.0 (#15450),0.49140802,Changed ModelCheckpoint version suffixes to start at 1 (#5008),  [create-pull-request] automated change   Add legacy to checkgroup   Co-authored-by: akihironitta akihironitta@users.noreply.github.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com,0
Update setuptools requirement from <=59.5.0 to <65.6.0 in /requirements (#15421),0.68660605,Setup: added requirement freeze for the next major version (#14480),Updates the requirements on setuptools to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: setuptools   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Implement freeze batchnorm with freezing track running stats (#15063),0.8006492,Implement freeze batchnorm with freezing track running stats by @PososikTeam in https://github.com/Lightning-AI/lightning/pull/15063,Co-authored-by: awaelchli aedu.waelchli@gmail.com,1
Update ipython[all] requirement from <8.5.1 to <8.6.1 in /requirements (#15419),0.5678959,Drop Python 3.6 support,Updates the requirements on ipython[all] to permit the latest version. - Release notes - Commits  updated-dependencies: - dependency-name: ipython[all]   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Fix Lightning package version label (#15447),0.70852387,Pkg: fix parsing versions by @Borda in https://github.com/Lightning-AI/lightning/pull/15401,,1
Update changelog for development post 1.8 (#15444),0.6264866,Full Changelog,Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Use sklearn in runif (#15426),0.45197445,"def on_fit_start(self, *args, **kwargs):", Use sklearn in runif test by removing sklearn dep remove repeated code seed,0
CI: Upload video artifacts from e2e tests (#14467),0.34474945,The point of this release is more bug fixes ahead of v 1.0.0. We now have CI tests on TPU thanks to @zcain117 from Google! :slightly_smiling_face:, CI: try upload e2e Artifacts Apply suggestions from code review  Co-authored-by: Akihiro Nitta nitta@akihironitta.com,0
Lightning 1.8.0 release (#15435),0.6686912,The core team is excited to announce the release of Lightning 1.8 :zap:,Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz,0
install lite for mypy & docs (#15437),0.5248917,There were two different ways of importing Lite in <= 1.9.0,,0
releasing RC2 (#15436),0.43054888,[1.6.4] - 2022-06-01,rc2,0
ci: update install lite & cut pkg dependency (#14517),0.50897783,There were two different ways of importing Lite in <= 1.9.0,"  ci: update install lite   try without lite in req file   ci: install   app   init   Revert ""app""   This reverts commit f3f09e7888163db9730012c9efd35d8f2617a0cf.   ci: cpu   ci: gpu   pkg   env   bench   trigger   notes   prune   set version   fix version   git reset   hpu, ipu   adjust   --hard   git checkout   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com   rc2   L   docs   hpu   Co-authored-by: awaelchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Luca Antiga luca.antiga@gmail.com",0
[App][CI] Delete apps older than 1 hours CI account (#15238),0.385925,App,Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Update notebooks submodule head (#15432),0.44296297,Renamed utils modules (#5199),,0
Allow force run app on cloud if loading locally errors (#15019),0.6336173,The utility lightning.app.utilities.cloud.is_running_in_cloud now returns True during the loading of the app locally when running with --cloud (#16045),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
[App] Remove --instance-types from cluster creation (#15314),0.8095416,- Removed the `--instance-types` option when creating clusters ([#15314](https://github.com/Lightning-AI/lightning/pull/15314)),"What does this PR do? Removes the ability to specify --instance-types when creating clusters. Instead, all clusters will be able to use every instance type supported by the platform.",1
set default work version v1.12 (#15431),0.64558375,Set version as today (#13906),,0
Fix TPU tests on master builds (#15349),0.63129467,This means we fixed many TPU bugs we hadn’t caught before because we had no tests.,,0
neptune.init deprecation fix (#15393),0.7580129,- The `NeptuneLogger` now uses `neptune.init_run` instead of the deprecated `neptune.init` to initialize a run ([#15393](https://github.com/Lightning-AI/lightning/pull/15393)),Co-authored-by: awaelchli aedu.waelchli@gmail.com,1
Fix DataLoader re-instantiation when attribute is array (#15409),0.78400683,- Fixed an issue with DataLoader re-instantiation when the attribute is an array and the default value of the corresponding argument changed ([#15409](https://github.com/Lightning-AI/lightning/pull/15409)),,1
CI: trigger PL for lite (#15402),0.49312228,Lite,,0
bump typing-extensions (#15405),0.5312754,Refactored setup for typing friendly (#6590),,0
ci: fix install lite on GPUs (#15375),0.47704613,This required installing nvidia/apex,,0
Fix skipped tests due to sklearn (#15311),0.49089634,Refactored setup_training and remove test_mode (#5388),Co-authored-by: awaelchli aedu.waelchli@gmail.com,0
"Update matplotlib requirement from <3.5.3,>3.1 to >3.1,<3.6.2 in /requirements (#15422)",0.55223763,Drop Python 3.6 support,Update matplotlib requirement in /requirements Updates the requirements on matplotlib to permit the latest version. - Release notes - Commits  updated-dependencies: - dependency-name: matplotlib   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Improve callback documentation for outputs and accumulate_grad_batches (Resolves #15315) (#15327),0.6024695,  * `Callback.on_batch_end` in favor of `Callback.on_train_batch_end`,Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: awaelchli aedu.waelchli@gmail.com,0
Update stale-bot message (#15410),0.45047122,Cleanup cluster waiting (#16054),fix stale bot language,0
Increase timeout in PyTorch CI jobs due to long grpcio installation (#15411),0.6118865,"Resolved schedule step bug for PyTorch Profiler (#6674, #6681)",increase timeout in pytorch jobs due to long installation,0
Fix default CloudCompute for flows (#15371),0.79081017,- Fixed a bug with a default CloudCompute for Lightning flows ([#15371](https://github.com/Lightning-AI/lightning/pull/15371)), Fix default CloudCompute for flows Unit test added,1
CI: Avoid using fixed container name (#15397),0.4486702,Remove unnecessary intermediate layers in Dockerfiles (#5697),Avoid using fixed container name,0
Fix issues when RichProgressBar disabled (#15376),0.6464206,- Removed `__getstate__` and `__setstate__` of `RichProgressBar` ([#11100](https://github.com/PyTorchLightning/pytorch-lightning/pull/11100)),,0
[App] Add ServeStreamlit work (#15400),0.48333544,Updated app URLs to the latest format (#16568),  Init   Updates   Add test for model building   Imports   Fix   Typing   Ignore serve streamlit in mypy ,0
Deprecate AllGatherGrad (#15364),0.70267767,Deprecation warning (#3844),,1
Add SSH key management to CLI (#15291),0.7007411,- Added support for managing SSH-keys via CLI ([#15291](https://github.com/Lightning-AI/lightning/pull/15291))," add cli commands for adding/ removing resources  as discussed with Adrian, we want to adopt ""lightning add"" and ""lightning remove"" for  ssh-keys, as the resource already exists.  implement ssh-key management one parameter for public key, optional name handle the case where a private key file was provided make ssh key-mgmt support classes protected re-order add ssh-key args change types signatures of add_key rename test cases update changelog  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",1
allow e2e test image to be changed via env variable (#15200),0.38793087,  * `env_prefix`,"as we patch our base images, this e2e image needs to be updated all the time as well. Instead of changing this with a PR all the time this PR makes the e2e container image version configurable through the ENV. Co-authored-by: Jirka jirka.borovec@seznam.cz",0
Add support for functions (#15098),0.61825716,"Add deprecated metric utility functions back to functional (#5067, #5068)",,0
[App] Fix cluster logic (#15383),0.59646547,Cluster creation and deletion now waits by default [#15458,,0
[App] Reduce import depths and add test (#15330),0.41971096,"Decoupled Appex (#4052, #4054, #4055, #4056, #4058, #4060, #4061, #4062, #4063, #4064, #4065)",Co-authored-by: thomas chaton thomas@grid.ai,0
[App] Add env variables to desactivate pull and push of the App State (#15367),0.5402664,- Added a layout endpoint to the Rest API and enable to disable pulling or pushing to the state ([#15367](https://github.com/Lightning-AI/lightning/pull/15367),,0
Update auto_scale_batch_size error message and docstring with LightningDataModule (#15351),0.59779745,- Added `BatchSizeFinder` callback ([#11089](https://github.com/Lightning-AI/lightning/pull/11089)),,0
Fix unpickle redirection (#15382),0.39390886,Changed overwrite to True (#16009),,0
ENG-1404: Hide region CLI flag for cluster creation (#15277),0.552715,- Fixed BYOC cluster region selector -> hiding it from help since only us-east-1 has been tested and is recommended ([#15277]https://github.com/Lightning-AI/lightning/pull/15277),Hide region CLI flag,0
App tests hang on Windows with Python 3.9 (#15385),0.5038182,Skipped testing with PyTorch 1.7 and Python 3.9 on Ubuntu (#11217),App tests hang on Windows with Python 3.8,0
Fix resetting internal bars in RichProgressBar after each trainer stage (#15377),0.7006946,- Fixed `RichProgressBar` progress validation bar total when using multiple validation runs within a single training epoch ([#11668](https://github.com/PyTorchLightning/pytorch-lightning/pull/11668)),,1
Create required group for app examples (#15332),0.38709474,Explicitly specify the process group backend if you choose to,,0
CI: switch cloud e2e tests to Prod (#15369),0.46891403,Removed implicit ui testing with testing.run_app_in_cloud in favor of headless login and app selection (#16741),,0
docker: drop pt 1.9 (#15345),0.63081676,Remove unnecessary intermediate layers in Dockerfiles (#5697),  docker: drop pt 1.9   Missed some   Last one   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Luca Antiga luca.antiga@gmail.com,0
Add JustPy Frontend (#15002),0.43262923,python script.py \,  update   update   update   update   changelog   update   update   update   update   update   update   update   update   uipdate   update   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
[App] Format client error ApiExceptions without a traceback (#15130),0.4842232,This release has breaking API changes. See #124 for all details. ,"What does this PR do? The lightning CLI uses the lightning APIs to create, read, update, and delete resources such as apps and clusters. If an API call fails due to client error (e.g. invalid input, unauthorized, unauthenticated) the API will return an HTTP status code in the 400s, along with a custom message in the response body. The CLI (usually) does not directly handle these exceptions. As a result the exception bubbles up through the click framework (which does not recognize the exception type) and to the python runtime, which produces a long traceback and dumps the raw exception to the user. This PR fixes the user experience. When our API fails on a client error, the CLI will display the message from the server without a traceback.",0
Fix import for OrderedDict in Python 3.7.0 (#15359),0.50930655,Removed dependency on pandas (#736), weird,0
fixing publish pypi (#15361),0.4277087,"Removed process_idx from the {DDPSpawnPlugin,TPUSpawnPlugin}.new_process methods (#10022)",,0
Update train_model_basic.rst (#15352),0.60318494,"Removed the deprecated Trainer.reset_train_val_dataloaders() in favor of Trainer.reset_{train,val}_dataloader (#16131)",,0
Fix missing secrets in legacy checkpoint workflow (#15358),0.5882918,all the checkpoint issues should be gone now (including backward support for old checkpoints),,0
LAI: creating mirror package (#15105),0.5854715,Release LAI docs as stable (#14250),"  placeholder   mirror + prune   makedir   setup   ci   ci   name   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   ci clean   empty   py   parallel   doctest   flake8   ci   typo   replace   clean   Apply suggestions from code review   re.sub   fix UI path   full replace   ui path?   replace   updates   regex   ci   fix   ci   path   ci   replace   Update .actions/setup_tools.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   also convert lightning_lite tests for PL tests to adapt mocking paths   fix app example test   update logger propagation for PL tests   update logger propagation for PL tests   Apply suggestions from code review   Revert ""update logger propagation for PL tests""   This reverts commit c1a5e119c740b5468daac63028de8aa799a177ac.   playwright   py   update import in tests   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   try edit import in overwrite   debug code   rev playwright   Revert ""try edit import in overwrite""   This reverts commit c02f766521c91454be36b19784c6a3ed2f715109.   ci: adjust examples   adjust examples cloud   mock lightning_app   Install assistant dependencies   lightning   setup   Apply suggestions from code review   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   Apply suggestions from code review   disable cache   move doctest to install   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   )   echo ./   ci   lru   revert disabling cache, prints   ci   prune ci jobs   prune ci jobs   training loop standalone tests   add sys modules cleanup fixture   make use of fixture   revert standalone   ci e2e   fix imports in lightning   fix imports of lightning in tests   Revert ""make use of fixture""   This reverts commit c15efdd205da2353187275a8d3da141d0ec0ec0a.   Revert other commits for fixtures   revert use of fixture   py3.9   fix mocking   fix paths   hack mocking   docs   Apply suggestions from code review   rev suggestion   Minor changes to the parametrizations   Update checkgroup with the new and changed jobs   include frontend dir   cli   fix imports and entry point   Revert standalone   rc1   e2e on staging   Revert ""Revert standalone""   This reverts commit 9df96685b866b1719fcdeb0b2e832255e3a5f8c0.   groups   to   ci: pt ver   docker   Apply suggestions from code review   Copy over changes from previous commit to other groups   Add back changes from bad merge   Uppercase step name everywhere   update   ci   ci: lai oldest   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Justus Schock justus.schock@posteo.de Co-authored-by: manskx ahmed.mansy156@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Luca Antiga luca.antiga@gmail.com",0
Fix exception when creating a multiprocessing Pool after importing Lightning (#15292),0.84466916,- Fixed an exception that would occur when creating a `multiprocessing.Pool` after importing Lightning ([#15292](https://github.com/Lightning-AI/lightning/pull/15292)),,1
Do not trigger PyTorch GPU tests on Lite test changes (#15348),0.6542892,  * Deprecated the `pytorch_lightning.utilities.device_parser.parse_gpu_ids` in favor of `lightning_lite.utilities.device_parser.parse_gpu_ids`,,0
[App] Show logs command to be standalone and re-usable (#15343),0.71676743,- Improved the show logs command to be standalone and re-usable ([#15343](https://github.com/Lightning-AI/lightning/pull/15343),,1
"Update pandas requirement from <1.5.1,>1.0 to >1.0,<1.5.2 in /requirements (#15263)",0.6118034,Removed dependency on pandas (#736),Update pandas requirement in /requirements Updates the requirements on pandas to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: pandas   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Fix PyTorch versions in Lite CI (#15338),0.76811874,- Include the `pytorch_lightning` version as a header in the CLI config files ([#12532](https://github.com/Lightning-AI/lightning/pull/12532)),  replace oldest in lite   Fix PyTorch versions in Lite CI   This will be moved to install pkg workflow in the mirror PR   1.13 fixes   Windows fix   sorting   Co-authored-by: otaj ota@lightning.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
Replace oldest versions also for Lite (#15337),0.5574765,There were two different ways of importing Lite in <= 1.9.0,,0
[App] Resolve Research Studio Bugs (#15313),0.49704,Updated app testing (#16000),,0
New skip conditions for unpickle-patching tests (#15329),0.41325954,Make the is_picklable function more robust (#17270), New running conditions for tests found one more mistake,0
Fix pickling issues with rich progress bar (#15319),0.6777971,- Fixed a pickling error when using `RichProgressBar` together with checkpointing ([#15319](https://github.com/Lightning-AI/lightning/pull/15319)),,0
Extend the detection of interactive mode (#15293),0.45272648,The following modes are supported:, extend interactive mode detection update test names changelog test,0
Pin test requirements to their current latest versions (#15157),0.48509872,We have upgrade Continues Integration to speed up the automatic testing. ,Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
"Drop ""full"" suffix in CI (#15320)",0.47462773,"- ""Native"" suffix removal ([#16490](https://github.com/Lightning-AI/lightning/pull/16490))",,0
Resolve Boring App Race Condition (#15324),0.5021396,Resolved a bug about a race condition when sending the work state through the caller_queue (#14074),,0
Mark internal Lite APIs as protected (#15307),0.52727985,  * The fetching classes are now marked as protected ([#16664](https://github.com/Lightning-AI/lightning/pull/16664)), mark internal lite apis as protected formatting docs update  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Enable and fix recent legacy checkpoint tests in CI (#12491),0.67265373,all the checkpoint issues should be gone now (including backward support for old checkpoints),,0
Remove non-existent Lite checks (#15325),0.45885545,Silenced some warnings. verified ddp refactors (#3483),,0
add cloudio pickle patching for unified package (#15309),0.47327623,  * `pl.utilities.cloud_io` ([#16438](https://github.com/Lightning-AI/lightning/pull/16438)),,0
Narrower CI timeouts (#15231),0.57297313,Increased TPU check timeout from 20s to 100s (#5598),Narrow CI timeouts,0
Fix typo in OS name (#15318),0.42823482,Renamed xxx_AVAILABLE as protected (#5082),,0
Prevent bug when launching apps on multiple clusters (#15226),0.76927733,- Fixed a bug when launching an app on multiple clusters ([#15226](https://github.com/Lightning-AI/lightning/pull/15226)),"Stops a bug when cross-launching an app between clusters. Currently the platform does not allow running multiple app instances. If you have app-1 running on cluster-1 and try to run it on cluster-2, the CLI will succeed but the app will never start. This PR prevents this disconnect. The app should not be uploaded / released if it won't run. An error is presented to the user explaining what happened and how to proceed (specify a different --name: e.g. app-2). Once the platform supports multiple app instances / running individual apps on multiple clusters, this PR can be reverted.",1
Update pl CPU testing matrix (#15312),0.51500654,"trainer = pl.Trainer(callbacks=DeviceStatsMonitor(cpu_stats=True), accelerator=""cpu"")","  Update pl CPU testing matrix   Remove standalone comment, could be confused   Heurisitc for PyTorch latest   partition rest   ckpgoup   These do not exist   This ALSO does not exist ",0
drop GH e2e cloud & add cron for Azure (#15306),0.36769056,Resolved a bug where the wrong client was passed to collect cloud logs (#14684), Fix GH e2e cloud drop gha cron,0
Add support for custom cloud compute configurations for Flows (#14831),0.75781053,- Added support for configuring flow cloud compute ([#14831](https://github.com/Lightning-AI/lightning/pull/14831)),"  use more recent lightning cloud launcher   allow LightningApp to use custom cloud compute for flows   feedback from adrian   adjust other cloud tests   update   update   update commens   Update src/lightning_app/core/app.py   Co-authored-by: Sherin Thomas sherin@grid.ai   Close profiler when StopIteration is raised (#14945)   Find last checkpoints on restart (#14907)   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   Remove unused gcsfs dependency (#14962)   Update hpu mixed precision link (#14974)   Signed-off-by: Jerome janand@habana.ai  Bump version of fsspec (#14975)  fsspec verbump   Fix TPU test CI (#14926)   Fix TPU test CI   +x first   Lite first to uncovert errors faster   Fixes   One more   Simplify XLALauncher wrapping to avoid pickle error   debug   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Debug commit successful. Trying local definitions   Require tpu for mock test   ValueError: The number of devices must be either 1 or 8, got 4 instead   Fix mock test   Simplify call, rely on defaults   Skip OSError for now. Maybe upgrading will help   Simplify launch tests, move some to lite   Stricter typing   RuntimeError: Accessing the XLA device before processes have spawned is not allowed.   Revert ""RuntimeError: Accessing the XLA device before processes have spawned is not allowed.""   This reverts commit f65107ebf3e062d497f1033bfbbd59774f2d253f.   Alternative boring solution to the reverted commit   Fix failing test on CUDA machine   Workarounds   Try latest mkl   Revert ""Try latest mkl""   This reverts commit d06813aa67cc161879775e24be24b735e2925555.   Wrong exception   xfail   Mypy   Comment change   Spawn launch refactor   Accept that we cannot lazy init now   Fix mypy and launch test failures   The base dockerfile already includes mkl-2022.1.0 - what if we use it?   try a different mkl version   Revert mkl version changes   Co-authored-by: awaelchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com   Trainer: fix support for non-distributed PyTorch (#14971)   Trainer: fix non-distributed use   Update CHANGELOG   fixes typing errors in rich_progress.py (#14963)   revert default cloud compute rename   allow LightningApp to use custom cloud compute for flows   feedback from adrian   update   resolve merge with master conflict   remove preemptible   update CHANGELOG   add basic flow cloud compute documentation   fix docs build   add missing symlink   try to fix sphinx   another attempt for docs   fix new test   Signed-off-by: Jerome janand@habana.ai Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Sherin Thomas sherin@grid.ai Co-authored-by: Ziyad Sheebaelhamd 47150407+ziyadsheeba@users.noreply.github.com Co-authored-by: otaj 6065855+otaj@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jerome Anand 88475913+jerome-habana@users.noreply.github.com Co-authored-by: awaelchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Adam J. Stewart ajstewart426@gmail.com Co-authored-by: DP 10988155+donlapark@users.noreply.github.com",1
Pick queue type only if specified (#15295),0.38181356,    # use the last 4 numbers in the job id as the id,Pick queue type only if specified (#15295),0
Run all tests in master (#15288),0.5098977,- Full tests that test specific functionality in trainer.,  example full tests on master   Modify checkgroup   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Resolve App e2es (#15302),0.53617084,"Resolved LightningApp(..., debug=True) (#14464)","  update   prune   examples   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
[App] testing lightning in lightning-app package (#15286),0.6317614,"Resolved LightningApp(..., debug=True) (#14464)",  hot fix   update   update   update   cmd   pkg   update   Apply suggestions from code review   update   update   Apply suggestions from code review   update   update   update   update   update   update   update   update   update   update   update   update   update   update   cleaning   Apply suggestions from code review   update   update   Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Allow sharing secrets on TPU tests (#15289),0.48152325,Add an environment variable to your app to read the secret:,,0
Remove examples and loggers from develop dependencies (#15282),0.51967406,Removed logger_connector legacy code (#6733),  Remove examples and loggers from develop dependencies   remove more references   Fix mypy   Keep logger file for docs mocking   Simpler fix   Fix docs build   Global testsetup   Matching files   Undo change   loggers as info   Clarify   Update requirements/pytorch/loggers.info   Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: otaj 6065855+otaj@users.noreply.github.com,0
Do not lose references of trainer in test (#15272),0.6946442,trainer.test(...),  Fix reference error   Skip flaky hanging test   .   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
fix requirements for package (#15285),0.5368709,Setup: added requirement freeze for the next major version (#14480),  fix requirements   https   verbose   Co-authored-by: Luca Antiga luca.antiga@gmail.com,0
Better handling connection interruption (#15267),0.5042374,Improved Lightning App connect logic by disconnecting automatically (#14532),"  config fixes   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
Use google-github-actions/get-gke-credentials@v0 (#15264),0.48834276,"GitHubComponent(api_token=os.environ[""API_TOKEN""])", Bump google-github-actions/get-gke-credentials from 0.2.1 to 0.8.2  Bumps google-github-actions/get-gke-credentials from 0.2.1 to 0.8.2. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: google-github-actions/get-gke-credentials   dependency-type: direct:production   update-type: version-update:semver-minor ... Signed-off-by: dependabot[bot] support@github.com   Update .github/workflows/tpu-tests.yml   Don't use deprecated credentials input   Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com,0
Include link to bug report template in GitHub bug issue (#15270),0.437649,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
CI: drop unused tests (#15280),0.56507206,Cleaning up stale logger tests (#3490),,0
Extend Lite CPU coverage (#15279),0.45003098,        # Let Lite setup your dataloader(s),,0
Add support for local connect terminal context (#15241),0.49051625,Reuse existing commands when running connect more than once (#15471),  update   update   update   update   update   update   update   update   update   update   update   Update .azure/app-cloud-e2e.yml   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  update  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Disable package parametrizations until they are fixed (#15273),0.46744305,Changed the default of find_unused_parameters to False in DDP (#5185),  Disable package parametrizations until they are fixed   Apply suggestions from code review   Reword   What does this mean?   Missed some   Update .github/workflows/ci-app-examples.yml   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   Update .github/workflows/ci-app-examples.yml   trigger ci   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: thomas chaton thomas@grid.ai,0
Update lightning_cli_advanced_2.rst (#15257),0.64970285,- Removed the deprecated `LightningCLI` arguments ([#16380](https://github.com/Lightning-AI/lightning/pull/16380)),Co-authored-by: Mauricio Villegas mauricio_ville@yahoo.com,0
CI: fix example imports for App as standalone (#15260),0.48074347,Removed implicit ui testing with testing.run_app_in_cloud in favor of headless login and app selection (#16741),"  cleaning   adjust examples   fix some imports   fix imports   ci   fixing   cmd_install._install_app   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   bloody queues   queue   mock   queue   scope   win   Co-authored-by: otaj ota@lightning.ai Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
CI name for TPU (#15258),0.5058619,TPU training (#2708),Co-authored-by: Akihiro Nitta nitta@akihironitta.com,0
Update to the latest playwright container v1.27.1 (#15129),0.45930824,- Improved the error message for installing tensorboard or tensorboardx ([#17053](https://github.com/Lightning-AI/lightning/pull/17053)), Update to the latest playwright container v1.27.1  Co-authored-by: Jirka jirka.borovec@seznam.cz,0
Ci: fix install pkg name (#15259),0.49958882,"Fixing missing packaging package in dependencies, which was affecting the only installation to a very blank system.",  Ci: fix install pkg name   param ,0
Fix and refactor test_deepspeed_engine_is_steppable test (#15251),0.56153154,Update the logic to check for accumulation steps with deepspeed (#9826),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
CI: testing monolotic package (#15213),0.39767835,"Changed setup() and teardown() stage argument to take any of {fit,validate,test,predict} (#6386)",testing monolithic package,0
Update mypy version (#15161),0.46913075,python script.py \, update mypy version type-ignore-comments more mypy-fix import-fix Update Lite too simpler implementation for flatten dict Fix rich progress Simplify rich test True None  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
CI: apps (#15242),0.4157583,"@awaelchli, @carmocca, @Chizuchizu, @frankier, @SeanNaren, @tchaton", CI: apps fix imports req doctest SQLModel fsspec azure docs falek8  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Use user executable for pip installation (#15230),0.45928335,pip install rich,,0
Fixes to the K-fold loop example (#15225),0.5674336,Refactored Loops,,0
[App] Optimise Queues Calls (#15235),0.55104035,Resolved a bug about a race condition when sending the work state through the caller_queue (#14074),,0
Fix PyPI releasing (#15243),0.4523619,- The `pyDeprecate` dependency is no longer installed ([#14472](https://github.com/Lightning-AI/lightning/pull/14472)),,0
App: Remove the unsupported params for CloudCompute (#14852),0.5414729,Resolved a bug where the wrong client was passed to collect cloud logs (#14684),,0
Avoid initializing optimizers during deepspeed evaluation (#14944),0.74546146,Support for manual optimization with DeepSpeed (#7970),,1
publishing workflow (#15239),0.4433434,Resolved a bug where the work statuses will grow quickly and be duplicated (#13970), publishing workflow Apply suggestions from code review  Co-authored-by: otaj 6065855+otaj@users.noreply.github.com,0
Migrate TPU tests to GitHub actions (#14687),0.46032906,The point of this release is more bug fixes ahead of v 1.0.0. We now have CI tests on TPU thanks to @zcain117 from Google! :slightly_smiling_face:,"  Migrate TPU tests to GitHub actions   No working dir   Keep _target   Dont skip draft   CHECK_SLEEP   Not yet   Remove recurrent cleanup script   Set secrets   a step cannot have both the uses and run keys   Version $PYTHON_VER was not found in the local cache   can't load package ... ($GOPATH not set)   The set-env command is disabled   Try updating go   Match timeout   simplify path   More cleanup   Install coverage. Unmark draft   Update .github/workflows/ci-pytorch-test-tpu.yml   DEBUG echo   Revert ""DEBUG echo""   This reverts commit 4011856e6ea076e45fe40b942c20ee63ed7433f3.   More debug   SSH   Im stupid   Remove always()   Forgot some   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Luca Antiga luca.antiga@gmail.com",0
Fix LightningCLI parse_env and description in subcommands (#15138),0.91404927,- Fixed `LightningCLI` parse_env and description in subcommands ([#15138](https://github.com/Lightning-AI/lightning/pull/15138)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
[LAI] Solve circular imports for combined package (#15234),0.5470625,Wrapped imports for traceability (#13924),,0
Proper check for availability of bagua (#15220),0.40319455,The Bagua Strategy,,0
Single source for the mypy version (#15224),0.43713352,Compatibility for Python 3.10,,0
Optimize required checks when Lite tests are modified (#15232),0.47222844,"Simplified ""should run validation"" logic (#7682)",,0
Resolve collectives test issues (#15195),0.49411237,"Deprecated Accelerator collective API barrier, broadcast, and all_gather in favor of calling the TrainingTypePlugin collective API directly (#9677)",Co-authored-by: otaj ota@lightning.ai,0
Renamed Mount root_dir Argument to mount_path (#15228),0.45551226,Changed Checkpoint path parameter from filepath to dirpath (#1016),  renamed Mount argument   fix tests   Apply suggestions from code review   Co-authored-by: Luca Antiga luca.antiga@gmail.com  updated examples as well  Co-authored-by: Luca Antiga luca.antiga@gmail.com,0
[App] Automate missing requirements installation for CLI (#15198),0.59829324,- Added support for adding requirements to commands and installing them when missing when running an app command ([#15198](https://github.com/Lightning-AI/lightning/pull/15198),  update   update   update   update   update   update   update   wording   Co-authored-by: Mansy ahmed.mansy156@gmail.com   update   update   update   update   update   update   update   update   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Mansy ahmed.mansy156@gmail.com,0
Assistant fixes (#15221),0.46895242,Renamed several Trainer atributes:  (#567),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
[App] Enable help without running application (#15196),0.5135581,- Added support getting CLI help for connected apps even if the app isn't running ([#15196](https://github.com/Lightning-AI/lightning/pull/15196),,0
[App] Pass LightningWork to LightningApp (#15215),0.82997906,- Added support to pass a `LightningWork` to the `LightningApp` ([#15215](https://github.com/Lightning-AI/lightning/pull/15215),  update   update   update   Apply suggestions from code review   Apply suggestions from code review   Apply suggestions from code review   ll   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Luca Antiga luca.antiga@gmail.com,1
Assistant for Unified Package (#15207),0.43404794,Parsed local package versions (#13933), Update assistant and workflow files Update .actions/assistant.py  Co-authored-by: otaj 6065855+otaj@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: otaj ota@lightning.ai,0
Collective's PREMUL_SUM support with PyTorch 1.13 (#15201),0.5726939,Support for this strategy is marked as beta in PyTorch., Collective's PREMUL_SUM support with PyTorch 1.13 Fix test Skip under 1.13,0
releasing v1.8 RC (#15205),0.5169822,The point of this release is more bug fixes ahead of v 1.0.0. We now have CI tests on TPU thanks to @zcain117 from Google! :slightly_smiling_face:,,0
Fix srun availability check (#15211),0.48936266,Resolve TPU miss rendezvous (#6781),command -v Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,0
Avoid underscore suffix in filenames (#15189),0.40199238,"- ""Native"" suffix removal ([#16490](https://github.com/Lightning-AI/lightning/pull/16490))",,0
[App] Authentication for HTTP queue (#15202),0.7074796,- Added authentication to HTTP queue ([#15202](https://github.com/Lightning-AI/lightning/pull/15202)),[App] Authentication for HTTP queue (#15202),1
"[LAI] Rename instances of ""lightning_app"" and ""pytorch_lightning"" (#15208)",0.6723006,- Removed deprecated `pytorch_lightning.callbacks.lr_monitor.LearningRateMonitor.lr_sch_names` ([#13353](https://github.com/Lightning-AI/lightning/pull/13353)), rename instances of lightning_app and pytorch_lightning solve failing azure tests wrong runif import lit_app  Co-authored-by: Jirka jirka.borovec@seznam.cz,0
[LAI] Make lite tests safe for combined package (#15204),0.55147356,Release LAI docs as stable (#14250),Make lite tests safe for combined package Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,0
Fix root logger propagation (#15206),0.61092186,Removed logger_connector legacy code (#6733),,0
CI: enable CI run for PT 1.13 (#15128),0.44197732,Enabled cp (upload) at project level (#16631), Apply suggestions from code review enable CI to run for PT 1.13  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
[LAI] Bump minimal requirements (#15203),0.525431,Release LAI docs as stable (#14250), bump minimal requirement include app requirements in oldest,0
Separate the concept of a Drive from that of a Mount (#15120),0.2969132,We cleaned up the properties related to device indices (#14829).,  added mount class and configured it into compute config   added mount to the cloud runtime dispatcher   raise error if s3 bucket is passed to a drive telling the user to utilize mounts   added example for app   udpated tests   updated tests   addressed code review comments   fix bug   bugfix   updates'   code review comments   updates   fixed tests after rename   fix tests ,0
Validate SRUN variables when launching in SLURM (#15011),0.7171054,Fix an issue with the SLURM srun detection causing permission errors (#15485),,1
switch LAI deployment branch (#15194),0.5808923,Release LAI docs as stable (#14250), switch LAI deployment branch update links,0
Efficient gradient accumulation in LightningLite (#14966),0.68793935,- Added `LightningLite.no_backward_sync` for control over efficient gradient accumulation with distributed strategies ([#14966](https://github.com/Lightning-AI/lightning/pull/14966)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Add Database Component (#14995),0.54730445,- Added an Database Component ([#14995](https://github.com/Lightning-AI/lightning/pull/14995),,0
Lite: setting extras & fix CI (#15192),0.47069538,There were two different ways of importing Lite in <= 1.9.0, extras test.txt doctest Apply suggestions from code review Fix imports Oops  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Protect Flow from exception during request processing (#15187),0.5771524,- Added a try / catch mechanism around request processing to avoid killing the flow ([#15187](https://github.com/Lightning-AI/lightning/pull/15187),,0
Add support for command descriptions (#15193),0.7545649,- Added support for adding descriptions to commands either through a docstring or the `DESCRIPTION` attribute ([#15193](https://github.com/Lightning-AI/lightning/pull/15193),,1
Standardize Lite's filenames (#15058),0.45970553,lite = LightningLite(...),,0
Move warning before deepcopying the hyperparameters (#15132),0.6181117,- Moved the warning about saving nn.Module in `save_hyperparameters()` to before the deepcopy ([#15132](https://github.com/Lightning-AI/lightning/pull/15132)),,0
CI: use native oldest for package (#15094),0.55932033,Parsed local package versions (#13933), use native oldest for package print adjust,0
Setup phased transition away from PyTorch version-specific handling of cuda availability and device counting (#15133),0.69306916,"- To avoid issues with forking processes, from PyTorch 1.13 and higher, Lightning will directly use the PyTorch NVML-based check for `torch.cuda.device_count` and from PyTorch 1.14 and higher, Lightning will configure PyTorch to use a NVML-based check for `torch.cuda.is_available`. ([#15110](https://github.com/Lightning-AI/lightning/pull/15110), [#15133](https://github.com/Lightning-AI/lightning/pull/15133))",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Add docs for distributed inference (#15149),0.49181896,Inference mode support,,0
fix(app): remove full story from the app template (#15166),0.51395607,Included app templates to the lightning and app packages (#13731),,0
Resolve s3 drive issue where the root folder doesn't need to exist locally (#15127),0.40039855,pwd: Return the current folder in your Cloud Platform Filesystem,,0
Fix collective tests with PyTorch 1.13 (#15167),0.646655,Skipped testing with PyTorch 1.7 and Python 3.9 on Ubuntu (#11217),,0
Fix locally failing lite tests (#15137),0.5066437,"- Removed support for the experimental `PL_FAULT_TOLERANT_TRAINING` environment flag ([#16516](https://github.com/Lightning-AI/lightning/pull/16516), [#16533](https://github.com/Lightning-AI/lightning/pull/16533))",,0
"Update gym[classic_control] requirement from <0.25.2,>=0.17.0 to >=0.17.0,<0.26.3 in /requirements (#15136)",0.5827836,Removed the deprecated TrainerLoggingMixin class (#8609),Update gym[classic_control] requirement in /requirements Updates the requirements on gym[classic_control] to permit the latest version. - Release notes - Commits  updated-dependencies: - dependency-name: gym[classic_control]   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
"Update torchmetrics requirement from <0.9.3,>=0.7.0 to >=0.7.0,<0.10.1 in /requirements (#15135)",0.7098979,    * Some configuration errors that were previously raised as `MisconfigurationException`s will now be raised as `ProcessRaisedException` (torch>=1.8) or as `Exception` (torch<1.8),Update torchmetrics requirement in /requirements Updates the requirements on torchmetrics to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: torchmetrics   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,1
Bump JamesIves/github-pages-deploy-action from 4.4.0 to 4.4.1 (#15152),0.46577933,Resolved a bug where the install command was not installing the latest version of an app/component by default (#14181),Bumps JamesIves/github-pages-deploy-action from 4.4.0 to 4.4.1. - Release notes - Commits  updated-dependencies: - dependency-name: JamesIves/github-pages-deploy-action   dependency-type: direct:production   update-type: version-update:semver-patch ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Update 8-bit optimizer docs (#15155),0.5931741,Refactored optimizer (#4658),,0
"Update comet-ml requirement from <3.31.8,>=3.1.12 to >=3.1.12,<3.31.16 in /requirements (#15081)",0.52865887,Using .comet.config file for CometLogger (#1913),Update comet-ml requirement in /requirements Updates the requirements on comet-ml to permit the latest version.  updated-dependencies: - dependency-name: comet-ml   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
"Update neptune-client requirement from <0.16.8,>=0.10.0 to >=0.10.0,<0.16.10 in /requirements (#15082)",0.7686762,"It now supports neptune-client 0.16.16 and neptune >=1.0, and we have replaced the log() method with append() and extend().",Update neptune-client requirement in /requirements Updates the requirements on neptune-client to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: neptune-client   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,1
Finishing touches to the graveyard (#15123),0.44256657,(#16002),,0
docs: temp drop S3 from index (#15099),0.44912833,Changed IoU remove_bg bool to ignore_index optional int (#3098),Co-authored-by: awaelchli aedu.waelchli@gmail.com,0
Force NVML-based CUDA check in PyTorch 1.14+ (#15110),0.6999954,"- To avoid issues with forking processes, from PyTorch 1.13 and higher, Lightning will directly use the PyTorch NVML-based check for `torch.cuda.device_count` and from PyTorch 1.14 and higher, Lightning will configure PyTorch to use a NVML-based check for `torch.cuda.is_available`. ([#15110](https://github.com/Lightning-AI/lightning/pull/15110), [#15133](https://github.com/Lightning-AI/lightning/pull/15133))",,0
[docs] Docs for ColossalaiStrategy (#15093),0.50694907,Colossal-AI,,0
Fix batch normalization statistics in StochasticWeightAveraging (#15113),0.8117418,- Fixed batch normalization statistics calculation in `StochasticWeightAveraging` callback ([#14866](https://github.com/Lightning-AI/lightning/pull/14866)),,1
Bump Python version for mypy check (#15126),0.553612,Drop Python 3.6 support,,0
Use per-package imports in docs (#15124),0.58116776,Wrapped imports for traceability (#13924),"  fix import to be true stand-alone   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
Deprecate tuning enum and trainer properties (#15100),0.8267182,| Enum TrainerFn.TUNING                                    | 1.10             | No longer supported         |,,1
Prepare changelog for 1.8 (#15111),0.64034796,Full Changelog,  Prepare changelog for 1.8   update   update   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Update obsolete URL in HPU docs (#15112),0.57974845,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",,0
Update examples that require the run() method (#15096),0.537603,    def run(self):,Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
Update docs regarding deprecation window (#15089),0.7579403,Learn more about our deprecation window here.,,1
CI: rename Azure workflow file (#15097),0.43883824,Renames model steps (#1051),,0
Add inference_mode flag to Trainer (#15034),0.69613075,- Added `inference_mode` flag to Trainer to let users enable/disable inference mode during evaluation ([#15034](https://github.com/Lightning-AI/lightning/pull/15034)),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Update tuner docs (#15087),0.75609154,You can find more documentation about the tuner here.,,1
chore: add model as recommended parameter for validate() (#15086),0.6082176,"Removed deprecated ModelCheckpoint arguments prefix, mode=""auto"" (#6162)",,0
Error messages for removed DataModule hooks (#15072),0.61661434,Removed deprecated model hooks (#3980),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Error messages for removed Logger APIs (#15067),0.7616445,Removed logger_connector legacy code (#6733),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
Error messages for removed Trainer mixin methods (#15065),0.7362846,Removed TrainerProperties mixin and moved property definitions directly into Trainer (#9495),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
Error messages for the remaining callback hooks (#15064),0.7408196,We removed some Callback hooks that were ambiguous to use Removed deprecated callback hooks (#14834):,,1
Error messages for unsupported Trainer attributes (#15059),0.81363606,"Removed deprecated trainer attributes - on_cpu, on_tpu, use_tpu, on_gpu, use_dp, use_ddp, use_ddp2, use_horovod, use_single_gpu (#7501)",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,1
Exclude Graveyard from noqa (#15084),0.35911626,# 2. Remove the `hiddens` argument,,0
secrets docs (#14951),0.5042833,"Introducing encrypted secrets (#14612), a feature requested by Lightning App users :tada:!",  secrets docs   Update docs/source-app/glossary/secrets.rst   Co-authored-by: Yurij Mikhalevich yurij@grid.ai  Apply suggestions from code review  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   Update secrets.rst   links   Co-authored-by: Yurij Mikhalevich yurij@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka jirka.borovec@seznam.cz,0
Introduce the graveyard :headstone:  (#15061),0.39066955,(#16002),,0
Filter APEX future warning (#15078),0.4209049,Moved ignore_scalar_return_in_dp warning suppression to the DataParallelPlugin class (#7421),,0
Add ColossalAI strategy (#14224),0.4864229,"trainer = Trainer(strategy=""colossalai"")",Co-authored-by: HELSON c2h214748@gmail.com Co-authored-by: rohitgr7 rohitgr1998@gmail.com Co-authored-by: otaj ota@lightning.ai Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Various test fixes (#15068),0.64038587,Updated app testing (#16000),,0
Remove unused Lite code (#15000),0.48711437,clean up data reset (#3161), Remove unused Lite code Remove duplicate import Group variable Fix monkeypatch,0
Move the _scan_checkpoints utility function (#9312),0.6318103,Removed deprecated checkpoint argument filepath (#5321),Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Mark AMP test as flaky (#15055),0.52142155,enable_pl_optimizer=False by default to temporarily fix AMP issues (#5163),,0
Remove old testing artifacts (#15052),0.6113373,Cleaning up stale logger tests (#3490),,0
Remove deprecated callback hooks (#14834),0.84851927,Removed deprecated callbacks (#3979),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: otaj ota@lightning.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
[Lite] precision_plugin -> precision (#15001),0.8449635,Precision Plugins (#5718),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Missing steps in run on your own machine docs (#15033),0.42771402,move run_pretrain_routine -> setup_training (#3294),,0
"Update gym[classic_control] requirement from <0.24.2,>=0.17.0 to >=0.17.0,<0.25.2 in /requirements (#14005)",0.58656406,Removed the deprecated TrainerLoggingMixin class (#8609),Update gym[classic_control] requirement in /requirements Updates the requirements on gym[classic_control] to permit the latest version. - Release notes - Commits  updated-dependencies: - dependency-name: gym[classic_control]   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Trim flaky amp test (#15051),0.4663717,enable_pl_optimizer=False by default to temporarily fix AMP issues (#5163),,0
Support Slurm Autorequeue for Array Jobs (#15040),0.803901,- Added support for requeueing slurm array jobs ([#15040](https://github.com/Lightning-AI/lightning/pull/15040)),Signed-off-by: Max Ehrlich max.ehr@gmail.com Co-authored-by: awaelchli aedu.waelchli@gmail.com,1
LightningCLI add --config option after parser init (#15048),0.70588946,LightningCLI.init_parser now returns the parser instance (#8721),,1
Remove deprecated on_load/save_checkpoint behavior (#14835),0.8093369,The signature and behavior of the on_load_checkpoint and on_save_checkpoint callback hooks have changed (#14835):,Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
Resolve interactions between CUDA tests (#15042),0.5456897,Moved torch.cuda.set_device() to enable collective calls earlier in setup (#8312),,0
Add tuner callback docs (#15030),0.8590375,New Tuner Callbacks,,1
More tests for TPU accelerator in Lite (#14960),0.60844624,reduced accelerator selection (#3211),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
docs: replacement of method type_as in docs to Tensor.to (#15027),0.63329935,  * The `PrecisionPlugin.backward` signature changed: The `closure_loss` argument was renamed to `tensor`,,0
Fix Broken Link in lightning_app.core.work.LightningWork (#15032),0.5368219,- Added support to pass a `LightningWork` to the `LightningApp` ([#15215](https://github.com/Lightning-AI/lightning/pull/15215),,0
Introduce base collective and main subclasses (#15016),0.5283904,"Base classes (#1326, #1877)",Co-authored-by: otaj ota@lightning.ai,0
Use torch.testing.assert_close everywhere (#15031),0.54753816,Prevent modification of torch.backends.cudnn.benchmark when Trainer(benchmark=...) is not set (#13154),remove unnecessary version check,0
feat: allow root path to run the app on /path (#14972),0.810217,Allowed root path to run the app on /path (#14972), feat: add base path uvicorn fix arg Add prefix update with base_path fix replace base path with root path Apply suggestions from code review  Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Kaushik B kaushikbokka@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
[App] HTTP Removing Queue health check from Individual App (#15023),0.5155934,- Added authentication to HTTP queue ([#15202](https://github.com/Lightning-AI/lightning/pull/15202)),  removing expensive health check from Queue abstraction   removing expensive health check from Queue abstraction ,0
[App/Feature] HTTP Queues (#14978),0.5071198,- Added an HTTPQueue as an optional replacement for the default redis queue ([#14978](https://github.com/Lightning-AI/lightning/pull/14978),[App/Feature] HTTP Queues (#14978),0
Support ddp_fork strategy with native AMP by attempting NVML-based CUDA availability assessment (#14984),0.6807582,- Added native AMP support for `ddp_fork` (and associated alias strategies) with CUDA GPUs ([#14983](https://github.com/Lightning-AI/lightning/pull/14983)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Add LRFinder callback (#13802),0.4888246,- Removed deprecated callback hooks ([#14834](https://github.com/Lightning-AI/lightning/pull/14834)), add BatchSizeFinderCallback callback enable fast_dev_run test keep tune and remove early_exit move exception to setup Apply suggestions from code review  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Laverne Henderson laverne.henderson@coupa.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
CI: Use self-hosted Azure GPU runners (#14632),0.5466759,Support auto_select_gpus with the accelerator and devices API (#12608), move config Apply suggestions from code review  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: otaj 6065855+otaj@users.noreply.github.com,0
Fix bug in upload file endpoint (#14924),0.653704,Add support to upload files to the Drive through an asynchronous upload_file endpoint (#14703),Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: otaj 6065855+otaj@users.noreply.github.com,0
Optimizable structural typing (#14994),0.4727779,The brittle argument parsing utilities (#16708),"  update optimizer typing   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   forgot one file   update types   hopefully_last   zero grad not required as can also be done on model   consistency with other typing annotations   revert for deepspeed   Update deepspeed.py   Update deepspeed.py   revert for base plugin   Update types.py   add protocol inheritance   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update typing for precision plugin   Update module.py   typo   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
CI: Reuse clear cache (#14593),0.42235243,Changed overwrite to True (#16009), Remove existing weekly reset logic clear cache every week Use main tag,0
Added support for custom parameters in subclasses of SaveConfigCallback (#14998),0.86239505,- Added support for custom parameters in subclasses of `SaveConfigCallback` ([#14998](https://github.com/Lightning-AI/lightning/pull/14998)),,1
Fix fork tests failing in environments with CUDA available (#14982),0.7037351,- Added a more descriptive error message when attempting to fork processes with pre-initialized CUDA context ([#14709](https://github.com/Lightning-AI/lightning/pull/14709)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Refactor XLA and TPU checks across codebase (#14550),0.45807713,Updated logic for checking TPUs availability (#6767),,0
Fix GPU tests that fail to raise expected configuration error when run in a CUDA environment (#14983),0.5859719,Moved torch.cuda.set_device() to enable collective calls earlier in setup (#8312),,0
[app] Add CloudCompute ID serializable within the flow and works state  (#14819),0.61614704,- Fixed a bug with a default CloudCompute for Lightning flows ([#15371](https://github.com/Lightning-AI/lightning/pull/15371)),,0
[App/Improvement] Cleaning up Queue abstraction (#14977),0.5154732,Simplified messaging in cloud dispatch (#16160),[App/Improvement] Cleaning up Queue abstraction (#14977),0
Fix commands and API test (#14947),0.54090565,- fixed all the .test() calls,,0
Fix ReduceLROnPlateau update issue while resuming from a checkpoint (#14702),0.61492604,Resuming from checkpoints (#16167),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Remove the deprecated device_stats_monitor_prefix_keys (#14890),0.7134008,- Removed the deprecated `device_stats_monitor_prefix_metric_keys` ([#14890](https://github.com/Lightning-AI/lightning/pull/14890)),"  Remove the deprecated device_stats_monitor_prefix_keys   Added pr no to changelog.md   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: otaj 6065855+otaj@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com",1
fixes typing errors in rich_progress.py (#14963),0.5373415,pip install rich,,0
Trainer: fix support for non-distributed PyTorch (#14971),0.85242057,- Fixed `Trainer` support for PyTorch built without distributed support ([#14971](https://github.com/Lightning-AI/lightning/pull/14971)), Trainer: fix non-distributed use Update CHANGELOG,1
Fix TPU test CI (#14926),0.573853,Increased TPU check timeout from 20s to 100s (#5598),"  Fix TPU test CI   +x first   Lite first to uncovert errors faster   Fixes   One more   Simplify XLALauncher wrapping to avoid pickle error   debug   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Debug commit successful. Trying local definitions   Require tpu for mock test   ValueError: The number of devices must be either 1 or 8, got 4 instead   Fix mock test   Simplify call, rely on defaults   Skip OSError for now. Maybe upgrading will help   Simplify launch tests, move some to lite   Stricter typing   RuntimeError: Accessing the XLA device before processes have spawned is not allowed.   Revert ""RuntimeError: Accessing the XLA device before processes have spawned is not allowed.""   This reverts commit f65107ebf3e062d497f1033bfbbd59774f2d253f.   Alternative boring solution to the reverted commit   Fix failing test on CUDA machine   Workarounds   Try latest mkl   Revert ""Try latest mkl""   This reverts commit d06813aa67cc161879775e24be24b735e2925555.   Wrong exception   xfail   Mypy   Comment change   Spawn launch refactor   Accept that we cannot lazy init now   Fix mypy and launch test failures   The base dockerfile already includes mkl-2022.1.0 - what if we use it?   try a different mkl version   Revert mkl version changes   Co-authored-by: awaelchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com",0
Bump version of fsspec (#14975),0.52002513,Native FSDP replaces Fairscale FSDP (#16400),fsspec verbump,0
Update hpu mixed precision link (#14974),0.67132473,Mixed precision overhaul (#16783),Signed-off-by: Jerome janand@habana.ai,0
Remove unused gcsfs dependency (#14962),0.47521,Removed dependency on pandas (#736),,0
Find last checkpoints on restart (#14907),0.6871307,"Automatically reload the ""last"" checkpoint",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Close profiler when StopIteration is raised (#14945),0.5392196,Moved profilers to their own file (#7822),,0
Simplify root node resolution for SLURM environment (#14912),0.53713626,    root_node = os.environ['SLURM_NODELIST'].split(' ')[0],Co-authored-by: Seppo Enarvi seppo.git@marjaniemi.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Introduce primitives for input/output dtype conversion in Lite Precision (#14792),0.5142795,Enable mixed precision in DDPFullyShardedStrategy when precision=16 (#12965),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: otaj 6065855+otaj@users.noreply.github.com,0
Remove deprecated on_init_start_end (#14867),0.6784247,Removed deprecated early_stop_callback (#3982),Co-authored-by: awaelchli aedu.waelchli@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: otaj 6065855+otaj@users.noreply.github.com,0
feat: option to add custom meta tags to the UI container (#14915),0.46196193,Implemented ready for components (#16129),,0
Fix fork skip condition in GitHub workflows (#14955),0.4298526,"- Include a version suffix for new ""last"" checkpoints of later runs in the same directory ([#12902](https://github.com/Lightning-AI/lightning/pull/12902))",Co-authored-by: otaj 6065855+otaj@users.noreply.github.com,0
An instance of SaveConfigCallback should only save the config once (#14927),0.701702,"- `SaveConfigCallback` instances should only save the config once to allow having the `overwrite=False` safeguard when using `LightningCLI(..., run=False)` ([#14927](https://github.com/Lightning-AI/lightning/pull/14927))",,1
Move type annotation into init (#14943),0.47643688,moves init apex from LM to apex connector (#3923),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Fix mypy typing errors in pytorch_lightning/trainer/trainer.py (#14204),0.7700882,- Fixed wrong typehint for `Trainer.lightning_optimizers` ([#11155](https://github.com/PyTorchLightning/pytorch-lightning/pull/11155)),Co-authored-by: otaj ota@lightning.ai Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Fix typo in checkgroup.yml (#14959),0.47057682,"Removed deprecated ModelCheckpoint arguments prefix, mode=""auto"" (#6162)",Fix typo,0
Refactor launching tests to use our launchers (#14954),0.5126914,Updated app testing (#16000),,0
CI: Combine conda and full testing into a single workflow (#14387),0.5172373,- Full tests that run multiple models in different configs,"  Remove conda job   Remove conda job from readme   Remove conda jobs from checkgroup   Remove conda from docker builds   Remove base-conda dockerfile   Rewrite the strategy matrix while keeping equivalent   Run the workflow on this branch   Revert ""Rewrite the strategy matrix while keeping equivalent""   This reverts commit e54298d60e57cffbf8107890987be3fe4a006c77.   Add PyTorch versions   Run on draft and disable unrelated costly CI   Revert ""Run the workflow on this branch""   This reverts commit 51ed8b905d8926b630dce4817124bd486135d3ec.   tmp: Lightweight relevant CI   Fix CI pathfilter   Update matrix   Drop skipping logic   pip list   reorder pip list   tmp: lightweight ci   Install specified pytorch   Fix torch installation   Uncomment steps   Increase timeout   bad merge   Revert ""Run on draft and disable unrelated costly CI""   This reverts commit eb5dc5e6bd07ba801eea34111052e7d31701fddc.   Update checkgroup   Update docs and remove Python/PyTorch versions   Remove pip-list   Fail if wrong pytorch version installed   Add Python 3.8, PyTorch 1.9 job   tmp: remove azure jobs   tmp: remove dockers   tmp: remove others   Run all combinations   Include oldest   Exclude no Python 3.10 distributions   tmp: no concurrency   tmp: double timeout   Add pytest log reporter   Add pytest-reportlog   Fewer jobs   Revert ""tmp: no concurrency""   This reverts commit 4a7978dcb3499ce754306580412110b7a42920cd.   fix artifact name   Revert test reports   Revert unrelated changes   Revert unrelated changes   Add the combination of ex-conda jobs   Update checkgroup   revert timeout   remove conda job   revert docker build workflow file   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",0
Hydra changes to lightning-lite (#14950),0.59693706,Temporarily removed support for Hydra multi-run (#15737),Co-authored-by: awaelchli aedu.waelchli@gmail.com,0
Run CI helpers' doctests in a workflow (#14498),0.40943813,"Refactored training_batch + tests to verify correctness (#2327, #2328)",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com,0
Removed the deprecated datamodule_checkpointhooks (#14909),0.7403777,Removed deprecated checkpoint argument filepath (#5321),Co-authored-by: awaelchli aedu.waelchli@gmail.com Co-authored-by: otaj 6065855+otaj@users.noreply.github.com,1
Comet.ml logger - add usage tracking (#14906),0.6720881,Using .comet.config file for CometLogger (#1913),Co-authored-by: Aliaksandr.Kuzmik AliaksandrK@comet.ml,0
Update quick start guide with latest info (#14880),0.55386823,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",Co-authored-by: thomas chaton thomas@grid.ai,0
Fixed docstring for unwatch method (#14920),0.43685335,Removed a redundant warning with ModelCheckpoint(monitor=None) callback (#9875),,0
Fix wandb save_dir is not overridden by None dir when using CLI (#14878),0.89577866,- Fixed wandb `save_dir` is overridden by `None` `dir` when using CLI ([#14878](https://github.com/Lightning-AI/lightning/pull/14878)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Fix MissingFieldException in offline mode (#14919),0.6126403,- Fixed MissingFieldException in offline mode for the `NeptuneLogger()` ([#14919](https://github.com/Lightning-AI/lightning/pull/14919)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: otaj 6065855+otaj@users.noreply.github.com,0
Fairscale integration tests for Lite (#14921),0.5494478,"For reference, FairScale's implementation can be used with",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Align ddp and ddp-spawn strategies in setting up the environment (#11073),0.74966204,"decoupled DDP, DDP spawn (#3733, #3766, #3767, #3774, #3802, #3806)",Co-authored-by: Kushashwa Ravi Shrimali kushashwaravishrimali@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Call LightningDataModule.load_state_dict hook while restoring checkpoint using LightningDataModule.load_from_checkpoint (#14883),0.8532995,- Fixed a missing call to `LightningDataModule.load_state_dict` hook while restoring checkpoint using `LightningDataModule.load_from_checkpoint` ([#14883](https://github.com/Lightning-AI/lightning/pull/14883)),,1
Simplify bug report template (#14925),0.4697126,Fixing critical bugs in newly added hooks and hparams assignment.,Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com,0
Fairscale import updates (#14721),0.45413575,Native FSDP replaces Fairscale FSDP (#16400), fairscale imports refactor to avoid meta package build issue  Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: thomas chaton thomas@grid.ai,0
DeepSpeed integration tests for Lite (#14901),0.49551564,Support for manual optimization with DeepSpeed (#7970),,0
Prepare v1.8.0rc0 (#14918),0.5068715,[1.1.8] - 2021-02-08,,0
Clean up CODEOWNERS for PL and Lite (#14942),0.42436278,Silenced some warnings. verified ddp refactors (#3483),  Clean up CODEOWNERS for PL and Lite   Update ,0
Skip CircleCI trigger for forks (#14930),0.36283016,Enable None model checkpoint default (#3669),,0
Prepare CI to run on 3090s (#14910),0.40073133,"    batch_size=32,",,0
Self-review of the recent Trainer changes (#14916),0.7616743,New Trainer Arguments: Strategy and Devices,,1
Make internal torchscript check a class attribute (#14904),0.47157812,- Fixed an issue with comparing torch versions when using a version of torch built from source ([#17030](https://github.com/Lightning-AI/lightning/pull/17030)),,0
"Introduce ckpt_path=""hpc"" keyword for checkpoint loading (#14911)",0.8041429,"- Introduce `ckpt_path=""hpc""` keyword for checkpoint loading ([#14911](https://github.,com/Lightning-AI/lightning/pull/14911))",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Docs section for SLURM troubleshooting (#14873),0.7025124,Read more about our SLURM integration here.,Co-authored-by: Laverne Henderson laverne.henderson@coupa.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
Remove deprecated LightningIPUModule (#14830),0.8059943,- Removed the deprecated `LightningIPUModule` ([#14830](https://github.com/Lightning-AI/lightning/pull/14830)), Remove deprecated LightningIPUModule chlog fix import Fix 1.10 depr test  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Fix type hints of tuner/batch_size_scaling.py (#13518),0.74186367,tuner.scale_batch_size(...),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: otaj ota@lightning.ai Co-authored-by: Jirka jirka.borovec@seznam.cz,1
Upgrade HPU image to release 1.6.1 (#14932),0.46082652,Moveed HPU broadcast override to the HPU strategy file (#17011),,0
Improve building times of IPU docker image (#14934),0.45013863,Remove unnecessary intermediate layers in Dockerfiles (#5697),,0
Fix App tests (#14922),0.8400891,Updated app testing (#16000),,1
Fix pkg version issue while compiling docs (#14914),0.6101459,Pkg: fix parsing versions by @Borda in https://github.com/Lightning-AI/lightning/pull/15401," Revert ""Add BatchSizeFinder callback (#11089)""  This reverts commit d1a3a3ebf543629fc38783d9f5e39f2bfb6b37d7.  Revert ""Revert ""Add BatchSizeFinder callback (#11089)""""  This reverts commit 9cc4695925d0ec934009b31b96d3fdc3c1ffd579.   remove pl   add torch   add numpy   rm packages   add packages   add packages   import from PL   import from PL   always install PL for doctests   remove unnecessary requirements   always install PL in editable mode   once more   another attempt   maybe fix app test?   Redundant checkgroup path   Revert ""maybe fix app test?""   This reverts commit 8210a43ef499b29a00a7cd10d1d4c55d1fa6829d.   speed up install deps   damn this   damn trio   Co-authored-by: otaj ota@lightning.ai Co-authored-by: otaj 6065855+otaj@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com",0
Better error message when trying to re-initialize CUDA in forked subprocess (#14709),0.7173631,- Added a more descriptive error message when attempting to fork processes with pre-initialized CUDA context ([#14709](https://github.com/Lightning-AI/lightning/pull/14709)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Move logic to error out on deprecation warnings into conftest (#14902),0.71015954,Deprecation warning (#3844),,1
Add BatchSizeFinder callback (#11089),0.7274987,- Added `BatchSizeFinder` callback ([#11089](https://github.com/Lightning-AI/lightning/pull/11089)),"  add BatchSizeFinderCallback callback   temp rm from init   skip with lr_finder tests   restore loops and intergrate early exit   enable fast_dev_run test   add docs and tests   keep tune and remove early_exit   add more tests   patch lr finder   disable skip   force_save and fix test   mypy and circular import fix   fix mypy   fix   updates   rebase   address reviews   add more exceptions for unsupported functionalities   move exception to setup   chlog   unit test   address reviews   Apply suggestions from code review   update   update   mypy   fix   use it as a util func   license   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   mypy   mypy   review   fix   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   fix   updates   updates   fix import   Protect callback attrs   don't reset val dataloader   update test   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: otaj 6065855+otaj@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com",1
Fix double precision support in Lite (#14827),0.72492313,- It is no longer needed to call `model.double()` when using `precision=64` in Lightning Lite ([#14827](https://github.com/Lightning-AI/lightning/pull/14827)),,1
Enable quick-start-app-e2e (#14542),0.44172257,This is how you enable it:,Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
fix connect/ disconnect without arguments (#14877),0.6634383,Connect and Disconnect node (#16700),,0
Remove the deprecated trainer.*_ckpt_path (#14897),0.83774245,trainer.ckpt_path = None,,1
Integration tests for Precision in Lite (#14815),0.5714143,Mixed precision overhaul (#16783),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,0
Remove deprecated trainer_optimizer_mixin (#14887),0.7856487,Removed deprecated Trainer argument enable_pl_optimizer and automatic_optimization (#6163),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Removed deprecated trainer_lightning_optimizers (#14889),0.8609868,- Removed the deprecated `Trainer.lightning_optimizers` ([#14889](https://github.com/Lightning-AI/lightning/pull/14889)),,1
Remove the deprecated trainer.call_hook (#14869),0.8068098,"  * Removed class methods from Trainer: `default_attributes()`, `from_argparse_args()`, `parse_argparser()`, `match_env_arguments()`, `add_argparse_args()`",,1
Removed the deprecated trainer_data_loading_mixin (#14888),0.79834455,Removed trainer.reset_*_dataloader() methods (#16726),,1
Update root Makefile to run all test projects (#14881),0.3571359,Support compiling a module after it was set up by Fabric (#17529),,0
Remove the deprecated trainer.verbose_evaluate (#14884),0.8201527,Made evaluate method private >> Trainer._evaluate(...). (#1260),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Remove the deprecated run_stage (#14870),0.58625156,Removed deprecated early_stop_callback (#3982),,0
Remove deprecated trainer.should_rank_save_checkpoint (#14885),0.89843446,Removed should_rank_save_checkpoint property from Trainer (#9433),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
Remove the deprecated profile_iterable (#14864),0.6199451,- Deprecated `BaseProfiler.profile_iterable` ([#12102](https://github.com/PyTorchLightning/pytorch-lightning/pull/12102)), remove profile_iterable remove imports remove depricated api update changelog  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Update documentation for the basic skills tutorial level 2 on how to validate and test a model (#14874),0.5444789,"Deprecated model steps training_end, validation_end and test_end (#1051, #1056)",,0
fixed comet -> mlflow typo in visualize/experiment_managers docs (#14843),0.5588006,Updated mlflow with using resolve_tags (#6746),fixed comet -> mlflow typo Co-authored-by: Devin Conathan devin.conathan@libertymutual.com,0
Cleaning requirement + git fix (#14863),0.43771103,some minor cleaning, drop duplicate docs requirements skip empty dir mypy for #14861,0
Make Trainer readable and debuggable (3/n) (#14871),0.7692549,Trainer code became harder to follow,"  clean trainer 3/n   clean trainer 3/n   clean trainer 3/n   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  clean trainer 3/n  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",1
Make Trainer readable and debuggable (2/n) (#14862),0.7724941,Trainer code became harder to follow,"  clean trainer 2/n   clean trainer 2/n   clean trainer 2/n   clean trainer 2/n   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",1
Make Trainer Debuggable and understandable again (1/n) (#14861),0.7649655,Trainer code became harder to follow,"  clean trainer 1/n   clean trainer 1/n   clean trainer 1/n   clean trainer 1/n   clean trainer 1/n   clean trainer 1/n   clean trainer 1/n   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   clean trainer 1/n   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",1
update changelog after App 0.6.2 (#14853),0.55599946,Full Changelog,,0
App: Update changelog post release (0.6.1) (#14844),0.55594915,Full Changelog,,0
Update Changelog post v1.7.7 (#14851),0.5451746,Full Changelog,,0
Improving Hydra+DDP support (#11617),0.70061237,better support for Hydra,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: rohitgr7 rohitgr1998@gmail.com Co-authored-by: Jirka jirka.borovec@seznam.cz,1
Integrate Lite Precision into PL (#14798),0.6498081,- Integrated the Lite Precision plugins into the PL Precision plugins - the base class in PL now extends the `lightning_lite.precision.Precision` base class ([#14798](https://github.com/Lightning-AI/lightning/pull/14798)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Remove the deprecated agg_and_log_metrics (#14840),0.7428967,Removed deprecated metrics (#8586),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Remove deprecated precision plugin checkpoint hooks (#14833),0.71227664,The PrecisionPlugin.backward hooks no longer takes a should_accumulate argument (#8328), Remove deprecated precision plugin checkpoint hooks chlog,1
Remove deprecated device attributes from Trainer (#14829),0.7338599,Removed obsolete self._device in Trainer (#1849), Remove deprecated device attributes from Trainer changelog  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
Attempt to query device count via NVML (#14631),0.5325677,device_stats = DeviceStatsMonitor(),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Updated the structure and applied feedback (#14734),0.48893464,Refactored the logic around manual and automatic optimization inside the optimizer loop (#7526),,0
Fix attribute error in SWA when running with Tuner (#14836),0.5866292,- Fixed an attribute error when running the tuner together with the `StochasticWeightAveraging` callback ([#14836](https://github.com/Lightning-AI/lightning/pull/14836)), Fix attribute error in SWA when running with Tuner changelog add better test  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Remove deprecated use_amp attributes (#14832),0.6340003,Deprecated the Trainer.amp_backend property, Remove deprecated use_amp attributes chlog,0
Tests for fixed TypeError (#14821),0.50678915,Raised ValueError when a None value is self.log-ed (#7771), tests for 14809 Apply suggestions from code review  Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Clean-up dtype management (#14823),0.47025353,clean up data reset (#3161),Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,0
Correct false error message relating to Trainer(precision) (#14828),0.74439543,Removed trainer.fit() return value of 1. It has no return now (#7237),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Updates links to components in the Gallery (#14807),0.45725012,"In the previous release, we added shorthand notation support for registered components. In this release, we added a flag to automatically register all available components:",,0
Update device attribute in Lite's module wrapper (#14822),0.6114714,Move lightning module to correct device type when using LightningDistributedWrapper (#6070),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Remove the model argument from Lite's optimizer_step via structural typing (#14810),0.6367645,    # Let Lite setup your model and optimizer,Co-authored-by: awaelchli aedu.waelchli@gmail.com,0
Set path filters in favor of the required-job action (#14294),0.48570722,Removed deprecated BaseProfiler.output_filename arg from it and its descendants in favor of dirpath and filename (#9214),Co-authored-by: Jirka jirka.borovec@seznam.cz,0
change cluster creation log message (#14672),0.5868212,Cluster creation and deletion now waits by default [#15458,Co-authored-by: Laverne Henderson laverne.henderson@coupa.com,0
"pkg, strict version (#14814)",0.57489604,Parsed local package versions (#13933),,0
Fix icon encoding in PL App pretty-print (#14795),0.34660828,  * `pl.utilities.seed` ([#16422](https://github.com/Lightning-AI/lightning/pull/16422)),Co-authored-by: awaelchli aedu.waelchli@gmail.com,0
Fixed TypeError on 1.7.6 when torch.distributed unavailable (#14809),0.7558829,- Raised exception in `init_dist_connection()` when torch distributed is not available ([#10418](https://github.com/PyTorchLightning/pytorch-lightning/pull/10418)), Fixed TypeError on 1.7.6 when distributed unavailable changelog,1
[mypy] start introducing mypy on lightning_app (#14267),0.5077708,- Add a `JustPyFrontend` to ease UI creation with `https://github.com/justpy-org/justpy` ([#15002](https://github.com/Lightning-AI/lightning/pull/15002)), fix mypy lightning_cli errors,0
"Update pandas requirement from <=1.4.3,>1.0 to >1.0,<1.5.1 in /requirements (#14787)",0.61612064,Removed dependency on pandas (#736),Signed-off-by: dependabot[bot] support@github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Set MLFlowLogger status to FAILED when training raises an error (#12292),0.71943676,"- The `MLFlowLogger.finalize()` now sets the status to `FAILED` when an exception occurred in `Trainer`, and sets the status to `FINISHED` on successful completion ([#12292](https://github.com/Lightning-AI/lightning/pull/12292))",Co-authored-by: Ritsuki Yamada ritsuki.yamada@uzabase.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Add backward-compatibility for LightningLite in PL (#14735),0.69911623,Updated compatibility for LightningLite to run with the latest DeepSpeed 0.7.0 (13967),,0
Move src/pytorch_lightning/lite to src/lightning_lite (#14735),0.72193736,Removed deprecated code in pytorch_lightning.utilities.meta (#16038),,1
CI: HPU support v1.6.0 release (#14794),0.5829003,Device Stats Monitoring support for HPUs, Update hpu-tests.yml to support v1.6.0 release Update Dockerfile,0
Surface Neptune installation problems to the user (#14715),0.44931266,"It now supports neptune-client 0.16.16 and neptune >=1.0, and we have replaced the log() method with append() and extend().",,0
Remove unused mixed precision class (#14790),0.61099285,Mixed precision overhaul (#16783),,0
"Update wandb requirement from <0.13.2,>=0.10.22 to >=0.10.22,<0.13.4 in /requirements (#14771)",0.62222695,Removed wandb logger's finalize method (#1193),Update wandb requirement in /requirements Updates the requirements on wandb to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: wandb   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Added args parameter to LightningCLI to ease running from within Python (#14596),0.892029,- Added `args` parameter to `LightningCLI` to ease running from within Python ([#14596](https://github.com/Lightning-AI/lightning/pull/14596)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka jirka.borovec@seznam.cz,1
"Update arrow requirement from <=1.2.2,>=1.2.0 to >=1.2.0,<1.2.4 in /requirements (#14770)",0.50205564,Pkg: fix parsing versions by @Borda in https://github.com/Lightning-AI/lightning/pull/15401,Update arrow requirement in /requirements Updates the requirements on arrow to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: arrow   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Clean-up PL's Lite imports (#14769),0.47493756,There were two different ways of importing Lite in <= 1.9.0,Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Introduce Upload File endpoint (#14703),0.722521,Add support to upload files to the Drive through an asynchronous upload_file endpoint (#14703),  update   update   update   update   update   update   update   update   update   update ,1
Removes the old HPO content (#14754),0.44522238,"Cleaning (#5948, #5949, #5950)",  Removes the old HPO content   Remove source-lit symlinks for HPO   drop ref   Co-authored-by: Jirka jirka.borovec@seznam.cz,0
Fix TQDMProgressBar usage in logging.rst (#14768),0.7186496,The TQDM progress bar now correctly shows the on_epoch logged values on train epoch end (#11069),,1
Move accelerator-specific parsing functions with their accelerators (#14753),0.6819645,Refactored Accelerators and Plugins (#5743),Co-authored-by: awaelchli aedu.waelchli@gmail.com,0
Fix TensorBoardLogger creating redundant experiment when finalizing (#14762),0.7463808,Remove explicit flush from tensorboard logger (#2126),Co-authored-by: Kushashwa Ravi Shrimali kushashwaravishrimali@gmail.com,1
Remove check num_slurm_tasks in Lite (#14761),0.59379315,- Removed fall-back to `LightningEnvironment` when number of SLURM tasks does not correspond to number of processes in Trainer ([#14300](https://github.com/Lightning-AI/lightning/pull/14300)),,0
User-friendly exception when LightningWork.run() method is missing (#14759),0.8377267,Improved the error message when the LightningWork is missing the run method (#14759),,1
Fix property setter override by default setter (#14259),0.49430567,    @property,,0
User-friendly exception if root flow does not override the run() method (#14760),0.5458262,Used raise .. from .. to explicitly chain exceptions (#3750),,0
Fix test suite when running on MPS-enabled hardware (#14708),0.5262126,Disabled optimizers setup during testing (#3059),,0
Bump Lightning Cloud to 0.5.7 (#14757),0.6158209,  * `pl.utilities.cloud_io` ([#16438](https://github.com/Lightning-AI/lightning/pull/16438)), Bump Lightning Cloud to 0.5.7 :tada: Fix link in changelog  Co-authored-by: Sherin Thomas sherin@grid.ai,0
Explicitly set which Probot job to run (#14756),0.44171482,Disabled lr_scheduler.step() in manual optimization  (#6825),,0
fix: keep pre versions (#14752),0.65024304,Set version as today (#13906),,0
Standalone Lite: Update LightningLite (#14726),0.57064414,from lightning.lite import LightningLite,,0
Fix boring app test: debug=True when running on the cloud (#14751),0.77259177,Removed implicit ui testing with testing.run_app_in_cloud in favor of headless login and app selection (#16741),debug=True for boring_app (dynamic app also has debug=True) Co-authored-by: thomas chaton thomas@grid.ai,1
Remove deprecated NeptuneLogger code (#14727),0.69914997,- Removed deprecated support for the old `neptune-client` API in the `NeptuneLogger` ([#14727](https://github.com/Lightning-AI/lightning/pull/14727)),,0
Remove deprecated torch_distributed_backend logic (#14693),0.7264061,Setting the torch-distributed backend, Remove deprecated torch_distributed_backend logic changelog mention deprecated imports  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz,1
Use valid pypi versions for install in assistant (#14750),0.42509013,Add missing python-multipart dependency (#17244),Co-authored-by: Kushashwa Ravi Shrimali kushashwaravishrimali@gmail.com,0
Fix CC-bot for non-forks (#14710),0.39473584,The *_epoch_end hooks were removed (#16520),,0
CI: update HPU pool (#14733),0.5414451,Moveed HPU broadcast override to the HPU strategy file (#17011),,0
Remove silent behavior when num_slurm_tasks does not correspond to number of processes in Trainer (#14300),0.7028983,  * Removed the `Trainer(num_processes=...)` argument, simplify logic remove hpc update add changelog more tests update test  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
Combine the pip install commands in conda workflow (#14744),0.41739756,Add missing python-multipart dependency (#17244),,0
update codeowners (#14718),0.4607924,"Updated DeepSpeed ZeRO (#6546, #6752, #6142, #6321)",,0
Update installation (#14732),0.5216509,Setup: added requirement freeze for the next major version (#14480), Update installation  Updates to use python -m pip install -U lightning and adds troubleshooting note  Apply suggestions from code review  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Fix Google Tag Manager for the Lightning App docs (#14731),0.6125081,Update the Lightning App docs (#13537), updates the Lightning App docs theme to the one without Pytorch Lightning docs Google Tag Manager hardcoded sets the GTM id in the conf.py for Lightning App docs,0
New bug form (#14193),0.56274533,"At last, lots of bug fixes (see below).",Created a YAML form to replace the MD form for issues.  Minor update to form Removed MD issue form More updates Renamed the file  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Docs: fix link to title (#14730),0.48500064,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ", lower,0
Invalid cache before listing drive when collecting component names (#13971),0.43679893,We cleaned up the properties related to device indices (#14829).,Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Standalone Lite: Connector (#14692),0.56864864,group connectors (#3472),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,0
Use the setter in the children recursively (#14724),0.40760478,- `Loop.restarting=...` now sets the value recursively for all subloops ([#11442](https://github.com/PyTorchLightning/pytorch-lightning/pull/11442)),,0
docs: Clarify versioning and API stability (#14549),0.5660989,No API changes - We commit to backward compatibility in the 2.0 series,  mv releases to a standalone page   Include release_policy in index   Update policy   mv releases to a standalone page   Include release_policy in index   Update policy   Update title   remove release_policy.rst   Update versioning   syntax   simplify wording   Include examples that don't follow X+2 rule   syntax   update   consistency   rm noninformative statement   .   Reduce redundancy in the deprecation process   grammar?   consistency   Update docs/source-pytorch/versioning.rst   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Support Injecting Secrets into Apps Running in the Cloud (#14612),0.70752645,Add --secret option to CLI to allow binding Secrets to app environment variables when running in the cloud (#14612),"Adds a new '--secret' flag to 'lightning run app': lightning run app --cloud --secret MY_SECRET=my-secret-name app.py When the Lightning App runs in the cloud, the 'MY_SECRET' environment variable will be populated with the value of the referenced Secret. The value of the Secret is encrypted in the database, and will only be decrypted and accessible to the Flow/Work processes in the cloud. Co-authored-by: Sherin Thomas sherin@grid.ai Co-authored-by: Noha Alon nohalon@gmail.com Co-authored-by: thomas chaton thomas@grid.ai",1
Remove mentions of @awaelchli from source code (#14425),0.47205734,"@awaelchli, @carmocca, @rohitgr7",  Remove a todo from trainer regarding exception handling   Remove mentions of TODO(@awaelchli) from code   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Add code-owners for standalone Lite package (#14694),0.3802381,"    name=""my-package"",",  Add Lite codeowners   remove Borda on request ,0
Bump lightning cloud for memory leak fix (#14697),0.8040333,Resolved the memory leak issue with the Lightning Cloud package and bumped the requirements to use the latest version (#14697),Bump lightning cloud for memory leak fix (#14697),1
Tune the checkgroup config (#14712),0.4622175,Distributed group defaults to WORLD if None (#5125), Tune the checkgroup config Lite does not support HPU and IPU atm Skip HPU as the server is down,0
Drop gatekeeper CI checks (#14717),0.43465984,Configuration Validator (#9779),,0
[CLI] Fix cluster logs with over 5000 entries (#14458),0.6456272,Fixing 5000 log line limitation for Lightning AI BYOC cluster logs (#14458),,0
Standalone Lite: DDP Spawn Strategy Family (#14675),0.7493993,"decoupled DDP, DDP spawn (#3733, #3817, #3819, #3927)",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
Resolve minor formatting issue (#14706),0.48499513,"In addition, we fixed:",,0
Standalone Lite: DDP Strategy Family  (#14670),0.6402149,decoupled DDP2 (#3816),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Add the probot check-group action (#14621),0.39870134,group prepare data hook (#3212),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Add easy access to state_dict in Lite module wrapper (#14629),0.74511087,"- In Lightning Lite, state-dict access to the module wrapper now gets passed through to the original module reference ([#14629](https://github.com/Lightning-AI/lightning/pull/14629))",Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Standalone Lite: DataParallel Strategy (#14681),0.5680895,The slow and clunky data-parallel strategy (#16748),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Add documentation for trainer.datamodule and dataloaders of a trainer object (#14600),0.7443963,"Improved the error messaging when passing Trainer.method(model, x_dataloader=None) with no module-method implementations available (#14614)", Update trainer.rst Update datamodule.rst,1
Standalone Lite: Single Device TPU Strategy (#14663),0.54973036,TPU core selection,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Added support for downloading wandb artifacts in the WandbLogger  (#14551),0.6973425,Removed the deprecated sync_step argument from WandbLogger (#8763), Added functions to the WandbLogger to download and use artifacts without having to access the experiment object Updated CHANGLELOG.md Added suggested changes Delete test_script  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: awaelchli aedu.waelchli@gmail.com,0
Standalone Lite: Strategy base classes and registry (#14662),0.4380627,"The new version renders the registries and the auto_registry flag, introduced in 1.6.0, unnecessary, so we have deprecated them.","  add accelerator implementations to lite   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   fix imports   rename registry argument   fix test   fix tests   remove duplicated test   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   fix tests   deprecation   deprecations   flake8   fixes   add mps to runif   fix tests   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  Apply suggestions from code review  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci   remove more   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   local import   undo device stats :(   fix import   stupid typehints   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   more refactors :(   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   fix   rename init_device to setup_device   remove unused import   make uppercase to differentiate from class   trick test after moving import locally   add base classes and registry   reg   registry   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   tests   update to other branches   resolve todo(lite)   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   add very basic unit tests   fix name assignment   Update src/lightning_lite/strategies/parallel.py   Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com   remove deprecated property   remove pre- and post backward for now   protecting the registry utility function   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  remove unused import  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com",0
fix mypy typing errors in pytorch_lightning.utilities.data.py (#13901),0.6994882,"  * Removed functions from `lightning.pytorch.utilities.parsing`: `import str_to_bool()`, `str_to_bool_or_int()`, `str_to_bool_or_str()`",Co-authored-by: otaj ota@lightning.ai,0
fix: e2e with short form after signup (#14689),0.38045844,Deprecated the Trainer.disable_validation property in favor of not Trainer.enable_validation (#8291),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Fix TPU CI for non-forks (#14688),0.5328327,Increased TPU check timeout from 20s to 100s (#5598),,0
Lightning cloud client call with key word arguments (#14685),0.8754473,LightningCloud client calls to use keyword arguments instead of positional arguments (#14685),,1
Fix incorrect imports in lightning docs (#14678),0.59970516,Pkg: fix parsing versions by @Borda in https://github.com/Lightning-AI/lightning/pull/15401,,0
Update ipython[all] requirement from <8.4.1 to <8.5.1 in /requirements (#14671),0.5483265,Add missing python-multipart dependency (#17244),Updates the requirements on ipython[all] to permit the latest version. - Release notes - Commits  updated-dependencies: - dependency-name: ipython[all]   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Break hpu graphs into two for better performance (#14656),0.8445509,Break HPU Graphs into two parts (forward + backward as one and optimizer as another) for better performance (#14656),Signed-off-by: Jerome janand@habana.ai Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,1
CI: install more OS (#14660),0.40260255,Setup: added requirement freeze for the next major version (#14480),,0
Remove legacy examples from logging docs (#14686),0.58168656,Removed logger_connector legacy code (#6733),,0
Fix mypy errors attributed to pytorch_lightning.profilers.pytorch (#14405),0.82581675,Removed the deprecated pytorch_lightning.profiler.* classes in favor of pytorch_lightning.profilers (#16059), remove toml ref fix conflicts small fix move assertion  Co-authored-by: rohitgr7 rohitgr1998@gmail.com,1
Lightning App Fixes from Training Studio App dev (#14532),0.5635918,Update the Lightning App docs (#13537),  update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update ,0
Content for Lightning with iOS and Android (#14038),0.5360068,Lightning App,"  Content for Lightning with iOS and Android   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   make link clickable   Update docs/source-app/glossary/ios_and_android.rst   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: awaelchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",0
Remove deprecated BaseProfiler and AbstractProfiler (#14404),0.61008716,- Removed the deprecated `BaseProfiler` and `AbstractProfiler` classes ([#14404](https://github.com/Lightning-AI/lightning/pull/14404)),Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Akihiro Nitta nitta@akihironitta.com,0
(hot fix) Resolve Boring App (#14684),0.5446068,"Resolved LightningApp(..., debug=True) (#14464)",  resolve_boring_app   update   update   update   update   update   update   update   update   update   update   update   update   update ,0
feat: LAI2-10296 check if user has sufficient credits to run an app from the cli (#14285),0.4865861,Introducing CLI commands for apps (#13602)!,,0
Update CODEOWNER of CI/CD (#14676),0.35212725,"If we forgot someone due to not matching the commit email with the GitHub account, let us know :]",Update CODEOWNER,0
Better error message when dataloader and datamodule is None (V2) (#14637),0.64582616,- Show a better error message when a custom `DataLoader` implementation is not well implemented and we need to reconstruct it ([#10719](https://github.com/PyTorchLightning/pytorch-lightning/pull/10719)),,0
Avoid warning when cloning tensor in self.log (#14599),0.5945571,- Show a better error message when a Metric that does not return a Tensor is logged ([#13164](https://github.com/Lightning-AI/lightning/pull/13164)),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Requirements: try Traitlets >= 5.3.0 (#14679),0.48203355,- Removed the deprecated `TrainerOptimizersMixin` ([#14887](https://github.com/Lightning-AI/lightning/pull/14887)), Traitlets >= 5.3.0 Apply suggestions from code review  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
CI: run CLI after install (#14659),0.45744574,"Anywhere in your program, you can now call the CLI directly:",,0
[CLI] Move storage from app prefix to project/app prefix (#14583),0.7995232,Application storage prefix moved from app_id to project_id/app_id (#14583),  Move storage from app prefix to project/app prefix: checking and legacy support   Changelog message   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
Remove deprecated LoggerCollection (#14283),0.7288151,Removed logger_connector legacy code (#6733),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
Update traitlets requirement from <5.2.0 as strict in /requirements (#14666),0.41372764,Setup: added requirement freeze for the next major version (#14480), Update traitlets requirement from <5.2.0 as strict in /requirements  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Make the SLURM Preemption/Timeout Signal Configurable (#14626),0.6905122,Control SLURM's re-queueing, Add parameter to change the preemption signal Make the signal connector use the custom signal from SLURMEnvironment  Signed-off-by: Max Ehrlich max.ehr@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Remove the deprecated weights_save_path Trainer argument (#14424),0.87594604,"Deprecated Trainer attribute ckpt_path, which will now be set by weights_save_path (#2681)",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Bump tj-actions/changed-files from 29.0.3 to 29.0.4 (#14650),0.43128684,"- Include a version suffix for new ""last"" checkpoints of later runs in the same directory ([#12902](https://github.com/Lightning-AI/lightning/pull/12902))",Bumps tj-actions/changed-files from 29.0.3 to 29.0.4. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: tj-actions/changed-files   dependency-type: direct:production   update-type: version-update:semver-patch ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
(app): Add load_state_dict and state_dict (#14100),0.7548636,Called on_load_checkpoint before loading state_dict (#4057),Co-authored-by: manskx ahmed.mansy156@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz,1
Removed from_argparse_args tests in test_cli.py (#14597),0.64056784,argparse_utils >> argparse,,0
Update psutil requirement from <=5.9.1 to <5.9.3 in /requirements (#14665),0.617127,The psutil package is now required for CPU monitoring (#17010),Updates the requirements on psutil to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: psutil   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
"Update docutils requirement from <0.19,>=0.16 to >=0.16,<0.20 in /requirements (#14664)",0.5374435,Setup: added requirement freeze for the next major version (#14480),Update docutils requirement in /requirements Updates the requirements on docutils to permit the latest version.  updated-dependencies: - dependency-name: docutils   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Remove skipping logic in PL CI (#14565),0.4565708,Simplify the PL examples structure (shallower and more readable) (#1247),Drop skipping logic,0
Removes timeout from streamlit e2e test (#14667),0.47050053,Increased TPU check timeout from 20s to 100s (#5598), Removes timeout from streamlit e2e test We have a timeout on the app view which waits for the button but it causes a refresh on the page which causes playwright to miss the button on each refresh. we can remove the timeout altogether since we have a time limit on the test itself in the CI setup  Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Add troubleshooting section to MPS docs (#14642),0.44818336,Allow logging of metrics together with hparams (#1630),Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,0
fixes mypy errors in trainer/supporters.py (#14633),0.62064326,Removed deprecated property Trainer.running_sanity_check in favor of Trainer.sanity_checking (#9209)," fixes mypy errors in trainer/supporters.py Fxes mypy error when accessing ""init"" directly add an assertion in lr_finder.py Make init calls reset in TensorRunningAccum Fixes formatting Add self.window_length to __init__  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
Fix mypy errors in pytorch_lightning/cli.py (#14653),0.73224753,Removed the deprecated pytorch_lightning.utilities.cli module in favor of pytorch_lightning.cli (#16116),,1
(app) Resolve a bug where the state changes isn't detected properly (#14465),0.5424459,"    # previously, only the state for this callback was passed in as argument",Co-authored-by: Mansy ahmed.mansy156@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Standalone Lite: Accelerators (#14578),0.71536666,Refactored into accelerator module:,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
set next App dev (#14609),0.46729046,"    devices=8, ",,0
"Run CircleCI with the HEAD sha, not the base (#14625)",0.43223688,def configure_sharded_model(self):," Run CircleCI with the HEAD sha, not the base Different solution",0
Standalone Lite: Precision Plugins (#14547),0.83223295,Precision Plugins (#5718),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,1
(app) Make Logging DEBUG mode lazy (#14464),0.57544005,Change default logger to a dedicated one (#1064),,0
Set running_torchscript recursively (#14657),0.61420804,        scheduler = torch.optim.lr_scheduler.OneCycleLR(,  Set running_torchscript recursively   CHANGELOG ,0
Standalone Lite: Launchers (#14555),0.4177542,There were two different ways of importing Lite in <= 1.9.0,Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Bump docker/build-push-action from 1.1.0 to 3 (#14651),0.5232254,"Show a message when BuildConfig(dockerfile=""..."") is passed but a Dockerfile file is already present in the Work (#15799)"," Bump docker/build-push-action from 1.1.0 to 3.1.1  Bumps docker/build-push-action from 1.1.0 to 3.1.1. - Release notes - Commits  updated-dependencies: - dependency-name: docker/build-push-action   dependency-type: direct:production   update-type: version-update:semver-major ... Signed-off-by: dependabot[bot] support@github.com  Revert ""Bump docker/build-push-action from 1.1.0 to 3.1.1""  This reverts commit 05f9bfb084fd00657d4396214938f448a3f9b143.  use v3  Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com",0
Integrate lightning_utilities is_overridden (#14620),0.7755893,- Integrate the `lightning_utilities` package (,,1
Drop duplicate docs requirements  (#14644),0.5574889,Drop duplicate metrics (#5014),Delete base.txt Co-authored-by: thomas chaton thomas@grid.ai,0
Add back-compatibility for checkpoint io plugins in pl/plugins/io (#14519),0.74484444,CheckpointIO Plugins,,1
Move checkpoint io plugins from pl/plugins/io to lite/plugins/io (#14519),0.64197356,CheckpointIO Plugins,,0
Standalone Lite: Cluster Environments (#14509),0.74199295,Support running on multiple clusters (#16016),,1
simplify storage import  (#14638),0.458804,Simplified data loader.,"  docs   t1   simple import   simple import   simple import   simple import   Update version.py   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
Remove deprecated test_tube dependency from environment.yml (#14617),0.48326215,- Removed the deprecated `TestTubeLogger` ([#12859](https://github.com/Lightning-AI/lightning/pull/12859)),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Bump carmocca/probot from 1 to 2 (#14336),0.43156886,@carmocca @jona-0 @kaushikb11 @Raalsky @rohitgr7 ,,0
Add bagua support for CUDA 11.6 images (#14529),0.5065314,"fabric = Fabric(accelerator=""cuda"", devices=8, strategy=""ddp"")",  Add support for bagua-cuda116   Remove bagua-cuda115 from installation   Co-authored-by: Akihiro Nitta nitta@akihironitta.com,0
Use TorchVision's Multi-weight Support and Model Registration API on Lightning (#14567),0.48776406,- Enabled `torch.inference_mode` for evaluation and prediction ([#12715](https://github.com/Lightning-AI/lightning/pull/12715)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
"Update s3fs requirement from <=2022.7.1,>=2022.5.0 to >=2022.5.0,<2022.8.3 in /requirements (#14585)",0.47447932,Setup: added requirement freeze for the next major version (#14480),Update s3fs requirement in /requirements Updates the requirements on s3fs to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: s3fs   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Avoid instantiating every accelerator in the registry (#14591),0.6780955,Registering Custom Accelerators, Avoid instantiating every accelerator in the registry when listing available ones,0
Update content for S3 persistent storage (#14060),0.40704438,Add support to upload files to the Drive through an asynchronous upload_file endpoint (#14703), Update content for S3 persistent storage Updates based on feedback Fix unstructured validation issue Updates based on feedback  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com Co-authored-by: Luca Antiga luca.antiga@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
CI: hotfix last version (#14627),0.48450452,The point of this release is more bug fixes ahead of v 1.0.0. We now have CI tests on TPU thanks to @zcain117 from Google! :slightly_smiling_face:,LooseVersion was not correctly evaluation RC and set it as last even though the full release is out...,0
Update checkgroup config (#14587),0.4697305,Configuration Validator (#9779),,0
Use the pull_request_target workflow event (#14603),0.34902894,"We've added the ability to turn the automatic resubmission on or off when a job gets interrupted by the SLURM controller (via signal handling). Users who prefer to let their code handle the resubmission (for example, when submitit is used) can now pass:", Use the pull_request_target workflow event Minor  cleanup ready_for_review,0
Move device parser tests inside Lite (#14586),0.63529533,"device parser (#3400, #3405)",,0
Refactor _get_rank utility to take strategy instead of trainer (#14546),0.6096165,"trainer = Trainer(strategy=ColossalAIStrategy(placement_policy=""cpu"", ...))",,0
PL: update changelog post 1.7.5 release (#14570),0.5962684,Full Changelog,Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Update fastapi requirement from <=0.79.0 to <0.83.0 in /requirements (#14576),0.49728197,Setup: added requirement freeze for the next major version (#14480),Updates the requirements on fastapi to permit the latest version. - Release notes - Commits  updated-dependencies: - dependency-name: fastapi   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
"Update neptune-client requirement from <0.16.4,>=0.10.0 to >=0.10.0,<0.16.8 in /requirements (#14582)",0.7685437,"It now supports neptune-client 0.16.16 and neptune >=1.0, and we have replaced the log() method with append() and extend().",Update neptune-client requirement in /requirements Updates the requirements on neptune-client to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: neptune-client   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,1
Fix(ci) ONC-114: reduce the ci load by only installing lmdb in tests (#14581),0.46564105,"Removed experimental fault-tolerance support (#16516, #16533)",reduce the ci load by only installing lmdb in tests,0
Standalone Lite: Remaining Utilities (#14492),0.48210278,There were two different ways of importing Lite in <= 1.9.0,Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Laverne Henderson laverne.henderson@coupa.com Co-authored-by: Felonious-Spellfire felonious.spellfire@gmail.com,0
Add compatibility when torch.distributed is not available (#14454),0.71855056,- Raised exception in `init_dist_connection()` when torch distributed is not available ([#10418](https://github.com/PyTorchLightning/pytorch-lightning/pull/10418)),Co-authored-by: Piero Coronica piero.coronica@mpcdf.mpg.de Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Remove old test artifacts (#14574),0.6008651,Cleaning up stale logger tests (#3490),,0
Add path filters to the TPU job (#14543),0.48180985,moved TPU xxx_step to backend (#3118),,0
Integrate lightning_utilities get_all_subclasses (#14575),0.6314539,- Integrate the `lightning_utilities` package (,,0
Integrate with lightning_utilities.core.enums (#14558),0.71019274,- Integrate the `lightning_utilities` package (,,1
Docs [Fix]: use bytes instead of strings while writing (#14505),0.3895176,Replaced _DataModuleWrapper with __new__ (#7289),  Fix doc examples: use bytes instead of strings while writing   Add a note (comment)   nit   Update any_server.rst   Update docs/source-app/workflows/add_server/any_server.rst   Update docs/source-app/workflows/add_server/any_server.rst   Update docs/source-app/workflows/add_server/any_server.rst   Apply suggestions from code review   Co-authored-by: Laverne Henderson laverne.henderson@coupa.com Co-authored-by: Ethan Harris ethanwharris@gmail.com Co-authored-by: thomas chaton thomas@grid.ai,0
"Update fsspec[http] requirement from !=2021.06.0,<2022.6.0,>=2021.05.0 to >=2021.05.0,!=2021.06.0,<2022.8.0 in /requirements (#14288)",0.47340864,Setup: added requirement freeze for the next major version (#14480),Update fsspec[http] requirement in /requirements Updates the requirements on fsspec[http] to permit the latest version. - Release notes - Commits  updated-dependencies: - dependency-name: fsspec[http]   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Integrate lightning_utilities.core.rank_zero (#14556),0.72594833,- Integrate the `lightning_utilities` package (,,1
Typo in major heading seen by newcomers (#14501),0.44592768,"In addition, we fixed:"," Typo in major heading seen by newcomers  Correct typo in one of the first major headings newcomers to Lightning see when they are considering migrating their code to use Lightning. I know this is a trivial change in terms of the text change itself, but I really think it's valuable for one of the most important landing pages that users first investigating Lightning see - to have rock-solid, professional text without obvious typos. Here was a typo in the main heading itself. I suggest fixing it straightaway via this PR. Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com",0
Cleanup Lite apply_funcs utilitites (#14560),0.5567629,  * `pl.utilities.apply_func` ([#16413](https://github.com/Lightning-AI/lightning/pull/16413)),,0
Integrate lightning_utilities.core.apply_func (#14537),0.7770364,- Integrate the `lightning_utilities` package (,,1
Integrate lightning_utilities.core.imports (#14475),0.8553538,- Integrate the `lightning_utilities` package (,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Mark the lite DeviceDtypeModuleMixin as protected (#14548),0.6081356,"- Deprecated the internal `pl.core.mixins.DeviceDtypeModuleMixin` class ([#14511](https://github.com/Lightning-AI/lightning/pull/14511), [#14548](https://github.com/Lightning-AI/lightning/pull/14548))",,0
Add auto wrapping support for DDPFullyShardedStrategy (#14383),0.7971691,- Added support for auto wrapping for `DDPFullyShardedStrategy` ([#14383](https://github.com/Lightning-AI/lightning/pull/14383)),,1
Deprecate pl/utilities/apply_func (#14516),0.68609434,  * `pl.utilities.apply_func` ([#16413](https://github.com/Lightning-AI/lightning/pull/16413)),,0
move pl/utilities/apply_func.py to pl/utilities/apply_func.py (#14516),0.5846438,  * `pl.utilities.apply_func` ([#16413](https://github.com/Lightning-AI/lightning/pull/16413)),,0
Setup: add requirement freeze for next major version (#14480),0.89549387,Setup: added requirement freeze for the next major version (#14480),,1
fixes typing errors in auto_restart.py (#13904),0.5150943,Removed support for self.log()ing a dictionary (#16389),Co-authored-by: otaj 6065855+otaj@users.noreply.github.com Co-authored-by: rohitgr7 rohitgr1998@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Bump actions/checkout from 2 to 3 (#14540),0.4179063,Increased TPU check timeout from 20s to 100s (#5598),Bumps actions/checkout from 2 to 3. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: actions/checkout   dependency-type: direct:production   update-type: version-update:semver-major ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Bump tj-actions/changed-files from 29.0.1 to 29.0.3 (#14541),0.43004358,"- Include a version suffix for new ""last"" checkpoints of later runs in the same directory ([#12902](https://github.com/Lightning-AI/lightning/pull/12902))",Bumps tj-actions/changed-files from 29.0.1 to 29.0.3. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: tj-actions/changed-files   dependency-type: direct:production   update-type: version-update:semver-patch ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Add path filters for azure PR jobs (#14544),0.47772932,# guarantees unique ports across jobs from same grid search,,0
Deprecate pl/utilities/cloud_io.py (#14515),0.63309926,  * `pl.utilities.cloud_io` ([#16438](https://github.com/Lightning-AI/lightning/pull/16438)),,0
move pl/utilities/cloud_io.py to lite/utilities/cloud_io.py (#14515),0.62739265,- Deprecated all functions in `pytorch_lightning.utilities.cloud_io` in favor of `lightning_lite.utilities.cloud_io` ([#14515](https://github.com/Lightning-AI/lightning/pull/14515)),,0
Deprecate pl/utilities/xla_device (#14514),0.71482337,xla_device_utils >> xla_device,,1
move pl/utilities/xla_device.py to lite/utilities/xla_device.py (#14514),0.71833444,xla_device_utils >> xla_device,,1
Deprecate pl/core/mixins/device_dtype_mixin and update imports (#14511),0.89560705,| Import pl.core.mixins.DeviceDtypeModuleMixin                                                           | 1.10             | No longer supported |,,1
Move test_dtype_device_mixin to lite (#14511),0.7362807,| Import pl.core.mixins.DeviceDtypeModuleMixin                                                           | 1.10             | No longer supported |,,1
Move pl/core/mixins/device_dtype_mixin.py to lite/utilities/device_dtype_mixin.py (#14511),0.80245036,| Import pl.core.mixins.DeviceDtypeModuleMixin                                                           | 1.10             | No longer supported |,,1
Squeeze tensor while logging (#14489),0.7308922,Squeezed tensor values when logging with LightningModule.log (#14489),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
pkg: include lite in PL (#14536),0.6465787,There were two different ways of importing Lite in <= 1.9.0, pkg: include lite in PL Apply suggestions from code review ci: nb dirs  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Add path filters for some non-required jobs (#14539),0.42878816,# guarantees unique ports across jobs from same grid search,,0
Fixed WandbLogger save_dir is not set after creation (#12748) (#14326),0.6826451,Removed the deprecated sync_step argument from WandbLogger (#8763),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Estimate stepping batches with max_steps if max_epochs is not set (#14317),0.6778969,Trainer.estimated_stepping_batches,Co-authored-by: Roberto Estevão robertode@microsoft.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Use a standalone test symlink for Lite (#14502),0.44688213,There were two different ways of importing Lite in <= 1.9.0,,0
[CLI] Fix status message on cluster creation (#14477),0.7503172,Cluster creation and deletion now waits by default [#15458, Fix message on BYOC cluster creation  Co-authored-by: thomas chaton thomas@grid.ai,1
Remove deprecated test_tube dependency (#14513),0.6106428,Deprecated the TestTubeLogger (#9065),,0
Pin protobuf (#14512),0.37977093,Barebones Trainer mode (#16854),,0
Updated basic debugging (#14488),0.5628596,DDP Debugging Improvements,Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Standalone Lite CI setup (#14451),0.45445073,        # Let Lite setup your dataloader(s),Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Clean up fairscale imports (#14476),0.48657152,clean up data reset (#3161),,0
Precise description of reload_dataloaders_every_n_epochs (#14245),0.57253474,Deprecated reload_dataloaders_every_epoch argument of Trainer in favor of reload_dataloaders_every_n_epochs (#5043),,0
Mark stage argument in hooks as required (#14064),0.5527157,Updated hooks arguments - breaking for setup and teardown (#2850),Co-authored-by: rohitgr7 rohitgr1998@gmail.com,0
[App] Handling s3 rate limiting in framework (#14411),0.37932825,- Disable loading dataloades if corresponding `limit_batches=0` ([#11576](https://github.com/PyTorchLightning/pytorch-lightning/pull/11576)),"bump of fsspec and s3fs to version supporting retry on ""SlowDown"" response",0
Update changelog after v1.7.4 release (#14479),0.6748089,Full Changelog,,0
[App][CLI] Fix lightning cli --version (#14433),0.6835653,- Removed the deprecated `LightningCLI` arguments ([#16380](https://github.com/Lightning-AI/lightning/pull/16380)), [App][CLI] Fix lightning cli --version,0
Dependency pinning (#14463),0.472624,Setup: added requirement freeze for the next major version (#14480), deps pinned Apply suggestions from code review  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Reset dataloaders on failure in tuner (#14372),0.69170046,Reset val_dataloader in tuner/batch_size_scaling (#9857),,0
Introduce lightning connect (#14452),1.0000001,Introduce lightning connect (#14452),"  update   update   update   update   Review of content   Formatting updates   Fomatting updates   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Updates based on new commits   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   update   update   Introduce lightning connect (#14183)   Co-authored-by: Luca Antiga luca.antiga@gmail.com Co-authored-by: Felonious-Spellfire felonious.spellfire@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Luca Antiga luca.antiga@gmail.com",1
CI: Azure clear workspace (#14460),0.43275172,Resolved a bug where the wrong client was passed to collect cloud logs (#14684),,0
Remove the unused pyDeprecate dependency (#14472),0.7440146,- The `pyDeprecate` dependency is no longer installed ([#14472](https://github.com/Lightning-AI/lightning/pull/14472)), Remove the unused pyDeprecate dependency CHANGELOG,1
Remove deprecated rank zero utilities (#14471),0.5868901,Removed deprecated: (#2760),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
update rng state save/load test to also run on cuda gpu (#14396),0.6674659,- Included `torch.cuda` rng state to the aggregate `_collect_rng_states()` and `_set_rng_states()` ([#14384](https://github.com/Lightning-AI/lightning/pull/14384)),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
CI: Reuse check schema (#14469),0.3691499,Changed type checker with explicit cast of ref_model object (#4457), rm _check-shema.yml Reuse devtools' check schema,0
Remove deprecated support for passing the warning category positionally (#14470),0.709774,- Removed deprecated support for passing the `rank_zero_warn` warning category positionally ([#14470](https://github.com/Lightning-AI/lightning/pull/14470)),,1
E2E fix for custom base image (#14468),0.36451805,    # 2: Convert the image into a PIL Image to bytes and encode it with base64,  new custom base image   image tag ,0
Cleanup TPU CI script error management (#14389),0.49084356,Resolve TPU miss rendezvous (#6781),,0
[CLI] Cluster logs CLI improvements: new log labels + test coverage increasing (#14459),0.57105446,Cleaning up stale logger tests (#3490),  Cluster logs improvements   Unit tests added   Labels for processing deletion errors ,0
CI: skip examples with draft (#14453),0.38092142,Disable saving checkpoints if not trained (#4372),,0
[App] fix panel requirements (#14450),0.47934896,Update the Lightning App docs (#13537),"  update base requirements   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   try main   Apply suggestions from code review   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   extract into separate function   drop   up   up   optional   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  .  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",0
CI: update TPU docker (#14448),0.5023103,moved TPU xxx_step to backend (#3118),,0
ci: drop group probot (#14456),0.43263376,Cleanup cluster waiting (#16054),,0
CI: set probot timeout (#14455),0.4416718,Increased TPU check timeout from 20s to 100s (#5598),,0
prepare space for fused docs (#14160),0.48457587,Docs improvements,  copy app conf   ci + req.   script symlink   wip   keep only App   add also PL   lightning   artifact ,0
update notebooks (#14340),0.41763335,"Updated DeepSpeed ZeRO (#6546, #6752, #6142, #6321)",,0
CI: add e2e cron job (#14402),0.39937478,    --lr_scheduler=OneCycleLR \, add e2e cron job trigger workflow_dispatch  Co-authored-by: Mansy ahmed.mansy156@gmail.com Co-authored-by: thomas chaton thomas@grid.ai,0
Unify rank zero messaging utilities (#14116),0.54888856,Rank-zero only EarlyStopping messages,,0
PanelFrontend and Panel Web UI Intermediate docs (#13531),0.5875803,Adds PanelFrontend to easily create complex UI in Python (#13531),Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Marc Skov Madsen masma@orsted.com Co-authored-by: Laverne Henderson laverne.henderson@coupa.com Co-authored-by: Felonious-Spellfire felonious.spellfire@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Mansy ahmed.mansy156@gmail.com Co-authored-by: Jirka jirka.borovec@seznam.cz,0
Azure: local id for e2e (#14432),0.3222856,Resolved a bug where the wrong client was passed to collect cloud logs (#14684),,0
Update has_len_all_ranks to use Strategy.root_device (#12144),0.5304793,Renamed Strategy.reduce to Strategy.all_reduce in all strategies (#16370),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Update GHA job names (#14400),0.48266664,    # use the last 4 numbers in the job id as the id, update CJ job names groups filter Apply suggestions from code review  Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
"Fix cloud e2e, artifacts and cleanup (#14392)",0.48165482,Resolved a bug where the wrong client was passed to collect cloud logs (#14684),,0
Remove the legacy get_deprecated_arg_names (#14415),0.62147963,Remove deprecated args to learning rate step function (#890),,0
Bump tj-actions/changed-files from 28 to 29.0.1 (#14430),0.42261183,"- Include a version suffix for new ""last"" checkpoints of later runs in the same directory ([#12902](https://github.com/Lightning-AI/lightning/pull/12902))",Bumps tj-actions/changed-files from 28 to 29.0.1. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: tj-actions/changed-files   dependency-type: direct:production   update-type: version-update:semver-major ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Removed deprecated Trainer.num_processes property in favour of Trainer.num_devices (#14423),0.81443024,- Removed deprecated `Trainer.num_processes` attribute in favour of `Trainer.num_devices` ([#14423](https://github.com/Lightning-AI/lightning/pull/14423)),Co-authored-by: awaelchli aedu.waelchli@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
Removed the deprecated Trainer.data_parallel_device_ids function in favour of Trainer.device_ids (#14422),0.8372495,- Deprecated `Trainer.data_parallel_device_ids` in favor of `Trainer.device_ids` ([#12072](https://github.com/PyTorchLightning/pytorch-lightning/pull/12072)),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
Removed the deprecated the trainer.lr_schedulers (#14408),0.85112387,- Removed the deprecated the `trainer.lr_schedulers` ([#14408](https://github.com/Lightning-AI/lightning/pull/14408)),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
Remove deprecated TrainerCallbackHookMixin (#14401),0.8103248,Removed the deprecated TrainerLoggingMixin class (#8609),  remove deprecated callback hook   changelog ,1
Fix mypy errors in pytorch_lightning/strategies/sharded.py (#14184),0.69871294,  * Removed the `pytorch_lightning.strategies.sharded.DDPShardedStrategy` (ddp_sharded) class,Co-authored-by: otaj ota@lightning.ai,0
Update changelog after v1.7.3 release (#14398),0.6688755,Full Changelog,,0
Change trainer.should_stop to not stop in between an epoch and run until min_steps/min_epochs only (#13890),0.82399356,- Changed `trainer.should_stop` to not stop in between an epoch and run until `min_steps/min_epochs` only ([#13890](https://github.com/Lightning-AI/lightning/pull/13890)),,1
[App][CI] Fix psutil requirement CI (#14413),0.6233439,The psutil package is now required for CPU monitoring (#17010),,0
Remove deprecated HPC model hooks (#14315),0.74945444,Removed deprecated model hooks (#3980),Co-authored-by: rohitgr7 rohitgr1998@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
Bump ravsamhq/notify-slack-action from 1 to 2 (#14290),0.41490918,Resolved a bug where the work statuses will grow quickly and be duplicated (#13970),Bumps ravsamhq/notify-slack-action from 1 to 2. - Release notes - Commits  updated-dependencies: - dependency-name: ravsamhq/notify-slack-action   dependency-type: direct:production   update-type: version-update:semver-major ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
"Update mlflow requirement from <1.28.0,>=1.0.0 to >=1.0.0,<1.29.0 in /requirements (#14311)",0.6954106,Fallback to module available check for mlflow (#17467),Update mlflow requirement in /requirements Updates the requirements on mlflow to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: mlflow   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Remove support for the deprecated torchtext legacy (#14375),0.7742063,- Removed deprecated support for old torchtext versions ([#14375](https://github.com/Lightning-AI/lightning/pull/14375)),,1
save checkpoints and profiler output to the first logger (#14325),0.79645896,"- When using multiple loggers, by default checkpoints and profiler output now get saved to the log dir of the first logger in the list ([#14325](https://github.com/Lightning-AI/lightning/pull/14325))",Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
Disable non blocking to device with MPS (#14368),0.51451266,Enable non-blocking for device transfers to GPU (#1843),"  disable non-blocking for mps due to race condition bug   fixed typo   fixed: unknown mps device for non arm systems   Removed unrobust test case   moved _MPS_DEVICES such that we used in apply_func   Resolve circular dependencies   Comment rewording   changed torchElasticEnvironment to a global import   simplified if statement to blocking device type   Added change to CHANGELOG   Update src/pytorch_lightning/utilities/apply_func.py   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   fixed mypy not detecting casting of device   Moved check into if statement to mainain original behavior   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",0
Remove the deprecated loop output format (#14373),0.56655705,Loop customization has a new face,,0
Fix mypy errors attributed to pytorch_lightning.core.datamodule (#13693),0.73082626,Removed the deprecated pytorch_lightning.core.lightning module in favor of pytorch_lightning.core.module (#16318),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: rohitgr7 rohitgr1998@gmail.com Co-authored-by: otaj ota@lightning.ai,1
Fix device parser logic to avoid creating CUDA context (#14319),0.5999466,"Raise MisconfigurationException when the accelerator is available but the user passes invalid ([]/0/""0"") values to the devices flag (#12708)",  let environment disable forking   add helper function and error messages   tests   changelog   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Fix restoring trainer after lr_find() (#14113),0.7138277,Removed trainer.fit() return value of 1. It has no return now (#7237),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Fix mypy errors attributed to pytorch_lightning.trainer.connectors.data_connector.py (#13806),0.7657719,- Moved `trainer.connectors.env_vars_connector._defaults_from_env_vars` to `utilities.argsparse._defaults_from_env_vars` ([#10501](https://github.com/PyTorchLightning/pytorch-lightning/pull/10501)),Co-authored-by: rohitgr7 rohitgr1998@gmail.com Co-authored-by: otaj 6065855+otaj@users.noreply.github.com,1
Raise an error when resuming training with Apex (#14341),0.6253888,1. Remove resume_from_checkpoint from the Trainer,,0
Update docs for train_bn in Basefinetuning.filter_params (#13938),0.5823096,"Removed the deprecated Trainer.reset_train_val_dataloaders() in favor of Trainer.reset_{train,val}_dataloader (#16131)",,0
Add back standalone task azure CI step (#14366),0.41711283,Resolved a bug where the wrong client was passed to collect cloud logs (#14684),,0
Remove unused utility function _parse_devices (#14386),0.6817163,"device parser (#3400, #3405)",,0
CI: Azure CPU pool (#14367),0.46522862,"adding compute environments (#3837, [#3842)", Azure CPU pool Fix empty env LAI vars  Co-authored-by: manskx ahmed.mansy156@gmail.com,0
Add auto wrapping for DDPFullyShardedNativeStrategy (#14252),0.75903416,- Added support for auto wrapping for `DDPFullyShardedNativeStrategy` ([#14252](https://github.com/Lightning-AI/lightning/pull/14252)),,1
Reset epoch progress with batch size scaler (#13846),0.99999994,Reset epoch progress with batch size scaler (#13846),Co-authored-by: Christian Schell christian.schell@uni-wuerzburg.de Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
Remove appropriate line from pyproject.toml as a followup to #13929 (#14397),0.44515792,Updated references to self.forward() to instead use the __call__ interface. (#1211),followup to #13929,0
Support sharded optimizer state dumping outside of sharded strategies (#14208),0.67141545,- Added support for saving sharded optimizer state dict outside of `DDPShardedStrategy` ([#14208](https://github.com/Lightning-AI/lightning/pull/14208)),,0
Fix mypy errors attributed to pytorch_lightning.demos.boring_classes (#14201),0.7037847,Removed the deprecated pytorch_lightning.profiler.* classes in favor of pytorch_lightning.profilers (#16059),Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: otaj ota@lightning.ai,1
Remove mps config for test (#14379),0.52242434,Refactored setup_training and remove test_mode (#5388),"  Remove mps config for test   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
Add torch.cuda rng state to seed save/load (#14384),0.7978538,- Included `torch.cuda` rng state to the aggregate `_collect_rng_states()` and `_set_rng_states()` ([#14384](https://github.com/Lightning-AI/lightning/pull/14384)),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
Fix LightningDataModule hparams parsing (#12806),0.6946671,Removed deprecated LightningModule hparams setter (#6207),Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
[CI] fix horovod tests (#14382),0.6183336,horovod deprecation (#16141),,0
Add document to showcase scaleout on hpu (#14357),0.39524174,Moveed HPU broadcast override to the HPU strategy file (#17011),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com,0
Fix silent TPU CI failures (#14034),0.5558425,Resolve TPU miss rendezvous (#6781),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
[CI] Trick Bagua into installing appropriate wheel in GPU tests (#14380),0.46356964,refactored GPU backend __step (#3120),Bagua trick needs to be replicated on everywhere applicable,0
Add a required job checker as an action (1/2) (#14363),0.5070375,"Simplified ""should run validation"" logic (#7682)",Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
release App 0.6.0 RC (#14370),0.50296354,[1.6.1] - 2022-04-13,  release App 0.6.0 RC   req ,0
[CLI] Adding opportunity to see basic cluster logs (#14334),0.56482494,Add support to see Lightning AI BYOC cluster logs (#14334),"  pinning starsessions   pinning starsessions   adding strict back to requirements.txt   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Duplicated   Basic implementation   Basic implementation   Basic implementation   Basic implementation   Common things moved to log helpers file   Decomposing logs reader classes for reusing   Setting colors for log levels   Manifest trimming   Changes added to CHANGELOG   Prettifications   Prettifications   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Logs function name change   Logs function name change   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   attempt to fix the pydanitc import   Tests + command name fixes   Extending tests   Adding limit argument   Unmerging CI fix   Unmerging CI fix   Adding fields for errors   Adding log level fixed field width   Adding absent typing + exeptions raising   Adding socket error logging   Addressing comments on cluster list function return value   Addressing comments on adding e2e tests   Adding version range for arrow package in reqs   New unit tests   arrow time parsing callback modified + unit tests   helpers updated   helpers updated   helpers updated   One more test   CMD test fix   CMD test fix   CMD test fix   CMD test fix   CMD test fix   LightningClient mocking   Flaky test removed   Co-authored-by: hhsecond sherin@grid.ai Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",0
Fix make test (#14273),0.45999098,Removed no return warning from val/test step (#6139),,0
CI: enable testing of react e2e (#14364),0.40970436,Removed implicit ui testing with testing.run_app_in_cloud in favor of headless login and app selection (#16741),enable testing of react e2e,0
Update for M1 Mac installations (#14350),0.44085342,"Defaults to ""mps"" when run on M1 or M2 Apple machines",  Update for M1 Mac installations   Apply suggestions from code review   Update PL installation   Update based on feedback   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Bring back access app state (#14258),0.43273497,"    # previously, we were able to return state here",  Recreated the access_app_state file   Update the site's TOC to include the file   Update code sample file path   Minor formatting update   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Sherin Thomas sherin@grid.ai,0
[CI] Bump CUDA in Docker images to 11.6.1 (#14348),0.55018973,  * Deprecated the `pytorch_lightning.utilities.device_parser.is_cuda_available` in favor of `lightning_lite.accelerators.cuda.is_cuda_available`,"  bump cuda in docker images to 11.6.1   PUSH TO HUB. REVERT THIS!   conda forge for 11.6   cuda 11.5   revert conda changes   11.6 back again   11.6 back again, all of them   maybe all passes now   maybe all passes now   final push   Revert ""PUSH TO HUB. REVERT THIS!""   This reverts commit 602bfce224cf22e24421448887844937e0aff9f0.  Apply suggestions from code review  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",0
chlog after App 0.5.6 & 0.5.7 (#14352),0.52464235,"Deprecated LightningLoggerBase.close, LoggerCollection.close in favor of LightningLoggerBase.finalize, LoggerCollection.finalize (#9422)",  chlog after App 0.5.6 & 0.5.7   . ,0
fix imports of collections.abc for py3.10 (#14345),0.5258778,Compatibility for Python 3.10,fix collections.abc for py3.10 Co-authored-by: Sherin Thomas sherin@grid.ai,0
adding CI for e2e on Azure (#14282),0.41879708,"adding compute environments (#3837, [#3842)",  start CI   wip   matrix   name   wip   prune   rm   ls   cache   dir   ls   name   cleaning   clone   git   git   if   private   .   local_id   clean   var   group check   ci ,0
[App] Add cloud platform exception (#13928),0.5461743,Resolved a bug where the wrong client was passed to collect cloud logs (#14684),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Add back support for logging in the gradient clipping hooks (#14298),0.6143652,Ensure that clip gradients is only called if the value is greater than 0 (#6330),  Add back support for logging in the gradient clipping hooks   Docs and CHANGELOG   Fix tests ,0
Bump tj-actions/changed-files from 24 to 28 (#14337),0.42420065,"- Include a version suffix for new ""last"" checkpoints of later runs in the same directory ([#12902](https://github.com/Lightning-AI/lightning/pull/12902))",Bumps tj-actions/changed-files from 24 to 28. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: tj-actions/changed-files   dependency-type: direct:production   update-type: version-update:semver-major ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Pinning starsessions to 1.x (#14333),0.9999998,Pinning starsessions to 1.x (#14333),The recent release of starsessions 2.0 has broken lightning app as some of the arguments are removed. This PR also fixes a bug in our setup tools that prevents our internal # strict parameter being considered. Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
Fix an issue to avoid the impact of sanity check on reload_dataloaders_every_n_epochs for validation (#13964),0.61011374,"- Fixed logging on `{test,validation}_epoch_end` with multiple dataloaders ([#11132](https://github.com/PyTorchLightning/pytorch-lightning/pull/11132))",,0
Fix mypy errors attributed to pytorch_lightning.callbacks.quantization (#13782),0.7082445,"        ""pytorch_lightning.callbacks_factory"": [",Co-authored-by: otaj 6065855+otaj@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Remove references to local app admin view (#14306),0.40510467,Application storage prefix moved from app_id to project_id/app_id (#14583),"The local app '/admin' route is being removed because it does not provide any value, so we need to remove references from it in the CLI.",0
Adjust mergify's number of reviewer rules (#14293),0.35991594,Fixing 5000 log line limitation for Lightning AI BYOC cluster logs (#14458),,0
Fix mypy errors attributed to pytorch_lightning.core.module.py (#13603),0.7945764,Removed the deprecated pytorch_lightning.core.lightning module in favor of pytorch_lightning.core.module (#16318),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Bump actions/upload-artifact from 2 to 3 (#14289),0.46537077,Add support to upload files to the Drive through an asynchronous upload_file endpoint (#14703),Bumps actions/upload-artifact from 2 to 3. - Release notes - Commits  updated-dependencies: - dependency-name: actions/upload-artifact   dependency-type: direct:production   update-type: version-update:semver-major ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Bump actions/setup-node from 2 to 3 (#14286),0.44588506,moved ___step_end hooks (#3130),Bumps actions/setup-node from 2 to 3. - Release notes - Commits  updated-dependencies: - dependency-name: actions/setup-node   dependency-type: direct:production   update-type: version-update:semver-major ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Bump actions/setup-python from 2 to 4 (#14287),0.47522944,from setuptools import setup,Bumps actions/setup-python from 2 to 4. - Release notes - Commits  updated-dependencies: - dependency-name: actions/setup-python   dependency-type: direct:production   update-type: version-update:semver-major ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Update ipython[all] requirement from <=8.1.1 to <8.4.1 in /requirements (#14281),0.5468409,Add missing python-multipart dependency (#17244),Updates the requirements on ipython[all] to permit the latest version. - Release notes - Commits  updated-dependencies: - dependency-name: ipython[all]   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Fix wrong num padding for RichProgressBar (#14296),0.6078355,- Fixed `RichProgressBar` progress when refresh rate does not evenly divide the total counter ([#11668](https://github.com/PyTorchLightning/pytorch-lightning/pull/11668)),,0
BYOC: fix default types for cluster instance types (#14260),0.6341765,- Removed the `--instance-types` option when creating clusters ([#15314](https://github.com/Lightning-AI/lightning/pull/15314)),,0
Deprecate on_colab_kaggle func (#14247),0.7611331,- Deprecated the `on_colab_kaggle` function ([#14247](https://github.com/Lightning-AI/lightning/pull/14247)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Update mising CODEOWNERS for the PL package (#14280),0.5465036,Release LAI docs as stable (#14250),  Update CODEOWNERS   Update .github/CODEOWNERS ,0
Fix mypy typing errors attributed to pytorch_lightning/demos/mnist_datamodule.py (#13929),0.6178783,- Added a warning when saving an instance of `nn.Module` with `save_hyperparameters()` ([#12068](https://github.com/PyTorchLightning/pytorch-lightning/pull/12068)),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
CI: Let dependabot check GHA updates weekly (#14274),0.41923732,Doing this minor release because correct validation metrics logging is critical.,,0
Forward extra keyword arguments in LightningDataModule.from_datasets (#14185),0.72515064,- Added support for passing extra init-parameters to the `LightningDataModule.from_datasets` ([#14185](https://github.com/Lightning-AI/lightning/pull/14185)),Co-authored-by: otaj ota@lightning.ai Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Fix access to logger attribute when multiple loggers are used (#14234),0.7157074,Change default logger to a dedicated one (#1064),  Fix access to logger attribute when multiple loggers are used   add changelog ,1
"Update scikit-learn requirement from <=1.1.1,>0.22.1 to >0.22.1,<1.1.3 in /requirements (#14276)",0.6197726,Removed dependency on scikit-learn (#801),Update scikit-learn requirement in /requirements Updates the requirements on scikit-learn to permit the latest version. - Release notes - Commits  updated-dependencies: - dependency-name: scikit-learn   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Enable on_before_batch_transfer for DPStrategy and IPUAccelerator (#14023),0.8304752,- Enabled `on_before_batch_transfer` for `DPStrategy` and `IPUAccelerator` ([#14023](https://github.com/Lightning-AI/lightning/pull/14023)),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Fixes note formatting and more (#14264),0.582813,Noteworthy changes:,Fixes formatting for a note and makes headings for arguments and parameters stand out,0
"Update torch requirement from <=1.12.0,>=1.9.* to >=1.9.0.a,<1.13.0 in /requirements (#14088)",0.7154027,    * Some configuration errors that were previously raised as `MisconfigurationException`s will now be raised as `ProcessRaisedException` (torch>=1.8) or as `Exception` (torch<1.8), Update torch requirement in /requirements  Updates the requirements on torch to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: torch   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com   Update base.txt   Update adjust_versions.py   Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com,1
Fix type check for non-standard schedulers in horovod (#14215),0.5180573,horovod deprecation (#16141),,0
"Update tensorboard requirement from <2.10.0,>=2.9.1 to >=2.9.1,<2.11.0 in /requirements (#14200)",0.75147885,Improved the error message for installing tensorboard or tensorboardx (#17053),Update tensorboard requirement in /requirements Updates the requirements on tensorboard to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: tensorboard   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,1
release LAI docs as stable (#14250),0.99999976,Release LAI docs as stable (#14250),,1
Update changelog after 1.7.2. release (#14251),0.62070036,Full Changelog,,0
Terminate process when main process raises error in ServableModuleValidator (#14217),0.54396415,ServableModule and its Servable Module Validator Callback,Co-authored-by: otaj 6065855+otaj@users.noreply.github.com,0
"Remove incorrect ""template"" information (#13911)",0.42255056,Remove MetricsHolder (#7909),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Update defaults for WandbLogger's run name and project name (#14145),0.79414034,"The default project name in WandbLogger is now ""lightning_logs"" (#14145)",,1
Allowed setting attributes on DataLoader and BatchSampler when instantiated inside *_dataloader hooks (#14212),0.7184255,- Allowed custom `BatchSampler`s when instantiated in `*_dataloader` hook [#13640](https://github.com/Lightning-AI/lightning/pull/13640)),,1
CI: docker focus on PL only (#14246),0.51147234,Remove unnecessary intermediate layers in Dockerfiles (#5697),  CI: docker focus on PL only   group ,0
Warn when http URLs are configured (#14233),0.4644462,warning_utils >> warnings,  add a warning   add test   add test   add changelog   remove todo   clarify http won't work in cloud   Apply suggestions from code review   Co-authored-by: Sherin Thomas sherin@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Sherin Thomas sherin@grid.ai,0
Add 'app' parameter into the command example (#14055),0.69024837,Add --app_args support from the CLI (#13625),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
[App] Moved app.py to root dir for lightning init app <app_name> template (#13853),0.884155,Unification of app template: moved app.py to root dir for lightning init app <app_name> template (#13853),"  Moved app.py to main app directory   updated docs   updated changelog   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",1
Docs BYOC content (#13976),0.56105405,Docs," BYOC content  Content for the upcoming BYOC feature   First DRAFT of BYOC content   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  Update docs/source-app/index.rst  Co-authored-by: thomas chaton thomas@grid.ai  Update docs/source-app/workflows/byoc/index.rst  Co-authored-by: thomas chaton thomas@grid.ai  Update docs/source-app/workflows/byoc/index.rst  Co-authored-by: thomas chaton thomas@grid.ai   Updates based on feedback   Updates based on feedback   Update external ID with note   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  Update docs/source-app/workflows/byoc/index.rst  Co-authored-by: Raphael Randschau nicolai86@users.noreply.github.com  Update docs/source-app/workflows/byoc/index.rst  Co-authored-by: Raphael Randschau nicolai86@users.noreply.github.com  Update docs/source-app/workflows/byoc/index.rst  Co-authored-by: Raphael Randschau nicolai86@users.noreply.github.com  Update docs/source-app/workflows/byoc/index.rst  Co-authored-by: Raphael Randschau nicolai86@users.noreply.github.com  Update docs/source-app/workflows/byoc/index.rst  Co-authored-by: Raphael Randschau nicolai86@users.noreply.github.com  Updates for terraform mod  Updates for terraform mod and arg pram split  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci  Update docs/source-app/workflows/byoc/index.rst  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update docs/source-app/workflows/byoc/index.rst  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   Update index.rst   Update docs/source-app/workflows/byoc/index.rst   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update docs/source-app/workflows/byoc/index.rst  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update docs/source-app/workflows/byoc/index.rst  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update content with table  Changed bullets into table based on feedback  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Raphael Randschau nicolai86@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com",0
"Revert ""Remove skipping logic in favor of path filtering (#14170)"" (#14244)",0.46944487,- Removed the deprecated code in:,,0
CI: clean building docs (#14216),0.44466037,Silenced some warnings. verified ddp refactors (#3483),  CI: clean building docs   group   . ,0
add more issues types (#14174),0.46495432,Made type hints public (#17100),  add more issues types   Update .github/ISSUE_TEMPLATE/config.yml   Co-authored-by: Mansy ahmed.mansy156@gmail.com  typo  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Mansy ahmed.mansy156@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Laverne Henderson laverne.henderson@coupa.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com,0
(app) Documentation fix for Work resources (#14182),0.57109666,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",,0
Fix install latest version of app/component (#14181),0.6880495,Resolved a bug where the install command was not installing the latest version of an app/component by default (#14181),  fix install latest version of app/component   Add changelog   a better testcase   Update src/lightning_app/CHANGELOG.md   Co-authored-by: mansy mansy@lightning.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Use fsdp module to initialize precision scalar for fsdp native (#14092),0.7356623,Avoided requiring the FairScale package to use precision with the fsdp native strategy (#14092),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Laverne Henderson laverne.henderson@coupa.com,1
Remove skipping logic in favor of path filtering (#14170),0.3891827,Update the logic to check for accumulation steps with deepspeed (#9826),,0
Add docs for fsdp_native (#14108),0.67799795,Native FSDP replaces Fairscale FSDP (#16400),,0
Avoid raising the sampler warning if num_replicas=1 (#14097),1.0,Avoid raising the sampler warning if num_replicas=1 (#14097),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: otaj 6065855+otaj@users.noreply.github.com,1
Replace unwrapping logic in strategies (#13738),0.52792865,- Replaced the unwrapping logic in strategies with direct access to unwrapped `LightningModule` ([#13738](https://github.com/Lightning-AI/lightning/pull/13738)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
Avoid false positive warning about using sync_dist when using torchmetrics (#14143),0.9972063,Avoided false positive warning about using sync_dist when using torchmetrics (#14143),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Profile batch transfer and gradient clipping hooks (#14069),0.5498599,Gradient Clipping Customization,,0
Fix mypy errors attributed to pytorch_lightning. strategies.sharded_spawn (#14102),0.70920086,- Removed `Strategy.init_optimizers` in favor of `Strategy.setup_optimizers` ([#11236](https://github.com/PyTorchLightning/pytorch-lightning/pull/11236)),Co-authored-by: rohitgr7 rohitgr1998@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: awaelchli aedu.waelchli@gmail.com,1
[CLI] adjust command description (#14130),0.5608185,- Added support for adding descriptions to commands either through a docstring or the `DESCRIPTION` attribute ([#15193](https://github.com/Lightning-AI/lightning/pull/15193), adjust CLI copy  Co-authored-by: RobertLaurella 99420295+RobertLaurella@users.noreply.github.com,0
Configure the check-group app (#14165),0.4379909,Configuration Validator (#9779),Co-authored-by: Jirka jirka.borovec@seznam.cz,0
Remove DeepSpeed version restriction from Lite (#13967),0.67775506,Updated compatibility for LightningLite to run with the latest DeepSpeed 0.7.0 (13967),,0
Fix saving hyperparameters in a composition where parent is not a LM or LDM (#14151),0.62234235,Moved save_hyperparameters to its own function (#7119),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
[CLI] change cluster creation cost savings mode default (#14132),0.6620376,Cluster creation and deletion now waits by default [#15458," [CLI] change cluster creation cost savings mode default  instead of having customers opt-into cost savings mode, we'll ask them to opt-out of cost savings mode.",0
Feature GRID-9731: Update Lightning Cloud.py Backend to Accept Drive Specs (2/2) (#14106),0.50937295,Add support to upload files to the Drive through an asynchronous upload_file endpoint (#14703),initial work adding drives to create work API from framework cloud dispatcher,0
"(app) Introduce configure_api and Post, Get, Delete, Put HttpMethods (#13945)",0.538352,"Add support for Lightning API through the configure_api hook on the LightningFlow and the Post, Get, Delete, Put with HttpMethods (#13945)",,0
Fix flaky test caused by weak reference (#14157),0.45017195,Cleaning up stale logger tests (#3490),,0
Fix entry point test for Python 3.10 (#14154),0.59697634,Compatibility for Python 3.10,,0
Convert subprocess test to standalone test (#14101),0.44360155,refactor eval loop to use hooks - use test_mode for if so we can split later (#3129),,0
Resolve e2es V3 (#14153),0.44454056,Refactored EpochResultStore (#5522),update,0
(app) Add s3 drive type (1/2) (#14002),0.43368655,Add support to upload files to the Drive through an asynchronous upload_file endpoint (#14703),"  Add S3 protocol and optimization field to the drive object   Add a list of drives to the work specification   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   add only protocol for s3 drives, no optimization arguments, and add tests   added trailing slash criteria   allow slash in s3 drives   fix   fixed test issues   Co-authored-by: Panos Lantavos-Stratigakis default-email@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Rick Izzo rick@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rick Izzo rlizzo@users.noreply.github.com",0
Update onnxruntime requirement from <=1.12.0 to <1.13.0 in /requirements (#14083),0.47715837,Setup: added requirement freeze for the next major version (#14480),Updates the requirements on onnxruntime to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: onnxruntime   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
"Update gcsfs requirement from <2022.6.0,>=2021.5.0 to >=2021.5.0,<2022.8.0 in /requirements (#14079)",0.48249695,Setup: added requirement freeze for the next major version (#14480),Update gcsfs requirement in /requirements Updates the requirements on gcsfs to permit the latest version. - Release notes - Commits  updated-dependencies: - dependency-name: gcsfs   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
relax docker requirement (#14009),0.5730464,Remove unnecessary intermediate layers in Dockerfiles (#5697),,0
(app) Remove ClickRunner (#14147),0.5081778,Remove MetricsHolder (#7909),,0
update chlog after 0.5.5 (#14133),0.49948597,Removed logger_connector legacy code (#6733),,0
Fix a bug that caused spurious AttributeError when multiple DataLoader classes are imported (#14117),0.6302257,"Did not always create a DataLoader during reinstantiation, but the same type as before (if a subclass of DataLoader) (#1346)",,0
Fix mypy errors attributed to pytorch_lightning.profilers.simple (#14103),0.8252584,Removed the deprecated pytorch_lightning.profiler.* classes in favor of pytorch_lightning.profilers (#16059),,1
Update CODEOWNERS (#14119),0.45982832,"Updated DeepSpeed ZeRO (#6546, #6752, #6142, #6321)",  Update CODEOWNERS   Cleanup and remove old sections   pl focus   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Remove duplicated test classes (#14122),0.51728886,Cleaning up stale logger tests (#3490),Remove duplicated classes,0
Avoid entry_points deprecation warning (#14052),0.73485154,Avoid metadata.entry_points deprecation warning on Python 3.10 (#14052),Co-authored-by: Adam J. Stewart ajstewart426@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com,1
Update Grid links to Lightning AI (#14081),0.53617036,"  [#14475](https://github.com/Lightning-AI/lightning/pull/14475),",  initial changes for lightning   Update .github/BECOMING_A_CORE_CONTRIBUTOR.md   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Update changelog after 1.7.1 release (#14127),0.59143674,Full Changelog,,0
(app) Run the flow only if the state has updated 1/2 (#14076),0.815767,Run the flow only if the state has changed from the previous execution (#14076),,1
Use websockets in e2es (#14138),0.33775938,Connect and Disconnect node (#16700),,0
CI/CD: Add CUDA version to docker image tags (#13831),0.4636204,  * Deprecated the `pytorch_lightning.utilities.device_parser.num_cuda_devices` in favor of `lightning_lite.accelerators.cuda.num_cuda_devices`,"  append cuda version to tags   revertme: push to hub   Update docker readme   Build base-conda-py3.9-torch1.12-cuda11.3.1   Use new images in conda tests   revertme: push to hub   Revert ""revertme: push to hub""   This reverts commit 0f7d534b2ae41e4bd227961a929c333c88e35f59.  Revert ""revertme: push to hub""  This reverts commit 46a05fccbb9b596aa98d5d68424917b5811c5b4f.   Run conda if workflow edited   Run gpu testing if workflow edited   Use new tags in release/Dockerfile   Build base-cuda and PL release images with all combinations   Update release docker   Update conda from py3.9-torch1.12 to py3.10-torch.1.12   Fix ubuntu version   Revert conda   revertme: push to hub   Don't build Python 3.10 for now...   Fix pl release builder   updating version contribute to the error? https://github.com/docker/buildx/issues/456   Update actions' versions   Update slack user to notify   Don't use 11.6.0 to avoid bagua incompatibility   Don't use 11.1, and use 11.1.1   Update .github/workflows/ci-pytorch_test-conda.yml   Co-authored-by: Luca Medeiros 67411094+luca-medeiros@users.noreply.github.com   Update trigger   Ignore artfacts from tutorials   Trim docker images to distribute   Add an image for tutorials   Update conda image 3.8x1.10   Try different conda variants   No need to set cuda for conda jobs   Update who to notify ipu failure   Don't push   update filenaem   Co-authored-by: Luca Medeiros 67411094+luca-medeiros@users.noreply.github.com",0
[App] Application logs in CLI (#13634),0.5911127,Add support for printing application logs using CLI lightning show logs <app_name> [components] (#13634),,0
Fix device placement when .cuda() called without specifying index (#14128),0.66438115,Moved torch.cuda.set_device() to enable collective calls earlier in setup (#8312),,0
CI: Update Windows version from 2019 to 2022 (#14129),0.4359842,Setup: added requirement freeze for the next major version (#14480),Update windows,0
CI: Replace _ of in GHA workflow filenames with - (#13917),0.41519424,Renames model steps (#1051),  Rename workflow files   Update docs   Fix azure badges   Update the main readme   bad rebase   Update doc ,0
Update collect env details and issue template (#14017),0.4151608,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Support checkpoint save and load with Stochastic Weight Averaging (#9938),0.7925612,Checkpoint saving and loading extensibility:,Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: Kushashwa Ravi Shrimali kushashwaravishrimali@gmail.com Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
Skip ddp fork tests on windows (#14121),0.6231332,Run ddp_spawn dataloader checks on Windows (#6930),,0
[CLI] fix cluster creation CLI requiring instance-type selection (#14056),0.7199117,- Removed the `--instance-types` option when creating clusters ([#15314](https://github.com/Lightning-AI/lightning/pull/15314)),"fix cluster creation CLI requiring instace-type selection we've marked instance_types as required=False, but the CLI calls split on the value. So if nothing is provided, we'll actually receive a runtime error, effectively rendering the flag as required. Co-authored-by: thomas chaton thomas@grid.ai",1
Reset all results on epoch end (#14061),0.63604414,Reset current progress counters when restarting an epoch loop that had already finished (#9371),,0
"Fix assert wandb Run when mode=""disabled"" (#14112)",0.71418685,WandbLogger does not force wandb reinit arg to True anymore and creates a run only when needed (#4648),,1
"Fix incorrect precision=""mixed"" being used with DeepSpeedStrategy and IPUStrategy (#14041)",0.7296906,Updated precision attributes in DeepSpeedPlugin (#10164),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Add missing codeowners for app package (#13542),0.41946137,Application storage prefix moved from app_id to project_id/app_id (#14583),,0
Prefix seed_everything log messages with rank info (#14031),0.77218246,- Added prefix to log message in `seed_everything` with rank info ([#14031](https://github.com/Lightning-AI/lightning/pull/14031)),Co-authored-by: Anton Shevtsov aeshevtsov@avito.ru Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Fix import in doctest example (#14067),0.45160782,Module imports are now catching ModuleNotFoundError instead of ImportError (#9867),,0
Fix regression on default value for find_unused_parameters (#14095),0.7691525,            find_unused_parameters=True,,1
Fix mypy errors attributed to pytorch_lightning.core.saving (#13932),0.74936104,Make sure save_dir can be empty str (#15638](https://github.com/PyTorchLightning/pytorch-lightning/issues/15638)),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: otaj 6065855+otaj@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
CI: Enable Python 3.10 in full CPU testing (#13829),0.59093475,Compatibility for Python 3.10, Update docker images to build,0
Resolve increased time. (#14074),0.51632416,Increased TPU check timeout from 20s to 100s (#5598),,0
Update CODEOWNERS (remove myself from defaults + some specifics) (#14084),0.37063277,Remove MetricsHolder (#7909),Update CODEOWNERS,0
Fix mypy errors  in pytorch_lightning/strategies/ddp.py (#13885),0.7330019,  * Removed the `pytorch_lightning.strategies.sharded.DDPShardedStrategy` (ddp_sharded) class,Co-authored-by: awaelchli aedu.waelchli@gmail.com,1
"Update tqdm requirement from <=4.63.0,>=4.57.0 to >=4.57.0,<4.65.0 in /requirements (#13875)",0.5432013,Setup: added requirement freeze for the next major version (#14480),Update tqdm requirement in /requirements Updates the requirements on tqdm to permit the latest version. - Release notes - Commits  updated-dependencies: - dependency-name: tqdm   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Fix dtype inference during gradient norm computation (#14051),0.54075736,- Fixed type promotion when tensors of higher category than float are logged ([#11401](https://github.com/PyTorchLightning/pytorch-lightning/pull/11401)),,0
"Fix: Start Lightning App on Cloud if Repo Begins With Name ""Lightning"" (#14025)",0.63074696,- Added support to start lightning app on cloud without needing to install dependencies locally ([#15019](https://github.com/Lightning-AI/lightning/pull/15019),,0
"Update wandb requirement from <0.12.20,>=0.10.22 to >=0.10.22,<0.13.2 in /requirements (#14080)",0.63042635,Removed the deprecated sync_step argument from WandbLogger (#8763),,0
Fix mypy errors attributed to pytorch_lightning.trainer.connectors.callback_connector.py (#13750),0.76171124,- Moved `trainer.connectors.env_vars_connector._defaults_from_env_vars` to `utilities.argsparse._defaults_from_env_vars` ([#10501](https://github.com/PyTorchLightning/pytorch-lightning/pull/10501)), Apply suggestions from code review  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: otaj 6065855+otaj@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
Add Promoted CLI to API Reference Section (#14072),0.4797085,Introducing CLI commands for apps (#13602)!,,0
Remove deprecated DistributedType and DeviceType enum classes (#14045),0.6259257,- Removed the deprecated `DistributedType` and `DeviceType` enum classes ([#14045](https://github.com/Lightning-AI/lightning/pull/14045)),,0
Run mypy with PyTorch 1.12 (#14044),0.6932664,"PyTorch 1.10 and Python 3.7 no longer supported (#16492, #16579)",,0
Freeze requirements for CI (#14007),0.5318772,Setup: added requirement freeze for the next major version (#14480),"  free requirements   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   typo   typo   ui   mypy   todo   mypy   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  mypy  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com",0
Cast to fp16 before moving to device with deepspeed (#14000),0.7202971,Casted tensors to fp16 before moving them to device with  DeepSpeedStrategy (#14000),,1
[CLI] add support for listing apps (#13987),0.739446,Add support for listing Lightning AI apps (#13987),"  add support for listing apps   update changelog with correct PR number   add tests for pagination   fix wrong mock on test_cli   ensure all enum values are accounted for   make AppManager and AppList protected, add limit to pagination calls   add restarting transition /w tests   add state transition not yet run with tests ",1
"feature(ui): Lightning AI doc theme update, integrates global header and footer with docs (#14053)",0.5103812,"  [#14537](https://github.com/Lightning-AI/lightning/pull/14537),",,0
relax redis requirement (#14008),0.5122572,- Added an HTTPQueue as an optional replacement for the default redis queue ([#14978](https://github.com/Lightning-AI/lightning/pull/14978),,0
Remove meta device utilities in favor of torchdistx (#13868),0.5700969,We cleaned up the properties related to device indices (#14829).,,0
CI: Update XLA from 1.9 to 1.12 (#14013),0.55761284,  * Deprecated the internal `XLADeviceUtils.xla_available` staticmethod,,0
Remove unused auto_collect_arguments class method (#14015),0.51441294,            find_unused_parameters=True,,0
Deprecate amp_level from Trainer (#13898),0.8974327,Deprecated the Trainer(amp_level=...) argument,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
[App] Increased DeepDiff's verbose level to properly handle dict changes (#13960),0.9330672,Increased DeepDiff's verbose level to properly handle dict changes (#13960),,1
CI/CD: Update base image nvidia/cuda from 11.1 to 11.1.1 (#14019),0.4217142,refactored GPU backend __step (#3120),update cuda_version 11.1 -> 11.1.1,0
Bump JamesIves/github-pages-deploy-action from 4.1.4 to 4.4.0 (#13953),0.4556979,Resolved a bug where the install command was not installing the latest version of an app/component by default (#14181),Bumps JamesIves/github-pages-deploy-action from 4.1.4 to 4.4.0. - Release notes - Commits  updated-dependencies: - dependency-name: JamesIves/github-pages-deploy-action   dependency-type: direct:production   update-type: version-update:semver-minor ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
"Update mlflow requirement from <1.27.0,>=1.0.0 to >=1.0.0,<1.28.0 in /requirements (#14024)",0.6957795,Fallback to module available check for mlflow (#17467),Update mlflow requirement in /requirements Updates the requirements on mlflow to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: mlflow   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Remove the deprecated DDP2 strategy (#14026),0.8245951,Removed support for the DDP2 strategy,,1
Cast only floating types with IPUs (#13983),0.5519577,Casted only floating point tensors to fp16 with IPUs (#13983),,0
[CLI] add support to run app on a specific cluster (#13894),0.71280015,Support running on multiple clusters (#16016),Add --cluster-id flag which can be passed to lightning run app if the --cloud flag is present. This allows you to run your Lightning AI apps on Lightning AI BYOC clusters running on your own cloud provider infrastructure. Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: Laverne Henderson laverne.henderson@coupa.com,1
Remove deprecated training type plugins (#14011),0.6960246,- Removed all deprecated training type plugins ([#14011](https://github.com/Lightning-AI/lightning/pull/14011)),  Remove deprecated training type plugins   update changelog   DDP2Plugin   Update src/pytorch_lightning/CHANGELOG.md ,0
"Update comet-ml requirement from <3.31.6,>=3.1.12 to >=3.1.12,<3.31.8 in /requirements (#13874)",0.5278142,Using .comet.config file for CometLogger (#1913),,0
Deprecate sheety API (#14004),1.0,Deprecate sheety API (#14004), deprecate sheety  Co-authored-by: manskx ahmed.mansy156@gmail.com,1
drop block contribution do app (#14010),0.39909208,App,,0
Raise an error if batch transfer hooks are overridden with IPUAccelerator (#13961),0.8340553,Raised a MisconfigurationException if batch transfer hooks are overriden with IPUAccelerator (13961),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Fix button selector for Lightning app e2e tests (#13984),0.46766898,- Removed the deprecated `test_transforms` argument from the `LightningDataModule` constructor ([#12773](https://github.com/Lightning-AI/lightning/pull/12773)),,0
Remove fp16 restriction in the docstring for DeepSpeedStrategy (#13919),0.77096736,Removed DeepSpeed FP16 Exception as FP32 is now supported (#8462),Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: awaelchli aedu.waelchli@gmail.com,1
Fix mypy typing errors in strategies/fully_sharded.py (#13941),0.58692247,Renamed Strategy.reduce to Strategy.all_reduce in all strategies (#16370),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: otaj 6065855+otaj@users.noreply.github.com,0
adding explain notes for requirements (#13872),0.4460165,Docs improvements, adding explain notes for requirements Apply suggestions from code review  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
Relax lightning app dependency requirements (#13998),0.6524756,Update the Lightning App docs (#13537), [App] Relax lightning app requirements  Co-authored-by: manskx ahmed.mansy156@gmail.com,0
Doc Terminology updates (#13972),0.5868859,"All documentation and examples are now recommending the new, less ambiguous names.",  Doc Terminology updates   API updates ,0
Fix mypy errors attributed to pytorch_lightning.loggers.neptune (#13692),0.7397579,Changed pytorch_lightning.logging to pytorch_lightning.loggers (#767),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: otaj ota@lightning.ai Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
Fix typing annotations for the ipu strategy (#13786),0.58293194,Refactored setup for typing friendly (#6590),Co-authored-by: otaj ota@lightning.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
Fix erroneous warning for unset max_epochs (#13262),0.65352917,Deprecated max_nb_epochs and min_nb_epochs (#567),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
Update version.py to 1.8.0dev (#13999),0.534927,"    version=""0.0.1"",",,0
Fix NeptuneLogger unusable after pytorch-lightning 1.7.0 (#13988),0.8470988,pytorch_lightning.loggers.neptune.NeptuneLogger is now consistent with the new neptune-client API; the old neptune-client API is supported by NeptuneClient from the neptune-contrib repo (#6867),Co-authored-by: rohitgr7 rohitgr1998@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Organize accelerator tests (#13986),0.64700496,reduced accelerator selection (#3211),,0
Make MPSAccelerator platform check expect arm64 (#13992),0.47936788,"Defaults to ""mps"" when run on M1 or M2 Apple machines",Co-authored-by: rohitgr7 rohitgr1998@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Reduce state size (#13970),0.5055976,    hiddens = ...  # 3. Choose the initial hidden state,,0
Validate the model input of trainer methods (#13892),0.76257604,trainer.test(model),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
Update docstrings for backward methods (#13886),0.5398629,Backward Incompatible Changes,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: thomas chaton thomas@grid.ai,0
Ignore _notebooks when running flake8 (#13990),0.40996355,Updated model_checkpoint's to_yaml to use fsspec open (#3801),,0
Lazy import check for hydra dependency (#13812),0.55765927,Temporarily removed support for Hydra multi-run (#15737),,0
Update changelog after v1.7.0 release (#13982),0.66688114,Full Changelog,,0
Bump tj-actions/changed-files from 23 to 24 (#13956),0.41464603,"- Include a version suffix for new ""last"" checkpoints of later runs in the same directory ([#12902](https://github.com/Lightning-AI/lightning/pull/12902))",Bumps tj-actions/changed-files from 23 to 24. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: tj-actions/changed-files   dependency-type: direct:production   update-type: version-update:semver-major ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Bump pypa/gh-action-pypi-publish from 1.5.0 to 1.5.1 (#13954),0.41769117,This release has breaking API changes. See #124 for all details. ,Bumps pypa/gh-action-pypi-publish from 1.5.0 to 1.5.1. - Release notes - Commits  updated-dependencies: - dependency-name: pypa/gh-action-pypi-publish   dependency-type: direct:production   update-type: version-update:semver-patch ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com,0
Prepare changelog for 1.7 release (#13979),0.65041125,Full Changelog,  Prepare changelog for 1.7.0 release   update links   fix conflicts   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,0
Fix resuming the tqdm progress bar  (#13962),0.68798745,Deprecated ProgressBar callback in favor of TQDMProgressBar (#10134),,0
Update version to 1.7.0 for release (#13977),0.5747997,Set version as today (#13906),,0
Fix MPS availability check (#13947),0.46827734,Resolve TPU miss rendezvous (#6781),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
Fix mypy typing errors in pytorch_lightning/strategies/tpu_spawn.py (#13813),0.6386835,- Fixed wrong typehint for `Trainer.lightning_optimizers` ([#11155](https://github.com/PyTorchLightning/pytorch-lightning/pull/11155)),Co-authored-by: awaelchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: otaj ota@lightning.ai,0
Make tbptt imports Python 3.10 compatible (#13973),0.6938965,Compatibility for Python 3.10,  Make tbptt imports Python 3.10 compatible   add chlog ,0
[CLI] add support for cluster management (#13835),0.6880443,Support running on multiple clusters (#16016),,0
Added support for HPU device stats monitor (#13819),0.87745655,Device Stats Monitoring support for HPUs," Added support for HPU device stats monitor  Signed-off-by: Jerome janand@habana.ai  Update changelog  Signed-off-by: Jerome janand@habana.ai  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci  Apply suggestions from code review  Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com  Update reference  Signed-off-by: Jerome janand@habana.ai  Apply suggestions from code review  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   fix alignment   add descriptions   Update hpu_intermediate.rst   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com",1
Snapshot selected globals and restore them in spawned process (#13921),0.41078046,- Fixed an issue causing deterministic algorighms and other globals to get reset in spawned processes ([#13921](https://github.com/Lightning-AI/lightning/pull/13921)),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Bump actions/cache from 2 to 3 (#13955),0.41905826,Removed deprecated callbacks (#3979),Bumps actions/cache from 2 to 3. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: actions/cache   dependency-type: direct:production   update-type: version-update:semver-major ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Bump actions/setup-python from 2 to 4 (#13952),0.47468284,Refactored setup_training and remove test_mode (#5388),Bumps actions/setup-python from 2 to 4. - Release notes - Commits  updated-dependencies: - dependency-name: actions/setup-python   dependency-type: direct:production   update-type: version-update:semver-major ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Fix some tiny typos in docs (#13939),0.5467836,Docs improvements,,0
Porting latest App docs update (#13680),0.6054621,Update the Lightning App docs (#13537)," PRs 909,910,911, and 912  moves last 4 commits to the private re;po to the OS repo  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci   Fix validation error   Fixes API links and validation issues   Update docs/source-app/examples/file_server/app.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   Fix Python validation errors   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",0
Docs update (#13959),0.6358575,Docs improvements,,0
pkg: parse local versions (#13933),0.8469225,Parsed local package versions (#13933),  pkg: parse local versions   offline   str   manifest   ci ,1
update codecov badge (#13852),0.4560034,"Updated DeepSpeed ZeRO (#6546, #6752, #6142, #6321)",,0
fixing build meta pkg flow (#13926),0.5225266,Pkg: fix parsing versions by @Borda in https://github.com/Lightning-AI/lightning/pull/15401, debug pulled version pre flags only meta,0
Fix deepspeed default precision plugin amp_level to O2 (#13897),0.78005356,- Fixed default `amp_level` for `DeepSpeedPrecisionPlugin` to `O2` ([#13897](https://github.com/Lightning-AI/lightning/pull/13897)),Co-authored-by: Akihiro Nitta nitta@akihironitta.com,1
(app) Introduce LightningTrainingComponent (#13830),0.69384015,Adds LightningTrainingComponent. LightningTrainingComponent orchestrates multi-node training in the cloud (#13830),,0
prune func calls in meta pkg init (#13742),0.46857214,      init_args:, prune func calls in meta pkg init move calling prune coped  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
meta pkg: set version as today (#13906),0.78392667,Set version as today (#13906),,1
meta pkg: wrap imports for traceability (#13924),0.8568283,Wrapped imports for traceability (#13924),,1
Improvements to standalone scripts (#13840),0.47413853,Introducing CLI commands for apps (#13602)!,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Update version for rc1 release (#13910),0.526444,Set version as today (#13906),,0
Update GitHub links to PL repo (#13849),0.49794316,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ","  update lightning links in docs   update links in chlog   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  Apply suggestions from code review  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update src/pytorch_lightning/README.md  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update src/pytorch_lightning/README.md  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   update   painful   badges   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  Update badges  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Akihiro Nitta nitta@akihironitta.com",0
Cast on host instead of IPU when using precision=16  (#13880),0.6102251,"- When training with `precision=16` on IPU, the cast has been moved off the IPU onto the host, making the copies from host to IPU cheaper ([#13880](https://github.com/Lightning-AI/lightning/pull/13880))",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
Fix mypy errors in strategies/ddp_spawn.py (#13865),0.60049963,"trainer = L.Trainer(strategy=""ddp_spawn"")",Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Organize model summary utilities (#13893),0.5174353,model_utils >> model_helpers,,0
Update MPS availability to include check for ARM processors (#13896),0.41845202,"Raise MisconfigurationException when the accelerator is available but the user passes invalid ([]/0/""0"") values to the devices flag (#12708)",Update MPS availability to include check for ARM chips,0
Support DeepSpeed <0.7.0 (#13859),0.69907415,"Implemented DeepSpeedPlugin._setup_model_and_optimizers (#10009, #10064)",Co-authored-by: awaelchli aedu.waelchli@gmail.com,0
Comet logger documentation improvements (#13847),0.7062698,Using .comet.config file for CometLogger (#1913),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Run GPU tests with PyTorch 1.12 (#13716),0.70993644,Enable PyTorch 1.7 compatibility (#3541),Co-authored-by: Jirka jirka.borovec@seznam.cz,1
add UI for install all (#13732),0.46471444,from setuptools import setup, add UI for install all,0
"Support DeepSpeed >=0.6.0, <0.6.5 (#13863)",0.6699829,"Implemented DeepSpeedPlugin._setup_model_and_optimizers (#10009, #10064)",Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Fix TPU testing and collect all tests (#11098),0.62963396,This means we fixed many TPU bugs we hadn’t caught before because we had no tests.,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,0
Allowed custom BatchSamplers when instantiated in *_dataloader hook (#13640),0.83682907,- Allowed custom `BatchSampler`s when instantiated in `*_dataloader` hook [#13640](https://github.com/Lightning-AI/lightning/pull/13640)),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Update version for rc0 release (#13877),0.5312868,Set version as today (#13906),,0
Add batch size script argument for standalone tests (#13841),0.5804063,"    batch_size=32,",Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Fixes various typing errors in pytorch_lightning/strategies/deepspeed.py (#13832),0.75233567,    * Renamed the `DeepSpeedPlugin` to `DeepSpeedStrategy` ([#11194](https://github.com/PyTorchLightning/pytorch-lightning/pull/11194)),Co-authored-by: otaj ota@lightning.ai Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
Change tests/README.md to reflect repo structure change (#13437),0.44186956,Pkg: fix parsing versions by @Borda in https://github.com/Lightning-AI/lightning/pull/15401,Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Disallow batch sampler with multiple IPU devices (#13854),0.6213802,- Disallowed using `BatchSampler` when running on multiple IPUs ([#13854](https://github.com/Lightning-AI/lightning/pull/13854)),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
"Update torchmetrics requirement from <0.9.2,>=0.7.0 to >=0.7.0,<0.9.3 in /requirements (#13528)",0.7074659,    * Some configuration errors that were previously raised as `MisconfigurationException`s will now be raised as `ProcessRaisedException` (torch>=1.8) or as `Exception` (torch<1.8),Update torchmetrics requirement in /requirements Updates the requirements on torchmetrics to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: torchmetrics   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,1
Check if the scheduler already has reduce_on_plateau (#13838),0.66517544,- Improved support for custom `ReduceLROnPlateau` scheduler if `reduce_on_plateau` is set by the user in scheduler config ([#13838](https://github.com/Lightning-AI/lightning/pull/13838)),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Update Github issues template (#13857),0.50781345,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",Co-authored-by: mansy mansy@lightning.ai,0
Skip code formatters on _notebooks submodule (#13867),0.4081542,Wrapped imports for traceability (#13924),,0
"Update torchtext requirement from <=0.12.0,>=0.10.* to >=0.10.0.a,<0.14.0 in /requirements (#13758)",0.7570133,- Removed deprecated support for old torchtext versions ([#14375](https://github.com/Lightning-AI/lightning/pull/14375)), Update torchtext requirement in /requirements  Updates the requirements on torchtext to permit the latest version. - Release notes - Commits  updated-dependencies: - dependency-name: torchtext   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com  Update requirements/pytorch/extra.txt  Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com,1
fixes typing in stochastic_weight_avg.py (follow-up of #13685) (#13860),0.5951985,Deprecated passing stochastic_weight_avg to the Trainer constructor in favor of adding the StochasticWeightAveraging callback directly to the list of callbacks (#8989),,0
Fix error handling in learning rate finder (#13845),0.7762918,- Fixed error handling in learning rate finder when not enough data points are available to give a good suggestion ([#13845](https://github.com/Lightning-AI/lightning/pull/13845)),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
CI: Correct test path to publish test results (#13862),0.4378982,Updated app testing (#16000),,0
Add lightning apps to info_packages of collect_env_details (#13815),0.6200997,Add support for listing Lightning AI apps (#13987),,0
Add support for async checkpointing (#13658),0.7996263,Asynchronous Checkpointing,,1
Fix wrong error message in ModelPruning (#13820),0.58609456,Changed ModelCheckpoint version suffixes to start at 1 (#5008),,0
fix mypy typing errors in pytorch_lightning/strategies/strategy.py (#13519),0.72524333,- Moved `Strategy` classes to the `strategies` directory ([#11226](https://github.com/PyTorchLightning/pytorch-lightning/pull/11226)),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: otaj ota@lightning.ai,1
Added new email to enforce the Code of Conduct (#13833),0.39421287,Updated governance docs,,0
fixes typing in pytorch_lightning/callbacks/stochastic_weight_avg.py (#13685),0.82735395,Renamed pytorch_lightning.callbacks.swa to pytorch_lightning.callbacks.stochastic_weight_avg (#6259),Co-authored-by: otaj 6065855+otaj@users.noreply.github.com,1
Fix mypy errors attributed to pytorch_lightning.core.mixins.device_dtype_mixin (#13704),0.80285954,- Removed deprecated `pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin` in favor of `pytorch_lightning.core.mixins.device_dtype_mixin.DeviceDtypeModuleMixin` ([#10442](https://github.com/PyTorchLightning/pytorch-lightning/pull/10442)),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
[App] Introduce Commands (#13602),0.6817285,Introducing CLI commands for apps (#13602)!,,0
Fix PyTorch spelling errors (#13774),0.65397006,  * Removed the `pytorch_lightning.loops.batch.TrainingBatchLoop`,  Fix PyTorch spelling errors   more ,0
Merge different gpu backends with accelerator='gpu' (#13642),0.65975285,Support auto_select_gpus with the accelerator and devices API (#12608),"  Rename GPUAccelerator to CUDAAccelerator   Add back GPUAccelerator and deprecate it   Remove temporary registration   accelerator connector reroute   accelerator_connector tests   update enums   lite support + tests   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   typo   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   move ""gpu"" support up before actual accelerator flag checks   Stupid arguments   fix tests   change exception type   fix registry test   pre-commit   CI: debug HPU flow (#13419)   Update the hpu-tests.yml to pull docker from vault  fire & sudo habana-gaudi-hpus Check the driver status on gaudi server (#13718)  Co-authored-by: arao arao@habana.ai Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Akarsha Rao 94624926+raoakarsha@users.noreply.github.com  Update typing-extensions requirement from <4.2.1,>=4.0.0 to >=4.0.0,<4.3.1 in /requirements (#13529)  Update typing-extensions requirement in /requirements Updates the requirements on typing-extensions to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: typing-extensions   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com  [pre-commit.ci] pre-commit suggestions (#13540)  updates: - github.com/psf/black: 22.3.0 → 22.6.0 Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com   [FIX] Native FSDP precision + tests (#12985)   Simplify fetching's loader types (#13111)   Include app templates to the lightning and app packages (#13731)   Include app templates to the package   Co-authored-by: mansy mansy@lightning.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Fix mypy typing errors in pytorch_lightning/callbacks/model_checkpoint.py (#13617)  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Fix typos initialize in docs (#13557)  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   Fix main progress bar counter when val_check_interval=int and check_val_every_n_epoch=None (#12832)   Fix mypy errors attributed to pytorch_lightning.loggers.tensorboard.py (#13688)   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Fix mypy errors attributed to pytorch_lightning.loggers.mlflow (#13691)  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: otaj 6065855+otaj@users.noreply.github.com  fix mypy errors for loggers/wandb.py (#13483)  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com   Fix gatekeeper minimum check (#13769)   changelog   changelog   fix order   move up again   add missing test   Co-authored-by: rohitgr7 rohitgr1998@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: arao arao@habana.ai Co-authored-by: Akarsha Rao 94624926+raoakarsha@users.noreply.github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com Co-authored-by: Sean Naren sean@grid.ai Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Mansy ahmed.mansy156@gmail.com Co-authored-by: mansy mansy@lightning.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Lee Jungwon 33821003+BongYang@users.noreply.github.com Co-authored-by: Nathaniel D'Amours 88633026+NathanielDamours@users.noreply.github.com Co-authored-by: Justin Goheen 26209687+JustinGoheen@users.noreply.github.com Co-authored-by: otaj 6065855+otaj@users.noreply.github.com Co-authored-by: Gautier Dagan s2234411@ed.ac.uk Co-authored-by: Akihiro Nitta nitta@akihironitta.com",0
Add missing docstring for LightningWork.stop() (#13368),0.62582844,Improved the error message when the LightningWork is missing the run method (#14759),,0
Fix typos in Checkpointing doc (#13827),0.6359708,all the checkpoint issues should be gone now (including backward support for old checkpoints),Co-authored-by: awaelchli aedu.waelchli@gmail.com,0
Fix mypy errors attributed to pytorch_lightning.loggers.comet (#13689),0.729944,Changed pytorch_lightning.logging to pytorch_lightning.loggers (#767),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: otaj ota@lightning.ai,1
Update LightningCLI test for new support in latest release of jsonargparse (#13805),0.79509914,- Changes in `LightningCLI` required for the new major release of jsonargparse v4.0.0 ([#10426](https://github.com/PyTorchLightning/pytorch-lightning/pull/10426)),,1
[Typo] update introduction.rst (#13791),0.510731,"In addition, we fixed:",,0
Rename spawn-based launchers (#13743),0.4522546,"- Redesigned process creation for spawn-based plugins (`DDPSpawnPlugin`, `TPUSpawnPlugin`, etc.) ([#10896](https://github.com/PyTorchLightning/pytorch-lightning/pull/10896))",,0
Lazy import check for neptune dependency (#13477),0.552031,"It now supports neptune-client 0.16.16 and neptune >=1.0, and we have replaced the log() method with append() and extend().",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com,0
Add ddp_notebook alias for ddp_fork (#13744),0.63100195,        Override to init DDP in a different way or use your own wrapper.,Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
Add logging messages to notify when FitLoop stopping conditions are met (#9749),0.6907315,- Added logging messages to notify when `FitLoop` stopping conditions are met ([#9749](https://github.com/Lightning-AI/lightning/pull/9749)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Promote the CLI out of utilities (#13767),0.58057064,Introducing CLI commands for apps (#13602)!,,0
App: prune requirements duplicity (#13739),0.86295664,Pruned requirements duplicity (#13739),prune requirements Co-authored-by: Akihiro Nitta nitta@akihironitta.com,1
Set default strategy to ddp_fork in interactive environments (#13746),0.66974795,"When using multiple devices, the strategy now defaults to ""ddp"" instead of ""ddp_spawn"" when none is set (#16388)",,0
Support setting the trainer reference recursively for ensembles (#13638),0.6828606,- Added support for recursively setting the `Trainer` reference for ensembles of `LightningModule`s ([#13638](https://github.com/Lightning-AI/lightning/pull/13638),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,0
Lazy import check for wandb dependency (#13474),0.51932955,Removed wandb logger's finalize method (#1193),,0
Add support for DDP fork (#13405),0.72819513,DDP custom implementation support (override these hooks):,Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
Update macos ci (#13794),0.42636395,The point of this release is more bug fixes ahead of v 1.0.0. We now have CI tests on TPU thanks to @zcain117 from Google! :slightly_smiling_face:,update macos ci,0
Fix to allow custom CheckpointIO with strategy classes (#13785),0.7090264,- Fixed and issue that prevented setting a custom `CheckpointIO` plugin with strategies ([#13785](https://github.com/Lightning-AI/lightning/pull/13785)),,1
Fix mypy errors attributed to pytorch_lightning/profilers/advanced.py (#13792),0.77200794,Removed the deprecated pytorch_lightning.profiler.* classes in favor of pytorch_lightning.profilers (#16059),Co-authored-by: otaj 6065855+otaj@users.noreply.github.com,1
Use correct python version in lightning component template (#13790),1.0000002,Use correct python version in lightning component template (#13790),use correct python version in lightning component template Co-authored-by: manskx ahmed.mansy156@gmail.com,1
Do not force sync_dist=True on epoch end (#13364),0.7148938,- Raised a warning instead of forcing `sync_dist=True` on epoch end ([13364](https://github.com/Lightning-AI/lightning/pull/13364)),Co-authored-by: rohitgr7 rohitgr1998@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Add auto_device_count and device name support (#13423),0.692613,        :param device_ids:,Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: awaelchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: mansy mansy@lightning.ai Co-authored-by: manskx mansy@lightning.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Mansy ahmed.mansy156@gmail.com Co-authored-by: otaj ota@lightning.ai Co-authored-by: Sean Naren sean@grid.ai Co-authored-by: Keiichi Kuroyanagi kuroyanagi.keiichi@gmail.com Co-authored-by: Martino Sorbaro martinosorb@users.noreply.github.com Co-authored-by: Wang Ran (汪然) wangr@smail.nju.edu.cn Co-authored-by: Rhys Goodall rhys.goodall@outlook.com Co-authored-by: Siyuan Li siyuanli.s.c@gmail.com Co-authored-by: Ekagra Ranjan ekagra.ranjan@gmail.com Co-authored-by: S. Kumano 54502860+s-kumano@users.noreply.github.com Co-authored-by: otaj 6065855+otaj@users.noreply.github.com Co-authored-by: Gautier Dagan gautierdagan2017@u.northwestern.edu Co-authored-by: Sherin Thomas sherinct@live.com Co-authored-by: Cyprien Ricque 48893621+Cyprien-Ricque@users.noreply.github.com Co-authored-by: Masahiro Wada argon.argon.argon@gmail.com Co-authored-by: nitinramvelraj 98356761+nitinramvelraj@users.noreply.github.com Co-authored-by: donlapark 10988155+donlapark@users.noreply.github.com Co-authored-by: Justin Goheen 26209687+JustinGoheen@users.noreply.github.com Co-authored-by: Shantam Gilra 64306405+shantam-8@users.noreply.github.com Co-authored-by: Bibhabasu Mohapatra 68384968+bibhabasumohapatra@users.noreply.github.com Co-authored-by: Jimmy Yao jiahaoyao.math@gmail.com Co-authored-by: Nikhil Shenoy nikhilshenoy98@gmail.com Co-authored-by: Sanjay Aradhyamath 57592361+samz5320@users.noreply.github.com,0
Fix gatekeeper minimum check (#13769),0.50177675,Configuration Validator (#9779),,0
fix mypy errors for loggers/wandb.py (#13483),0.695649,Changed WandbLogger(log_model={True/'all'}) to log models as artifacts (#6231),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com,0
Fix mypy errors attributed to pytorch_lightning.loggers.mlflow (#13691),0.7517803,Changed pytorch_lightning.logging to pytorch_lightning.loggers (#767),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: otaj 6065855+otaj@users.noreply.github.com,1
Fix mypy errors attributed to pytorch_lightning.loggers.tensorboard.py (#13688),0.70754266,Changed pytorch_lightning.logging to pytorch_lightning.loggers (#767),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Fix main progress bar counter when val_check_interval=int and check_val_every_n_epoch=None (#12832),0.79920065,- Fixed main progress bar counter when `val_check_interval=int` and `check_val_every_n_epoch=None` ([#12832](https://github.com/Lightning-AI/lightning/pull/12832),,1
Fix typos initialize in docs (#13557),0.49812573,Syntax changes are: ,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Fix mypy typing errors in pytorch_lightning/callbacks/model_checkpoint.py (#13617),0.7044261,- Deprecated `ModelCheckpoint.save_checkpoint` in favor of `Trainer.save_checkpoint` ([#12456](https://github.com/PyTorchLightning/pytorch-lightning/pull/12456)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Include app templates to the lightning and app packages (#13731),0.94886076,Included app templates to the lightning and app packages (#13731), Include app templates to the package  Co-authored-by: mansy mansy@lightning.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Simplify fetching's loader types (#13111),0.5274117,"Refactor dataloading, supports infinite dataloader (#955)",,0
[FIX] Native FSDP precision + tests (#12985),0.69187355,Native FSDP replaces Fairscale FSDP (#16400),,0
[pre-commit.ci] pre-commit suggestions (#13540),0.5955204,@akihironitta @awaelchli @Borda @carmocca @kaushikb11 @krshrimali @mauvilsa @otaj @pre-commit-ci @rohitgr7 @semaphore-egg @tkonopka @wayi1,updates: - github.com/psf/black: 22.3.0 → 22.6.0 Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
"Update typing-extensions requirement from <4.2.1,>=4.0.0 to >=4.0.0,<4.3.1 in /requirements (#13529)",0.49178338,Pkg: fix parsing versions by @Borda in https://github.com/Lightning-AI/lightning/pull/15401,Update typing-extensions requirement in /requirements Updates the requirements on typing-extensions to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: typing-extensions   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
CI: debug HPU flow (#13419),0.48327288,tests for val loop flow (#2605), Update the hpu-tests.yml to pull docker from vault fire & sudo habana-gaudi-hpus Check the driver status on gaudi server (#13718)  Co-authored-by: arao arao@habana.ai Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Akarsha Rao 94624926+raoakarsha@users.noreply.github.com,0
Use global_step while restoring logging step for old checkpoints (#13645),0.8219928,- Used `global_step` while restoring logging step for old checkpoints ([#13645](https://github.com/Lightning-AI/lightning/pull/13645)),Co-authored-by: Akihiro Nitta nitta@akihironitta.com,1
CI: fix ignored paths (#13733),0.5115456,- all the file path errors with loggers (txs @awaelchli),,0
Add back GPUAccelerator and deprecate it,0.52217835,- Removed deprecated `GPUStatsMonitor` callback ([#12554](https://github.com/Lightning-AI/lightning/pull/12554)),,0
Rename GPUAccelerator to CUDAAccelerator,0.5638751,Removed the deprecated pytorch_lightning.accelerators.GPUAccelerator in favor of pytorch_lightning.accelerators.CUDAAccelerator (#16050),,0
Remove conda installation guide from lightning docs (#13727),0.44245788,| Argument LightningCLI(auto_registry=...)                                                                   | 1.9             | Not necessary anymore                           |,Co-authored-by: Akihiro Nitta nitta@akihironitta.com,0
CI: Fix false negatives by the labeler (#13722),0.42700356,"Refactored training_batch + tests to verify correctness (#2327, #2328)","  Add change to src/lightning_app/*/   Update filter   Update config   Revert ""Add change to src/lightning_app/*/""   This reverts commit e411de153b5f3fb204b5c9be11f0a8d0d15c1269.  Examples  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",0
CI: revert skip jobs (#13715),0.45738664,"Removed experimental fault-tolerance support (#16516, #16533)",revert skip jobs,0
Allow CUDA and IPU tests without the CI environment var (#13676),0.496677,Removed implicit ui testing with testing.run_app_in_cloud in favor of headless login and app selection (#16741),,0
define app 0.6 (#13699),0.5752208,App,  define app 0.6   req   fix link   txt   ex   gk   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   Apply suggestions from code review   lapp 0.5.2   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
add codeownser for docs - @Felonious-Spellfire (#13717),0.43300682,Docs,,0
Change AWS credentials to Lightning ones (#13703),0.56831676,Add a secret to your Lightning account in lightning.ai (read more here),,0
Run standalone tests in batches (#13673),0.6011137,"nb_test_batches to num_test_batches,",,0
Fix trainer.predict(return_predictions=False) does not track batch_indices (#13629),0.82064724,- Fixed `Trainer.predict(return_predictions=False)` to track prediction's batch_indices ([#13629](https://github.com/Lightning-AI/lightning/pull/13629)), Pull request for fixing issue #13580 chlog and test disable track for epoch  Co-authored-by: rohitgr7 rohitgr1998@gmail.com,1
Update docs README on how to build docs (#13465),0.6551891,Docs improvements,update docs README on how to build docs Co-authored-by: Jeroen Delcour jeroen@track32.nl,0
Bump pypa/gh-action-pypi-publish from 1.4.1 to 1.5.0 (#13621),0.41192484,This release has breaking API changes. See #124 for all details. ,Bumps pypa/gh-action-pypi-publish from 1.4.1 to 1.5.0. - Release notes - Commits  updated-dependencies: - dependency-name: pypa/gh-action-pypi-publish   dependency-type: direct:production   update-type: version-update:semver-minor ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Add CI app cloud e2e & fix setup UI download (#13499),0.43566924,Resolved a bug where the install command was not installing the latest version of an app/component by default (#14181), Add CI app e2e update UserID fix apps cleanup download frontend inside setup.py  Co-authored-by: mansy mansy@lightning.ai Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Move s-rog (myself) from core to alumni (#13694),0.43357402,Moved TrainsLogger to Bolts (#2384),Update governance.rst,0
Fix mypy errors attributed to pytorch_lightning.utilities.distributed (#13678),0.7314306,Removed the deprecated pytorch_lightning.utilities.cli module in favor of pytorch_lightning.cli (#16116),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
CI: debug TPU failing tests (#13679),0.5727833,This means we fixed many TPU bugs we hadn’t caught before because we had no tests.,"  list pytest   docs   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   list   test   fix GK   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
bump base NGC image (#13346),0.33183008,    # 2: Convert the image into a PIL Image to bytes and encode it with base64,,0
Add --app_args support from the CLI (#13625),1.0,Add --app_args support from the CLI (#13625),,1
add testing PT 1.12 (#13386),0.60746104,Updated app testing (#16000), add testing PT 1.12 Fix quantization tests Fix another set of tests Fix check since https://github.com/pytorch/pytorch/pull/80139 is only going to be available for 1.13 Skip this test for now for 1.12  Co-authored-by: SeanNaren sean@grid.ai,0
CI: Update docs-related workflows (#13547),0.4843391,Docs improvements, update  Co-authored-by: Jirka jirka.borovec@seznam.cz,0
Remove deprecated Strategy.post_dispatch (#13461),0.63861036,Renamed Strategy.reduce to Strategy.all_reduce in all strategies (#16370),  Remove deprecated Strategy.post_dispatch   changelog   remove unused imports ,0
CI: adjust gatekeeper (#13604),0.4728244,Configuration Validator (#9779),adjust gatekeeper,0
Introduce ServableModuleValidator Callback (#13614),0.7187473,"trainer = Trainer(..., callbacks=[ServableModuleValidator()])",  wip   wip   wip   wip   wip   wip   wip   wip   Update tests/tests_pytorch/serve/test_servable_module_validator.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update tests/tests_pytorch/serve/test_servable_module_validator.py  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update src/pytorch_lightning/serve/servable_module_validator.py  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update src/pytorch_lightning/serve/servable_module_validator.py  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update src/pytorch_lightning/serve/servable_module_validator.py  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   Typing improvements   wip   update doc   update   update   update   update   update   update   update   update   update   update   update   Update examples/pl_servable_module/production.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   update   update   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Add lightning_app docs (#13656),0.81259674,Update the Lightning App docs (#13537),"  update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   update   update   update   update   update   update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update latest docs   remove image   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   ci   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   make   Update .gitignore   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update .github/workflows/docs-deploy.yml  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   update   update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   update   update   update   update   update   Update docs/source-app/_templates/theme_variables.jinja   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/api_reference/api_references.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/api_reference/api_references.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/api_reference/api_references.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/code_samples/convert_pl_to_app/train.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/core_api/lightning_app/communication_content.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/examples/model_server_app/model_server_app_content.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/get_started/training_with_apps.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/get_started/training_with_apps.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/get_started/training_with_apps.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/examples/hpo/hpo.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/core_api/lightning_app/communication_content.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/core_api/lightning_app/communication_content.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/core_api/lightning_app/dynamic_work_content.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/examples/github_repo_runner/github_repo_runner_content.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/examples/github_repo_runner/github_repo_runner_content.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/examples/github_repo_runner/github_repo_runner_step_2.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/examples/github_repo_runner/github_repo_runner_step_4.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/examples/model_server_app/model_server.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   update   Update docs/source-app/core_api/lightning_app/dynamic_work_content.rst   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/examples/github_repo_runner/app.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/examples/github_repo_runner/app.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   update   update   update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com",1
Fix mypy errors attributed to pytorch_lightning/loops/epoch/training_epoch_loop.py (#13555),0.71953964,Removed pytorch_lightning/trainer/training_loop.py (#7985),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Run only CUDA tests on Azure GPU CI (#13651),0.5506838,Moved torch.cuda.set_device() to enable collective calls earlier in setup (#8312),,0
[Hot Fix] Resolve Skipper CI (#13670),0.42169517,Resolve TPU miss rendezvous (#6781),,0
Remove a bool type assignment in lr_finder.py (#13652),0.5865451,move lr_finder (#3434),,0
CI: adjust GPU timeout (#13628),0.5410043,refactored GPU backend __step (#3120),,0
Bump actions/cache from 2 to 3 (#13619),0.41179082,Removed deprecated callbacks (#3979),,0
Optimize CI for PL tests to run only when PL is modified (#13661),0.45742792,test_percent_check in favour of limit_test_batches,,0
Simplify predict attribute (#13635),0.4401839,Simplify optimization Logic (#4984),,0
Simplify TPUSpawn rank management (#11163),0.54777795,Enabled manual optimization for TPUs (#8458),Co-authored-by: Kaushik B kaushikbokka@gmail.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,0
Increase typing_extensions minimal version (#13657),0.42794842,| Import pl.core.mixins.DeviceDtypeModuleMixin                                                           | 1.10             | No longer supported |,  Increase typing_extensions requirement   it's rich! ,0
CI: use testing with PL released (#13647),0.5619545,The point of this release is more bug fixes ahead of v 1.0.0. We now have CI tests on TPU thanks to @zcain117 from Google! :slightly_smiling_face:,,0
Change the return type of tune() in trainer.py to TypedDict (#13631),0.78195107,trainer.tune() now returns the tuning result (#7258),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
"Revert ""Update Lightning App docs (#13537)"" (#13655)",0.6965061,Update the Lightning App docs (#13537),This reverts commit cd31ba3f87d51b1e09e466c34b9b530809b38ddd.,0
Update Lightning App docs (#13537),0.98532844,Update the Lightning App docs (#13537),"  update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   update   update   update   update   update   update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update latest docs   remove image   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   ci   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   make   Update .gitignore   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update .github/workflows/docs-deploy.yml  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   update   update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   update   update   update   update   update   Update docs/source-app/_templates/theme_variables.jinja   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/api_reference/api_references.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/api_reference/api_references.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/api_reference/api_references.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/code_samples/convert_pl_to_app/train.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/core_api/lightning_app/communication_content.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/examples/model_server_app/model_server_app_content.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/get_started/training_with_apps.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/get_started/training_with_apps.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/get_started/training_with_apps.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/examples/hpo/hpo.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/core_api/lightning_app/communication_content.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/core_api/lightning_app/communication_content.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/core_api/lightning_app/dynamic_work_content.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/examples/github_repo_runner/github_repo_runner_content.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/examples/github_repo_runner/github_repo_runner_content.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/examples/github_repo_runner/github_repo_runner_step_2.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/examples/github_repo_runner/github_repo_runner_step_4.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/examples/model_server_app/model_server.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   update   Update docs/source-app/core_api/lightning_app/dynamic_work_content.rst   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/examples/github_repo_runner/app.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/examples/github_repo_runner/app.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   update   update   update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com",1
Hotfix mypy checking (#13653),0.46206003,Do not override PYTHONWARNINGS (#4700),mypy hotfix,0
Remove deprecated max_steps=None (#13591),0.6932518,"Deprecated setting Trainer(max_steps=None); To turn off the limit, set Trainer(max_steps=-1) (default) (#9460)",  Remove max_steps=None   Update changelog   Update docs   Unused import   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Enable using any Sampler in distributed environment in Lite (#13646),0.8218705,- Enabled using any Sampler in distributed environment in Lite ([#13646](https://github.com/Lightning-AI/lightning/pull/13646)),,1
fix mypy typing errors in pytorch_lightning/strategies/parallel.py (#13556),0.66570413,    * Renamed the `ParallelPlugin` to `ParallelStrategy` ([#11123](https://github.com/PyTorchLightning/pytorch-lightning/pull/11123)),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
fix mypy typing errors in pytorch_lightning/strategies/horovod.py (#13570),0.6881781,- Fixed check for horovod module ([#12377](https://github.com/PyTorchLightning/pytorch-lightning/pull/12377)),,0
Remove deprecated LightningDistributed (#13549),0.82882375,- Removed deprecated `LightningDistributed` ([#13549](https://github.com/Lightning-AI/lightning/pull/13549)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Update CHANGELOG after the 1.6.5 release (#13641),0.6462134,This section outlines notable changes that are not backward compatible with previous versions. The full list of changes and removals can be found in the Full Changelog below.,,0
Remove deprecated ClustertEnvironment methods (#13458),0.65184456,Deprecated ClusterEnvironment.creates_children() in favor of ClusterEnvironment.creates_processes_externally (property) (#10106),  Remove deprecated ClustertEnvironment methods   update changelog   ignore typing error   Co-authored-by: Akihiro Nitta nitta@akihironitta.com,0
Remove deprecated LightningModule.on_post_move_to_device (#13548),0.9368826,Deprecated the LightningModule.on_post_move_to_device method (#9525),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Remove deprecated Trainer.slurm_job_id (#13459),0.75957227,- Removed deprecated `Trainer.slurm_job_id` in favor of `SLURMEnvironment.job_id` ([#13459](https://github.com/Lightning-AI/lightning/pull/13459)),,1
fix mypy typing errors in pytorch_lightning/strategies/dp.py (#13564),0.6884074,- Fixed wrong typehint for `Trainer.lightning_optimizers` ([#11155](https://github.com/PyTorchLightning/pytorch-lightning/pull/11155)),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: otaj ota@lightning.ai,0
Removed deprecated pytorch_lightning.overrides.distributed.IndexBatchSamplerWrapper.batch_indices  (#13565),0.9234586,- Deprecated the access to the attribute `IndexBatchSamplerWrapper.batch_indices` in favor of `IndexBatchSamplerWrapper.seen_batch_indices` ([#10870](https://github.com/PyTorchLightning/pytorch-lightning/pull/10870)),  Removed the deprecated   method   Removed deprecated  IndexBatchSamplerWrapper.batch_indices   Update src/pytorch_lightning/CHANGELOG.md   Missed code   Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Bump docker/setup-buildx-action from 1 to 2 (#13618),0.56505406,"Show a message when BuildConfig(dockerfile=""..."") is passed but a Dockerfile file is already present in the Work (#15799)",Bumps docker/setup-buildx-action from 1 to 2. - Release notes - Commits  updated-dependencies: - dependency-name: docker/setup-buildx-action   dependency-type: direct:production   update-type: version-update:semver-major ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Bump actions/upload-artifact from 2 to 3 (#13622),0.45519862,Add support to upload files to the Drive through an asynchronous upload_file endpoint (#14703),Bumps actions/upload-artifact from 2 to 3. - Release notes - Commits  updated-dependencies: - dependency-name: actions/upload-artifact   dependency-type: direct:production   update-type: version-update:semver-major ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Bump codecov/codecov-action from 1 to 3 (#13620),0.40624058,- Removed the deprecated code in:,,0
Remove add_to_queue and remove_from_queue from LightningModule (#13600),0.7863933,- Removed the deprecated `LightningModule.add_to_queue` and `LightningModule.get_from_queue` method ([#13600](https://github.com/Lightning-AI/lightning/pull/13600)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
CI: hotfix gatekeeper (#13606),0.43715435,Configuration Validator (#9779),  CI: hotfix gatekeeper   no min   min 1 ,0
Remove redundant GPU test (#13623),0.5861711,refactored GPU backend __step (#3120),Remove redundant test,0
CI: Update labeler bot (#13624),0.40602043,Renamed several Trainer atributes:  (#567),Update labeler,0
Fix default value for enable_progress_bar in docs (#13584),0.60734594,Changed the default progress bar to print to stdout instead of stderr (#531),fix typo,0
CI: Enable dependabot for GitHub Actions (#13589),0.36814493,Add missing python-multipart dependency (#17244),  Enable dependabot on GHA   Update comment   Update PR limit   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Fix mypy errors attributed to pytorch_lightning.loggers.base.py (#13494),0.81917524,Changed pytorch_lightning.logging to pytorch_lightning.loggers (#767),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Fix mypy errors attributed to pytorch_lightning.loggers.csv_logs.py (#13538),0.754604,Changed pytorch_lightning.logging to pytorch_lightning.loggers (#767),Co-authored-by: Akihiro Nitta nitta@akihironitta.com,1
CI/CD: Refactor building docker images (#13576),0.6099316,Remove unnecessary intermediate layers in Dockerfiles (#5697), Refactor docker builds in CI Reduce duplicate and merge two workflows push: bool expression Extend timeout for ipu builds Update concurrency group Define env for push to hub Rename workflow Fix bool expressions Remove unnecessary trigger paths Remove unused env var Update job names Trim timeout rename  Co-authored-by: Jirka jirka.borovec@seznam.cz,0
Restore log step during restart (#13467),0.6151113,- Used `global_step` while restoring logging step for old checkpoints ([#13645](https://github.com/Lightning-AI/lightning/pull/13645)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Adds is last batch (#13550),0.42857224,"    batch_size=32,",Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,0
Remove deprecated pytorch_lightning.core.decorators.parameter_validation (#13514),0.9202063,- Removed deprecated `pytorch_lightning.core.decorators.parameter_validation` from `decorators` ([#13514](https://github.com/Lightning-AI/lightning/pull/13514)),  Removal of depreciated code from decorators   Update CHANGELOG.md   Removed imports ,1
setup: set default metadata (#13571),0.45186982,from setuptools import setup,,0
CI: Update mypy workflow (#13574),0.40753245,"Renamed step_idx to step, epoch_idx to epoch, max_num_epochs to max_epochs and min_num_epochs to min_epochs (#589)",  Fix pyproject.toml   Add TODO   Update mypy workflow ,0
CI: Add PR labeler (#13475),0.397462,Renamed several Trainer atributes:  (#567),  Add pr labeler   Triger on docs change   Make mutually exclusive   Add requirements   files   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
cd: releasing packages (#13489),0.4857062,This release includes:,  assist   split   need   inverse   checkout   json   include   unzip   mirror   fire   twine   for tar -xf   clean   3.8   ls ,0
Fix TPU circleci tests (#13432),0.5649286,Resolve TPU miss rendezvous (#6781),  Fix TPU circleci tests   Fix TPU circleci tests   Fix TPU circleci tests   Fix TPU circleci tests   Fix TPU circleci tests   Fix rank issue   Fix rank issue   debug alternative fix   Revert properties   Co-authored-by: awaelchli aedu.waelchli@gmail.com,0
CI: Define reusable workflow - check schema (#13562),0.3547456,Renames model steps (#1051),  Decouple schema check   Update workflow name   Don't run if dir not found ,0
Fix mypy errors attributed to pytorch_lightning.loggers.logger.py (#13541),0.83300185,Changed pytorch_lightning.logging to pytorch_lightning.loggers (#767),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com,1
fix mypy typing errors in pytorch_lightning/tuner/lr_finder.py (#13513),0.6561628,Removed the deprecated pytorch_lightning.profiler.* classes in favor of pytorch_lightning.profilers (#16059),Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,0
Remove deprecated on_keyboard_interrupt (#13438),0.73020935,Deprecated on_keyboard_interrupt callback hook in favor of new on_exception hook (#9260),Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
fix mypy typing errors in pytorch_lightning/strategies/single_tpu.py (#13534),0.69926757,- Fixed wrong typehint for `Trainer.lightning_optimizers` ([#11155](https://github.com/PyTorchLightning/pytorch-lightning/pull/11155)),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: awaelchli aedu.waelchli@gmail.com,0
Fix type hints of callbacks/finetuning.py (#13516),0.55874276,Updated references to self.forward() to instead use the __call__ interface. (#1211),,0
fix mypy typing errors in pytorch_lightning/strategies/ddp2.py (#13535),0.68917406,"| Argument Trainer(strategy=""ddp2"") and class pytorch_lightning.strategies.DDP2Strategy                    | 1.8             | No longer supported                             |", fix typing in strategies/ddp2.py Use quotes instead of future.annotations for forward references  Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,0
fix mypy typing errors in pytorch_lightning/strategies/single_device.py (#13532),0.6926039,- Deprecated `Trainer.devices` in favor of `Trainer.num_devices` and `Trainer.device_ids` ([#12151](https://github.com/PyTorchLightning/pytorch-lightning/pull/12151)), fix typing in strategies/single_device.py Make assert statement more explicit  Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,0
code-owners for App (#13497),0.4424347,"If we forgot someone due to not matching the commit email with the GitHub account, let us know :]",  condensers for App   pkg   TestsUpdate .github/CODEOWNERS   Co-authored-by: Mansy ahmed.mansy156@gmail.com Co-authored-by: Mansy ahmed.mansy156@gmail.com,0
Add CI for app examples (#13495),0.44223094,Removed implicit ui testing with testing.run_app_in_cloud in favor of headless login and app selection (#16741), Add CI for app examples  Co-authored-by: manskx mansy@lightning.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz,0
Remove redundant progress bar refresh (#13462),0.65922904,Better progress bar (#16695),,0
Add CI for python lightning app Python unit tests (#13491),0.5757365,Removed implicit ui testing with testing.run_app_in_cloud in favor of headless login and app selection (#16741),"  Update lightning_app src   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update lightning app tests   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   add CI   update tests   requirements   fix version tests   todo   fix tests   fix tests   fix tests   fix tests   fix formatting   Co-authored-by: mansy mansy@lightning.ai Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: awaelchli aedu.waelchli@gmail.com",0
lightning entry point (#13490),0.5609586,Introduce lightning connect (#14452),,0
fix mypy typing errors in pytorch_lightning.setup.py (#13472),0.6926807,Removed the deprecated pytorch_lightning.core.lightning module in favor of pytorch_lightning.core.module (#16318),Fix typing in _load_py_module function,0
Move deepspeed summary test to correct folder (#13478),0.53643703,- Fixed deepspeed keeping old sub-folders in same ckpt path ([#12194](https://github.com/PyTorchLightning/pytorch-lightning/pull/12194)),,0
Remove redundant shebang from source files (#13479),0.49478132,Parse all lines in app file looking for shebangs to run commands (#15714),,0
ci: drop false download artifact (#13473),0.39937532,Resolved a bug where the install command was not installing the latest version of an app/component by default (#14181),,0
Add lightning app examples (#13456),0.6141535,Update the Lightning App docs (#13537),"  add lightning app examples   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   fix CI   rm init   restucture app examples   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  img  Co-authored-by: mansy mansy@lightning.ai Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz",0
adding LAI test (#13321),0.41115785,@eng-yue @ethanwharris @Borda @awaelchli @carmocca,"  tests   ci   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update tests   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: mansy mansy@lightning.ai",0
fix mypy typing errors in lightning/trainer/optimizers.py (#13470),0.7396198,- Fixed wrong typehint for `Trainer.lightning_optimizers` ([#11155](https://github.com/PyTorchLightning/pytorch-lightning/pull/11155)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
More clear docs for LightningDataModule (#13464),0.68204236,- Added profiling of `LightningDataModule` hooks ([#12971](https://github.com/Lightning-AI/lightning/pull/12971))," More clear docs for LightningDataModule  More clear docs for the methods add_argparse_args and from_argparse_args of the class LightningDataModule.  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
Typo in trainer/supporters.py (#13455),0.6016591,"Removed deprecated trainer attributes - on_cpu, on_tpu, use_tpu, on_gpu, use_dp, use_ddp, use_ddp2, use_horovod, use_single_gpu (#7501)",Typo,0
Remove unused argument model (#13454),0.5918314,Refactor Model backward (#2276),Remove unused argument model in the doc of verify_loop_configurations.,0
Typo in tuner/lr_finder.py (#13453),0.7344407,tuner.lr_find(...),Typo,1
Fix typo in Loop.replace docstring (#13452),0.4661832,refactored inner eval loop (#3141),Typo,0
Fix typo in _block_parallel_sync_behavior docstring (#13451),0.5924242,Moved block_ddp_sync_behaviour out of TrainingBatchLoop to loop utilities (#9192),Typo,0
CI: abstract and make full pkg check (#13460),0.40675944,Parsed local package versions (#13933), cut actions sequence ver,0
Add BaseModelCheckpoint class to inherit from (#13024),0.5916806,"Removed deprecated ModelCheckpoint arguments prefix, mode=""auto"" (#6162)",Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Simplify list extension (#13435),0.4439032,    # 2. Add the outputs to the list,,0
fixed doc of timer (#13393),0.44299638,Enforce an epoch scheduler interval when using SWA (#6588), fix doc of timer,0
Add flash[image] dependency in Active learning example (#13442),0.3481986,- Added support for using custom Trainers that don't include callbacks using the CLI ([#13138](https://github.com/Lightning-AI/lightning/pull/13138)),,0
"Update wandb requirement from <0.12.19,>=0.8.21 to >=0.8.21,<0.12.20 in /requirements (#13415)",0.6379646,Removed wandb logger's finalize method (#1193),Co-authored-by: Jirka jirka.borovec@seznam.cz,0
Remove unused docstring parameter device (#13448),0.67133033,We cleaned up the properties related to device indices (#14829).,,0
Set timeout for DDPSpawnStrategy (#13383),0.69817615,"- Added a `timeout` argument to `DDPStrategy` and `DDPSpawnStrategy`. ([#13244](https://github.com/Lightning-AI/lightning/pull/13244), [#13383](https://github.com/Lightning-AI/lightning/pull/13383))",Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Fix validation when accelerator is a string (#13417),0.6589526,Ensure accelerator is valid if running interactively (#5970),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
Convert validation loop config warnings to PossibleUserWarning (#13377),0.7639643,-  Converted validation loop config warnings to `PossibleUserWarning` ([#13377](https://github.com/Lightning-AI/lightning/pull/13377)),Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
"Update comet-ml requirement from <=3.28.2,>=3.1.12 to >=3.1.12,<3.31.6 in /requirements (#13414)",0.54061234,Using .comet.config file for CometLogger (#1913),Update comet-ml requirement in /requirements Updates the requirements on comet-ml to permit the latest version.  updated-dependencies: - dependency-name: comet-ml   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Call set_epoch for distributed batch samplers (#13396),0.79995435,The loops now call .set_epoch() also on batch samplers if the dataloader has one wrapped in a distributed sampler (#13396),Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
Remove remaining old-style AcceleratorConnector properties (#13412),0.66555595,AcceleratorConnector rewrite,Co-authored-by: Jirka jirka.borovec@seznam.cz,0
Unpin protobuf version and update tensorboard version (#13259),0.67102706,Improved the error message for installing tensorboard or tensorboardx (#17053),  Remove protobuf from base req   Update tensorboard version   Co-authored-by: Jirka jirka.borovec@seznam.cz,0
Fix docstring typo (#13447),0.44549376,Docs improvements,,0
fix PL release docker (#13439),0.47588754,Pkg: fix parsing versions by @Borda in https://github.com/Lightning-AI/lightning/pull/15401,,0
Modified python version check to accommodate for legacy version styles (#13420),0.68125224,Use correct python version in lightning component template (#13790),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Update gather_all_tensors to handle tensors of different sizes (#12630),0.81309927,Auto convert tensors to contiguous format when gather_all (#4907),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: awaelchli aedu.waelchli@gmail.com,1
[Docs] Fix README.md in lightning/examples/pl_basics (#13380),0.58906007,Update the Lightning App docs (#13537),  Change the path of the command execution folder from mnist_examples to convert_from_pt_to_pl   Add a guide to add PYTHONPATH   Fix Lightning Lite link   Remove duplicate   Add note   Co-authored-by: Akihiro Nitta nitta@akihironitta.com,0
Remove support for DDP2 strategy (#12705),0.8652758,Removed support for the DDP2 strategy,Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Jirka jirka.borovec@seznam.cz,1
Add model summary when using DeepSpeed Stage 3 (#13427),0.762385,- Fixed Model Summary when using DeepSpeed Stage 3 ([#13427](https://github.com/Lightning-AI/lightning/pull/13427)),,1
CI: fix requirements freeze (#13441),0.6981349,Setup: added requirement freeze for the next major version (#14480),"  allow freeze   ci   typo   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  ipu  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
Rename old references to training type plugin in tests (#13421),0.53769755,Refactored setup_training and remove test_mode (#5388),,0
Better errors for logging corner cases (#13164),0.5888127,Un-balanced logging properly supported (#5119),,0
[CLI] Support custom trainers without callbacks (#13138),0.8395207,- Added support for using custom Trainers that don't include callbacks using the CLI ([#13138](https://github.com/Lightning-AI/lightning/pull/13138)),,1
"Update numpy requirement from <1.22.5,>=1.17.2 to >=1.17.2,<1.23.1 in /requirements (#13413)",0.50067234,Dropped official support/testing for older PyTorch versions <1.3 (#1917),Update numpy requirement in /requirements Updates the requirements on numpy to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: numpy   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
"Update neptune-client requirement from <0.16.3,>=0.10.0 to >=0.10.0,<0.16.4 in /requirements (#13416)",0.76887006,"It now supports neptune-client 0.16.16 and neptune >=1.0, and we have replaced the log() method with append() and extend().",Update neptune-client requirement in /requirements Updates the requirements on neptune-client to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: neptune-client   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,1
"Remove unnecessary endpoint logic, rename collaborative to hivemind (#13392)",0.6973485,"    * Removed unnecessary endpoint logic, renamed `collaborative` to `hivemind` ([#13392](https://github.com/Lightning-AI/lightning/pull/13392))","  Remove endpoint after collaborate app/dht CLI   Fix references, change filename   Add CHANGELOG.md   Address review   Co-authored-by: Jirka jirka.borovec@seznam.cz",0
Clarify when and why LearningRateScheduler is called (#13267),0.5896637,"If you need to customize the learning rate scheduler configuration, you can do so by overriding:",Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
create meta package [RFC] (#13327),0.44048098,Meta Module,  placeholder   move setup_tools & abstract about   adjust lightning-app   notes   lightning about   lightning init   CI check   ci   install   adjust manifest & mv chlog   manifest   pkg   mv setup   parse_requirements   lit   ci - pytorch   wrap func   ci   cd draft   generate lit   pkg   utf-8   root pkg   req.   ver   mypy   try check   meta pkg   meta pkg - vars   meta pkg - pruning   meta pkg - fixing   fix PL for meta   multi-line wrapper   hack manifest   ci   fix docstr   fixing   ci & mypy   links ,0
Merge pull request #13123 from Lightning-AI/mps_accelerator,0.6111875,"  [#14537](https://github.com/Lightning-AI/lightning/pull/14537),",MPS Accelerator,0
Use workflow name for docs page redirector (#13193),0.33969292,"All documentation and examples are now recommending the new, less ambiguous names.",Fix docs link,0
Rename CollaborativeStrategy to HivemindStrategy (#13388),0.70941633,    * Renamed `CollaborativeStrategy` to `HivemindStrategy` ([#13388](https://github.com/Lightning-AI/lightning/pull/13388)),,1
Fix mypy errors for model summary utilities (#13384),0.5133767,import my_code.models,,0
Reroute profiler to profilers (#12308),0.66124743,Moved profilers to their own file (#7822),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com,0
Rename profiler to profilers (#12308),0.6966027,Moved profilers to their own file (#7822),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com,0
Fix typo in CI/CD doc (#13196),0.46443033,Syntax changes are: , fix typo in ci cd doc fix path to release-docker.yaml Update instance type Update gpu instance type  Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
DummyLogger can be called with unknown methods (#13224),0.5819311,- Added support for calling unknown methods with `DummyLogger` ([#13224](https://github.com/Lightning-AI/lightning/pull/13224),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
"Update fsspec[http] requirement from !=2021.06.0,<=2022.2.0,>=2021.05.0 to >=2021.05.0,!=2021.06.0,<2022.6.0 in /requirements (#13366)",0.47213453,Setup: added requirement freeze for the next major version (#14480),Update fsspec[http] requirement in /requirements Updates the requirements on fsspec[http] to permit the latest version. - Release notes - Commits  updated-dependencies: - dependency-name: fsspec[http]   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Improved Deepspeed Imports (#13223),0.66632855,Support for manual optimization with DeepSpeed (#7970),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka jirka.borovec@seznam.cz,0
"Update numpy requirement from <=1.22.3,>=1.17.2 to >=1.17.2,<1.22.5 in /requirements (#13361)",0.4853759,Dropped official support/testing for older PyTorch versions <1.3 (#1917),Update numpy requirement in /requirements Updates the requirements on numpy to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: numpy   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
"Update omegaconf requirement from <=2.1.*,>=2.0.5 to >=2.0.5,<2.3.0 in /requirements (#13167)",0.49411005,Setup: added requirement freeze for the next major version (#14480),Update omegaconf requirement in /requirements Updates the requirements on omegaconf to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: omegaconf   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
"Update hydra-core requirement from <=1.1.*,>=1.0.5 to >=1.0.5,<1.3.0 in /requirements (#13309)",0.6695046,Temporarily removed support for Hydra multi-run (#15737),Update hydra-core requirement in /requirements Updates the requirements on hydra-core to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: hydra-core   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
"Update horovod requirement from !=0.24.0,<=0.24.3,>=0.21.2 to >=0.21.2,!=0.24.0,<0.25.1 in /requirements (#13363)",0.5109481,Setup: added requirement freeze for the next major version (#14480),Update horovod requirement in /requirements Updates the requirements on horovod to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: horovod   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Adds Sampler Wrappers for custom samplers in distributed environment (#12959),0.7842747,It is now possible to use custom samplers in a distributed environment without the need to set replace_ddp_sampler=False and wrap your sampler manually with the DistributedSampler.,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
"Update gcsfs requirement from <=2022.2.0,>=2021.5.0 to >=2021.5.0,<2022.6.0 in /requirements (#13168)",0.4796694,Setup: added requirement freeze for the next major version (#14480),Update gcsfs requirement in /requirements Updates the requirements on gcsfs to permit the latest version. - Release notes - Commits  updated-dependencies: - dependency-name: gcsfs   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
"Update gym[classic_control] requirement from <=0.23.1,>=0.17.0 to >=0.17.0,<0.24.2 in /requirements (#13307)",0.58373755,Removed the deprecated TrainerLoggingMixin class (#8609),Update gym[classic_control] requirement in /requirements Updates the requirements on gym[classic_control] to permit the latest version. - Release notes - Commits  updated-dependencies: - dependency-name: gym[classic_control]   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
"Update wandb requirement from <0.12.17,>=0.8.21 to >=0.8.21,<0.12.19 in /requirements (#13308)",0.6441141,Removed wandb logger's finalize method (#1193),Update wandb requirement in /requirements Updates the requirements on wandb to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: wandb   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
Add XLAEnvironment plugin (#11330),0.5813229,  * Deprecated the internal `XLADeviceUtils.xla_available` staticmethod, add xla environment class add api reference integrate use xenv remove properties  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Kaushik B kaushikbokka@gmail.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
Fix changelog of setting back benchmark=False by default (#13194),0.6125438,"Changed default for DeepSpeed CPU Offload to False, due to prohibitively slow speeds at smaller scale (#6262)",Update changelog,0
Remove pytorch lightning.callbacks.lr monitor.learning rate monitor.lr_sch_names (#13353),0.85951585,- Removed deprecated `pytorch_lightning.callbacks.lr_monitor.LearningRateMonitor.lr_sch_names` ([#13353](https://github.com/Lightning-AI/lightning/pull/13353)),Co-authored-by: Raymond G Schireman raymond.schireman@uvm.edu Co-authored-by: Akihiro Nitta nitta@akihironitta.com,1
Improve support for custom DataLoaders when instantiated in *_dataloader hook (#12981),0.7701892,- Improved support for custom `DataLoader`s when instantiated in `*_dataloader` hook ([#12981](https://github.com/Lightning-AI/lightning/pull/12981)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com,1
"Update torchmetrics requirement from <=0.7.2,>=0.4.1 to >=0.4.1,<0.9.2 in /requirements (#13275)",0.7219113,    * Some configuration errors that were previously raised as `MisconfigurationException`s will now be raised as `ProcessRaisedException` (torch>=1.8) or as `Exception` (torch<1.8),Update torchmetrics requirement in /requirements Updates the requirements on torchmetrics to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: torchmetrics   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,1
Fix LightningCLI signature parameter resolving for some lightning classes (#13283),0.91475713,- Fixed ``LightningCLI`` signature parameter resolving for some lightning classes ([#13283](https://github.com/Lightning-AI/lightning/pull/13283)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
CI: Build new HPU docker image (#13352),0.4151541,Remove unnecessary intermediate layers in Dockerfiles (#5697),Build new hpu docker image,0
[BUG] estimated_stepping_batches requires distributed comms in configure_optimizers for DeepSpeedStrategy (#13350),0.6477846,"Implemented DeepSpeedPlugin._setup_model_and_optimizers (#10009, #10064)",,0
Use run name for logging with WandbLogger (#12604),0.8013414,"- The `WandbLogger` will now use the run name in the logs folder if it is provided, and otherwise the project name  ([#12604](https://github.com/Lightning-AI/lightning/pull/12604))",Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com,1
Deprecate CLI registries and update documentation (#13221),0.5997158,"The new version renders the registries and the auto_registry flag, introduced in 1.6.0, unnecessary, so we have deprecated them.",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Future 5/n: Move requirements (#13306),0.53171456,move specific accelerator code (#3457),Co-authored-by: Jirka jirka.borovec@seznam.cz,0
Update HPU Dockerfile to latest version (#13344),0.45098114,Moveed HPU broadcast override to the HPU strategy file (#17011),,0
Update old PL links (#13349),0.5055989,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",,0
Enable timeout for DDPStrategy (#13244),0.6800889,"- Added a `timeout` argument to `DDPStrategy` and `DDPSpawnStrategy`. ([#13244](https://github.com/Lightning-AI/lightning/pull/13244), [#13383](https://github.com/Lightning-AI/lightning/pull/13383))",,0
EarlyStopping logging on rank 0 only (#13233),0.6754419,Removed restrictions in the Trainer that loggers can only log from rank 0; the existing logger behavior has not changed (#8608),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
update tutorials (#13268),0.44400853,1. Create the Trainer,,0
Fix torch.distributed._sharded_tensor DeprecationWarning (#13261),0.7416381,Corrected call to torch.no_grad (#5124),,1
Added multi-optimizer tests with hpu (#13217),0.6278708,Refactored optimizer (#4658),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Make plugin compatible for hpu release1.5 (#13338),0.47262985,Moveed HPU broadcast override to the HPU strategy file (#17011),,0
Fix ci for IPU (#13340),0.4886067,Raised a MisconfigurationException if batch transfer hooks are overriden with IPUAccelerator (13961),,0
[DOCS] Updated link for lightning_lite.rst (#13331),0.6164018,Update the Lightning App docs (#13537),Updated link,0
Docs for LAI (#13312),0.65990484,Release LAI docs as stable (#14250),"  edit   docs   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   fixing   clean generated   ignore   pre-commit   ci   ci   Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
temp disable install app from source (#13318),0.46350774,Resolved a bug where the install command was not installing the latest version of an app/component by default (#14181),setup,0
CI: adjust names & skip app if pytorch (#13317),0.51556903,Improved PyTorchProfiler chrome traces names (#8009),"  CI: renames azure   check   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   skip lightning_app   ignore   .   true block   ci   ci   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
Add root README (#13313),0.41395256,# figure out the root node addr,"  add main page   update   move   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   fix link   copy over from main repo   formatting   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz",0
Update README and approval config (#13311),0.49371916,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ","  Update README and approval config   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
CI: block app contrib. (#13310),0.44193026,Application storage prefix moved from app_id to project_id/app_id (#14583),  CI: block app contrib.   format   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Copy README.md in preparation for the new one (#13305),0.5275933,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",,0
Delete leftover directory after rename (#13302),0.49604854,Rename failed -> error in tables (#15608),Delete pytorch_lightning/demos directory,0
Introduce Lightning App (#13303),0.75576216,Lightning App,"  introduce Lightning App   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz",1
Fix repository links (#13304),0.492364,Updated app URLs to the latest format (#16568),"  GH org rename Lightning-AI   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  repo name  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
Future 4/n: test & legacy in test/ folder (#13295),0.45886338,Deprecated filepath in ModelCheckpoint (#4213),"  move: legacy >> test/   move: tests >> test/   rename unittests   update CI   tests4pl   tests_pytorch   proxi   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   ci   link   cli   standalone   fixing   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   .   Apply suggestions from code review   Co-authored-by: Akihiro Nitta nitta@akihironitta.com  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci   alone   test -> tests   Standalone fixes   ci   Update   More fixes   Fix coverage   Fix mypy   mypy   Empty-Commit   Fix   mypy just for pl   Fix standalone   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com",0
Remove unused test argument (#13296),0.559203,Removed no return warning from val/test step (#6139),,0
"Update wandb requirement from <=0.12.11,>=0.8.21 to >=0.8.21,<0.12.17 in /requirements (#13051)",0.63061607,Removed the deprecated sync_step argument from WandbLogger (#8763), Update wandb requirement in /requirements  Updates the requirements on wandb to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: wandb   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com,0
Future 3/n: docs adjustment (#13299),0.53144956,Docs improvements,  docs: rename source >> source-PL   docs: fix typing   readthedocs   update paths & codeowners   source-pytorch   ci   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Future 2/n: stand-alone examples (#13294),0.43262693,Support for warn-level determinism,  move: pl_examples >> src/   convert pl_examples package to plain examples   update CI for examples   ci   missing   install ,0
Future 1/n: package in src/ folder (#13293),0.42783242,Moved accelerators and plugins to its legacy pkg (#5645),"  move: pytorch_lightning >> src/   update setup & install   update CI   ci   update CI for examples   Self review   mypy   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   ci   make   docs   typo   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   ci: gpu   .   hpu   typing   docs   tpu   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
Drop PyTorch 1.8 support (#13155),0.956255,Drop PyTorch 1.9 support (#15347),  Drop PyTorch 1.8 support   Missed update   Skip profiler test until supported   Upgrade ipu dockerfile pytorch version   Update XLA version ,1
Remove test's proxy boring classes import (#13297),0.44285294,- fixed all the .test() calls,Co-authored-by: Jirka jirka.borovec@seznam.cz,0
Standardize tensor annotation (#13292),0.55253077,- Fixed type promotion when tensors of higher category than float are logged ([#11401](https://github.com/PyTorchLightning/pytorch-lightning/pull/11401)),unify torch.Tensor >> Tensor Co-authored-by: Jirka jirka.borovec@seznam.cz,0
Fix deploy production intermediate doc (#13204),0.4520606,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",,0
"Update rich requirement from !=10.15.*,<=12.0.0,>=10.2.2 to >=10.2.2,!=10.15.0.a,<13.0.0 in /requirements (#13047)",0.57434154,Improved exception message if rich version is less than 10.2.2 (#10839), Update rich requirement in /requirements  Updates the requirements on rich to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: rich   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com,0
Update CI setup (#13291),0.49241275,Setup: added requirement freeze for the next major version (#14480), drop mamba use legacy GPU machines,0
Create  hpu-ci-runner Dockerfile (#13239),0.46512502,    hparams_file='/path/to/hparams_file.yaml',  Create  hpu-ci-runner Dockerfile   Add ENTRYPOINT script 'start.sh' to hpu-ci-runner   rename dirs   ci   add docker   Fix build failure   Fix build failure   Fix title of nightly ci runner build   Fix comments   Fix comments   Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Akihiro Nitta nitta@akihironitta.com,0
Merge pull request #13250 from PyTorchLightning/ci/rm-base,0.60773784,Full commit list: https://github.com/PyTorchLightning/pytorch-lightning/compare/1.6.0...1.7.0,CI: Remove simple test ci_test-base.yml,0
Fixing small typo in documentation. (#13215),0.5782631,Syntax changes are: ,Fixing typo in documentation.,0
Add deprecation path for the old Loop module (#13043),0.64222115,- Removed support for loop customization,,0
Rename loops/base.py to loops/loop.py (#13043),0.56368303,| Import pytorch_lightning.loops.base.Loop                                                                   | 1.9             | pytorch_lightning.loops.loop.Loop             |,,0
Explicitly mention disabling validation in trainer docs (#13148),0.79205287,Deprecated the Trainer.disable_validation property in favor of not Trainer.enable_validation (#8291),Co-authored-by: Akihiro Nitta nitta@akihironitta.com,1
Decouple pulling legacy checkpoints from existing GHA workflows and docker files (#13185),0.5854372,all the checkpoint issues should be gone now (including backward support for old checkpoints), Add pull-legacy-checkpoints action Replace pulls with the new action and script Simplify,0
update NGC docker (#13136),0.43880808,Pkg: fix parsing versions by @Borda in https://github.com/Lightning-AI/lightning/pull/15401, update docker Apply suggestions from code review  Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Update CHANGELOG after the 1.6.4 release (#13201),0.65965736,This section outlines notable changes that are not backward compatible with previous versions. The full list of changes and removals can be found in the Full Changelog below.,,0
LightningCLI natively support callback list append (#13129),0.7520535,- `LightningCLI` changed to use jsonargparse native support for list append ([#13129](https://github.com/Lightning-AI/lightning/pull/13129)),  LightningCLI natively support callback list append.   Update jsonargparse version   Handle case where callbacks is not a list.   Fix PEP8 issue.   Handle mypy false positive   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Fix initialization of optimizers in DDP Strategy (#11952),0.6238702,Refactored optimizer (#4658),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Fix epoch logging on train epoch end (#13025),0.7226509,Remove epoch from trainer.logged_metrics (#9904),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Register torch's unresolvable import paths in cli module (#13153),0.688447,import torch,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com,0
Fix typo in cluster_advanced.rst (#13173),0.5834507,Dropped name column from cluster list (#15721),,0
Remove the deprecated logger.close (#13149),0.7640107,"Deprecated LightningLoggerBase.close, LoggerCollection.close in favor of LightningLoggerBase.finalize, LoggerCollection.finalize (#9422)",  refactor:removed the close instance from the LoggerCollection class   Also logger.close()   Update CHANGELOG   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Updates docs to clarify train_dataloader usage by batch_size_finder (#13171),0.6430453,"Overriding the on_train_batch_{start,end} hooks in conjunction with taking a dataloader_iter in the training_step no longer errors out and instead shows a warning (#16062)",  batchfinder clarification   fix according to precommit hook ,0
Fix instruction for cherry pick way of fixing mixed branches (#13186),0.41645873,Mixed precision overhaul (#16783),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Specify Trainer(benchmark=False) in parity benchmarks (#13182),0.64087373,  * Removed the `Trainer(tpu_cores=...)` argument,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Removed weights_summary argument from Trainer (#13070),0.8519887,Changed the default value of the Trainer argument weights_summary from full to top (#2029),Co-authored-by: rohitgr7 rohitgr1998@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
[CLI] Respect existing seed by default (#13110),0.67475855,seed_everything now fails when an invalid seed value is passed instead of selecting a random seed (#8787),,0
Fix logging's step values when multiple dataloaders are used during evaluation (#12184),0.62837124,- Fixed logging to loggers with multiple eval dataloaders ([#12454](https://github.com/PyTorchLightning/pytorch-lightning/pull/12454)),Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
add @akihironitta among CI/CD owners (#13139),0.49848002,"@awaelchli, @carmocca, @rohitgr7",add @akihironitta,0
Fix not running test codes (#13089),0.5395697,- Code coverage (99%),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Avoid changing the current cudnn.benchmark value (#13154),0.72438383,Prevent modification of torch.backends.cudnn.benchmark when Trainer(benchmark=...) is not set (#13154),,1
"Revert ""Update deepspeed requirement from <0.6.0 to <0.7.0 in /requirements (#13048)"" (#13177)",0.57512593,"Updated DeepSpeed ZeRO (#6546, #6752, #6142, #6321)",This reverts commit e19babc9869a9de584731d0106bee41f02c71584.,0
Fix standalone test collection (#13177),0.46162307,Refactored setup_training and remove test_mode (#5388),,0
xfail flaky quantization test blocking CI (#13177),0.4542676,"Resolved schedule step bug for PyTorch Profiler (#6674, #6681)",,0
Pin protobuf version (#13177),0.41895118,Renamed TPUHalfPrecisionPlugin to TPUBf16PrecisionPlugin (#10026),,0
Document CI/CD (#12980),0.44274998,"    'path/to/checkpoint.ckpt',",  Add README.md to .github/workflows/   Link CI/CD doc page from contributing.md   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
"Update typing-extensions requirement from <=4.1.1,>=4.0.0 to >=4.0.0,<4.2.1 in /requirements (#13054)",0.4841863,Pkg: fix parsing versions by @Borda in https://github.com/Lightning-AI/lightning/pull/15401, Update typing-extensions requirement in /requirements  Updates the requirements on typing-extensions to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: typing-extensions   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com,0
Update deepspeed requirement from <0.6.0 to <0.7.0 in /requirements (#13048),0.64586025,"Implemented DeepSpeedPlugin._setup_model_and_optimizers (#10009, #10064)", Update deepspeed requirement from <0.6.0 to <0.7.0 in /requirements  Updates the requirements on deepspeed to permit the latest version. - Release notes - Commits  updated-dependencies: - dependency-name: deepspeed   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com,0
"Update tensorboard requirement from <=2.8.0,>=2.2.0 to >=2.2.0,<2.10.0 in /requirements (#13049)",0.76839817,Improved the error message for installing tensorboard or tensorboardx (#17053), Update tensorboard requirement in /requirements  Updates the requirements on tensorboard to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: tensorboard   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com,1
"Update jsonargparse[signatures] requirement from <=4.7.1,>=4.7.1 to >=4.7.1,<4.7.4 in /requirements (#13052)",0.64626807,- Changes in `LightningCLI` required for the new major release of jsonargparse v4.0.0 ([#10426](https://github.com/PyTorchLightning/pytorch-lightning/pull/10426)), Update jsonargparse[signatures] requirement in /requirements  Updates the requirements on jsonargparse[signatures] to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: jsonargparse[signatures]   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com,0
"Update matplotlib requirement from <=3.5.1,>3.1 to >3.1,<3.5.3 in /requirements (#13053)",0.5239884,Drop Python 3.6 support, Update matplotlib requirement in /requirements  Updates the requirements on matplotlib to permit the latest version. - Release notes - Commits  updated-dependencies: - dependency-name: matplotlib   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com,0
"Update neptune-client requirement from <=0.15.2,>=0.10.0 to >=0.10.0,<0.16.3 in /requirements (#13056)",0.77394617,"It now supports neptune-client 0.16.16 and neptune >=1.0, and we have replaced the log() method with append() and extend().", Update neptune-client requirement in /requirements  Updates the requirements on neptune-client to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: neptune-client   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com,1
"Update mlflow requirement from <=1.24.0,>=1.0.0 to >=1.0.0,<1.27.0 in /requirements (#13127)",0.69508684,Fallback to module available check for mlflow (#17467), Update mlflow requirement in /requirements  Updates the requirements on mlflow to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: mlflow   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz,0
Removed flush_logs_every_n_steps argument from Trainer (#13074),0.8986101,Rename Trainer arguments row_log_interval >> log_every_n_steps and log_save_interval >> flush_logs_every_n_steps (#3748),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Fix torchelastic detection with non-distributed installations (#13142),0.63833356,    * Some configuration errors that were previously raised as `MisconfigurationException`s will now be raised as `ProcessRaisedException` (torch>=1.8) or as `Exception` (torch<1.8),  Fix torchelastic detection under Mac   CHANGELOG ,0
Fix import statement in tutorial  (#13130),0.50376576,"Separated utils: imports & enums (#5256, #5874)",fix tutorial Co-authored-by: Jiny491 jiny491@gmail.com,0
Add accelerator method teardown() (#11935),0.7058083,- Added `teardown()` method to `Accelerator` ([#11935](https://github.com/Lightning-AI/lightning/pull/11935)),"  refactor strategies to use accelerator   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci  Apply suggestions from code review  Co-authored-by: Aki Nitta nitta@akihironitta.com  Apply suggestions from code review  Co-authored-by: Aki Nitta nitta@akihironitta.com   update changelog   remove changes to strategies   update changelog   update device-specific teardowns   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   partially revert   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Update strategy.py   swap hpu   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   add missing log   unused import   order matters for tpu?   Co-authored-by: edward-io me@edward.io Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Aki Nitta nitta@akihironitta.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com",1
Use coverage>=6.4 (#13132),0.6171805,- Code coverage (99%),,0
Rename min_gpus to min_cuda_gpus (#13133),0.6027676,"Deprecated the use of Trainer(gpus=""i"") (string) for selecting the i-th GPU; from v1.5 this will set the number of GPUs instead of the index (#6388)", rename min_gpus to min_cuda_gpus,0
Avoid firewall message from find_free_network_port (#13113),0.5202124,    default_port = 12910,,0
Update trainer profiler typehint to use Profiler instead of the deprecated BaseProfiler (#13084),0.7624288,Removed support for passing a bool value to profiler argument of Trainer (#6164),  Fix trainer profiler typehint   Remove unused import of deprecated BaseProfiler   Update CHANGELOG.md   Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
GPU CI: Increase timeout from 65 to 100min (#13104),0.548771,refactored GPU backend __step (#3120),,0
Add deprecation path for the old Callback module (#13031),0.7695051,Removed deprecated callbacks (#3979),,1
Rename callbacks/base.py to callbacks/callback.py (#13031),0.5994423,| Import pytorch_lightning.callbacks.base.Callback                                                           | 1.9             | pytorch_lightning.callbacks.callback.Callback |,,0
Update docs about reduce_fx (#13101),0.59498894,Deprecated self.log(sync_dist_op) in favor of self.log(reduce_fx). (#7891),,0
Enable all ddp params for hpu parallel strategy (#13067),0.6195218,Made DDP the default if no backend specified with multiple GPUs (#1789),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Add deprecation path for the old LightningModule module (#12740),0.6246574,Removed deprecated LightningModule hparams setter (#6207),,0
Update core/lightning.py to core/module.py (#12740),0.65598285,- Deprecated `pytorch_lightning.core.lightning.LightningModule` in favor of `pytorch_lightning.core.module.LightningModule` ([#12740](https://github.com/Lightning-AI/lightning/pull/12740)),,0
Fix CLI test interaction (#13037),0.5285771,Removed implicit ui testing with testing.run_app_in_cloud in favor of headless login and app selection (#16741),,0
Removed process_position argument from Trainer Class (#13071),0.79059464,  * Removed the `Trainer(num_processes=...)` argument,,1
CI: Azure - multiple configs (#12984),0.40792423,  * `save_config_multifile`, CI: Azure - multiple configs names benchmark Apply suggestions from code review  Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Remove unused hook keyword argument (#13059),0.63966817,Updated hooks arguments - breaking for setup and teardown (#2850),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Fix materialize_module recursively setting its child module (#12870),0.40509173,  * Removed the deprecated `pl_module` argument from the distributed module wrappers, Don't set materialized child to child's child Update CHANGELOG  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Fix number of references to LightningModule (#12897),0.58371437,Deprecated LightningModule.model_size (#8343),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
GPU CI: Increase timeout from 55min to 65min (#13064),0.5324867,refactored GPU backend __step (#3120),,0
Fix typo ddp_spawn_sharded -> ddp_sharded_spawn (#13062),0.6895183,  * Removed the `pytorch_lightning.strategies.sharded_spawn.DDPSpawnShardedStrategy` (ddp_sharded_spawn) class,,0
Fix version freeze comparison (#13057),0.5209764,Setup: added requirement freeze for the next major version (#14480),,0
Remove twine dependency from requirements (#13050),0.4802342,Add missing python-multipart dependency (#17244),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Improved the jsonargparse[signatures] availability variable (#13035),0.56036067,- Changes in `LightningCLI` required for the new major release of jsonargparse v4.0.0 ([#10426](https://github.com/PyTorchLightning/pytorch-lightning/pull/10426)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Use hpu hmp for bf16 alone (#13028),0.5595366,BFloat16 Support, Use hpu hmp for bf16 alone Update changelog  Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
add freeze for development and full range for install (#12994),0.5516786,Setup: added requirement freeze for the next major version (#14480),"  freeze versions   unfreeze   dependabot   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   fix all req   ...   use base   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   fix refs   Apply suggestions from code review   Co-authored-by: Akihiro Nitta nitta@akihironitta.com   Apply suggestions from code review   dockers   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com",0
Fix docs' TensorBoardLogger instantiation (#13038),0.8400371,Changed the default logger to TensorBoardLogger (#609),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
"Remove the deprecated on_{train,val,test,predict}_dataloader hooks (#13033)",0.77013516,"- Removed the deprecated `on_{train,val,test,predict}_dataloader` hooks from the `LightningModule` and `LightningDataModule` ([#13033](https://github.com/Lightning-AI/lightning/pull/13033))",Co-authored-by: rohitgr7 rohitgr1998@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Provide access to unwrapped model in Lite (#12597),0.40005395,- Replaced the unwrapping logic in strategies with direct access to unwrapped `LightningModule` ([#13738](https://github.com/Lightning-AI/lightning/pull/13738)),Co-authored-by: Akihiro Nitta nitta@akihironitta.com,0
Fix double precision during evaluation (#12983),0.7253812,    precision=16,,1
"Fix typo: ""optimizeres"" -> ""optimizers"" (#13030)",0.6423483,Refactored optimizer (#4658),,0
Avoid redundant callback restore warning while tuning (#13026),0.9999999,Avoid redundant callback restore warning while tuning (#13026),,1
Docs for Collaborative Training (#12996),0.53180206,TPU training (#2708),  Add documentation for collaborative_training   Add strategies   Fix formatting   use accelerator API   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   Fix link   Try to fix label   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   Fix sequence   Apply suggestions from code review   Co-authored-by: RobertLaurella 99420295+RobertLaurella@users.noreply.github.com   Address reviews   Address code reviews   Update docs/source/strategies/collaborative_training_expert.rst   Co-authored-by: Akihiro Nitta nitta@akihironitta.com  Update docs/source/strategies/collaborative_training_intermediate.rst  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: RobertLaurella 99420295+RobertLaurella@users.noreply.github.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com,0
Remove deprecated checkpoint_callback flag in Trainer (#13027),0.8968221,Deprecated checkpoint_callback from the Trainer constructor in favor of enable_checkpointing (#9754),  Removed lines pertinent to checkpoint_callback   removed checkpoint callback flag   Updated Change Log   Removed deprecation test for checkpoint_callback argument   updated line in the simple_classif_training.py   Updated docs   updated simple_classif_training.py removing enable_checkpointing ,1
Track CPU stats with DeviceStatsMonitor (#11795),0.71671,CPU stats monitoring,Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Kaushik B kaushikbokka@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
HPU & TPU doesn't support torch.inference_mode (#13014),0.7586469,- Fixed an issue with unsupported torch.inference_mode() on hpu backends by making it use no_grad ([#13014](https://github.com/Lightning-AI/lightning/pull/13014))," HPU doesn't support torch.inference_mode  Signed-off-by: Jerome janand@habana.ai  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci  Update doc and changelog  Signed-off-by: Jerome janand@habana.ai   Update pytorch_lightning/trainer/trainer.py   Revert back to HPU available   Address reviews   Signed-off-by: Jerome janand@habana.ai   Update pytorch_lightning/trainer/trainer.py   Update pytorch_lightning/trainer/trainer.py   Add TPU accelerator condition   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Kaushik B kaushikbokka@gmail.com",1
parse strategies as own extras (#12975),0.52159184,The brittle argument parsing utilities (#16708),  parse strategies as own extras   prune devel   Update Makefile   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  revert parse_requirements  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
add RobertLaurella as docs owner (#12973),0.42049217,"If we forgot someone due to not matching the commit email with the GitHub account, let us know :]",,0
"Remove on_train_batch_{start,end}(dataloader_idx=...) (#12977)",0.71935594,"Changed the seq of on_train_batch_end, on_batch_end & on_train_epoch_end, on_epoch_end hooks (#5688)",,1
Profile LightningDataModule hooks (#12971),0.7104832,- Added profiling of `LightningDataModule` hooks ([#12971](https://github.com/Lightning-AI/lightning/pull/12971)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Fix default int values being float (#12989),0.5458758,Changed iou [func] to allow float input (#4704),,0
Add profiling to dataloader next() (#12124),0.6068792,- Added profiling to the loops' dataloader `__next__` calls ([#12124](https://github.com/Lightning-AI/lightning/pull/12124)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Unpin CUDA docker image for GPU CI (#12373),0.49355173,- Added an argument `include_cuda` in `pytorch_lightning.utilities.seed.isolate_rng` to disable managing `torch.cuda`'s rng ([#16423](https://github.com/Lightning-AI/lightning/pull/16423)), unpin CUDA docker image for GPU CI Apply suggestions from code review  Co-authored-by: Aki Nitta nitta@akihironitta.com Co-authored-by: Akihiro Nitta akihiro@pytorchlightning.ai Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Break lazy accumulation of graphs (#12938),0.37493905,Update the logic to check for accumulation steps with deepspeed (#9826),Co-authored-by: Kaushik B kaushikbokka@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
[1/2] Collaborative Strategy (#12842),0.45021623,- Hivemind Strategy,,0
[FSDP] Adding Native FSDP Strategy (#12447),0.7932475," * `strategy=""fsdp_native""` is now `strategy=""fsdp""`",,1
"Add support for reloading the last checkpoint saved by passing ckpt_path=""last"" (#12816)",0.81226707,"In certain scenarios, like when running in a cloud spot instance with fault-tolerant training enabled, it is useful to load the latest available checkpoint. It is now possible to pass the string ckpt_path=""last"" in order to load the latest available checkpoint from the set of existing checkpoints.",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Add a method signature check for setup (#12960),0.4839161,Replace MisconfigurationException with warning in ModelCheckpoint Callback (#4560),Co-authored-by: otaj ota@grid.ai Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Raise an exception when using DeepSpeed with an invalid accelerator (#12699),0.6960767,- Added a friendly error message when attempting to use `DeepSpeedStrategy` on unsupported accelerators ([#12699](https://github.com/Lightning-AI/lightning/pull/12699)),Co-authored-by: manjirou maxim.mametkulov@halbestunde.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
Update Strategy doc (#12950),0.49331343,Updated governance docs,  Update Strategy doc   Update strategy references ,0
Merge pull request #12723 from PyTorchLightning/req/strategies,0.5792125,Full commit list: https://github.com/PyTorchLightning/pytorch-lightning/compare/1.6.0...1.7.0,Separate strategies' requirements,0
Support predict_dataset in LightningDataModule.from_datasets (#12942),0.8258444,- Added missing `predict_dataset` argument in `LightningDataModule.from_datasets` to create predict dataloaders ([#12942](https://github.com/Lightning-AI/lightning/pull/12942)),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
Update CHANGELOG after the 1.6.3 release (#12968),0.65473443,This section outlines notable changes that are not backward compatible with previous versions. The full list of changes and removals can be found in the Full Changelog below.,,0
simplify _copy_trainer_model_properties (#12788),0.6157363,Allow easy trainer re-instantiation (#7508),,0
Fix zero division error for empty dataloaders (#12885),0.680431,"Raised MisconfigurationException when total length of dataloader across ranks is zero, and give warning when total length is non-zero, but only local rank length is zero. (#9827)",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Callback collection through entry points (#12739),0.63619846,Read more about callback entry points in our docs.,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: thomas chaton thomas@grid.ai,0
Fix fit loop restart logic to enable resume using the checkpoint (#12821),0.67460704,Call reset_on_restart in the loop's reset hook instead of when loading a checkpoint (#9561),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Fix TQDMProgressBar reset and update to show correct time estimation (#12889),0.77715665,- Fixed `TQDMProgressBar` reset and update to show correct time estimation (2/2) ([#13962](https://github.com/Lightning-AI/lightning/pull/13962)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
[FIX] Enable mixed precision in the Fully Sharded Strategy when precision=16 (#12965),0.6170529,"- Changed arguments for precision settings (from [64|32|16|bf16] to [""64-true""|""32-true""|""16-mixed""|""bf16-mixed""]) ([#16767](https://github.com/Lightning-AI/lightning/pull/16767))",  Fix fully sharded mixed precision setter   Add CHANGELOG.md ,0
Construct the hook kwargs inside each loop (#12100),0.5927333,moved hooks around in eval loop (#3195),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
Add LightningDataModule.load_from_checkpoint to load datamodules directly from checkpoint (#12550),0.9326676,- Added `LightningDataModule.load_from_checkpoint` to support loading datamodules directly from checkpoint ([#12550](https://github.com/Lightning-AI/lightning/pull/12550)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: otaj ota@grid.ai,1
Support CLI shorthand natively (#12614),0.6316004,Introducing CLI commands for apps (#13602)!,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Prefix log_metrics keys with class name in callbacks (#12228),0.6033014,Removed legacy code to include step dictionary returns in callback_metrics. Use self.log_dict instead. (#6682),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Enforce eval shuffle warning only for default samplers (#12653),0.8885808,Enforced eval shuffle warning only for default samplers in DataLoader (#12653),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Remove duplicate boring classes (#12951),0.5087837,Drop duplicate metrics (#5014),,0
fix enumerate usage (#12949),0.3880968,"Separated utils: imports & enums (#5256, #5874)",,0
Add missing super().__init__() calls (#12948),0.7657702,    super().__init__(),,1
Add hook test for reloading with max epochs (#12932),0.648527,# 3. Rename the hook to `on_*_epoch_end`,,0
Fuse_modules in a qat-respecting way (#12891),0.44330418,Unified module names in Utils (#5199),"  Fuse_modules in a qat-respecting way   Add compatibility for PyTorch <1.11   In older pytorch versions, fuse_modules used the Module.training flag to determine wheter fusion should be QAT-compliant or not, refer https://github.com/pytorch/pytorch/releases/tag/v1.11.0   Add CHANGELOG for pull #12891   Fix conditional import of fuse_modules_qat   torch.ao.quantization.fuse_modules_qat was actually added in torch 1.11.  Update CHANGELOG.md  Co-authored-by: Akihiro Nitta nitta@akihironitta.com",0
Update nvidia gpg key to fix nightly docker builds (#12930),0.4002629,Removed deprecated pytorch_lightning.utilities.memory.get_gpu_memory_map in favor of pytorch_lightning.accelerators.cuda.get_nvidia_gpu_stats (#15617), Update gpg key Use curl instead of wget Install key manually,0
Override optimizer_zero_grad when using the IPUStrategy (#12913),0.72713614,Stopped optimizer_zero_grad from being called after IPU execution (#12913),Co-authored-by: Sean Naren sean@grid.ai Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
Exclude the CHANGELOG from the pre-commit size check (#12931),0.49771947,Doing this minor release because correct validation metrics logging is critical.,,0
Fix pickling of KFoldLoop (#12441),0.5462329,Tested pickling (#1636),  allow pickling of KFoldLoop   Update pl_examples/loop_examples/kfold.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Nicolas Berger nicolas.berger@inait.ai Co-authored-by: Kushashwa Ravi Shrimali kushashwaravishrimali@gmail.com,0
Remove use of jsonargparse internals (#12918),0.5847137,- Changes in `LightningCLI` required for the new major release of jsonargparse v4.0.0 ([#10426](https://github.com/PyTorchLightning/pytorch-lightning/pull/10426)),,0
Versioning of last checkpoins (#12902),0.5737029,"Versioning of ""last"" checkpoints",  last checkpoint versioning   changelog   Simplify test   Update CHANGELOG.md   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update CHANGELOG.md  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Threading support for legacy loading of checkpoints (#12814),0.7538613,Checkpoint saving and loading extensibility:,,1
Invoke parent DDP configuration for torch>1.10.2 (#12912),0.6524138,Swaped torch.load for fsspec load in DDP spawn backend (#3787),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Print ragged dict of metrics in EvaluationLoop._print_results properly (#12857),0.54295063,Removed legacy code to include step dictionary returns in callback_metrics. Use self.log_dict instead. (#6682),  first fix   full bugfix + tests   Apply Adrian's suggestion   Add test with tensor(0)   Minor code simplification   change sorting to make the comment correct   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
ShardedGradScaler should only be set for FP16 (#12915),0.53161335," * `strategy=""fsdp_native_full_shard_offload""` is now `strategy=""fsdp_cpu_offload""`",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Remove reference to genindex and search in index.rst (#12919),0.36730218,Changed epoch indexing from 1 instead of 0 (#2206),,0
Merge pull request #12920 from PyTorchLightning/rename/lightning_extension,0.6206549,  * ([#10403](https://github.com/PyTorchLightning/pytorch-lightning/pull/10403)),Update docs conf with renamed lightning extension,0
Support automatic seeding of the LightningCLI (#12822),0.71381396,LightningCLI V2,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Fix typo in predict_step docs (#12911),0.6131407,"Changed setup() and teardown() stage argument to take any of {fit,validate,test,predict} (#6386)",,0
Use cmake installed with apt (#12907),0.32774752,Add missing python-multipart dependency (#17244),,0
Update CHANGELOG after the 1.6.2 release (#12904),0.65453583,This section outlines notable changes that are not backward compatible with previous versions. The full list of changes and removals can be found in the Full Changelog below.,,0
Update the link to jsonargparse's link_arguments (#12898),0.5930302,- Changes in `LightningCLI` required for the new major release of jsonargparse v4.0.0 ([#10426](https://github.com/PyTorchLightning/pytorch-lightning/pull/10426)),,0
Update TPU Accelerator docs (#12850),0.66003656,Update Gradient Clipping for the TPU Accelerator (#6576),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
clean up side menu (#12892),0.42312503,some minor cleaning,Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Kaushik B kaushikbokka@gmail.com,0
Fix tests related to DDP communication hooks (#12878),0.6270357,DDP Debugging Improvements,  Fix ddp_comm_hook tests   Refactor ddp_comm_hook tests   Co-authored-by: Akihiro Nitta akihiro@pytorchlightning.ai,0
Fix trainer.logger deprecation message (#12671),0.8152848,"Deprecated passing flush_logs_every_n_steps as a Trainer argument, instead pass it to the logger init if supported (#9366)",,1
Fix to ensure the checkpoint states are saved in a common filepath with deepspeed (#12887),0.67201006,Removed deprecated checkpoint argument filepath (#5321),Co-authored-by: Akihiro Nitta nitta@akihironitta.com,0
"Support deterministic=""warn"" in Trainer for Pytorch 1.11+ (#12588)",0.81292003,"In Pytorch 1.11, operations that do not have a deterministic implementation can be set to throw a warning instead of an error when ran in deterministic mode. This is now supported by our Trainer:",Co-authored-by: carmocca carlossmocholi@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com,1
Use a single instance of rich.console.Console throughout the codebase (#12886),0.9858208,Use only a single instance of rich.console.Console throughout codebase (#12886),,1
Update jsonargparse to unblock master (#12884),0.48123625,- Changes in `LightningCLI` required for the new major release of jsonargparse v4.0.0 ([#10426](https://github.com/PyTorchLightning/pytorch-lightning/pull/10426)),Co-authored-by: Mauricio Villegas mauricio_ville@yahoo.com,0
Fix reference to basic level doc (#12848),0.48136845,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",,0
Raise better error when calling Trainer.save_checkpoint without a model attached (#12772),0.786803,Removed passing a ModelCheckpoint instance to Trainer(checkpoint_callback) (#6166),  add error message   add test   changelog   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Update deepspeed and fairscale versions (#12860),0.6235394,"Updated DeepSpeed ZeRO (#6546, #6752, #6142, #6321)",  Fix deepspeed installation   Adapt to deepspeed>=0.5.9   Fix fairscale installation   Co-authored-by: Akihiro Nitta akihiro@pytorchlightning.ai,0
Clarify fast_dev_run docs (#12751),0.65197694,Updated fast_dev_run to accept integer representing num_batches (#4629),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
Fixed encoding issues on terminals that do not support unicode characters (#12828),0.356021,"At last, lots of bug fixes (see below).",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Fix false positive deprecation warning from register_ddp_comm_hook (#12846),0.6891899,Silenced some warnings. verified ddp refactors (#3483),  Use new rank_zero_debug   Fix and move import statement to the top ,0
Remove deprecated TestTubeLogger (#12859),0.8624121,Deprecated the TestTubeLogger (#9065),  remove deprecated test_tube logger   remove testube from logger init   remove relevant testtube tests   update CHANGELOG with removal of deprecated TestTubeLogger ,1
Add missing f prefix to f-strings (#12869),0.47765204,Remove beta arg from F1 class and functional (#5076),,0
Use an uniform call hook style in the loops (#12742),0.5978583,moved hooks around in eval loop (#3195),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Remove the deprecated get_progress_bar_dict (#12839),0.7357255,- Removed deprecated `get_progress_bar_dict` property from `LightningModule` ([#12839](https://github.com/Lightning-AI/lightning/pull/12839)),Co-authored-by: Raymond G Schireman raymond.schireman@uvm.edu Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Refactor PredictionLoop.on_run_start for consistency (#12732),0.5997857,"Refactored prediction loop interface; added new classes PredictionLoop, PredictionEpochLoop (#7700, #8077)",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Add required for positional arguments in argparse logic (#12504),0.62312955,Args should come after the last positional argument (#1807),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Call on_predict_model_eval hook through method (#12741),0.5845951,"Marked several methods in PredictionLoop as protected: on_predict_start, on_predict_epoch_end, on_predict_end, on_predict_model_eval (#9516)",Co-authored-by: carmocca carlossmocholi@gmail.com,0
Fix formatting issue in Trainer docs (#12777),0.60567397,Trainer code became harder to follow,,0
"Improve LigtningEnum, etc. (#12750)",0.45423734,Updated metrics to use LightningEnum (#5689),,0
Remove deprecated LightningDataModule.val_transforms (#12763),0.83691704,- Removed the deprecated `val_transforms` argument from the `LightningDataModule` constructor ([#12763](https://github.com/Lightning-AI/lightning/pull/12763)),  remove val_transform from datamodule.py   remove val_transforms from tests   update docs   update changelog   remove unused imports   Co-authored-by: Raymond G Schireman raymond.schireman@uvm.edu Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Update building docker images (#12837),0.52743065,Remove unnecessary intermediate layers in Dockerfiles (#5697),Co-authored-by: Akihiro Nitta akihiro@pytorchlightning.ai,0
Fix support for ModelCheckpoint monitors with dots (#12783),0.69527555,Allow ModelCheckpoint monitor to be None (#3633),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Remove the deprecated LightningDataModule.test_transforms  (#12773),0.91310704,- Removed the deprecated `test_transforms` argument from the `LightningDataModule` constructor ([#12773](https://github.com/Lightning-AI/lightning/pull/12773)),Co-authored-by: Raymond G Schireman raymond.schireman@uvm.edu Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com,1
[Docs] Example for auto_insert_metric_name (#12649),0.54346246,"docs for all Metrics (#2184, #2209)",Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
docs refactor 4/n (fix broken links) (#12825),0.62778455,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",  docs refactor 4/n (fix broken links)   docs refactor 4/n (fix broken links)   docs refactor 4/n (fix broken links) ,0
Enable inference mode for evaluation (#12715),0.7107081,Inference mode support, Enable inference mode for evaluation better name Update CHANGELOG.md  Co-authored-by: Akihiro Nitta nitta@akihironitta.com,1
Support val_check_interval values higher than number of training batches  (#11993),0.6946955,val_percent_check in favour of limit_val_batches,Co-authored-by: rohitgr7 rohitgr1998@gmail.com,0
Horovod tests do not make sense for 1 gpu (#12710),0.5441375,Changed HorovodPlugin.all_gather to return a torch.Tensor instead of a list (#9696),Co-authored-by: Akihiro Nitta nitta@akihironitta.com,0
Make standalone tests less verbose (#12684),0.48118567,Cleaning up stale logger tests (#3490),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Fix import error when distributed module not available (#12794),0.6425622,  * Removed the deprecated `pl_module` argument from the distributed module wrappers,  add distributed.is_available checks to avoid errors when not available   Update CHANGELOG   Update pytorch_lightning/strategies/ddp.py   Co-authored-by: Akihiro Nitta nitta@akihironitta.com  Update pytorch_lightning/strategies/ddp.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
Better error mesage and type checking for gpus arg and devices arg in Trainer (#12530),0.70129377,  * Removed the `Trainer(gpus=...)` argument,Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
general docs improvements (#12747),0.8321017,Docs improvements,,1
Add Changelog entry for #12716 (#12813),0.6213805,Full Changelog,,0
docs refactor 3/n (#12795),0.5899761,Docs improvements,"  updated titles + css   updated titles + css   levels structure   levels structure   levels structure   adding level indexes   finished intro guide layout   finished intro guide layout   general titles   general titles   added movie   added movie   finished 15 mins   levels   added core levels   added core levels   fixed api reference on the left   gpu guides   gpu guides   gpu guides   gpu guides   precision   hpu guide   added ipu   added ipu   added ipu   added ckpt docs   finished basic logging   intermediate   intermediate   intermediate   fixed   fixed margins   fixed margins   fixed margins   fixed margins   fixed margins   fixed margins   fixed margins   fixed margins   fixed margins   added logger stuff   added logger stuff   added logger stuff   added logger stuff   added logger stuff   ic   added inconsolata   added inconsolata   added inconsolata   added inconsolata   added inconsolata   added inconsolata   added inconsolata   updated menu   added basic cloud docs   added basic cloud docs   added basic cloud docs   added basic cloud docs   ic   ic   ic   ic   ic   ic   ic   ic   ic   ic   ic   ic   added demos folder   added demos folder   added demos folder   added demos folder   added demos folder   added demos folder   twocolumns directive   twocols   twocols   registry   registry   registry   registry   registry   registry   registry   registry   registry   registry   registry   registry   registry   registry   registry   registry   registry   registry   registry   cleaning up   cleaning up   cleaning up   cleaning up   cleaning up   cleaning up   cleaning up   cleaning up   cleaning up   updated titles + css   levels structure   adding level indexes   finished intro guide layout   general titles   added movie   finished 15 mins   levels   added core levels   fixed api reference on the left   gpu guides   precision   hpu guide   added ipu   added ckpt docs   finished basic logging   intermediate   fixed margins   added logger stuff   ic   added inconsolata   updated menu   added basic cloud docs   ic   added demos folder   twocolumns directive   registry   cleaning up   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   deconflict   deconflict   deconflict   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Add testsetup sections wherever needed; fix errors in building docs   pre-commit fixes   Fix duplicate label   minor nit with pre-commit   Fix labels   More changes...   require   debug & cli   prec & model & visu   fix references   fix references   fix refs   fix refs - model_parallel   fix references   prune testsetup with global   refs in index   Fix duplicate label errors   Update orphan docs   Update orphan docs   Update orphan docs   fix links   Fix genindex and search index   fix refs   fix refs   Fix index rst related issues   fix refs   inc to rst   Fix links ref   fix more references   fix refs   deconflict   errors   errors   errors   fix refs   fix refs   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   fix warnings   Fix LightningCLI errors   Fix LightningCLI errors   Fix LightningCLI errors   Fix LightningCLI errors   fix doc build   Duplicate Label fix (docs) (#12800)   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   ignore typing in demo folder   Ignore demos for mypy   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Kushashwa Ravi Shrimali kushashwaravishrimali@gmail.com Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: rohitgr7 rohitgr1998@gmail.com Co-authored-by: Kaushik B kaushikbokka@gmail.com Co-authored-by: otaj ota@grid.ai",0
update chlog after 1.6.1 release (#12802),0.5118755,Removed logger_connector legacy code (#6733),,0
"Remove the deprecated LightningDataModule.size, LightningDataModule.dims (#12780)",0.8935316,- Removed the deprecated `dim` and `size` arguments from the `LightningDataModule` constructor([#12780](https://github.com/Lightning-AI/lightning/pull/12780)),,1
Remove deprecated dataloader_idx argument from on_train_batch_start/end callback hooks (#12769),0.7462897,"Overriding the on_train_batch_{start,end} hooks in conjunction with taking a dataloader_iter in the training_step no longer errors out and instead shows a warning (#16062)", remove dataloader_idx fix tests Update CHANGELOG.md  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
Merge pull request #12766 from PyTorchLightning/docs/slack,0.6299207,Full commit list: https://github.com/PyTorchLightning/pytorch-lightning/compare/1.7.0...1.8.0,update slack link,0
Enable running slow tests on Windows in CI (#12761),0.47392076, - Development run: `Trainer(fast_dev_run=True)`,,0
Add dataclass support to _extract_batch_size (#12573),0.7430705,- Added dataclass support to `extract_batch_size` ([#12573](https://github.com/Lightning-AI/lightning/pull/12573)),Co-authored-by: Akihiro Nitta nitta@akihironitta.com,1
Inspect correct function in wrap_init (#12716),0.49097633,def __init__(self):,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
Fix version comparison for python version in pl.utilities.imports (#12754),0.58581096,Use correct python version in lightning component template (#13790),,0
Workaround for actions/checkout in conda CI jobs (#12758),0.3723593,We also added support for setting the checkpoint path statefully:,,0
build more dockers & slack fails (#12675),0.5398551,"Show a message when BuildConfig(dockerfile=""..."") is passed but a Dockerfile file is already present in the Work (#15799)", build dockers add slack Apply suggestions from code review  Co-authored-by: Akihiro Nitta nitta@akihironitta.com,0
Support auto_select_gpus with accelerator and devices api (#12608),0.9909085,Support auto_select_gpus with the accelerator and devices API (#12608),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Remove pytorch_lightning.core.memory.get_gpu_memory_map (#12644),0.89279187,"- Removed the deprecated `pytorch_lightning.core.memory.{get_gpu_memory_map,get_memory_profile}` ([#12659](https://github.com/Lightning-AI/lightning/pull/12659))",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Remove deprecated XLAStatsMonitor (#12688),0.655556,  * Deprecated the internal `XLADeviceUtils.xla_available` staticmethod,Co-authored-by: rohitgr7 rohitgr1998@gmail.com,0
Update deepspeed precision test (#12727),0.7460107,Updated precision attributes in DeepSpeedPlugin (#10164),,1
Minor grammatical updates to the Pull Request Template (#12719),0.5127273,Syntax changes are: ,,0
Update Rich github links (#12718),0.5008418,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",,0
Remove support for passing strategy name to plugins (#12700),0.62009096,- Removed support for passing strategy names or strategy instances to the plugins Trainer argument ([#12700](https://github.com/Lightning-AI/lightning/pull/12700)),"  remove more code   update tests   remove unsupported test   remove unsupported test   remove dead enum values   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   add changelog   fix pep   add xfail test   remove comment   Remove support for passing strategy name to plugins   remove unused import   chlog   improve comment   update chlog   fix merge error   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
Remove support for passing strategy strings to accelerator (#12696),0.6544113,Enabled passing in custom accelerators (#4050),Co-authored-by: Kushashwa Ravi Shrimali kushashwaravishrimali@gmail.com,0
Remove the deprecated pl.callbacks.ProgressBar (#12658),0.63248867,Deprecated ProgressBar callback in favor of TQDMProgressBar (#10134),Co-authored-by: Raymond G Schireman raymond.schireman@uvm.edu Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Raise MisconfigurationException when the accelerator is available but… (#12708),0.8028029,"Raise MisconfigurationException when the accelerator is available but the user passes invalid ([]/0/""0"") values to the devices flag (#12708)",,1
Remove deprecated get_memory_profile (#12659),0.56895715,"Deprecated log_gpu_memory, gpu_metrics, and util funcs in favor of DeviceStatsMonitor callback (#9921)",,0
CI: check docker requires (#12677),0.550378,"Show a message when BuildConfig(dockerfile=""..."") is passed but a Dockerfile file is already present in the Work (#15799)", check docker requires ci update bagua conda cuda,0
fix import failer (#12676),0.60832214,Module imports are now catching ModuleNotFoundError instead of ImportError (#9867),,0
Remove unused AcceleratorConnector argument (#12686),0.6603064,AcceleratorConnector rewrite,  remove unused AcceleratorConnector argument   update lite ,0
Run main progress bar independent of val progress bar in TQDMProgressBar (#12563),0.9619161,Run main progress bar updates independent of val progress bar updates in TQDMProgressBar (#12563),Co-authored-by: carmocca carlossmocholi@gmail.com,1
"Fix flaky test, that is not consistent on some configurations (#12707)",0.5597881,Updated app testing (#16000),,0
Update LightningCLI tests to reflect changes in jsonargparse 4.6.0 (#12704),0.75055236,- Changes in `LightningCLI` required for the new major release of jsonargparse v4.0.0 ([#10426](https://github.com/PyTorchLightning/pytorch-lightning/pull/10426)),,1
Add docs for Trainer.state (#12663),0.6867214,adding Trainer.tune() (#3293),Co-authored-by: Akihiro Nitta nitta@akihironitta.com,0
fix typing in BaguaStrategy (#12692),0.53739405,Refactored setup for typing friendly (#6590),,0
"Deprecate num_processes,gpus, tpu_cores, and ipus from the Trainer constructor (#11040)",0.8061396,"Removed deprecated trainer attributes - on_cpu, on_tpu, use_tpu, on_gpu, use_dp, use_ddp, use_ddp2, use_horovod, use_single_gpu (#7501)",,1
Remove deprecated GPUStatsMonitor callback (#12554),0.7550807,Deprecated GPUStatsMonitor and XLAStatsMonitor in favor of DeviceStatsMonitor callback (#9924),Co-authored-by: Aki Nitta nitta@akihironitta.com Co-authored-by: Raymond G Schireman raymond.schireman@uvm.edu Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,1
Remove deprecated LayerSummary and ModelSummary (#12593),0.66261524,Deprecated mode parameter in ModelSummary in favor of max_depth (#8062),Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Fix tests failing on a single GPU (#11753),0.5529573,"Deprecated the use of Trainer(gpus=""i"") (string) for selecting the i-th GPU; from v1.5 this will set the number of GPUs instead of the index (#6388)",Co-authored-by: akihiro@grid.ai akihiro@grid.ai Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
Fix a typo in warning message inside Trainer.reset_train_dataloader (#12645),0.83163357,"Removed the deprecated Trainer.reset_train_val_dataloaders() in favor of Trainer.reset_{train,val}_dataloader (#16131)",Co-authored-by: kd li_jide_ok@126.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Allow self-reviewing to rerun gatekeeper (#12672),0.36099946,"Simplified ""should run validation"" logic (#7682)",,0
remove deprecated model_size from LightningModule (#12641),0.9375621,Deprecated LightningModule.model_size (#8343),,1
Fix inconsistency when user specifies weights_save_path (#12372),0.59769756,"Deprecated Trainer attribute ckpt_path, which will now be set by weights_save_path (#2681)",Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Rename loggers/base.py to loggers/logger.py and changed LightningLoggerBase to Logger (#12014),0.7877444,Removed the deprecated pytorch_lightning.loggers.base module in favor of pytorch_lightning.loggers.logger (#16120),"  Deprecate: initial LightningLoggerBase to Logger, tests/loggers pass   Refactor moved LightningLoggerBase to logger.py and removed base.py in loggers   Recreated base.py for safer backwards compatibility   Renamed test_base.py to test_logger.py and added test_base.py to test deprecation warning.   Renamed tests/loggers/test_base.py to tests/loggers/test_logger.py   Order all list in loggers/init alphabetically   minor: change deprecation warning of loggers.logger.base   fixed failing tests and formating   Update the documentation   move deprecation test to deprecated_api files   backward compatibility and deprecations tests for all functionality in loggers/logger.py   fix PEP8 issues.   Update CHANGELOG.md   Skip mypy on renamed file   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Aki Nitta nitta@akihironitta.com Co-authored-by: carmocca carlossmocholi@gmail.com",1
Don't raise a warning when nn.Modules are not saved under hparams (#12669),0.98816156,Don't raise a warning when nn.Module is not saved under hparams (#12669),,1
Remove train_transforms in LightningDataModule (#12662),0.8166466,- Removed the deprecated `train_transforms` argument from the `LightningDataModule` constructor([#12662](https://github.com/Lightning-AI/lightning/pull/12662)),Co-authored-by: manjirou maxim.mametkulov@halbestunde.com Co-authored-by: Kushashwa Ravi Shrimali kushashwaravishrimali@gmail.com,1
Update ci_pr-gatekeeper.yml (#12661),0.44288045,Doing this minor release because correct validation metrics logging is critical., Update ci_pr-gatekeeper.yml,0
Add bfloat16 support to DeepSpeedStrategy (#12508),0.6600141,Removed DeepSpeed FP16 Exception as FP32 is now supported (#8462),Co-authored-by: carmocca carlossmocholi@gmail.com,0
Remove deprecated automatic logging of gpu metrics (#12657),0.7639053,- Removed the deprecated automatic logging of GPU stats by the logger connector ([#12657](https://github.com/Lightning-AI/lightning/pull/12657)),Co-authored-by: carmocca carlossmocholi@gmail.com,1
Clear env before test cases requiring empty env (#12561),0.5019232,Only check versions / env when not in the cloud (#15504),"  clearing env vars in a test to allow compatibility with ""make test""   added clear=True to more mock environments in testcases   Co-authored-by: Akihiro Nitta nitta@akihironitta.com",0
"Revert ""ci(Mergify): configuration update (#12599)"" (#12642)",0.4322365,Pkg: fix parsing versions by @Borda in https://github.com/Lightning-AI/lightning/pull/15401,This reverts commit 184518c2fab188a9679a5b9d73ba95e3a8097280.,0
add PR Gatekeeper (#12646),0.44332278,"If we forgot somebody or you have a suggestion, find us on Discord :zap:","  add PR Gatekeeper   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  Apply suggestions from code review  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com",0
[pre-commit.ci] pre-commit suggestions (#12613),0.6023155,@akihironitta @awaelchli @Borda @carmocca @kaushikb11 @krshrimali @mauvilsa @otaj @pre-commit-ci @rohitgr7 @semaphore-egg @tkonopka @wayi1,,0
Add version header to CLI config files (#12532),0.7057917,A header with the version that generated the config is now included.,,1
Clarification on omegaconf's interpolation support in LightningCLI (#12571),0.44393262,Resolve interpolation bug with Hydra (#5406) ,  Clarification on omegaconf's interpolation support in LightningCLI.   Update docs/source/common/lightning_cli.rst   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com,0
Update rank_zero_only decorator for LSF environments (#12587),0.5085631,- Removed deprecated support for passing the `rank_zero_warn` warning category positionally ([#14470](https://github.com/Lightning-AI/lightning/pull/14470)),,0
Fix LightningLite.run signature for arbitrary arguments (#12629),0.6800781,You had to subclass LightningLite and implement run(),,0
Minor fix with data_path flag to imagenet script (examples) (#12637),0.45563394,Removed deprecated BaseProfiler.output_filename arg from it and its descendants in favor of dirpath and filename (#9214),,0
fix val accuracy printing in lite example (#12632),0.5124358,- Integrated the Lite Precision plugins into the PL Precision plugins - the base class in PL now extends the `lightning_lite.precision.Precision` base class ([#14798](https://github.com/Lightning-AI/lightning/pull/14798)),,0
Expose deprecated arguments from logger base interface (#12609),0.62019086,"Finally, loggers are also now configurable with shorthand:",,0
Set a less strict pre-commit Python version (#12627),0.55705786,Do not override PYTHONWARNINGS (#4700),,0
Remove deprecated 'terminate_on_nan' argument from Trainer (#12553),0.8399588,Deprecated Trainer argument terminate_on_nan in favor of detect_anomaly(#9175),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Fix rich main progress bar update (#12618),0.68985677,New Rich Progress Bar,,0
Mark swa_lrs argument in StochasticWeightAveraging callback as required (#12556),0.84729415,- Marked `swa_lrs` argument in `StochasticWeightAveraging` callback as required ([#12556](https://github.com/Lightning-AI/lightning/pull/12556)),,1
"Update docs for Strategy, Accelerator, Plugins (#12582)",0.6271019,"In this release, we've made some large changes to achieve that goal. Not to worry, though! The only users affected by these changes are those who use custom implementations of Accelerator and Strategy (TrainingTypePlugin) as well as certain Plugins. In particular, we want to highlight the following changes:",  wip plugins and strategies   updates   updates   update   update   reset   remove transition   update references   update references to plugins   update registry usage   mention default setting for plugins ,0
ci(Mergify): configuration update (#12599),0.42221075,"Renamed step_idx to step, epoch_idx to epoch, max_num_epochs to max_epochs and min_num_epochs to min_epochs (#589)", ci(Mergify): configuration update  Signed-off-by: Jirka Borovec    draft   team   code    eden    Apply suggestions from code review   Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com   Update .github/mergify.yml   +krshrimali   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Apply suggestions from code review  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
add about log_dict info in docs (#12552),0.69583035,"Our logging mechanism previously supported log(""key"", {""something"": 123}) (not using log_dict). However, this added significant complexity to the implementation with little benefit, as these keys could not be monitored by our Callbacks and most logger implementations do not support this notation. If you were using this feature with a compatible logger, you can still publish data directly to the Logger using self.logger.log_metrics().",,0
Fix LightningCLI tests with invalid subclass arguments order. (#12570),0.5851037,- Fixed ``LightningCLI`` signature parameter resolving for some lightning classes ([#13283](https://github.com/Lightning-AI/lightning/pull/13283)),,0
Refactor gpu test to use RunIf (#12605),0.5506903,refactored GPU backend __step (#3120),,0
enable tests that were never running (#12585),0.573834,Disabled optimizers setup during testing (#3059),,0
Split jobs into two workflows (#12449),0.430935,    # use the last 4 numbers in the job id as the id,,0
Link full code example for LightningLite (#12568),0.6734828,LightningLite:,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Bump version of black to 22.3.0 (#12542),0.44284338,Set version as today (#13906), bump black version to 22.3.0,0
Remove the deprecated LightningModule.summarize (#12559),0.88826007,- Removed the deprecated `summarize` method from the `LightningModule` ([#12559](https://github.com/Lightning-AI/lightning/pull/12559)),,1
Remove deprecated prepare_data_per_node in Trainer (#12536),0.73972285,"Deprecated prepare_data_per_node flag on Trainer and set it as a property of DataHooks, accessible in the LightningModule and LightningDataModule (#8958)", remove deprecated prepare_data_per_node from Trainer remove deprecated test for prepare_data_per_node remove doc for deprecated prepare_data_per_node remove inconsistency test remove deprecated prepare_data_per_node remove doc mentioning Trainer(prepare_data_per_node) update changelog remove unused code  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: rohitgr7 rohitgr1998@gmail.com,1
Avoid calling average_parameters multiple times per optimizer step (#12452),1.0000001,Avoid calling average_parameters multiple times per optimizer step (#12452),,1
Enable validation during overfitting (#12527),0.8444835,Validation now runs in overfitting mode,Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
Remove deprecated stochastic_weight_avg argument from Trainer (#12535),0.8470112,Deprecated passing stochastic_weight_avg to the Trainer constructor in favor of adding the StochasticWeightAveraging callback directly to the list of callbacks (#8989),Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: ananthsub 2382532+ananthsub@users.noreply.github.com,1
Remove deprecated progress_bar_refresh_rate from Trainer constructor (#12514),0.87673086,"Deprecated passing progress_bar_refresh_rate to the Trainer constructor in favor of adding the ProgressBar callback with refresh_rate directly to the list of callbacks, or passing enable_progress_bar=False to disable the progress bar (#9616)", Remove progress_bar_refresh_rate from Trainer constructor changelog,1
Support strategy argument being case insensitive (#12528),1.0,Support strategy argument being case insensitive (#12528),,1
Update index.rst (#12519),0.3800529,Changed epoch indexing from 1 instead of 0 (#2206),,0
general doc updates 2/n (#12517),0.7165989,Docs improvements, updated titles + css,1
Update version after the 1.6.0 release (#12512),0.5694866,Set version as today (#13906),,0
Add Danielle Pintz and Siyu Wang as core maintainers (#12515),0.52580047,"If we forgot someone or have any suggestion, let us know in Slack :zap:",,0
Cleanup CHANGELOG (#12507),0.62519556,Full Changelog,,0
Merge pull request #12509 from RobertLaurella/patch-1,0.44303152,"merge backends (#3476, #3477, #3478, #3480, #3482)",Update index.rst,0
Remove TPU Availability check from parse devices (#12326),0.54760325,"device parser (#3400, #3405)", Remove TPU Availability check from parse devices Update tests,0
Prepare for the 1.6.0 release,0.5981063,Here is a selection of important changes that are not backward compatible with versions < 1.5. The full list of changes and removals are listed in the changelog at the bottom.,,0
Fix titles capitalization in docs,0.32523656,Docs improvements,,0
Update Plugins doc (#12440),0.5581933,apex plugin (#3502),Co-authored-by: rohitgr7 rohitgr1998@gmail.com,0
Update CI in README.md (#12495),0.47091734,The point of this release is more bug fixes ahead of v 1.0.0. We now have CI tests on TPU thanks to @zcain117 from Google! :slightly_smiling_face:,,0
Add usage of Jupyter magic command for loggers (#12333),0.63175523,    logger.do_something(),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Add typing to LightningModule.trainer (#12345),0.6955229,Removed deprecated property LightningModule.datamodule in favor of Trainer.datamodule (#9233),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.comk-Pro.local,0
Fix warning message formatting in save_hyperparameters (#12498),0.7795755,Add ignore param to save_hyperparameters (#6056),,1
Remove AcceleratorConnector.tpu_cores and Deprecate Trainer.tpu_cores (#12437),0.81181675,  * Removed the `Trainer(tpu_cores=...)` argument,,1
Add HPU Accelerator column to the precision doc (#12499),0.5799775,"To simplify this process, we've deprecated the per-accelerator properties to have accelerator agnostic properties. For example:",,0
Add notes to Trainer docs when devices flag is not defined (#12155),0.5954934,Removed obsolete self._device in Trainer (#1849),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
set_epoch for validation and prediction data loaders (#12197),0.6795978,| on_epoch_start             | on_validation_epoch_start    | ," set_epoch for prediction and evaluation minor fix in the test, warning msg was changed  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com",0
Fix reference link in Benchmarking doc (#12486),0.500154,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",,0
Deprecate ModelCheckpoint.save_checkpoint (#12456),0.9179615,Removed the deprecated model argument from ModelCheckpoint.save_checkpoint (#8688),,1
Update Slack link (#12460),0.51387084,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",,0
Update type hint for log_model (#12223),0.49403912,Minor improvements to apply_to_collection and type signature of log_dict (#7851),,0
update apply_to_collections to support dataclass inputs (#11889),0.68366015,- Added support for dataclasses in `apply_to_collections` ([#11889](https://github.com/PyTorchLightning/pytorch-lightning/pull/11889)),Co-authored-by: rohitgr7 rohitgr1998@gmail.com,0
Prevent last checkpoint being deleted after resumed training with changed dirpath (#12225),0.7113884,- Fixed an issue where `ModelCheckpoint` could delete last checkpoint from the old directory when `dirpath` has changed during resumed training ([#12225](https://github.com/PyTorchLightning/pytorch-lightning/pull/12225)),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
Fix _module_available to detect horovod.torch properly (#12377),0.7121163,- Fixed check for horovod module ([#12377](https://github.com/PyTorchLightning/pytorch-lightning/pull/12377)),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz,1
Doc Fix: Passing datamodule argument to trainer.tune is supported (#12406),0.78709394,"Standardized the dataloaders arguments of trainer.{fit,valdiate,test,tune} (#7431)",Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com,1
Fix logging to loggers with multiple eval dataloaders (#12454),0.7286874,- Fixed logging to loggers with multiple eval dataloaders ([#12454](https://github.com/PyTorchLightning/pytorch-lightning/pull/12454)),,1
Prioritize Previously-Used GPUs During Auto-Selection (#10485),0.7893791,Support auto_select_gpus with the accelerator and devices API (#12608),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Add cpu device parser to validate cpu devices (#12160),0.7200644,"device parser (#3400, #3405)",,1
fix tqdm standalone test (#12493),0.44808477,Updated app testing (#16000),,0
Support loading a checkpoint with QAT (#11346),0.702392,    # put all logic related to loading a checkpoint here,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.comk-Pro.local,1
Update old device flags (#12471),0.60428715,We cleaned up the properties related to device indices (#14829).,,0
Docs clean up 1/n (#12477),0.53306335,Docs,,0
Fix current epoch value override on restart (#12429),0.68587005,Renamed reset_on_epoch to reset_on_run (#9658),,0
Run HPU tests only with yml (#12469) (#12478),0.60664225,    hparams_file='/path/to/hparams_file.yaml', Run HPU tests only with yml (#12469)  Execute supported tests serially Signed-off-by: Jerome janand@habana.ai,0
Update standalone tests (#12472),0.60220027,Updated app testing (#16000),,0
Drop PyTorch 1.7 support (#12432),0.9653611,Drop PyTorch 1.7 support,,1
fix title levels (#12470),0.41899735,"In addition, we fixed:",,0
Allow log to an existing run ID in MLflow with MLFlowLogger (#12290),0.88204354,- Allow logging to an existing run ID in MLflow with `MLFlowLogger` ([#12290](https://github.com/PyTorchLightning/pytorch-lightning/pull/12290)),Co-authored-by: bruno.cabado bruno.cabado@cinfo.es Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
Update tests in strategies directory in preparation for #11040 (#12467),0.55631226,Updated app testing (#16000),,0
Update remaining tests in test_accelerator_connector in preparation for #11040 (#12466),0.6148008,Removed deprecated connect_precision_plugin and connect_training_type_plugin from Accelerator (#9019),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Fix SWA LR scheduler not being stepped (#12446),0.62499976,Changed lr schedule step interval behavior to update every backwards pass instead of every forwards pass (#1477),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Update environment variable for cache reset for consistency (#12455),0.4793734,Renamed reset_on_epoch to reset_on_run (#9658),,0
Deprecate Trainer.gpus (#12436),0.7925889,"Removed deprecated trainer attributes - on_cpu, on_tpu, use_tpu, on_gpu, use_dp, use_ddp, use_ddp2, use_horovod, use_single_gpu (#7501)",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
"Update Introduction docs page to ""Lightning in 2 Steps"" (#12357)",0.55802065,Update the Lightning App docs (#13537),Co-authored-by: Aki Nitta nitta@akihironitta.com,0
Fix: Update sphinx-autodoc-typehints minimal version (#12468),0.83770317,Pinned sphinx-autodoc-typehints with <v1.15 (#11400),,1
Update tests/models/*.py to use devices instead of gpus or ipus (#11470),0.5540356,Deprecated Trainer(auto_select_gpus=...) in favor of pytorch_lightning.accelerators.find_usable_cuda_devices (#16147),,0
Preparing for 1.6.0rc1 (#12453),0.55480665,[1.6.1] - 2022-04-13,,0
Avoid fallback on CPU if no devices are provided (#12410),0.7382031,No longer fallback to CPU with no devices,,1
Add and update readme for docs and tests (#12348),0.5694548,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",,0
Update TQDM progress bar tracking with multiple dataloaders (#11657),0.72922146,Run main progress bar updates independent of val progress bar updates in TQDMProgressBar (#12563),,1
Update tpu_cores flag with accelerator and devices flag (#12158),0.6069481,"The new devices argument is now agnostic to all accelerators, but the previous arguments gpus, tpu_cores, ipus are still available and work the same as before. In addition, it is now also possible to set devices=""auto"" or accelerator=""auto"" to select the best accelerator available on the hardware.",,0
Create the loss accumulator directly on the device (#12430),0.57402134,- Modified `supporters.py` so that in the accumulator element (for loss) is created directly on the device ([#12430](https://github.com/PyTorchLightning/pytorch-lightning/pull/12430)),Co-authored-by: Ivan Svogor ivan.svogor@iarai.ac.at Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
Update fit_loop.py (#12450),0.6158422,$ python trainer.py fit \,,0
Add support for Habana accelerator (HPU) (#11808),0.8254544,- Added support for Habana Accelerator (HPU) ([#11808](https://github.com/PyTorchLightning/pytorch-lightning/pull/11808)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: Aki Nitta nitta@akihironitta.com Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: four4fish 88516121+four4fish@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Kaushik B kaushikbokka@gmail.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: jjenniferdai 89552168+jjenniferdai@users.noreply.github.com Co-authored-by: Kushashwa Ravi Shrimali kushashwaravishrimali@gmail.com Co-authored-by: Akarsha Rao 94624926+raoakarsha@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Carlos Mocholí carlossmocholi@gmail.comk-Pro.local,1
Collect and run all IPU tests (#11170),0.4289208,- Full tests that test specific functionality in trainer.,  Collect and run all ipu tests   Update azure pipeline   Increase pytest verbosity   Update RunIf   Co-authored-by: Akihiro Nitta nitta@akihironitta.com,0
Centralize DDP speedups in docs (#12448),0.57435083,Speeding up DDP with find_unused_parameters (#16611),,0
Remove unused devbot (#12338),0.5775739,Remove MetricsHolder (#7909), Remove devbot,0
Fix default labels in issue templates (#12434),0.36574215,Changed ModelCheckpoint version suffixes to start at 1 (#5008),,0
Remove AcceleratorConnector.parallel_devices (#12075),0.72662055,- Removed `AcceleratorConnector.parallel_device_ids` property ([#12072](https://github.com/PyTorchLightning/pytorch-lightning/pull/12072)),,1
Remove AcceleratorConnector.devices (#12435),0.59120643,"Raise MisconfigurationException when the accelerator is available but the user passes invalid ([]/0/""0"") values to the devices flag (#12708)",,0
Replace PostLocalSGDOptimizer with a dedicated model averaging component (#12378),0.72259945,- Replaced PostLocalSGDOptimizer with a dedicated model averaging component ([#12378](https://github.com/PyTorchLightning/pytorch-lightning/pull/12378)),,1
[docs] Update checkpointing.rst and callbacks.rst for Stateful support (#12351),0.6530055,The signature and behavior of the on_load_checkpoint and on_save_checkpoint callback hooks have changed (#14835):,,0
Enable tpu device ids test (#12428),0.5501894,Enabled manual optimization for TPUs (#8458),,0
"[3/3] Update lightning callbacks to Stateful, deprecations for old on_save/load_checkpoint signatures (#11887)",0.76724553,- Removed support for returning a value in `Callback.on_save_checkpoint` in favor of implementing `Callback.state_dict` ([#14835](https://github.com/Lightning-AI/lightning/pull/14835)),,1
ModelCheckpoint's save_last now ignores every_n_epochs (#12418),0.78076404,Changed defaults of save_top_k and save_last to None in ModelCheckpoint (#3680),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
Add AcceleratorRegistry (#12180),0.71934265,"AcceleratorRegistry.register(""sota_accelerator"", SOTAAccelerator, x=123)",Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com,1
Fix: freeze Jinja2 version (#12444),0.40593225,"At last, lots of bug fixes (see below).",,0
Update tests/accelerators/*.py to use devices instead of gpus or ipus (#11817),0.67780215,"The new devices argument is now agnostic to all accelerators, but the previous arguments gpus, tpu_cores, ipus are still available and work the same as before. In addition, it is now also possible to set devices=""auto"" or accelerator=""auto"" to select the best accelerator available on the hardware.",,0
Do not print empty evaluation result tables (#12427),0.5460764,- Do not print an empty table at the end of the `EvaluationLoop` ([#12427](https://github.com/PyTorchLightning/pytorch-lightning/pull/12427)),,0
Do not configure launcher if processes are launched externally (#12431),0.45964924,"Disabled strict loading in multiprocessing launcher (""ddp_spawn"", etc.) when loading weights back into the main process (#16365)",,0
Add Progress Bar to docs (#11359),0.6995845,Better progress bar (#16695),Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Update logging docs (#12245),0.6192851,Change default logger to a dedicated one (#1064),  Update logging docs   fix text size issue   put back Log Writing Frequency section ,0
Remove manual optimization find_unused_parameters override (#12425),0.71623236,            find_unused_parameters=True,,1
Call Strategy.process_dataloader in data_connector.py (#12251),0.620099,def tng_dataloader(self):,,0
Remove Accelerator.parallel_device_ids and deprecate Trainer.data_parallel_device_ids (#12072),0.78340924,"Removed deprecated trainer attributes - on_cpu, on_tpu, use_tpu, on_gpu, use_dp, use_ddp, use_ddp2, use_horovod, use_single_gpu (#7501)",,1
Use debug instead of detail logging for per-iteration hooks (#12281),0.57038635,moved hooks around in eval loop (#3195),,0
Update gpus flag with accelerator and devices flag (#12156),0.6113382,Support auto_select_gpus with the accelerator and devices API (#12608),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
fix merge issue (#12420),0.60722715,"merge backends (#3476, #3477, #3478, #3480, #3482)",,0
Update version for rc0 release (#12423),0.5041709,Set version as today (#13906),,0
Add docs and message for DDP static graph (#12411),0.54836524,DDP(2) backend (#2796),,0
Fix deepspeed keeping old sub-folders in same ckpt path (#12194),0.83903015,- Fixed deepspeed keeping old sub-folders in same ckpt path ([#12194](https://github.com/PyTorchLightning/pytorch-lightning/pull/12194)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
fix returning logged metrics instead of callback metrics during evaluation (#12224),0.75288737,Removed callback metrics from test results obj (#2994),Co-authored-by: thomas chaton thomas@grid.ai,1
Label new issues as needs triage by default (#12403),0.42106062,Fixing critical bugs in newly added hooks and hparams assignment., Add needs triage to issues by default,0
Raise a warning when nn.Module instance is saved with save_hyperparameters() (#12068),0.72754776,Don't raise a warning when nn.Module is not saved under hparams (#12669),,1
Update Slack link (#12421),0.5237708,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",,0
Update hash for caching (#12405),0.38809627,    # Don't forget to clear the memory for the next epoch!,Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Update Trainer config tests to use acelerator and devices (#12152),0.68115336,Removed obsolete self._device in Trainer (#1849),,0
Do not mark LightningModule methods as abstract (#12381),0.6554511,- The `ServableModule` is now an abstract interface ([#17000](https://github.com/Lightning-AI/lightning/pull/17000)),  do not mark LightningModule methods as abstract   add concrete test ,0
Pin setup-gcloud to v0 instead of master (#12375),0.41029346,"No longer need to set master port, Lightning does it for you using the job id."," Pin setup-gcloud to v0 instead of master.  setup-gcloud will be updating the branch name from master to main in a future release. Even though GitHub will establish redirects, this will break any GitHub Actions workflows that pin to master. This PR updates your GitHub Actions workflows to pin to v0, which is the recommended best practice. Co-authored-by: Akihiro Nitta nitta@akihironitta.com",0
Remove AccleratorConnector.num_ipus and deprecate Trainer.ipus (#12386),0.6936265,Removed obsolete self._device in Trainer (#1849),,0
Fix TPU CI (#12419),0.63706464,Resolve TPU miss rendezvous (#6781),,0
Add profiling for on_load_checkpoint/on_save_checkpoint callback and LM hooks (#12149),0.71700364,The signature and behavior of the on_load_checkpoint and on_save_checkpoint callback hooks have changed (#14835):,,1
Remove AcceleratorConnector.tpu_id (#12387),0.6949727,- Removed `AcceleratorConnector.tpu_id` property ([#12387](https://github.com/PyTorchLightning/pytorch-lightning/pull/12387)),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Remove AcceleratorConnector.num_processes and deprecate Trainer.num_processes (#12388),0.78093135,  * Removed the `Trainer(num_processes=...)` argument,,1
mergify: drop ready if conflicts (#12396),0.37368876,"Unify sLURM/TorchElastic under backend plugin (#4578, #4580, #4581, #4582, #4583)",,0
Deprecate BaseProfiler in favor of Profiler (#12150),0.6175028,Deprecated bool values in Trainer's profiler parameter (#3656),Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Kaushik B kaushikbokka@gmail.com,0
Clarify what's the PyTorch profiler used in docs (#12392),0.6892594,Deprecated pytorch_lightning.profiler in favor of pytorch_lightning.profilers (#16059),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
Reduce number of times optimizers are instantiated with FSDP (#12267),0.6445801,Generalized Optimizer validation to accommodate both FSDP 1.x and 2.x (#16733),,0
Update nightly GPU benchmark pool (#12366),0.5601752,"Deprecated log_gpu_memory, gpu_metrics, and util funcs in favor of DeviceStatsMonitor callback (#9921)",,0
Remove AcceleratorConnector.num_gpus and deprecate Trainer.num_gpus (#12384),0.80413604,  * Removed the `Trainer(gpus=...)` argument,,1
Refactor TorchElasticEnvironment.detect to use torch.distributed.is_torchelastic_launched (#12376),0.6418396,Corrected call to torch.no_grad (#5124),  Refactor TorchElasticEnvironment.detect to use native utility from torch.distributed   fix version and tests   fix version   Update tests/accelerators/test_accelerator_connector.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
CI: update prune_pkgs (#12382),0.44117337,Pruned requirements duplicity (#13739),,0
Update fairscale version (#11567),0.55767864,"For reference, FairScale's implementation can be used with",Co-authored-by: Aki Nitta nitta@akihironitta.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Fix deepspeed scheduler initialization (#12031),0.6019852,- Fixed the lr-scheduler state not being dumped to checkpoint when using the deepspeed strategy ([#11307](https://github.com/PyTorchLightning/pytorch-lightning/pull/11307)),,0
Remove AcceleratorConnector.root_gpu and deprecate Trainer.root_gpu (#12262),0.76982564,"Removed deprecated trainer attributes - on_cpu, on_tpu, use_tpu, on_gpu, use_dp, use_ddp, use_ddp2, use_horovod, use_single_gpu (#7501)",,1
[2/n] add Stateful functionality support for Callbacks (#12232),0.65733546,"    # Now, the state for this callback gets passed to this new method",,0
update docs for ModelCheckpoint save_last (#12332),0.7975867,Removed the deprecated save_function property in ModelCheckpoint (#8680),,1
Deprecate Trainer.devices in favor of Trainer.num_devices and  Trainer.device_ids  (#12151),0.88652056,Trainer.num_devices and Trainer.device_ids,,1
Avoid rich 10.15.0 and 10.15.1 (#12293),0.57531345,Improved exception message if rich version is less than 10.2.2 (#10839),  Update rich version   Update requirements/extra.txt   Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,0
Docker: fix NCCL building Horovod (#12318),0.4442974,Remove unnecessary intermediate layers in Dockerfiles (#5697), Horovod w. MPI nccl_built fix,0
Add support for specifying process group backend to relevant distributed strategies (#11745),0.6782036,Explicitly specify the process group backend if you choose to,,0
Deprecate Trainer.use_amp (#12312),0.86014366,Deprecated the Trainer(amp_level=...) argument,,1
Deprecate LightningModule.use_amp (#12315),0.76809144,Deprecated the LightningModule.optimizer_step(using_native_amp=...) argument,,1
Fix the case where logger=None is passed to Trainer (#12249),0.80141145,self.log-ing without a Trainer reference now raises a warning instead of an exception (#9733),,1
Pin Docker image for testing on GPUs (#12368),0.4316581,Enable non-blocking for device transfers to GPU (#1843), Pin docker image sha,0
Fix CLI snippet in the docs (#12275),0.47443372,Introducing CLI commands for apps (#13602)!,,0
CI: fix running PT 1.11 (#12304),0.48737967,The point of this release is more bug fixes ahead of v 1.0.0. We now have CI tests on TPU thanks to @zcain117 from Google! :slightly_smiling_face:, fix fire horovod assistant cmake u20 cuda -j2 fix mypy  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
check trainerfn == FITTING before configuring sync_batchnorm (#11919),0.7529768,Automatically set sync_batchnorm for training_type_plugin (#6536),Co-authored-by: edward-io me@edward.io Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Aki Nitta nitta@akihironitta.com,1
Only allow one value for each plugin type in plugins flag (#12083),0.4571398,Allowed separate config files for parameters with class type when LightningCLI is in subclass_mode=False (#10286),,0
unify logger testing (#9081),0.6163864,Cleaning up stale logger tests (#3490),Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
aggregate multiple helper scripts to single CLI (#11147),0.49649167,Support passing lists of callbacks via command line (#8815), nightly release min version fire,0
CI: sanity check for req. pkgs (#11819),0.42399845,Pkg: fix parsing versions by @Borda in https://github.com/Lightning-AI/lightning/pull/15401, CI: sanity check for req. pkgs scripts rename gcsfs ? rich ! install extra move set -e  Co-authored-by: Aki Nitta nitta@akihironitta.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
CI: enable testing for PT 1.11 (#11792),0.6012084,The point of this release is more bug fixes ahead of v 1.0.0. We now have CI tests on TPU thanks to @zcain117 from Google! :slightly_smiling_face:, enable PT 1.11 horovod Apply suggestions from code review  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Aki Nitta nitta@akihironitta.com,0
Skip horovod 0.24.0 only (#12248),0.5998905,horovod deprecation (#16141), try skip horovod 0.24.0 only HOROVOD_BUILD_CUDA_CC_LIST fix test  Co-authored-by: Aki Nitta nitta@akihironitta.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Support passing storage_options in trainer.save_checkpoint() API (#11891),0.7525369,- Added optional `storage_options` argument to `Trainer.save_checkpoint()` to pass to custom `CheckpointIO` implementations ([#11891](https://github.com/PyTorchLightning/pytorch-lightning/pull/11891)),,1
Have the outputs match the loops format (#12182),0.6580594,Loop Customization,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Drop PyTorch 1.7 testing from the CI (#12191),0.8093996,Drop PyTorch 1.7 support,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Aki Nitta nitta@akihironitta.com,1
Add LightningCLI(auto_registry) (#12108),0.86282384,LightningCLI(auto_registry=True),,1
add Azure HPU agent (#12258),0.46601835,Moveed HPU broadcast override to the HPU strategy file (#17011),,0
Deprecate LightningDataModule.on_save/load_checkpoint (#11893),0.8138398,- Removed the deprecated `LightningDataModule.on_save/load_checkpoint` hooks ([#14909](https://github.com/Lightning-AI/lightning/pull/14909)),,1
Integrate global step with progress tracking (#11805),0.6629915,Progress tracking,,0
Add callout items to the Docs landing page (#12196),0.5060736,Read more about callback entry points in our docs.,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
Disable tuner with distributed strategies (#12179),0.7213886,- Disabled tuner with distributed strategies ([#12179](https://github.com/PyTorchLightning/pytorch-lightning/pull/12179)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
fix str to_device section in converting.rst (#12243),0.5104976,"device parser (#3400, #3405)",,0
Remove accelerator hooks from being called in call_hook (#12237),0.6977129,Accelerator hooks are called regardless if LightningModule overrides the same hooks (#7826),,0
Add Strategy page to docs (#11441),0.44991702,Docs improvements,Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: rohitgr7 rohitgr1998@gmail.com Co-authored-by: Kushashwa Ravi Shrimali kushashwaravishrimali@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
"Fix ""Get Started"" at the top being 404 (#12210)",0.42760056,Moved base req. to root (#4219),,0
"Remove ""Optional"" hint from non-None arguments (#12214)",0.52203304,Sanitize None params during pruning (#6836),,0
Replace eval() with ast.literal_eval() for security (#12212),0.49619672,Enabled no returns from eval (#2446),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
Move ipu precision flag check to IPUPrecisionPlugin init (#12148),0.5564343,Changed iou [func] to allow float input (#4704),,0
Update configuration_validator.py (#12123),0.7089627,Configuration Validator (#9779),,1
Deprecate LightningModule.on_pretrain_routine_{start/end} (#12122),0.80040133,- Deprecated `LightningModule.on_pretrain_routine_start` and `LightningModule.on_pretrain_routine_end` hooks in favor of `on_fit_start` ([#12122](https://github.com/PyTorchLightning/pytorch-lightning/pull/12122)),,1
Deprecate AbstractProfiler in favor of BaseProfiler (#12106),0.5633005,- Removed the deprecated `BaseProfiler` and `AbstractProfiler` classes ([#14404](https://github.com/Lightning-AI/lightning/pull/14404)),,0
Mark logger_connector as protected (#12195),0.7246405,Removed logger_connector legacy code (#6733),,1
CI: update poplar sdk version (#12226),0.46148294,Resolved a bug where the install command was not installing the latest version of an app/component by default (#14181),,0
Remove data_pipeline attribute patch (#12204),0.5032858,move prepare_data to data connector (#3307),,0
Deprecate LoggerCollection in favor of trainer.loggers (#12147),0.78243095,"Deprecated passing flush_logs_every_n_steps as a Trainer argument, instead pass it to the logger init if supported (#9366)",Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Pin horovod to <0.24 (#12234),0.5959133,horovod deprecation (#16141),,0
add state_dict/load_state_dict to base Callback (#11998),0.7556279,"+def load_state_dict(self, state):",,1
Check parallel_devices passed through strategy is consistent with the accelerator flag (#12105),0.64086324,"Raise MisconfigurationException when the accelerator is available but the user passes invalid ([]/0/""0"") values to the devices flag (#12708)",,0
Rename some doc files (#12163),0.4520706,Rename failed -> error in tables (#15608),,0
Deprecate PrecisionPlugin.on_save/load_checkpoint (#11978),0.7355988,Deprecated PrecisionPlugin.master_params() in favor of PrecisionPlugin.main_params() (#10105),,1
add accelerator.is_available() check (#12104),0.69279605,Ensure accelerator is valid if running interactively (#5970),Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Kaushik B kaushikbokka@gmail.com,0
Remove is_global_zero check in training_epoch_loop (#12134),0.63889134,"def training_epoch_end(self, outputs):",,0
Improve mechanism to reset the seed after sanity check (#11870),0.6371708,seed_everything now fails when an invalid seed value is passed instead of selecting a random seed (#8787),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
Add SyncBatchNormPlugin (#11754),0.46557307, * `NativeSyncBatchNorm` is now `TorchSyncBatchNorm`,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,0
Update API references for Callbacks (#11350),0.72157574,Removed deprecated callbacks (#3979),Co-authored-by: rohitgr7 rohitgr1998@gmail.com,1
Add Lightning Ecosystem to docs (#11399),0.5799887,Update the Lightning App docs (#13537),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Spelling and grammar updates in documentation (#11861),0.612612,Docs improvements,Co-authored-by: rohitgr7 rohitgr1998@gmail.com,0
Update API references for Core API (#11352),0.6066999,This release has breaking API changes. See #124 for all details. ,,0
Deprecate weights_save_path from the Trainer constructor  (#12084),0.85816264,"Deprecated Trainer attribute ckpt_path, which will now be set by weights_save_path (#2681)",Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Do not prefetch when possible (#12101),0.6092421,    def prefetch_batches(self):,,0
Add missing import statements in lightning_module.rst (#11946),0.56811225,import lightning as L,,0
Stop loading a few properties if checkpoint's dirpath has changed (#12045),0.664995,Changed Checkpoint path parameter from filepath to dirpath (#1016),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Add support for pluggable Accelerators (#12030),0.70374167,Registering Custom Accelerators,Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
[CLI] Fix SaveConfigCallback with DDP spawn (#12011),0.63347983,Changed the default of find_unused_parameters back to True in DDP and DDP Spawn (#6438),,0
"Fix LightningModule.{un,}toggle_model when only 1 optimizer is used (#12088)",0.8948858,"- Fixed `LightningModule.{un,}toggle_model` when only 1 optimizer is used ([#12088](https://github.com/PyTorchLightning/pytorch-lightning/pull/12088))",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Add estimated_stepping_batches property to Trainer (#11599),0.90668344,Trainer.estimated_stepping_batches,,1
fix to avoid common hook warning if no hook is overridden (#12131),0.63135445,- Fixed to avoid common hook warning if no hook is overridden ([#12131](https://github.com/PyTorchLightning/pytorch-lightning/pull/12131)),,0
Update validation_epoch_end docs (#12099),0.773473,validation_end >> validation_epoch_end,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Add issue triage config (#11880),0.46540764,Configuration Validator (#9779),,0
Clean loop fetching usage (#12103),0.4866295,Moved result teardown to the loops (#8245),,0
Refactor Horovod NCCL check (#11948),0.48350677,"refactored Horovod backend (#3121, #3122)",,0
Unit test for CLI with subcommands and a common default config file (#12061),0.58569443,- Full tests that run multiple models in different configs,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Remove AcceleratorConnector.num_nodes  (#12107),0.7169858,- Removed `AcceleratorConnector.num_nodes` ([#12107](https://github.com/PyTorchLightning/pytorch-lightning/pull/12107)),Co-authored-by: Aki Nitta nitta@akihironitta.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
[IPU] Support manually instantiating the poptorch.DataLoader (#12116),0.6584803,- No longer set a `DistributedSampler` to the `poptorch.DataLoader` when IPUs are used ([#12114](https://github.com/PyTorchLightning/pytorch-lightning/pull/12114)),,0
Remove AcceleratorConnector.use_dp (#12112),0.67538154,Deprecated access to the AcceleratorConnector.configure_slurm_ddp method and marked it as protected (#10101),,0
Remove AcceleratorConnector.has_tpu  (#12109),0.728541,- Removed `AcceleratorConnector.tpu_id` property ([#12387](https://github.com/PyTorchLightning/pytorch-lightning/pull/12387)),,1
Remove AcceleratorConnector.has_ipu (#12111),0.7004067,- Removed `AcceleratorConnector.has_ipu` property ([#12111](https://github.com/PyTorchLightning/pytorch-lightning/pull/12111)),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
remove AcceleratorConnector.use_ipu (#12110),0.7086307,- Removed `AcceleratorConnector.has_ipu` property ([#12111](https://github.com/PyTorchLightning/pytorch-lightning/pull/12111)),,1
add parity test for sync batchnorm (#12021),0.56574595,Automatically set sync_batchnorm for training_type_plugin (#6536),,0
Refactor codebase to use trainer.loggers over trainer.logger when needed (#11920),0.751082,"The loggers used to be wrapped by a LoggerCollection object, so that when you accessed trainer.logger you could log to all of them simultaneously. However, this ""magic"" caused confusion and errors among users and we decided to simplify this (#14283):",,1
Return early in reset_seed (#11983),0.56642246,seed_everything now fails when an invalid seed value is passed instead of selecting a random seed (#8787),,0
"Refactor tests, skip if sklearn not available (#12093)",0.49313706,Refactored setup_training and remove test_mode (#5388),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Deprecate BaseProfiler.profile_iterable (#12102),0.7326608,- Deprecated `BaseProfiler.profile_iterable` ([#12102](https://github.com/PyTorchLightning/pytorch-lightning/pull/12102)),,1
[IPU] Do not use DistributedSampler (#12114),0.6245847,- Disallowed using `BatchSampler` when running on multiple IPUs ([#13854](https://github.com/Lightning-AI/lightning/pull/13854)),,0
Remove AccleratorConnector.device_type (#12081),0.52998817,- Removed the `AcceleratorConnector.device_type` property ([#12081](https://github.com/PyTorchLightning/pytorch-lightning/pull/12081)),  remove AccleratorConnector.device_type   Update CHANGELOG.md   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Avoid patching common DataHooks to the LightningModule (#10603),0.57358557,- Improved support for custom `DataLoader`s when instantiated in `*_dataloader` hook ([#12981](https://github.com/Lightning-AI/lightning/pull/12981)),,0
"Deprecate callback hooks on_pretrain_routine_{start,end} (#11794)",0.79824114,Deprecate early_stop_callback Trainer argument (#3845),Co-authored-by: rohitgr7 rohitgr1998@gmail.com,1
Default benchmark based on deterministic flag (#11944),0.5405039,- Make `benchmark` flag optional and set its value based on the deterministic flag ([#11944](https://github.com/PyTorchLightning/pytorch-lightning/pull/11944)),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
Add support for dataloader_iter to validate and test steps (#11546),0.7276331,"validation_step, val_dataloader are now optional.   ",Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
Fix passing _ddp_params_and_buffers_to_ignore (#11949),0.6692766,Changed the default of find_unused_parameters to False in DDP (#5185),,0
Add bagua installation in dockerfile (#11283),0.38731414,Remove unnecessary intermediate layers in Dockerfiles (#5697),Co-authored-by: Aki Nitta nitta@akihironitta.com Co-authored-by: Jirka jirka.borovec@seznam.cz,0
add properties to check for trainer state in pytorch profier (#12063),0.66201985,trainer = L.Trainer(ipus=1),,0
Set the last global step saved only when actually saving (#12057),0.55017316,+ global_step += 1,,0
Update warnings for available accelerators not being used (#11909),0.65418017,"To support new accelerator and stategy features, we completely rewrote our internal AcceleratorConncetor class. No backwards compatibility was maintained so it is likely to have broken your code if it was using this class.",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Further clean up aggregation logic (#12053),0.44367528,Drop duplicate metrics (#5014),,0
Fixed a small issue in the documentation for FeatureExtractorFreezeUnfreeze (#12049),0.49673933,This release fixes that core issue,,0
[CLI] Add config -c argument (#12039),0.5978552,"Anywhere in your program, you can now call the CLI directly:",,0
Improve annotations for _defaults_from_env_vars decorator (#11888),0.5265426,"Deprecated description, env_prefix and env_parse parameters in LightningCLI.__init__ in favour of giving them through parser_kwargs (#15651)",Co-authored-by: rsokl ryan.soklaski@ll.mit.edu Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,0
Add callback to manage fault-tolerance checkpoints (#11862),0.69630694,Asynchronous Checkpointing,Co-authored-by: Danielle Pintz 38207072+daniellepintz@users.noreply.github.com,0
Fix is_interactive_compatible logic after AcceleratorConnector rewrite (#12008),0.6773784,AcceleratorConnector rewrite,"  fix is_interactive_compatible   improve tests   update message   address review   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
Remove Trainer._device_type (#11992),0.8865665,Removed obsolete self._device in Trainer (#1849),,1
remove ddp procs collection from script launcher (#12029),0.5168502,        Override to init DDP in a different way or use your own wrapper.,,0
Avoid loading dataloaders if limit_batches=0 (#11576),0.64562756,- Disable loading dataloades if corresponding `limit_batches=0` ([#11576](https://github.com/PyTorchLightning/pytorch-lightning/pull/11576)),,0
Remove DDPSpawnStrategy.get_mp_spawn_kwargs in favor of launchers (#11966),0.6691037,Merged the implementation of DDPSpawnStrategy into DDPStrategy and removed DDPSpawnStrategy (#14952),,0
Update RichProgressBarTheme after detecting light theme on colab (#10993),0.88290405,- Update `RichProgressBarTheme` styles after detecting light theme on colab ([#10993](https://github.com/PyTorchLightning/pytorch-lightning/pull/10993)),,1
Add XLA Profiler section to docs (#11436),0.5355722,xla_device_utils >> xla_device,Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com,0
Move strategy tests from accelerators to strategies directory (#11329),0.50302184,- Removed support for passing strategy names or strategy instances to the accelerator Trainer argument ([#12696](https://github.com/Lightning-AI/lightning/pull/12696)),,0
Remove calls to profile model_forward (#12032),0.51885223,remove obscure forward call in eval + CPU backend ___step (#3123),Co-authored-by: ananthsub ananth.subramaniam@gmail.com,0
"Deprecate agg_key_funcs, agg_default_func, and update_agg_funcs from LightningLoggerBase (#11871)",0.858124,- Deprecated `agg_key_funcs` and `agg_default_func` parameters from `LightningLoggerBase` ([#11871](https://github.com/PyTorchLightning/pytorch-lightning/pull/11871)),Co-authored-by: Danielle Pintz 38207072+daniellepintz@users.noreply.github.com,1
Fix typos on new-project page (#11942),0.5138968,"In addition, we fixed:",Co-authored-by: Chaddie chaddie.paik@webtoonscorp.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Update tests/plugins/*.py to use devices instead of gpus or ipus (#11872),0.53923404,  * Deprecated the `pytorch_lightning.utilities.device_parser.determine_root_gpu_device` in favor of `lightning_lite.utilities.device_parser.determine_root_gpu_device`,,0
Fix missing imports in converting.rst (#11945),0.39432153,Add missing python-multipart dependency (#17244),,0
PyTorch documentation updates (#11739),0.71477836,"Following our 4 PyTorch release window, this release supports PyTorch 1.8 to 1.11. Support for PyTorch 1.7 has been removed.",Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Ethan Harris ethanwharris@gmail.com Co-authored-by: thomas chaton thomas@grid.ai,1
Clarify val_check_interval description (#11951),0.5154512,Trainer(val_check_interval=100),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
add default args in Trainer methods doc (#11614),0.8632527,Trainer Argument Defaults,,1
Refactor signature for launcher (#11967),0.48633116,Changed signatures:,,0
Update Bagua section example (#11899),0.43809667,The Bagua Strategy,,0
Fix docstrings of on_fit_start/end #12016,0.49561653,  * Removed the `FitLoop.split_idx` property,,0
Update PT to PL conversion doc (#11397),0.48474336,Simplify the PL examples structure (shallower and more readable) (#1247),Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk,0
CI: fix upload-artifact (#11962),0.45874405,- Fixed a bug where the upload files endpoint would raise an error when running locally ([#14924](https://github.com/Lightning-AI/lightning/pull/14924)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Describe the behavior with limit_*_batches=1|1.0 (#11950),0.5567224,Disabled training when limit_train_batches=0 (#4371),,0
Support manual optimization step profiling without a trainer reference (#11883),0.6956817, - Profiling: `Trainer(profiler=...)`,,0
Remove Trainer._strategy_type (#11990),0.74577796,"  * Removed class methods from Trainer: `default_attributes()`, `from_argparse_args()`, `parse_argparser()`, `match_env_arguments()`, `add_argparse_args()`",,1
Add back deterministic support in accelerator_connector (#11999),0.66226184,Removed deprecated connect_precision_plugin and connect_training_type_plugin from Accelerator (#9019),Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,0
Restore test after #11448 (#11986),0.60712504,Skip restore from resume_from_checkpoint while testing (#5161),,0
Fix import error when running doctests for RL examples (#12010),0.37724695,Module imports are now catching ModuleNotFoundError instead of ImportError (#9867),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Support gradient accumulation using Horovod's backward_passes_per_step (#11911),0.6235761,- Enable gradient accumulation using Horovod's `backward_passes_per_step` ([#11911](https://github.com/PyTorchLightning/pytorch-lightning/pull/11911)),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Refactor Strategy._move_optimizer_states as utility functions (#11758),0.6183742,"  * Removed `optimizer_idx` argument from `Strategy.{optimizer_step,backward}` and all of its overrides in subclasses",Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: thomas chaton thomas@grid.ai,0
Deprecate and remove calls to agg_and_log_metrics (#11832),0.70928043,Removed deprecated metrics (#8586),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
"Remove log_text, and log_image from LightningLoggerBase API  (#11857)",0.7910608,- Removed `log_text` and `log_image` from the `LightningLoggerBase` API ([#11857](https://github.com/PyTorchLightning/pytorch-lightning/pull/11857)),,1
Rewrite accelerator_connector (#11448),0.76532173,AcceleratorConnector rewrite,,1
Refactor early stopping test (#11866),0.5470296,Early stopping checks on_validation_end (#1458),,0
Add process launchers (#11643),0.47148246,Introducing CLI commands for apps (#13602)!,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Update horovod.py (#11917),0.6304886,horovod deprecation (#16141),,0
Fix environment variable order for global rank determination (#11406),0.6995175,- Fixed environment variable priority for global rank determination ([#11406](https://github.com/PyTorchLightning/pytorch-lightning/pull/11406)),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Update ddp.py (#11929),0.60741407,DDP(2) backend (#2796),,0
fix typos (#11937),0.53933096,"In addition, we fixed:",,0
Support optimizer step progress tracking with manual optimization (#11848),0.6535617,          optimizer.step(),,0
Small cleanup when dataloader states are saved (#11843),0.61016154,refactored dataloader process hook (#3139),,0
Unblock GPU CI (#11934),0.6186327,Enable non-blocking for device transfers to GPU (#1843),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
Update trainer.rst,0.6514655,API changes to the trainer,,0
Pin myst-parser<0.17,0.40950066,Parsed local package versions (#13933),,0
Introduce Stateful PrecisionPlugin (#11638),0.6783527,- PrecisionPlugin.{post_optimizer_step},Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com,0
Update typing for CometLogger.experiment (#11836),0.6009151,Using .comet.config file for CometLogger (#1913),,0
Refine the pytorch profiler (#11268),0.70627433,"Resolved schedule step bug for PyTorch Profiler (#6674, #6681)",,1
Fix master merge conflict (#11858),0.5088679,"merge backends (#3476, #3477, #3478, #3480, #3482)",,0
Revert saving the dataloader and result collection by default (#11842),0.5806335,      return DataLoader(...),,0
Fix current_epoch value on training end (#8578),0.7877902,Made training_epoch_end behave like validation_epoch_end (#1357),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
update changelog after 1.5.10 release (#11830),0.6037421,Full Changelog,,0
"Make default logger name ""lightning_logs"" (#11762)",0.7230475,Change default logger to a dedicated one (#1064),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Fix horovod installation base-cuda Dockerfile (#11811),0.47680715,  * Deprecated the `pytorch_lightning.utilities.device_parser.num_cuda_devices` in favor of `lightning_lite.accelerators.cuda.num_cuda_devices`,  pip install --user   add checks   rm unrelated comment   consistent format   Fail if horovod not found   Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Update module path for LightningDeprecationWarning in setup.cfg (#11793),0.6125205,- Deprecated `pytorch_lightning.utilities.warnings.LightningDeprecationWarning` in favor of `pytorch_lightning.utilities.rank_zero.LightningDeprecationWarning`,,0
Add Accelerator.is_available() interface requirement (#11797),0.67986876,Removed deprecated property AcceleratorConnector.is_using_torchelastic in favor of TorchElasticEnvironment.is_using_torchelastic() (#9729),,0
Fix save_hyperparameters when no parameters need saving (#11827),0.872632,Add ignore param to save_hyperparameters (#6056),,1
Create loggers property for Trainer and LightningModule (#11683),0.8347175,- Added a `loggers` property to `LightningModule` which retrieves the `loggers` property from `Trainer` ([#11683](https://github.com/PyTorchLightning/pytorch-lightning/pull/11683)),Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Danielle Pintz 38207072+daniellepintz@users.noreply.github.com,1
feat(wandb): support distributed modes (#11650),0.5104061,The following modes are supported:,Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
Move data fetcher ownership to the loops (#11621),0.5187665,- Moved ownership of the data fetchers from the DataConnector to the Loops ([#11621](https://github.com/PyTorchLightning/pytorch-lightning/pull/11621)),,0
bug fix #10872 (#10965),0.58747345,"At last, lots of bug fixes (see below).",Co-authored-by: louie.kim louie.kim@kakaocorp.comlouie.kim@kakaocorp.com Co-authored-by: Jirka jirka.borovec@seznam.cz,0
Update inference doc (#11428),0.5680138,Inference mode support,,0
Update tests/trainer/*.py to use devices instead of gpus or ipus (#11697),0.72804314,- Deprecated `Trainer.gpus` in favor of `Trainer.device_ids` or `Trainer.num_devices` ([#12436](https://github.com/PyTorchLightning/pytorch-lightning/pull/12436)),,1
Return the output of the optimizer step (#11711),0.70286,    optimizer1.step(),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
remove todos (#11804),0.5483106,Remove MetricsHolder (#7909),,0
Faster callback configuration validator checks (#11785),0.57918334,Configuration Validator (#9779),Co-authored-by: rohitgr7 rohitgr1998@gmail.com,0
Configure native deepspeed schedulers with interval='step' (#11788),0.7538477,"- Configure native Deepspeed schedulers with interval='step' ([#11788](https://github.com/PyTorchLightning/pytorch-lightning/pull/11788)), ([#12031](https://github.com/PyTorchLightning/pytorch-lightning/pull/12031))",,1
update slack link,0.4754892,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",,0
Delete test_on_before_accelerator_backend_setup (#11803),0.6547931,Renamed all backends to Accelerator (#4066),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
Introduce Stateful DataModule (#11637),0.5781925,Support shorthand notation to instantiate datamodules (#10011),,0
bug fix: restore_optimizers correctly handles non-mapping values in optimizer.state.values() (#11757),0.58876294,Changed default behaviour of configure_optimizers to use no optimizer rather than Adam. (#1279),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
Fix to avoid moving batch to device for DataParallel (#11780),0.7077911,"With DPStrategy, the batch is not explicitly moved to the device (#11780)",Co-authored-by: ananthsub ananth.subramaniam@gmail.com,1
Deprecate on_epoch_start/on_epoch_end hook (#11578),0.8044483,# 3. Rename the hook to `on_*_epoch_end`,,1
Use fsspec in checkpoint connector for fault-tolerant training (#11776),0.6233716,Updated model_checkpoint's to_yaml to use fsspec open (#3801),,0
Update DDPStrategy to use optimizers property from within class (#11777),0.6679879,Merged the implementation of DDPSpawnStrategy into DDPStrategy and removed DDPSpawnStrategy (#14952),,0
reduce only loss with dp (#11594),0.5755475,        return loss0,Co-authored-by: Aki Nitta nitta@akihironitta.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Deprecate on_before_accelerator_backend_setup callback hook (#11655),0.80469126,  * `Callback.on_before_accelerator_backend_setup` in favor of `Callback.setup`,Co-authored-by: rohitgr7 rohitgr1998@gmail.com,1
Centralize rank_zero_only utilities into their own module (#11747),0.5535191,- Added `rank_zero` module to centralize utilities ([#11747](https://github.com/PyTorchLightning/pytorch-lightning/pull/11747)), Centralize rank_zero_only utilities into their own module  Fixes #11746   PossibleUserWarning   Update test_warnings.py   update imports   more imports   Update CHANGELOG.md   Update mlflow.py   Update cli.py   Update api_references.rst   Update meta.py   add deprecation tests   debug standalone   fix standalone tests   Update CHANGELOG.md ,0
Small improvements to TB and CSV loggers (#11764),0.5694973,Refactored logging, small improvements to TB and CSV loggers addr comments remove redundant lines and update tests  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Aki Nitta nitta@akihironitta.com,0
Keep is_global_zero definitions in sync across strategy and trainer (#11761),0.54591876,"Deprecated Trainer(strategy=""horovod"")",,0
Update TPU Spawn to use root_device instead of LightningModule's device (#11750),0.58883786,- Renamed `strategy='tpu_spawn'` to `strategy='xla'` and `strategy='tpu_spawn_debug'` to `strategy='xla_debug'` ([#16781](https://github.com/Lightning-AI/lightning/pull/16781)),,0
Allow Horovod teardown() to complete gracefully if exception thrown in callback setup (#11752),0.6194109,- Fixed an issue where `HorovodStrategy.teardown()` did not complete gracefully if an exception was thrown during callback setup [#11752](https://github.com/PyTorchLightning/pytorch-lightning/pull/11752),,0
Use root_device in XLAStatsMonitor callback (#11749),0.6131873,xla_device_utils >> xla_device,,0
CI: fix valid schema (#11744),0.45374358,Changed ModelCheckpoint version suffixes to start at 1 (#5008),,0
Use root_device in DeviceStatsMonitor callback  (#11748),0.66505384,Equivalent to DeviceStatsMonitor()," Use trainer.strategy.root_device in favor of LightningModule.device in DeviceStatsMonitor  Minor refactor to use the strategy's own root_device instead of the LightningModule's device property. Attempts at manual model parallelization by extending this plugin will face difficulties with the assumption that the LightningModule has all of its parameters on the same device.  For those use cases, it is critical to remove the assumption that the module has a device property (device in general goes against PyTorch module's design principles: - https://github.com/pytorch/pytorch/issues/7460 - https://github.com/PyTorchLightning/pytorch-lightning/pull/1790#discussion_r423459412",0
Update HorovodStrategy to use optimizers property from within class (#11728),0.7084141,Deprecated the HorovodStrategy class,,1
Remove legacy dead code in DDP script launch (#11678),0.56949323,        Override to init DDP in a different way or use your own wrapper.,Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
Allow access to ckpt_path within context of fit() (#11696),0.5419092,"trainer.ckpt_path = ""/path/to/checkpoint.ckpt""",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Support no pre-fetching (#11606),0.50997645,- Added support for no pre-fetching to `DataFetcher` ([#11606](https://github.com/PyTorchLightning/pytorch-lightning/pull/11606)),,0
Remove self._log_dir from BaseProfiler (#11740),0.6583842,Removed deprecated BaseProfiler.output_filename arg from it and its descendants in favor of dirpath and filename (#9214),,0
Use the strategy's root_device instead of the LightningModule's device property (#11734),0.62138116,- Fixed an issue with `LightningLite.setup()` not setting the `.device` attribute correctly on the returned wrapper ([#14822](https://github.com/Lightning-AI/lightning/pull/14822)),,0
Improve the result printing at the end of evaluation (#11332),0.5400839,Moved result teardown to the loops (#8245),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com,0
WandbLogger's log_image can use step argument (#11716),0.66057175,Changed WandbLogger(log_model={True/'all'}) to log models as artifacts (#6231),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
"Add Trainer(strategy=""bagua"") (#11146)",0.8279933,"trainer = pl.Trainer(strategy=""bagua"")",Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Sean Naren sean@grid.ai Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: thomas chaton thomas@grid.ai,1
Minor refactors to init_dist_connection (#11733),0.55773866,"def init_ddp_connection(self, proc_rank, world_size):",,0
Deprecate on_batch_start/on_batch_end callback hooks (#11577),0.7244855,  * `Callback.on_batch_end` in favor of `Callback.on_train_batch_end`,,1
Update debugging doc (#11445),0.5417967,DDP Debugging Improvements,Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
added warning for distributedsampler in case of evaluation (#11479),0.7205633,- Added a warning when using `DistributedSampler` during validation/testing ([#11479](https://github.com/PyTorchLightning/pytorch-lightning/pull/11479)),,1
Update tests/tuner/*.py to use devices instead of gpus or ipus (#11520),0.5736606,"Deprecated pytorch_lightning.tuner.auto_gpu_select.{pick_single_gpu,pick_multiple_gpus} in favor of pytorch_lightning.accelerators.find_usable_cuda_devices (#16147)",Co-authored-by: Jirka jirka.borovec@seznam.cz,0
Update CHANGELOG after the 1.5.9 release (#11558),0.62682605,Full Changelog,,0
Fix to avoid val progress bar disappear after validate (#11700),0.602924,Run main progress bar updates independent of val progress bar updates in TQDMProgressBar (#12563),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Fix rich with uneven refresh rate tracking (#11668),0.6082979,- Fixed `RichProgressBar` progress when refresh rate does not evenly divide the total counter ([#11668](https://github.com/PyTorchLightning/pytorch-lightning/pull/11668)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Avoid enforcing shuffle=False for eval dataloaders (#11575),0.8338884,Enforced eval shuffle warning only for default samplers in DataLoader (#12653),,1
Move progress bar disabling out of the Trainer (#11377),0.8003517,Decoupled the progress bar from trainer. It is a callback now and can be customized or even be replaced entirely (#1450).,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
Refine style guide (#11394),0.48597693,Syntax changes are: ,,0
Fix mid-epoch warning call while resuming (#11556),0.6318938,"Removed the on_epoch guard from the ""should stop"" validation check (#7701)",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka jirka.borovec@seznam.cz,0
Lazy initialize Strategy.parallel_devices (#11572),0.50972515,"When using multiple devices, the strategy now defaults to ""ddp"" instead of ""ddp_spawn"" when none is set (#16388)",Co-authored-by: Aki Nitta nitta@akihironitta.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Fix rich progress bar render only on main pbar (#11690),0.6457335,New Rich Progress Bar,,0
Replace occurrences of on_before_accelerator_backend_setup_called with setup  (#11568),0.7150043,  * `Callback.on_before_accelerator_backend_setup` in favor of `Callback.setup`,Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
Improving instructions in finetuning docstring (#10484),0.54588807,Docs improvements,,0
Add change log for Accelerator (#11591),0.55418736,Full Changelog,,0
Allow a CombinedLoader as the training data in DDP (#11648),0.684945,- Added support for DDP when using a `CombinedLoader` for the training data ([#11648](https://github.com/PyTorchLightning/pytorch-lightning/pull/11648)),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka jirka.borovec@seznam.cz,0
Clarify what the default values for log are based on hooks (#11611),0.5603697,Un-balanced logging properly supported (#5119),,0
fix available modules (#11526),0.6587302,Renamed utils modules (#5199),,0
Update new-project.rst (#11593),0.41623113,Set version as today (#13906),"A small typo in one of the code blocks. ""self.encoded"" should be replaced by ""self.encoder""",0
Fix typo in TensorBoardLogger.log_metrics error message (#11595),0.75225365,Changed the default logger to TensorBoardLogger (#609),,1
Update req. for pyDeprecate version flexibility (#11629),0.64057356,- The `pyDeprecate` dependency is no longer installed ([#14472](https://github.com/Lightning-AI/lightning/pull/14472)),,0
[CLI] Support shorthand for loggers (#11533),0.7361349,"Finally, loggers are also now configurable with shorthand:",,1
Removed subsection in LightningDataModule (#11675),0.55636644,- Removed the deprecated `LightningIPUModule` ([#14830](https://github.com/Lightning-AI/lightning/pull/14830)),,0
Add typing for utilities/memory.py (#11545),0.44309402,Removed support for self.log()ing a dictionary (#16389),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
Deprecate on_configure_sharded_model callback hook for v1.6 (#11627),0.7777258,  * `Callback.on_configure_sharded_model` in favor of `Callback.setup`,Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Danielle Pintz 38207072+daniellepintz@users.noreply.github.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Mark CheckpointConnector as protected (#11550),0.664932,Refactored CheckpointConnector to offload validation logic to the CheckpointIO plugin (#9045),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Refactor get_filesystem to use native fsspec API (#11708),0.5176605,Used fsspec instead of gfile for all IO (#3320),,0
Remove experiment property from abstract class (#11603),0.5075768,No need for experiment object in trainer.   ,Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Fix rich progress bar metric render on epoch end (#11689),0.73152566,The Rich progress bar now correctly shows the on_epoch logged values on train epoch end (#11689),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: Jirka jirka.borovec@seznam.cz,1
Rename _SupportsStateDict --> _Stateful Protocol (#11469),0.50784826,"Removed experimental fault-tolerance support (#16516, #16533)",,0
[CLI] Fix bug that forces overriding configure_optimizers (#11672),0.696714,Improved error messages for invalid configure_optimizers returns (#3587),,0
update no_warning_call utility in tests (#11557),0.62724054,warning_utils >> warnings,,0
Decouple utilities from LightningLoggerBase (#11484),0.68248993,"Deprecated LightningLoggerBase.close, LoggerCollection.close in favor of LightningLoggerBase.finalize, LoggerCollection.finalize (#9422)",Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Jirka jirka.borovec@seznam.cz,0
Rename Strategy.lr_schedulers to Strategy.lr_scheduler_configs (#11549),0.65684783,    --lr_scheduler=anneal_strategy=linear,,0
"Set the state before saving ""last"" or ""none"" checkpoints (#11481)",0.7287365,    # put all logic related to saving a checkpoint here,Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
Modernize the ImageNet example (#11691),0.5661222,Updated semantic segmentation example with custom u-net and logging (#1371),Co-authored-by: Aki Nitta nitta@akihironitta.com Co-authored-by: Jirka jirka.borovec@seznam.cz,0
Teardown all internal components on exception (#11620),0.5378927,Removed teardown from ParallelPlugin (#8943),,0
Sort simple profiler summary based on mean duration (#11671),0.6510501,Changed Simple Profiler report to order by percentage time spent + num calls (#4880),,0
Fix val_loop run on restart (#11552),0.7185539,Enabling val/test loop disabling (#2692),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Changes in preparation to #8578 (#11562),0.44597822,Noteworthy changes:,,0
Add typing to data fetching (#11515),0.46368873,Refactored setup for typing friendly (#6590),,0
CI: use concurrency (#11351),0.48265848,Inter Batch Parallelism,,0
Prepare for torch==1.11 (#11679),0.57393086,Corrected call to torch.no_grad (#5124),,0
Fix apex installation path in Dockerfile (#11596),0.43752837,Remove unnecessary intermediate layers in Dockerfiles (#5697),  empty commit   Specify apex installation target directory   pip install --user ,0
Update torch version matrix (#11649),0.6610031,- Fixed an issue with comparing torch versions when using a version of torch built from source ([#17030](https://github.com/Lightning-AI/lightning/pull/17030)),,0
Merge pull request #11635 from PyTorchLightning/ci/fix-term,0.6151711,Removed deprecated code in pytorch_lightning.utilities.meta (#16038),Pin coverage<6.3,0
Remove useless pass and abc (#11522),0.4223295,"Cleaning (#5948, #5949, #5950)",,0
Let Accelerator inherit from ABC to make sure abstractmethod takes effect (#11521),0.61693215,Enabled passing in custom accelerators (#4050),,0
Construct the hook kwargs inside each loop (#11511),0.5965096,moved hooks around in eval loop (#3195),,0
Update test_fetching.py (#11551),0.43681428,python script.py test,,0
Refactor fetching function (#11516),0.44476998,Refactoring,,0
Fix checkpoint values when saving and resetting the tuner state (#11518),0.75894856,The tuner now usees a unique filename to save a temporary checkpoint (#9682),,1
Fixed wandb logger documentation (#11540),0.7777734,Removed wandb logger's finalize method (#1193),,1
Tests: Fail on FutureWarning (#11541),0.53624076,Removed no return warning from val/test step (#6139),,0
[CLI] Save only the configuration used (#11532),0.6982765,  * `save_config_filename`,,0
Remove access to _short_id in NeptuneLogger (#11517),0.77296025,- Removed access to `_short_id` in `NeptuneLogger` ([#11517](https://github.com/PyTorchLightning/pytorch-lightning/pull/11517)),,1
Mark SignalConnector as protected (#11513),0.49703917,- Marked the `lightning.pytorch.trainer.connectors.signal_connector.HandlersCompose` class as protected ([#17008](https://github.com/Lightning-AI/lightning/pull/17008)),Co-authored-by: rohitgr7 rohitgr1998@gmail.com,0
Remove Strategy.on_tpu property (#11536),0.6196221,- Removed `Strategy.on_tpu` property ([#11536](https://github.com/PyTorchLightning/pytorch-lightning/pull/11536)),,0
Remove Strategy.on_gpu (#11537),0.64377713,- Removed `Strategy.on_gpu` ([#11537](https://github.com/PyTorchLightning/pytorch-lightning/pull/11537)),,0
Update Model Parallel doc (#11465),0.509819,Enhanced load_from_checkpoint to also forward params to the model (#1307),,0
Add a direct link to built docs in CI (#11514),0.4997937,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",,0
improve simple profiler output (#11414),0.5883739,Changed Simple Profiler report to order by percentage time spent + num calls (#4880),,0
Use a dataclass as the scheduler config (#11443),0.54045916,"        scheduler = TanhLRScheduler(optimizer, ...)",,0
Replacing latest tag with 0.4.0 for lightning-bolts links (#10440),0.48575383,Pkg: fix parsing versions by @Borda in https://github.com/Lightning-AI/lightning/pull/15401,,0
[CLI] Add unit test with a model that has a parameter with lazy_instance default. (#11509),0.5159345,Users who relied ontrainer.test(ckpt_path=None)to load the latest model need to change their code totrainer.test(model)` and pass the model reference directly.,,0
Update tests/callbacks/*.py to use devices instead of gpus or ipus (#11387),0.5843208,"Deprecated log_gpu_memory, gpu_metrics, and util funcs in favor of DeviceStatsMonitor callback (#9921)",Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,0
Move the lightning_optimizers ownership to the Strategy (#11444),0.6990529,- Moved ownership of the lightning optimizers from the `Trainer` to the `Strategy` ([#11444](https://github.com/PyTorchLightning/pytorch-lightning/pull/11444)),,0
Disable attaching samplers when using IterableDataset (#11507),0.83462346,Disabled sampler replacement when using IterableDataset (#11507),,1
Update profiler doc (2/n) (#11430),0.68068093,Moved profilers to their own file (#7822),Co-authored-by: Aki Nitta nitta@akihironitta.com,0
Update community section (#11510),0.4127638,"Updated DeepSpeed ZeRO (#6546, #6752, #6142, #6321)",,0
Address black version conflict (#11505),0.42175215,Parsed local package versions (#13933),,0
Change the default prog_bar=False to True in LightningModule.log_grad_norm (#11472),0.8616984,- Set the `prog_bar` flag to False in `LightningModule.log_grad_norm` ([#11472](https://github.com/PyTorchLightning/pytorch-lightning/pull/11472)), Reset on_step flag to True in log_grad_norm updated change log  Co-authored-by: Aki Nitta nitta@akihironitta.com,1
Update incorrect entries in changelog (#11501),0.47042975,Full Changelog,,0
Simplify data fetching (#11466),0.42770058,Improved string conversion for ResultCollection (#8622),,0
Update tests/core/*.py to use devices instead of gpus or ipus (#11433),0.55665284,"The new devices argument is now agnostic to all accelerators, but the previous arguments gpus, tpu_cores, ipus are still available and work the same as before. In addition, it is now also possible to set devices=""auto"" or accelerator=""auto"" to select the best accelerator available on the hardware.", update tests for v2 Update Pass devices to kwargs add accelerator to kwargs Fix testing with cpu on GPU env  Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: Aki Nitta nitta@akihironitta.com,0
update load_from_checkpoint docstrings (#11467),0.7332999,Refactor load in checkpoint connector (#4593),,1
Update speed docs (#11044),0.6026188,"Updated DeepSpeed ZeRO (#6546, #6752, #6142, #6321)",Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Aki Nitta nitta@akihironitta.com,0
update tests for v2 (#11487),0.63817763,Updated app testing (#16000),,0
update tests for v2 (#11486),0.62320155,Updated app testing (#16000),,0
update tests for v2 (#11485),0.6312433,Updated app testing (#16000),,0
update tutorials (#11402),0.45448387,training forward refactor (#3134),,0
Set Loop.restarting recursively (#11442),0.7053719,Set Loop.restarting=False at the end of the first iteration (#8362), Set Loop.restarting recursively Docs CHANGELOG Update pytorch_lightning/loops/epoch/training_epoch_loop.py Co-authored-by: Aki Nitta nitta@akihironitta.com,1
Update introduction docs (#11140),0.63041687,Docs improvements,,0
Upgrade Ubuntu version from 18.04 to 20.04 (#11395),0.29270867,Setup: added requirement freeze for the next major version (#14480),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
Fix compatibility with old checkpoints and fault-tolerance enabled (#11439),0.7980515,all the checkpoint issues should be gone now (including backward support for old checkpoints),,1
Update tests/utilities/*.py to use devices instead of gpus or ipus (#11458),0.5668993,  * Deprecated the `pytorch_lightning.utilities.device_parser.num_cuda_devices` in favor of `lightning_lite.accelerators.cuda.num_cuda_devices`,,0
Update utilities API references (#11450),0.53227973,This release has breaking API changes. See #124 for all details. ,,0
Add typing to accelerators/gpu.py (#11333),0.70807636,"    accelerator=""gpu"", ",,1
Update training tricks docs (#11169),0.6259302,Deprecated the Trainer.train_loop property in favor of Trainer.fit_loop (#8025),,0
Refine debugging doc (#11390),0.5807942,DDP Debugging Improvements,,0
Refine remote fs doc (#11393),0.40315872,Using gfile to support remote directories (#2164),,0
Avoid in-place ops during logging result updates (#11401),0.6010737,Refactored logging,Co-authored-by: rohitgr7 rohitgr1998@gmail.com,0
move profiler docs (#11431),0.7554351,Moved profilers to their own file (#7822),,1
Update tests/checkpointing/*.py to use devices instead of gpus or ipus (#11408),0.59988666,Deprecated the use of CheckpointConnector.hpc_load() in favor of CheckpointConnector.restore() (#7652),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Don't use old testing packages in CI (#11366),0.48782724,The point of this release is more bug fixes ahead of v 1.0.0. We now have CI tests on TPU thanks to @zcain117 from Google! :slightly_smiling_face:,,0
Fix inconsistent exceptions raised with no rich installed (#11360),0.6670276,Improved exception message if rich version is less than 10.2.2 (#10839),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
Add LightningModule.lr_scheduler_step (#10249),0.90820575,LightningModule.lr_scheduler_step,Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
Fix torch.distributed._* import statements in tests (#11416),0.6984766,import torch,Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
Add example of getting DataLoader from within LightningModule (#10903),0.67977065,"In previous versions, Lightning required that the DataLoader instance set its input arguments as instance attributes. This meant that custom DataLoaders also had this hidden requirement. In this release, we do this automatically for the user, easing the passing of custom loaders:",Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Roger Shieh sh.rog@protonmail.ch Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Add typing to TQDMProgressBar (#11369),0.64877254,Deprecated ProgressBar callback in favor of TQDMProgressBar (#10134),,0
Update BECOMING_A_CORE_CONTRIBUTOR.md (#11337),0.49320334,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com,0
Add DETAIL logs for batch use cases (#11008),0.5987689,- Add new `DETAIL` log level to provide useful logs for improving monitoring and debugging of batch jobs ([#11008](https://github.com/PyTorchLightning/pytorch-lightning/pull/11008)),,0
Update evaluation docs (#11173),0.49753833,Docs improvements,Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Update child modules docs (#11198),0.5075978,Renamed utils modules (#5199),,0
pin sphinx-autodoc-typehints version to v1.15 (#11400),0.92215735,Pinned sphinx-autodoc-typehints with <v1.15 (#11400),,1
Merge pull request #11388 from PyTorchLightning/ci/mergify-team,0.6060666,Full commit list: https://github.com/PyTorchLightning/pytorch-lightning/compare/1.7.0...1.8.0,Update mergify team name,0
Update test_pruning.py to use devices instead of gpus or ipus (#11341),0.54798985,"Parsing of the gpus Trainer argument has changed: gpus=""n"" (str) no longer selects the GPU index n and instead selects the first n devices (#8770)",,0
Update test_pruning.py to use devices instead of gpus or ipus (#11339),0.5500349,"Parsing of the gpus Trainer argument has changed: gpus=""n"" (str) no longer selects the GPU index n and instead selects the first n devices (#8770)",,0
Update test_gpu_stats_monitor.py to use devices instead of gpus or ipus (#11340),0.65506405,"Deprecated log_gpu_memory, gpu_metrics, and util funcs in favor of DeviceStatsMonitor callback (#9921)",,0
Update API references in doc (#11357),0.5698365,Removed deprecated API (#2073),,0
Skip testing with PyTorch 1.7 and Python 3.9 on Ubuntu (#11217),0.95395166,Skipped testing with PyTorch 1.7 and Python 3.9 on Ubuntu (#11217),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Move Conversational AI (Nemo) to Examples section (#11344),0.37769926,- The Trainer no longer accepts positional arguments to ([#17022](https://github.com/Lightning-AI/lightning/pull/17022)),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
"Move epoch_{start,end} hooks from TrainingEpochLoop to FitLoop (#11201)",0.77871203,- The epoch start/end hooks are now called by the `FitLoop` instead of the `TrainingEpochLoop` ([#11201](https://github.com/PyTorchLightning/pytorch-lightning/pull/11201)),,1
Move newly added Trainer methods to be with other methods (#11335),0.6995461,Allow easy trainer re-instantiation (#7508),,0
Add Accelerators section to Lightning docs (#10755),0.7398213,We've made it easier for users to try out new accelerators by enabling support for registering custom Accelerator classes in Lightning.,,1
Integrate progress tracking into the progress bar (#11213),0.7850324,progress bar,,1
Fix typing in pl.callbacks.xla_stats_monitor (#11219),0.49480498,  * Deprecated the internal `XLADeviceUtils.xla_available` staticmethod,Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
Changed hook doctstring (#11345),0.5134304,moved ___step_end hooks (#3130),,0
Fix restoring lr scheduler states with deepspeed strategy (#11322),0.73076177,- Fixed the lr-scheduler state not being dumped to checkpoint when using the deepspeed strategy ([#11307](https://github.com/PyTorchLightning/pytorch-lightning/pull/11307)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: thomas chaton thomas@grid.ai,1
Extend the deprecation of Trainer(resume_from_checkpoint) (#11334),0.8170293,Deprecated checkpoint_callback from the Trainer constructor in favor of enable_checkpointing (#9754),,1
"Remove the ""Verify linked issue"" bot (#11338)",0.48931247,Silenced some warnings. verified ddp refactors (#3483),,0
Update changelog after 1.5.8 release (#11336),0.60813767,Full Changelog,,0
Rename _distrib_type to _strategy_type (#11328),0.47136933,"All TrainingTypePlugins have been renamed to Strategy (#11120). Strategy is a more appropriate name because it encompasses more than simply training communcation. This change is now aligned with the changes we implemented in 1.5, which introduced the new strategy and devices flags to the Trainer.",Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Fix frozen dataclass instance error in apply_to_collection (#10927),0.55153745,- Added support for dataclasses in `apply_to_collections` ([#11889](https://github.com/PyTorchLightning/pytorch-lightning/pull/11889)),Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
Add LightningLite to README (#11164),0.6871635,from lightning.lite import LightningLite,,0
Deprecate TrainerDataLoadingMixin and move logic to DataConnector (#11282),0.71937406,Removed the deprecated TrainerLoggingMixin class (#8609),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Aki Nitta nitta@akihironitta.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Update LightningCLI(trainer_defaults=...) doc (#11309),0.75725937,The trainer.lightning_module reference is now properly set at the very beginning of a run (#8536),Co-authored-by: Mauricio Villegas mauricio_ville@yahoo.com,1
Parametrize deepspeed hook test (#11308),0.55286956,Do not fail if batch size could not be inferred for logging when using DeepSpeed (#10438),,0
Update definition of optimizer in introduction_guide.rst (#10822),0.6595614,Refactored optimizer (#4658),Co-authored-by: rohitgr7 rohitgr1998@gmail.com,0
Update optimizer configuration info message in DeepSpeedStrategy (#11327),0.6472894,"Implemented DeepSpeedPlugin._setup_model_and_optimizers (#10009, #10064)",,0
Update optimization docs (#11174),0.66126174,Refactored optimizer (#4658),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: thomas chaton thomas@grid.ai,0
Add typing to some utility files (#11316),0.58840007,Refactored setup for typing friendly (#6590),,0
Fix exception message for FSDP running on CPU (#11325),0.6272142,Native FSDP replaces Fairscale FSDP (#16400),,0
Update precision docs (#11010),0.71370536,Precision Plugins (#5718),Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,1
Modify LSFEnvironment to use more reliable environment variable (#10825),0.490064,Changed LSFEnvironment to use LSB_DJOB_RANKFILE environment variable instead of LSB_HOSTS for determining node rank and main address (#10825),Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Introduce StrategyRegistry (#11233),0.5356997,"All TrainingTypePlugins have been renamed to Strategy (#11120). Strategy is a more appropriate name because it encompasses more than simply training communcation. This change is now aligned with the changes we implemented in 1.5, which introduced the new strategy and devices flags to the Trainer.",Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Rename ttp -> strategy (#11312),0.56885034,"All TrainingTypePlugins have been renamed to Strategy (#11120). Strategy is a more appropriate name because it encompasses more than simply training communcation. This change is now aligned with the changes we implemented in 1.5, which introduced the new strategy and devices flags to the Trainer.",,0
Fix min/max logging default value (#11310),0.5463731,Change default logger to a dedicated one (#1064),,0
Update strategy registry docs (#11311),0.47159043,"All TrainingTypePlugins have been renamed to Strategy (#11120). Strategy is a more appropriate name because it encompasses more than simply training communcation. This change is now aligned with the changes we implemented in 1.5, which introduced the new strategy and devices flags to the Trainer.",,0
Fix lr scheduler state not being dumped to checkpoint in deepspeed strategy (#11307),0.7779646,- Fixed the lr-scheduler state not being dumped to checkpoint when using the deepspeed strategy ([#11307](https://github.com/PyTorchLightning/pytorch-lightning/pull/11307)),,1
Raise a warning if evaulation is triggered with best ckpt in case of multiple checkpoint callbacks (#11274),0.60868835,Saved checkpoints will no longer use the type of a Callback as the key to avoid issues with unpickling (#6886),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Rename training plugin test files & names to strategy (#11303),0.5537331,"All TrainingTypePlugins have been renamed to Strategy (#11120). Strategy is a more appropriate name because it encompasses more than simply training communcation. This change is now aligned with the changes we implemented in 1.5, which introduced the new strategy and devices flags to the Trainer.",,0
Add typing for utilities/enums.py (#11298),0.5747614,Deprecates the pytorch_lightning.utilities.enums.AMPType enum,,0
Reset the total fit-validation batch progress on epoch (#11244),0.7049576,Reset epoch progress with batch size scaler (#13846),,1
"Remove profile(""training_step_and_backward"") (#11222)",0.67961717,Changed default value of the max_steps Trainer argument from None to -1 (#9460),,0
Update logic to make sure logged_metrics always contain tensors (#11270),0.72860515,- Show a better error message when a Metric that does not return a Tensor is logged ([#13164](https://github.com/Lightning-AI/lightning/pull/13164)),,1
Add opt_idx to scheduler config if not assigned by user (#11247),0.53028315,Disabled lr_scheduler.step() in manual optimization  (#6825),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Avoid non-blocking GPU->CPU copies. (#11288),0.65474397,Enable non-blocking for device transfers to GPU (#1843),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
[pre-commit.ci] pre-commit suggestions (#11301),0.59836364,@akihironitta @awaelchli @Borda @carmocca @kaushikb11 @krshrimali @mauvilsa @otaj @pre-commit-ci @rohitgr7 @semaphore-egg @tkonopka @wayi1,,0
Update strategy import statements (#11238),0.5088538,Renamed Strategy.reduce to Strategy.all_reduce in all strategies (#16370),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Merge pull request #11228 from PyTorchLightning/docs/strategies-code-owners,0.6032322,- Moved ownership of the lightning optimizers from the `Trainer` to the `Strategy` ([#11444](https://github.com/PyTorchLightning/pytorch-lightning/pull/11444)),Update CODEOWNERS for pl/strategies,0
Group metrics generated by DeviceStatsMonitor for better visualization (#11254),0.6919931,device_stats = DeviceStatsMonitor(),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Fix data fetcher selection (#11294),0.4934777,"Accessing dataloaders (#16726, #16800)",,0
Remove Strategy.optimizer_zero_grad (#11246),0.79254514,"  * Removed `optimizer_idx` argument from `Strategy.{optimizer_step,backward}` and all of its overrides in subclasses",,1
Add deprecation path for renamed training type plugins (#11227),0.5966818,- Removed all deprecated training type plugins ([#14011](https://github.com/Lightning-AI/lightning/pull/14011)),Co-authored-by: Kaushik B kaushikbokka@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Remove hpc_save (#11101),0.6010475,Used checkpoint_connector.hpc_save in SLURM (#4217),,0
"Revert ""[CI] Comment flaky tests (#10084)"" (#10580)",0.4378219,"Removed experimental fault-tolerance support (#16516, #16533)"," Revert ""[CI] Comment flaky tests (#10084)""  This reverts commit ed9802643c5485a0f07d8376009410ae76076cc4.",0
Fix KFold example (#11230),0.3509555,Renamed ModelCheckpoint's attributes best to best_model_score and kth_best_model to kth_best_model_path (#1799),,0
Fix _should_reload_dl_epoch causing inconsistent validation dataloader reloading (#11036),0.6163483,- Do not update on-plateau schedulers when reloading from an end-of-epoch checkpoint ([#14702](https://github.com/Lightning-AI/lightning/pull/14702)),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Remove Strategy.init_optimizers (#11236),0.7631846,"  * Removed `optimizer_idx` argument from `Strategy.{optimizer_step,backward}` and all of its overrides in subclasses",,1
refactor _configure_schedulers (#11245),0.59201485,Disabled lr_scheduler.step() in manual optimization  (#6825),,0
Fix CLI race condition saving the config (#11199),0.539394,  * `save_config_overwrite`,,0
Reset the progress tracking state after sanity checking (#11218),0.6925427,Do not reset the progress tracking dataclasses total counters (#8475),,0
Rename training_type_plugin file to strategy (#11239),0.70094657,"All TrainingTypePlugins have been renamed to Strategy (#11120). Strategy is a more appropriate name because it encompasses more than simply training communcation. This change is now aligned with the changes we implemented in 1.5, which introduced the new strategy and devices flags to the Trainer.",Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Update strategy import statements (#11231),0.50335944,Renamed Strategy.reduce to Strategy.all_reduce in all strategies (#16370),,0
Fix docs to fix #11081 (#11229),0.5255692,Release LAI docs as stable (#14250),,0
Deprecate TrainerOptimizersMixin and move functionality to core/optimizer.py (#11155),0.80383056,- Deprecated `TrainerOptimizersMixin` and moved functionality to `core/optimizer.py`([#11155](https://github.com/PyTorchLightning/pytorch-lightning/pull/11155)),,1
Rename AcceleratorConnector.training_type_plugin to AcceleratorConnector.strategy (#11212),0.74715686,- The accelerator and training type plugin setup hooks no longer have a model argument.,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Return only unique names/versions for LoggerCollection (#10976),0.6631336,- `LoggerCollection` returns only unique logger names and versions ([#10976](https://github.com/PyTorchLightning/pytorch-lightning/pull/10976)),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
Introduce strategies directory for Training Strategies (#11226),0.6075333,"All TrainingTypePlugins have been renamed to Strategy (#11120). Strategy is a more appropriate name because it encompasses more than simply training communcation. This change is now aligned with the changes we implemented in 1.5, which introduced the new strategy and devices flags to the Trainer.",Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Update pre-commit hook versions (#11202),0.5159749,Set version as today (#13906),,0
Include Lezwon in alumni (#11223),0.37061644,"If we forgot someone or have any suggestion, let us know in Slack :zap:",,0
Remove explicit isinstance checks in strategies for checkpoint io (#11177),0.5950261,Refactored CheckpointConnector to offload validation logic to the CheckpointIO plugin (#9045),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Fix typing in pl.callbacks.lr_monitor (#10802),0.56528974,"trainer = pl.Trainer(callbacks=DeviceStatsMonitor(cpu_stats=False), accelerator=""gpu"")",Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
Fix BF16 teardown for TPU precision plugin (#10990),0.6101769,Renamed TPUHalfPrecisionPlugin to TPUBf16PrecisionPlugin (#10026),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: thomas chaton thomas@grid.ai,0
Update changelog after 1.5.7 release (#11204),0.6333725,Full Changelog,Co-authored-by: rohitgr7 rohitgr1998@gmail.com,0
Deprecate Trainer.training_type_plugin in favor of trainer.strategy (#11141),0.8072332,- Removed the deprecated `Trainer.training_type_plugin` property in favor of `Trainer.strategy` ([#14011](https://github.com/Lightning-AI/lightning/pull/14011)),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Rename ParallelPlugin to ParallelStrategy (#11123),0.7057226,    * Renamed the `ParallelPlugin` to `ParallelStrategy` ([#11123](https://github.com/PyTorchLightning/pytorch-lightning/pull/11123)),,1
Rename the DDPSpawnShardedPlugin to DDPSpawnShardeedStrategy (#11210),0.8521212,Merged the implementation of DDPSpawnStrategy into DDPStrategy and removed DDPSpawnStrategy (#14952),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Rename SingleDevicePlugin to SingleDeviceStrategy (#11181),0.5814082,    * Renamed the `SingleDevicePlugin` to `SingleDeviceStrategy` ([#11182](https://github.com/PyTorchLightning/pytorch-lightning/pull/11182)),Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Renamed the DDPSpawnPlugin to DDPSpawnStrategy (#11145),0.85474396,Merged the implementation of DDPSpawnStrategy into DDPStrategy and removed DDPSpawnStrategy (#14952),,1
Rename the DataParallelPlugin to DataParallelStrategy (#11183),0.65675986,Implemented DataParallelPlugin._setup_model (#10010),,0
Rename SingleTPUPlugin to SingleTPUStrategy (#11182),0.5981662,    * Renamed the `SingleTPUPlugin` to `SingleTPUStrategy` ([#11182](https://github.com/PyTorchLightning/pytorch-lightning/pull/11182)),,0
Renamed the DDP2Plugin to DDP2Strategy (#11185),0.7419462,Merged the implementation of DDPSpawnStrategy into DDPStrategy and removed DDPSpawnStrategy (#14952),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Fix master import conflict (#11203),0.5090627,Module imports are now catching ModuleNotFoundError instead of ImportError (#9867),,0
Deprecate TrainerCallbackHookMixin (#11148),0.7972689,Removed the deprecated TrainerLoggingMixin class (#8609),,1
Renamed DDPShardPlugin to DDPShardStrategy (#11187),0.68544143,Merged the implementation of DDPSpawnStrategy into DDPStrategy and removed DDPSpawnStrategy (#14952),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Remove should_rank_save_checkpoint property from TTP (#11070),0.72337556,Removed should_rank_save_checkpoint property from Trainer (#9433),,1
Drop Python 3.6 support (#11117),0.9497644,Drop Python 3.6 support,,1
Rename TPUSpawnPlugin to TPUSpawnStrategy (#11190),0.7286765,    * Renamed the `TPUSpawnPlugin` to `TPUSpawnStrategy` ([#11190](https://github.com/PyTorchLightning/pytorch-lightning/pull/11190)),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Move TrainerCallbackHookMixin.on_save/load_checkpoint  to Trainer and rename for clarity (#11179),0.75913197,Trainer now calls on_load_checkpoint() when resuming from a checkpoint (#1666),,1
Rename IPUPlugin to IPUStrategy (#11193),0.6473776,    * Renamed the `IPUPlugin` to `IPUStrategy` ([#11193](https://github.com/PyTorchLightning/pytorch-lightning/pull/11193)),,0
Rename DeepSpeedPlugin to DeepSpeedStrategy (#11194),0.7280602,"Implemented DeepSpeedPlugin._setup_model_and_optimizers (#10009, #10064)",,1
Suppress Warning in PredictionEpochLoop (#11189),0.63102746,"Marked several methods in PredictionLoop as protected: on_predict_start, on_predict_epoch_end, on_predict_end, on_predict_model_eval (#9516)",Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Rename HorovodPlugin to HorovodStrategy (#11195),0.5889197,    * Renamed the `HorovodPlugin` to `HorovodStrategy` ([#11195](https://github.com/PyTorchLightning/pytorch-lightning/pull/11195)),,0
Avoid torch amp cuda warning with bf16 on cpu (#11161),0.6346594,Moved torch.cuda.set_device() to enable collective calls earlier in setup (#8312),Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Rename DDPPlugin to DDPStrategy (#11142),0.679111,Merged the implementation of DDPSpawnStrategy into DDPStrategy and removed DDPSpawnStrategy (#14952),  Raname DDPPlugin to DDPStrategy   Change ddp_plugin to ddp_strategy   update changelog   rename occurences in docs   rename more occurrences   fix line too long   more fixes   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Move CheckpointConnector.fault_tolerant_auto_save_path out of CheckpointConnector.hpc_resume_path (#11092),0.80499417,Deprecated the use of CheckpointConnector.hpc_load() in favor of CheckpointConnector.restore() (#7652),,1
update optimizer_step example in docs (#10420),0.68712544,          optimizer.step(),,0
Delete legacy multinode tests (#11175),0.55115205,Updated Multinode Warning (#16091),,0
Rename restore_checkpoint_after_pre_dispatch to restore_checkpoint_after_setup (#11166),0.687948,Changed resume_from_checkpoint warning to an error when the checkpoint file does not exist (#7075),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Fix setter usage for checkpoint io and precision in TTP (#11071),0.5813499,set the checkpoint path with a setter,Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
3/n Simplify spawn plugins: Merge pre_dispatch and setup logic  (#11137),0.66458905,"    * The hooks/callbacks `prepare_data`, `setup`, `configure_sharded_model` and `teardown` now run under initialized process group for spawn-based plugins just like their non-spawn counterparts",,0
Set spawn_method on initialization (#11162),0.59131056,"    * All spawn-based plugins now spawn processes immediately upon calling `Trainer.{fit,validate,test,predict}`",,0
Rename DDPFullyShardedPlugin to DDPFullyShardedStrategy (#11143),0.6199175,    * Renamed the `DDPFullyShardedPlugin` to `DDPFullyShardedStrategy` ([#11143](https://github.com/PyTorchLightning/pytorch-lightning/pull/11143)),  Rename DDPFullyShardedPlugin to DDPFullyShardedStrategy   update fsdp_plugin to fsdp_strategy   update changelog   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Add required states for resumed ModelCheckpoint GC (#10995),0.7180431,Forced ModelCheckpoint callbacks to run after all others to guarantee all states are saved to the checkpoint (#5731),"  Add required states for resumed ModelCheckpoint GC   Add backwards compatibility with legacy cktps   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com  Add test to check if attrs are written to ckpt  Note that we do not yet check for proper loading/reinstantiation of ModelCheckpooint based on the ckpt written to disk   Test if attributes are restored properly from ckpt   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  Fix broken test_callbacks_state_fit_ckpt_path  ModelCheckpoint is configured to save after every epoch, but trainer.fit is called with max_steps = 1 Note there may be a better way of doing this, where ModelCheckpoint is called after training_step   Update test_restore.py   Update test_restore.py   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Check that all attributes are restored properly   revert changes, use fix on master   Convert to proper unit test   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Refactor test_mode_checkpoint_saveload_ckpt   First save, then load ckpt.  Instantiate ModelCheckpoint twice.  Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com",1
Include hook's object name when profiling (#11026),0.6048218,- Changed profiler to index and display the names of the hooks with a new pattern []. ([#11026](https://github.com/PyTorchLightning/pytorch-lightning/pull/11026)),,0
Rename the TrainingTypePlugin base to Strategy (#11120),0.8144127,"All TrainingTypePlugins have been renamed to Strategy (#11120). Strategy is a more appropriate name because it encompasses more than simply training communcation. This change is now aligned with the changes we implemented in 1.5, which introduced the new strategy and devices flags to the Trainer.",Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: four4fish 88516121+four4fish@users.noreply.github.com,1
Safely disable profiler (#11167),0.60302544,Moved profilers to their own file (#7822),,0
Fix evaluation logging on epoch end with multiple dataloaders (#11132),0.7006634,"- Fixed logging on `{test,validation}_epoch_end` with multiple dataloaders ([#11132](https://github.com/PyTorchLightning/pytorch-lightning/pull/11132))",,1
Prune EvalModelTemplate (#11153),0.51030946,Sanitize None params during pruning (#6836),,0
rename _call_ttp_hook to _call_strategy_hook (#11150),0.5063513,We removed some Callback hooks that were ambiguous to use Removed deprecated callback hooks (#14834):,,0
Refactor plugin tests whose assertions don't need to run in on_fit_start hook (#11149),0.5188691,- Full tests that run multiple models in different configs,,0
Add support for returning callback from LightningModule.configure_callbacks (#11060),0.77621084,- Added support for returning a single Callback from `LightningModule.configure_callbacks` without wrapping it into a list ([#11060](https://github.com/PyTorchLightning/pytorch-lightning/pull/11060)),,1
Fix tpu spawn plugin test (#11131),0.5960112,- Renamed `strategy='tpu_spawn'` to `strategy='xla'` and `strategy='tpu_spawn_debug'` to `strategy='xla_debug'` ([#16781](https://github.com/Lightning-AI/lightning/pull/16781)),,0
Fixed NeptuneLogger when using DDP (#11030),0.7717008,Enable NeptuneLogger to work with distributed_backend=ddp (#1753),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
Fix CVE-2020-1747 and CVE-2020-14343 (#11099),0.54702204,- Fixed security vulnerabilities CVE-2020-1747 and CVE-2020-14343 caused by the `PyYAML` dependency ([#11099](https://github.com/PyTorchLightning/pytorch-lightning/pull/11099)),,0
Mark all result classes as protected (#11130),0.5086881,  * The fetching classes are now marked as protected ([#16664](https://github.com/Lightning-AI/lightning/pull/16664)),,0
Enable logging hparams only if there are any (#11105),0.7434637,Allow logging of metrics together with hparams (#1630),,1
Deprecate Trainer.verbose_evaluate (#10931),0.78856707,Deprecated trainer.running_sanity_check in favor of trainer.sanity_checking (#4945),,1
Reset the current progress tracking state during double evaluation (#11119),0.6383074,Do not reset the progress tracking dataclasses total counters (#8475),,0
Prune EvalModelTemplate (3/n) (#10971),0.5033511,Sanitize None params during pruning (#6836),,0
Fix AttributeError when using CombinedLoader in prediction (#11111),0.63601416,- Move `Strategy.process_dataloader` function call from `fit/evaluation/predict_loop.py` to `data_connector.py` ([#12251](https://github.com/PyTorchLightning/pytorch-lightning/pull/12251)),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
remove redundant methods in RichProgressBar (#11100),0.59528625,New Rich Progress Bar,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Save the loop progress state by default (#10784),0.77523875,Save the loops state with the checkpoint (opt-in) (#8362),,1
Fix typing for utilities.warnings (#11115),0.58936113,warning_utils >> warnings,,0
Fix signal teardown outside main thread (#11124),0.5080463,Removed teardown from ParallelPlugin (#8943),,0
Track the evaluation loop outputs in the loop (#10928),0.59587777,final inner eval loop hooks (#3154),,0
Mark Trainer.run_stage as protected (#11000),0.67111695,Made evaluate method private >> Trainer._evaluate(...). (#1260),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Remove partitioning of model in ZeRO 3 (#10655),0.4756415,Refactor Model backward (#2276),,0
Add typing for trainer.logger (#11114),0.752599,print(trainer.logger),,1
Avoid the deprecated onnx.export(example_outputs=...) in torch 1.10 (#11116),1.0000002,Avoid the deprecated onnx.export(example_outputs=...) in torch 1.10 (#11116),,1
Merge pull request #11046 from PyTorchLightning/docs/security,0.59431034,"- Fixed security vulnerability ""CWE-94: Improper Control of Generation of Code (Code Injection)"" ([#12212](https://github.com/PyTorchLightning/pytorch-lightning/pull/12212))",Add security contact,0
Remove leftover clean_logger call in tests (#11080),0.74872875,Cleaning up stale logger tests (#3490),,1
Initialize ModelCheckpoint state as early as possible (#11108),0.7491385,Forced ModelCheckpoint callbacks to run after all others to guarantee all states are saved to the checkpoint (#5731),,1
Remove obsolete pre_dispatch in DDPSpawnShardedPlugin (#10988),0.793347,Merged the implementation of DDPSpawnStrategy into DDPStrategy and removed DDPSpawnStrategy (#14952),,1
Teardown sync-batchnorm after training (#11078),0.64865184,Automatically set sync_batchnorm for training_type_plugin (#6536),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Add Loop.replace (#10324),0.49376848,Set Loop.restarting=False at the end of the first iteration (#8362),Co-authored-by: tchaton thomas@grid.ai Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
Remove redundant special case for disabling the progress bar on TPU (#11061),0.50088733,Changed progress_bar_refresh_rate trainer flag to disable progress bar when setting to 0. (#1108),,0
Update mypy (#11096),0.41205472,python script.py \,,0
Improve checkpoint docs (#10916),0.675396,all the checkpoint issues should be gone now (including backward support for old checkpoints),,0
Update changelog after 1.5.6 release (#11094),0.6223896,Full Changelog,,0
Fix intellisense for LightningCLI (#11075),0.6870728,- Removed the deprecated `LightningCLI` arguments ([#16380](https://github.com/Lightning-AI/lightning/pull/16380)),,0
3/n Move accelerator into Strategy (#11022),0.6995744,move specific accelerator code (#3457),"  remove training_step() from accelerator   remove test, val, predict step   move   wip   accelerator references   cpu training   rename occurrences in tests   update tests   pull from adrian's commit   fix changelog merge pro   fix accelerator_connector and other updates   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   fix doc build and some mypy   fix lite   fix gpu setup environment   support customized ttp and accelerator   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   fix tpu error check   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   fix precision_plugin initialization to recognisze cusomized plugin   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Update bug_report_model.py   Update accelerator_connector.py   update changelog   allow shorthand typing references to pl.Accelerator   rename helper method and add docstring   fix typing   Update pytorch_lightning/trainer/connectors/accelerator_connector.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update tests/accelerators/test_accelerator_connector.py  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update tests/accelerators/test_cpu.py  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci   fix pre commit complaint   update typing to long ugly path   spacing in flow diagram   remove todo comments   docformatter   Update pytorch_lightning/plugins/training_type/training_type_plugin.py   revert test changes   improve custom plugin examples   remove redundant call to ttp attribute   it is no longer a property  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci  Apply suggestions from code review  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com",0
Support torch 1.10.1 (#11095),0.71451104,- Fixed an issue with comparing torch versions when using a version of torch built from source ([#17030](https://github.com/Lightning-AI/lightning/pull/17030)),,1
Deprecate Trainer.should_rank_save_checkpoint property (#11068),0.9106087,Removed should_rank_save_checkpoint property from Trainer (#9433),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Update the TQDM progress bar on_train_epoch_end (#11069),0.84345454,The TQDM progress bar now correctly shows the on_epoch logged values on train epoch end (#11069),,1
Standardize model attribute access in training type plugins (#11072),0.52109003,Allowed training type plugin to delay optimizer creation (#6331),,0
make RichProgressBar more flexible with Rich.Console (#10875),0.69613373,New Rich Progress Bar,Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: thomas chaton thomas@grid.ai,0
Fix sanity check for RichProgressBar (#10913),0.6856024,New Rich Progress Bar,,0
Remove dead check in ModelCheckpoint (#10930),0.73992956,"Removed deprecated ModelCheckpoint arguments prefix, mode=""auto"" (#6162)",,1
Simplify several profile calls (#11031),0.4387188,Changed Simple Profiler report to order by percentage time spent + num calls (#4880),,0
Fix support for logging within callbacks returned from LightningModule (#10991),0.810755,The coverage of self.log-ing in any LightningModule or Callback hook has been improved (#8498),Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
Added variable interpolation and troubleshooting to LightningCLI doc (#11009),0.555158,LightningCLI V2,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
Fix the num_batches value in warning (#10980),0.73296964,"    batch_size=32,",,1
Clean up last ModelCheckpoint makedirs call to IOPlugin (#11035),0.5933374,Deprecated filepath in ModelCheckpoint (#4213),,0
[DeepSpeed] fix flag forwarding in DeepSpeedPlugin (#10899),0.6257931,"Implemented DeepSpeedPlugin._setup_model_and_optimizers (#10009, #10064)",Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Sean Naren sean@grid.ai Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Add missing self to setup hook example (#11041),0.5265711,Updated hooks arguments - breaking for setup and teardown (#2850),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com,0
Update data docs (#11042),0.47883508,Docs improvements,Co-authored-by: Aki Nitta nitta@akihironitta.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Fix support for CombinedLoader while checking for warning raised with eval dataloaders (#10994),0.6884114,Removed deprecation warnings being called for on_{task}_dataloader (#9279),,0
Convert warning message to debug-level info in spawn plugins (#10864),0.5621789,"    * The hooks/callbacks `prepare_data`, `setup`, `configure_sharded_model` and `teardown` now run under initialized process group for spawn-based plugins just like their non-spawn counterparts",Co-authored-by: four4fish 88516121+four4fish@users.noreply.github.com,0
Removed duplicated file extension when uploading model checkpoints with NeptuneLogger (#11015),0.82461905,- Removed duplicated file extension when uploading model checkpoints with `NeptuneLogger` ([#11015](https://github.com/PyTorchLightning/pytorch-lightning/pull/11015)),,1
move optional section (#11011),0.4572062,move lr_finder (#3434),,0
2/n Simplify spawn plugins: Spawn immediately (#10896),0.6146455,Skip broadcast if distributed not initialized for the spawn plugins (#8017),,0
Remove _call_accelerator_hook Trainer method (#10999),0.7454122,Deprecated Trainer attribute accelerator_backend in favor of accelerator (#6034),,1
Deprecate on_hpc_{save/load} hooks (#10911),0.6398809,Fixing critical bugs in newly added hooks and hparams assignment.,"  first commit   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update pr #   test filterwarnings   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   add a todo comment   updates   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  `` Update pytorch_lightning/core/saving.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  `` Update pytorch_lightning/core/saving.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci  model --> LightningModule Update pytorch_lightning/core/saving.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  model --> LightningModule Update pytorch_lightning/core/saving.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com",0
"Remove start_{training,evaluating,predicting} from HorovodPlugin  (#10989)",0.59225047,- Removed `Trainer(strategy='horovod')` support ([#16150](https://github.com/Lightning-AI/lightning/pull/16150)),,0
Deprecate callback hooks on_init_start and on_init_end (#10940),0.71182597,Removed deprecated early_stop_callback (#3982),,1
Deprecate call_hook (#10979),0.7305789,Removed deprecated callbacks (#3979),,1
Update Changelog after 1.5.5 release (#10977),0.6250217,Full Changelog,,0
Prune EvalModelTemplate (1/n) (#10969),0.5172187,Sanitize None params during pruning (#6836),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
remove EvalModelTemplate from tests (#10970),0.53508806,Removed no return warning from val/test step (#6139),,0
Follow-up changes to #10575 (#10957),0.5258974,Noteworthy changes:,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Sharing Datasets Across Process Boundaries (#10951),0.4953876,It is now possible to resolve those limitations by enabling manual fault tolerance where you can write your own logic and specify how exactly to checkpoint your own datasets and samplers. You can do so using this environment flag:,,0
Fix typing in pl.plugins.environments (#10943),0.48496222,Refactored setup for typing friendly (#6590),,0
Remove TrainingTypePlugin.post_dispatch in favor of teardown (#10939),0.73853624,Changed teardown() in Accelerator to allow training_type_plugin to customize teardown logic (#7579),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,1
fix TypeError cause failure in singal_connector teardown (#10961),0.5391399,"Changed setup() and teardown() stage argument to take any of {fit,validate,test,predict} (#6386)",,0
4/n Move Accelerator into strategy - remove X_step() from accelerator (#10890),0.6332447,move specific accelerator code (#3457),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Change temporary spawn checkpoint name (#10934),0.53339845,Call reset_on_restart in the loop's reset hook instead of when loading a checkpoint (#9561),,0
Add Trainer.state to spawn queue #10937,0.6570866,"trainer = Trainer(strategy=""ddp_spawn"", accelerator=""gpu"", devices=2)",,0
Fix spawn plugins not deleting temp checkpoint (#10935),0.59011847,Call reset_on_restart in the loop's reset hook instead of when loading a checkpoint (#9561),,0
Move implementation of LightningModule.add_to_queue/get_from_queue (#10936),0.7812273,- Removed the deprecated `LightningModule.add_to_queue` and `LightningModule.get_from_queue` method ([#13600](https://github.com/Lightning-AI/lightning/pull/13600)),,1
Fix typing in pl.trainer.config_validator (#10803),0.7629956,Deprecated the Trainer.disable_validation property in favor of not Trainer.enable_validation (#8291),,1
2/n Move Accelerator into strategy - remove dispatch functions from Accelerator (#10885),0.66972554,reduced accelerator selection (#3211),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
added UserWarnings if max_epochs not set in the Trainer class (#10700),0.7857059,- Added a warning that shows when `max_epochs` in the `Trainer` is not set ([#10700](https://github.com/PyTorchLightning/pytorch-lightning/pull/10700)),,1
Fix some missing code in step-by-step walk through (#10519),0.5699762,Refactored training loop (#2336),Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Don't import torch_xla.debug for torch-xla<1.8 (#10836),0.6535088,import torch,,0
triger ci only with pull request (#10932),0.46553874,The point of this release is more bug fixes ahead of v 1.0.0. We now have CI tests on TPU thanks to @zcain117 from Google! :slightly_smiling_face:,,0
Re-design call_hook interface (#10575),0.61961985,Support for user-defined callbacks (#889 and #950),,0
Sort out the dataloader idx logic for evaluation (#10923),0.6909839,"Working with multiple dataloaders (#16800, #16753)",Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Resolve: 'DummyExperiment' object does not support item assignment (#10917),0.3978442,"Deprecated DataModule properties: has_prepared_data, has_setup_fit, has_setup_validate, has_setup_test, has_setup_predict, has_teardown_fit, has_teardown_validate, has_teardown_test, has_teardown_predict (#7657)",Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Unroll dict input before call Accelerator X_steps (#10908),0.60514176,"def serve_step(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:",Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Simplify some ddp-spawn tests #10921,0.73964614,"decoupled DDP, DDP spawn (#3733, #3766, #3767, #3774, #3802, #3806)",,1
Fix filtration logic for eval results with multiple dataloaders (#10810),0.607349,"Working with multiple dataloaders (#16800, #16753)",Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
Remove setup_optimizers_in_pre_dispatch logic (#10906),0.6373485,Executing the optimizer_closure is now required when overriding the optimizer_step hook (#9360),,0
Fix prepare_data implementation in BoringDataModule (#10915),0.540002,move prepare_data to data connector (#3307),,0
Disable eval dataloaders replacement during overfitting (#10877),0.5960827,Enforced eval shuffle warning only for default samplers in DataLoader (#12653),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Fix mathjax in RTD build (#10889),0.35045922,Progress bar metrics tensors are now converted to float (#5692), Update docs.txt Update conf.py Update .readthedocs.yml,0
Fix retrieval of batch indices when dataloader num_workers > 0 (#10870),0.6589067,Reset the dataloaders on OOM failure in batch size finder to use the last successful batch size (#14372),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
1/n Simplify spawn plugins: Simplify handling of multiprocessing queue (#10034),0.6674131,"- Redesigned process creation for spawn-based plugins (`DDPSpawnPlugin`, `TPUSpawnPlugin`, etc.) ([#10896](https://github.com/PyTorchLightning/pytorch-lightning/pull/10896))",Co-authored-by: thomas chaton thomas@grid.ai,0
Fix deadlinks in docs (#10739),0.4629202,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk,0
Fix schedule reset logic in pytorch profiler (#10837),0.8688523,"Resolved schedule step bug for PyTorch Profiler (#6674, #6681)",,1
Removed unnecessary _move_optimizer_state method overrides (#10849),0.69518363,"  * Removed `optimizer_idx` argument from `Strategy.{optimizer_step,backward}` and all of its overrides in subclasses","  Update tpu tp share same logic with ttp   run test   Update tpu_spawn.py   debug   Add changelog   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   Update training_type_plugin.py   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  Update training_type_plugin.py  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
3/n Move Accelerator into strategy - remove model_sharded_context() (#10886),0.68436605,Removed call_configure_sharded_model_hook property from Accelerator and TrainingTypePlugin (#9612),  3/n Move Accelerator into strategy - remove model_sharded_context()   update ttp function   update changelog   update changelog   Co-authored-by: ananthsub ananth.subramaniam@gmail.com,0
Remove precision_plugin pre_dispatch() method (#10887),0.6989665,Precision Plugins (#5718),  Remove precision_plugin pre_dispatch() method   update changelog ,0
Add separate CI job for slow tests (#10830),0.46672204,test_percent_check in favour of limit_test_batches,,0
Remove redundant None check from spawn plugins (#10855),0.6247338,Skip broadcast if distributed not initialized for the spawn plugins (#8017),Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Add missing deprecation rerouting for add_to_queue in TPUSpawnPlugin #10854,0.68468696,"Removed process_idx from the {DDPSpawnPlugin,TPUSpawnPlugin}.new_process methods (#10022)",,0
[CLI] Add support for ReduceLROnPlateau (#10860),0.7027462,We have also added support for the ReduceLROnPlateau scheduler with shorthand notation:,,1
Fixed uploading best model checkpoint in NeptuneLogger (#10369),0.7412662,- Removed duplicated file extension when uploading model checkpoints with `NeptuneLogger` ([#11015](https://github.com/PyTorchLightning/pytorch-lightning/pull/11015)),,1
Disable validation completely when overfit_batches>0 (#9709),0.6993482,set validation to a fix number of batches,Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Remove return_result argument from DDPSpawnPlugin.spawn() (#10867),0.7261905,- Removed argument `return_result` from the `DDPSpawnPlugin.spawn()` method ([#10867](https://github.com/PyTorchLightning/pytorch-lightning/pull/10867)),,1
Fix selection of standalone tests (#10857),0.5166863,Removed implicit ui testing with testing.run_app_in_cloud in favor of headless login and app selection (#16741),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
Raise exception if rich is less than 10.2.2 (#10839),0.86038387,Improved exception message if rich version is less than 10.2.2 (#10839),,1
Correct SLURM wrong import (#10842),0.6265188,Fix an issue with the SLURM srun detection causing permission errors (#15485),,0
Add job_name as a staticmethod in SLURMEnvironment class (#10698),0.64024323,# use slurm job id for the port number,Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
Update changelog after v1.5.4 release (#10843),0.6612752,Full Changelog,,0
Fix SignalConnector._has_already_handler check for callable type (#10483),0.54697216,- Marked the `lightning.pytorch.trainer.connectors.signal_connector.HandlersCompose` class as protected ([#17008](https://github.com/Lightning-AI/lightning/pull/17008)),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
Restore signals on teardown (#10611),0.54970527,Moved device-specific teardown logic from training loop to accelerator (#5973),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Fix typing in pl.overrides.distributed (#10797),0.574498,  * Removed the deprecated `pl_module` argument from the distributed module wrappers,"  fix typing   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
Update logging docs (#10734),0.6293931,Change default logger to a dedicated one (#1064),Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,0
Disable batch_size extraction for torchmetric instances (#10815),0.9197157,Disabled batch_size extraction for torchmetric instances because they accumulate the metrics internally (#10815),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
Save and load with CheckpointIO in DDPSpawn plugins (#10846),0.6558304,CheckpointIO Plugins,,0
Improve typing for loops (#10780),0.6852094,Refactored Loops,,0
Remove references to torchtext.legacy from PyTorch Lightning (#10724),0.7304213,- Deprecated the calls to `pytorch_lightning.utiltiies.meta` functions in favor of built-in https://github.com/pytorch/torchdistx support ([#13868](https://github.com/Lightning-AI/lightning/pull/13868)),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Skip hanging spawn tests (#10838),0.52295524,Cleaning up stale logger tests (#3490),  Skip hanging spawn tests   Docstring fix   Add back to TPU spawn ,0
Do not require omegaconf to run tests (#10832),0.43925208,Disabled optimizers setup during testing (#3059),,0
Update LiteOptimizer signature after optimizer changes in TrainingTypePlugin (#10708),0.6432926,- Moved optimizer related logics from `Accelerator` to `TrainingTypePlugin` ([#10596](https://github.com/PyTorchLightning/pytorch-lightning/pull/10596)),,0
Minor changes in preparation for saving the loops state (#10783),0.66983867,The Loop's state is now included as part of the checkpoints saved by the library. This enables finer restoration of custom loops.,,0
Fix typing in pl.callbacks.timer (#10798),0.6122548,def MyCallback(pl.Callback):,,0
Remove legacy configurations (#10829),0.50942314,Removed logger_connector legacy code (#6733),,0
Improve @RunIf docs (#10828),0.4489888,Docs improvements,,0
Update notebooks submodule (#10827),0.41929245,Replaced _DataModuleWrapper with __new__ (#7289),,0
Add test for job_id (#10774),0.69195396,    # use the last 4 numbers in the job id as the id,,0
2/n Move Precision Plugin into strategy - move optimizer related logics (#10596),0.6716995,"The Accelerator and PrecisionPlugin have moved into Strategy. All strategies now take an optional parameter accelerator and precision_plugin (#11022, #10570).",Co-authored-by: Danielle Pintz 38207072+daniellepintz@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Update extensions doc (#10778),0.47817153,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
1/n Move Accelerator into strategy - move batch_to_device to strategy (#10649),0.61520064,move specific accelerator code (#3457),"  1/n Integrate Device Specific Accelerator Logic with strategy - move batch_to_device to strategy   add changelog   add model is not none check   Apply suggestions from code review   Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   Update CHANGELOG.md   Update test_datamodules.py   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Update test_hooks.py   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  Update dp.py  Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
Fix default logging levels for train step specific hooks (#10756),0.65694374,"Deprecated passing flush_logs_every_n_steps as a Trainer argument, instead pass it to the logger init if supported (#9366)",,0
Do not sanity check on reload (#10785),0.50317466,Call reset_on_restart in the loop's reset hook instead of when loading a checkpoint (#9561),,0
update NGC (#10770),0.48554447,Update the Lightning App docs (#13537),,0
[CLI] Add support for --key.help=class (#10767),0.47275537,lightning add ssh-key CLI command has been transitioned to lightning create ssh-key,,0
Fix typing in pl.overrides.fairscale (#10799),0.52041316,  * Removed the `pytorch_lightning.overrides.fairscale.LightningShardedDataParallel` class,"  update typing in fairscale   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
Fix typing in pl.core.mixins.hparams_mixin (#10800),0.5615604,| Import pl.core.mixins.DeviceDtypeModuleMixin                                                           | 1.10             | No longer supported |,"  fix typing in hparams mixin   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  unused import  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
Fix typing in pl.overrides.data_parallel (#10796),0.628549,PL_INTER_BATCH_PARALLELISM=1 python train.py,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Mark outputs as protected in the evaluation loops (#10781),0.6209311,"Marked several methods in EvaluationEpochLoop as protected: on_evaluation_batch_start, evaluation_step, evaluation_step_end (#9516)",Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
Tune Conda CI timeout and other minor improvements (#10769),0.4479696,"Resolved schedule step bug for PyTorch Profiler (#6674, #6681)",,0
Fix reference link to s3fs (#10737),0.42420763,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",,0
Add remote filesystems to docs (#10752),0.53384364,Using gfile to support remote directories (#2164),Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Update changelog after 1.5.3 release (#10744),0.62568015,Full Changelog,,0
Consolidate state when retrieving sharded state dict in Lite (#10746),0.62264717,"+def load_state_dict(self, state):",Co-authored-by: thomas chaton thomas@grid.ai,0
Improve typing for plugins (#10742),0.6814685,Refactored setup for typing friendly (#6590),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
Improve typing for Lite (#10743),0.7088858,Refactored setup for typing friendly (#6590),"  improve typing in pytorch_lightning/lite   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  include lite again  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",1
Fault Tolerant: Add support for fault tolerant dataloader validator (#10465),0.66512734,enabled multiple dataloaders for validation.    ,,0
Add a custom PossibleUserWarning category (#10675),0.44974768,warning_utils >> warnings,,0
Improve typing for loops (#10749),0.6937372,Refactored Loops,  Improve typing for loops   Free memory ,0
Improve typing for logging (#10748),0.7030598,Refactored logging,Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,1
Avoid optional instances in Loops (#10735),0.5621687,Refactored Loops,  Avoid optional instances in Loops   More cleanup ,0
Remove dead code in TrainingEpochLoop (#10750),0.61816365,Removed Warning from trainer loop (#1634),,0
Rename special to standalone (#10779),0.3909386,"- ""Native"" suffix removal ([#16490](https://github.com/Lightning-AI/lightning/pull/16490))",,0
Delete TensorBoardLogger experiment before spawning the processes. (#10777),0.7547021,Changed the default logger to TensorBoardLogger (#609),,1
Fault Tolerant: move signal to SIGTERM (#10605),0.6525717,- Fault Tolerant relies on `signal.SIGTERM` to gracefully exit instead of `signal.SIGUSR1` ([#10605](https://github.com/PyTorchLightning/pytorch-lightning/pull/10605)),,0
Fix compare version for packages (#10762),0.66509855,Parsed local package versions (#13933),,0
Fault Tolerant Manual: Add support for DDP (#10638),0.6319221,- Fault Tolerant Manual,,0
Deprecate DeviceType in favor of _AcceleratorType (#10503),0.6900979,"The new devices argument is now agnostic to all accelerators, but the previous arguments gpus, tpu_cores, ipus are still available and work the same as before. In addition, it is now also possible to set devices=""auto"" or accelerator=""auto"" to select the best accelerator available on the hardware.",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Be explicit with mypy ignores (#10751),0.48335603,python script.py test,  Ignore mypy only for failing files   Comment ,0
Configure mypy to install dependencies in CI and update pyproject.toml (#10682),0.49409014,Add missing python-multipart dependency (#17244),  mypy install deps   fix deps   add examples   fix type errors   fix type error   fix   fix   update pyproject.toml ,0
Improve error message on TypeError during DataLoader reconstruction (#10719),0.64610255,- Show a better error message when a custom `DataLoader` implementation is not well implemented and we need to reconstruct it ([#10719](https://github.com/PyTorchLightning/pytorch-lightning/pull/10719)),,0
Fault Tolerant Manual: Enable the feature (#10707),0.74947506,- Fault Tolerant Manual,,1
Support re-instantiation for custom DataLoader in Lightning (#10680),0.8424616,- Improved support for custom `DataLoader`s when instantiated in `*_dataloader` hook ([#12981](https://github.com/Lightning-AI/lightning/pull/12981)),Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,1
Fault Tolerant Manual: utilities cleanup (#10703),0.577969,- Fault Tolerant Manual,Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
Update LightningDataModule docs (#10678),0.7476013,- Added profiling of `LightningDataModule` hooks ([#12971](https://github.com/Lightning-AI/lightning/pull/12971)),,1
Use PrecisionType enum instead of checking raw values  (#10704),0.59971285,    precision=16,"  use precision type   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
Fault Tolerant Manual: Add loading to reload the states (#10699),0.55985004,Called on_load_checkpoint before loading state_dict (#4057),Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
LiteDataLoader wrapper improvements (#10297),0.5404707,Make the _LiteDataLoader an iterator and add supports for custom dataloader (#10279),,0
Fault Tolerant Manual: Add support for collecting states across processes (#10639),0.5572102,A new stateful API,,0
Update DeepSpeed precision handling after moving PrecisionPlugin (#10657),0.88526624,Updated precision attributes in DeepSpeedPlugin (#10164),,1
Fault Tolerant Manual: Add stateful dataloader iter (#10674),0.6849154,Fault Tolerance has limitations that require specific information about your data-loading structure.,,0
Move Colab setup to ProgressBar (#10542),0.5615008,Better progress bar (#16695),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Fault Tolerant Manual: Add _rotate_worker_indices utility (#10647),0.54681337,"To simplify reading and debugging the codebase, we removed the experimental support for fault-tolerance which was under the PL_FAULT_TOLERANT_TRAINING= environment flag. We are looking at ways to re-implement this. If you are interested in this feature, don't hesistate to reach out to us or create a feature request.",,0
Update LightningModule docs (#10637),0.71539277,Update the Lightning App docs (#13537),,1
Fault Tolerant Manual: Add is_obj_stateful utility (#10646),0.5785012,Fault Tolerance has limitations that require specific information about your data-loading structure.,,0
remove import of datasets separately since unused (#10668),0.51225877,"DataModules now avoid duplicate {setup,teardown,prepare_data} calls for the same stage (#7238)",,0
Update DDPShardedPlugin precision handling after moving PrecisionPlugin (#10658),0.68036747,Enable mixed precision in DDPFullyShardedStrategy when precision=16 (#12965),,0
refactor slurm_job_id (#10622),0.7031653,# use slurm job id for the port number,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com,1
Use Set operations in Environment.detect (#10673),0.46431538,trainer = Trainer(detect_anomaly=True),,0
Raise an error if batch_size cannot be inferred from current batch (#10541),0.644457,Do not fail if batch size could not be inferred for logging when using DeepSpeed (#10438),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Small improvements to _init_debugging_flags (#10620),0.54126,      init_args:,,0
Fix move_metrics_to_cpu with evaluation (#10631),0.69133675,Removed the Trainer(move_metrics_to_cpu=True) argument (#16358),,0
1/n Add FaultTolerantMode (#10645),0.44141653,Manual Fault-tolerance,,0
Fix docs filterwarnings snippet (#10671),0.518722,Do not override PYTHONWARNINGS (#4700),,0
Remove metrics references from docs (#10567),0.68609893,"Removed experimental Metric API (#3868, #3943, #3949, #3946), listed changes before final removal:",,0
update bug_report model links and notebook (#10665),0.50404114,Changed ModelCheckpoint version suffixes to start at 1 (#5008),,0
Moved env_vars_connector._defaults_from_env_vars to utilities.argsparse._defaults_from_env_vars (#10501),0.61047184,- Moved `trainer.connectors.env_vars_connector._defaults_from_env_vars` to `utilities.argsparse._defaults_from_env_vars` ([#10501](https://github.com/PyTorchLightning/pytorch-lightning/pull/10501)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
LiteDataLoader code improvements and docs (#10625),0.50614595,Make the _LiteDataLoader an iterator and add supports for custom dataloader (#10279),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
"Remove the ""_precision"" suffix from some precision plugin files (#10052)",0.7256895,Precision Plugins (#5718),,1
Check torch.distributed availability before sharded tensor state dict hook registration (#10621),0.6269594,Removed registration of ShardedTensor state dict hooks in LightningModule.__init__ with torch>=2.1 (#16892),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Fix misleading ModelCheckpoint documentation on every_n_epochs parameter (#10421),0.8462716,Deprecated ModelCheckpoint(every_n_val_epochs) in favor of ModelCheckpoint(every_n_epochs) (#8383),,1
LightningCLI changes for jsonargparse>=4.0.0 (#10426),0.8683341,- Changes in `LightningCLI` required for the new major release of jsonargparse v4.0.0 ([#10426](https://github.com/PyTorchLightning/pytorch-lightning/pull/10426)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: thomas chaton thomas@grid.ai,1
Make _get_nvidia_gpu_stats public (#10406),0.62983483,"Deprecated log_gpu_memory, gpu_metrics, and util funcs in favor of DeviceStatsMonitor callback (#9921)",,0
Use new GitHub labels (#10552),0.4064544,@akihironitta @awaelchli @Borda @carmocca @kaushikb11 @krshrimali @mauvilsa @otaj @pre-commit-ci @rohitgr7 @semaphore-egg @tkonopka @wayi1,,0
Fix batch size extraction when set by the user in LightningModule.log (#10408),0.65455794,- Added dataclass support to `extract_batch_size` ([#12573](https://github.com/Lightning-AI/lightning/pull/12573)),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
Added boring model as a ipynb so it can be updated (#10521),0.47458184,adding Trainer.tune() (#3293),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
Respect the passed dtype with self.log (#10076),0.5979524,Raised ValueError when a None value is self.log-ed (#7771),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
MANIFEST.in and setup.py clean-up (#7614),0.44862303,A setup.py file for a callbacks plugin package could look something like this:,,0
Lite: Don't pop value if they don't exist (#10613),0.38969284,Prevent WandbLogger from dropping values (#5931),,0
Extract dataloader utilities from TrainerDataLoadingMixin (#10145),0.6964589,Removed trainer.reset_*_dataloader() methods (#16726),,0
Introduce ClusterEnvironment.detect() (#10564),0.68624604,Deprecated ClusterEnvironment.creates_children() in favor of ClusterEnvironment.creates_processes_externally (property) (#10106),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Remove redundant fit call from accelerator connector test (#10626),0.60440904,Remove hardcoding of local rank in accelerator connector (#6878),,0
Add refresh_rate to RichProgressBar (#10497),0.7609383,- Fixed `RichProgressBar` progress when refresh rate does not evenly divide the total counter ([#11668](https://github.com/PyTorchLightning/pytorch-lightning/pull/11668)),Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Don't register signal in thread (#10610),0.40466297,Resolved a bug about a race condition when sending the work state through the caller_queue (#14074),,0
Move benchmarks into the test directory (#10614),0.45475343,Moved profilers to their own file (#7822),,0
Fix failing master due to an interction between PRs (#10627),0.38376856,"To resolve fault-tolerance issues, we changed where the current epoch value gets increased.",,0
Use update_wrapper in test_hooks.py (#10578),0.5288865,refactor eval loop to use hooks - use test_mode for if so we can split later (#3129),,0
1/n Move precision plugin into strategy - update reference (#10570),0.703927,"The Accelerator and PrecisionPlugin have moved into Strategy. All strategies now take an optional parameter accelerator and precision_plugin (#11022, #10570).","  1/n move precision plugin into strategy - update reference   update precision plugin reference in tpu_spawn   add missing reference in error message   add back removed license line   update references in tests   update reference in trainer   update return annotation for precision_plugin property on TTP   simplify access to precision plugin reference in sharded plug   add changelog   remove precision property from ttp and add deprecation message   fix make doc and update precision reference   simplify a reference to precision   accidentally overridden Adrian's change, now add it back  Update CHANGELOG.md  add Adrian's change back  Update accelerator precision  Add Adrian's change back  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci  Add none check for precision plugin  just to be safe   Update ipu.py   update precision_plugin param deprecation message   Update accelerator.py   Remove deprecated warning    Tests will fail after 9940 Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",1
Refactor progress bar initialization to avoid extra attribute set on Trainer (#10553),0.7330384,Decoupled the progress bar from trainer. It is a callback now and can be customized or even be replaced entirely (#1450).,,1
Control automatic resubmission on SLURM (#10601),0.80229795,"We've added the ability to turn the automatic resubmission on or off when a job gets interrupted by the SLURM controller (via signal handling). Users who prefer to let their code handle the resubmission (for example, when submitit is used) can now pass:",,1
log metrics for correct dataloader only (#10522),0.6039711,"Working with multiple dataloaders (#16800, #16753)",Co-authored-by: tchaton thomas@grid.ai Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Update changelog after 1.5.2 release (#10590),0.6291647,Full Changelog,,0
Improve code quality in AcceleratorConnector._configure_slurm_ddp  (#10102),0.7780866,Deprecated access to the AcceleratorConnector.configure_slurm_ddp method and marked it as protected (#10101),,1
Fail the test when a DeprecationWarning is raised (#9940),0.70066845,Deprecation warning (#3844),,1
Simplify hanging queue test (#10591),0.468857,Resolved a bug about a race condition when sending the work state through the caller_queue (#14074),,0
Use single quotes in action job (#10579),0.45890856,    # use the last 4 numbers in the job id as the id,,0
Support special test parametrizations (#10569),0.45807552,Flattening Wandb Hyperparameters (#2459),,0
Fix caplog with logger.propagate=False (#10577),0.5902751,Changed WandbLogger(log_model={True/'all'}) to log models as artifacts (#6231),,0
Tune cc-bot settings (#10544),0.40939334,"If you need to customize the learning rate scheduler configuration, you can do so by overriding:",,0
Fix propagation of device and dtype properties in Lite modules (#10559),0.5937548,Move lightning module to correct device type when using LightningDistributedWrapper (#6070),,0
Mock GPU accelerator connector tests (#10554),0.5366139,"accelerator connector methods x/n (#3469, #3470, #3474)",,0
Fix scripting causing false positive deprecation warnings (#10555),0.67969334,Deprecation warning (#3844),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
[DeepSpeed] Do not fail if batch size could not be inferred for logging (#10438),0.967482,Do not fail if batch size could not be inferred for logging when using DeepSpeed (#10438),,1
Don't collapse Lightning API section (#10545),0.5264012,- Added a layout endpoint to the Rest API and enable to disable pulling or pushing to the state ([#15367](https://github.com/Lightning-AI/lightning/pull/15367),,0
remove deprecated reload_dataloaders_every_epoch from Trainer (#10481),0.89843804,Deprecated reload_dataloaders_every_epoch argument of Trainer in favor of reload_dataloaders_every_n_epochs (#5043),,1
Fix loop examples after Accelerator API removals (#10514),0.6452783,Refactored Accelerators and Plugins (#5743),,0
"Skip strategy=ddp_spawn, accelerator=cpu, python>=3.9 tests (#10550)",0.6333664,"Setting Trainer(accelerator=""ddp_cpu"") now does not spawn a subprocess if num_processes is kept 1 along with num_nodes > 1 (#9603)",,0
fix overfit_batch sampler replacement logic (#10486),0.6497115,overfit_pct in favour of overfit_batches,Co-authored-by: thomas chaton thomas@grid.ai,0
Fix to_torchscript() causing false positive deprecation warnings (#10470),0.6777372,- Fixed an issue with comparing torch versions when using a version of torch built from source ([#17030](https://github.com/Lightning-AI/lightning/pull/17030)),,0
Avoid deprecated progress_bar_refresh_rate usage (#10520),0.6515446,Changed progress_bar_refresh_rate trainer flag to disable progress bar when setting to 0. (#1108),Co-authored-by: Danielle Pintz 38207072+daniellepintz@users.noreply.github.com,0
Resolve instantiation problem with init_meta_context (#10493),0.49470687,def __init__(self):,,0
Remove deprecated disable_validation property from Trainer (#10450),0.8719121,Deprecated the Trainer.disable_validation property in favor of not Trainer.enable_validation (#8291),,1
Deprecate DistributedType in favor of StrategyType (#10505),0.6582537,- Deprecated `DistributedType` in favor of `_StrategyType` ([#10505](https://github.com/PyTorchLightning/pytorch-lightning/pull/10505)),,0
Remove deprecated stochastic_weight_avg example from the docs (#10502),0.63215256,Deprecated passing stochastic_weight_avg to the Trainer constructor in favor of adding the StochasticWeightAveraging callback directly to the list of callbacks (#8989),,0
Enable the auto-cc bot (#10531),0.41969442,This is how you enable it:,,0
Update configs with new GitHub labels (#10532),0.4712028,A header with the version that generated the config is now included.,Co-authored-by: thomas chaton thomas@grid.ai,0
Update issues templates (#10537),0.42862958,Included app templates to the lightning and app packages (#13731),,0
Remove deprecated is_overridden(model=...) (#10507),0.9046017,Deprecated is_overridden(model=...) in favor of is_overridden(instance=...) (#7918),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Remove deprecated hpc_load in CheckpointConnector (#10525),0.8525865,Deprecated the use of CheckpointConnector.hpc_load() in favor of CheckpointConnector.restore() (#7652),Co-authored-by: Aki Nitta nitta@akihironitta.com,1
shutdown workers on failure (#10463),0.49174875,"    num_workers=10,",,0
Drop torch 1.6 testing (#10390),0.5829263,- Fixed an issue with comparing torch versions when using a version of torch built from source ([#17030](https://github.com/Lightning-AI/lightning/pull/17030)),"  Drop torch 1.6 support   Drop 1.6 support   Update CHANGELOG   Fixes   Split change   Undo change   1.7 -> 1.7.1   https://github.com/pytorch/pytorch/issues/47354   Force trigger nightly   Update .github/workflows/events-nightly.yml   Co-authored-by: Aki Nitta nitta@akihironitta.com   Revert 1.7.1 change - try wildcard   Update adjust versions and test it   Undo test changes   Revert ""Undo test changes""   This reverts commit 3a6acadd115e86f02d83a788f1978372ab6764f3.  Update CHANGELOG.md  Co-authored-by: Aki Nitta nitta@akihironitta.com",0
remove deprecated signature for transfer_batch_to_device (#10480),0.5993934,Don't convert namedtuple to tuple when transferring the batch to target device (#1589),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Remove deprecated mode argument from ModelSummary (#10449),0.81685275,Deprecated mode parameter in ModelSummary in favor of max_depth (#8062),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
Squeeze the early stopping monitor (#10461),0.57960606,Squeeze the early stopping monitor to remove empty tensor dimensions (#10461),,0
remove deprecated train_loop (#10482),0.77058226,Removed deprecated TrainResult (#5323),  remove deprecated train_loop   chlog ,1
Change attributes of RichProgressBarTheme dataclass (#10454),0.6387919,New Rich Progress Bar,Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,0
Update tests to avoid the deprecated weights_summary (#10446),0.5111473,Function stat_scores_multiple_classes is deprecated in favor of stat_scores (#4839),,0
Upgrade CI after the 1.10 release (#10075),0.53082716,This release is a buffer in case 1.0 breaks any compatibility for people who upgrade. 0.10.0 has all the bug fixes and features of 1.0 but is 100% backward compatible. The 1.0 release following in the next 24 hours. ,,0
"Remove deprecated utilities.distributed.rank_zero_{warn,deprecation} (#10451)",0.7101902,"Deprecated importing rank_zero_{warn,deprecation} directly from pytorch_lightning.utilities.distributed (#8085)",,1
Remove more deprecated methods from base Accelerator class (#10448),0.81449115,- Removed deprecated passthrough methods and properties from `Accelerator` base class:,,1
Fix support for dataclasses with ClassVar/InitVar in apply_to_collection (#9702),0.6656764,- Added support for dataclasses in `apply_to_collections` ([#11889](https://github.com/PyTorchLightning/pytorch-lightning/pull/11889)),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
Remove deprecated profiler import (#10443),0.6581215,Moved profilers to their own file (#7822),,0
Update Changelog for v1.5.1 (#10439),0.6170366,This section outlines notable changes that are not backward compatible with previous versions. The full list of changes and removals can be found in the Full Changelog below.,  Missing Changelogs   Add 1.5.1 entry to changelog   Co-authored-by: Kaushik B kaushikbokka@gmail.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,0
Enable distributed training with CombinedDataLoader and max_size_cycle (#10374),0.6527159,The argument distributed_backend has been removed from the Trainer in favor of the new accelerator and strategy arguments (#10017).,  solve combinedloader   update   update changelog   update on comments   resolve iterable dataset support   update test description   update   update on comments   update   Accelerator auto   Address review   Refactor   Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
Remove deprecated task_idx (#10441),0.6642576,Removed deprecation warnings being called for on_{task}_dataloader (#9279),,0
Remove deprecated DeviceDtypeModuleMixin import (#10442),0.75600964,| Import pl.core.mixins.DeviceDtypeModuleMixin                                                           | 1.10             | No longer supported |,,1
Make monitor required arg of EarlyStopping callback (#10328),0.851836,Deprecated default value of monitor argument in EarlyStopping callback to enforce monitor as a required argument (#7907),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Do not autodetach extras (#10424),0.46864516,Deprecated automatically detaching returned extras with grads (#7994),  Do not autodetach extras   Update CHANGELOG   Use foo ,0
Resolve workers being forcelly deleted with persistent_workers=True (#10434),0.5168592,"    num_workers=10,",Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
"[Fault Tolerance] Don't check the len of a dataset, but its instance. (#10432)",0.54092103,Fault Tolerance has limitations that require specific information about your data-loading structure.,,0
Remove deprecated self.log arguments (#10423),0.7111142,Removed support for self.log()ing a dictionary (#16389),,1
Convert property into attribute (#10412),0.55868495,    @property,,0
Fix converting only float type tensors in Lite (#10429),0.6668002,- Fixed type promotion when tensors of higher category than float are logged ([#11401](https://github.com/PyTorchLightning/pytorch-lightning/pull/11401)),  fix   less code   add test case   add test cases   update input   add test cases   add type hint   add changelog note   Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,0
Fix deadlocks for distributed training for RichProgressBar (#10428),0.6375597,- Fixed the `RichProgressBar` crashing when used with distributed strategies ([#15376](https://github.com/Lightning-AI/lightning/pull/15376)),,0
disable step logging in epoch hooks (#10409),0.7360855,Explicitly disallow calling self.log(on_epoch=False) during epoch-only or single-call hooks (#7874),  disable step logging in epoch hooks   chlog   Apply suggestions from code review   chlog ,1
Raise exceptions when torch distributed is not available (#10418),0.6623386,    * Some configuration errors that were previously raised as `MisconfigurationException`s will now be raised as `ProcessRaisedException` (torch>=1.8) or as `Exception` (torch<1.8),  Raise exceptions when torch distributed is not avalible   add changelog ,0
Remove deprecated accelerator pass through functions in Accelerator (#10403),0.8210273,- Removed deprecated passthrough methods and properties from `Accelerator` base class:,Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Hyperparameters docs refresh (#10280),0.5884596,Moved save_hyperparameters to its own function (#7119),Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
"Rename ""master"" methods to ""main"" in ClusterEnvironment plugins (#10103)",0.55992645,- Removed deprecated ClusterEnvironment properties `master_address` and `master_port` in favor of `main_address` and `main_port` ([#13458](https://github.com/Lightning-AI/lightning/pull/13458)),"  rename occurrences of master port, master address, maser node, master process   rename properties   add property decorators   occurrences in docs   update changelog   update changelog   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   add lost method   create deprecation   add changelog   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   fix typo (but it was already there!!!)   Apply suggestions from code review   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com   add todo   update more occurences   add types   add missing import   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com",0
Drop torch 1.6 support (#10367),0.68135095,    * Some configuration errors that were previously raised as `MisconfigurationException`s will now be raised as `ProcessRaisedException` (torch>=1.8) or as `Exception` (torch<1.8),,0
Revert part of #10279 (#10376),0.4708437,Refactor Model backward (#2276),,0
Fix pickling error with CSVLogger (#10388),0.74458647,- pickling errors with loggers (txs @awaelchli),  Don't store csv.Dictwriter in ExperimentWriter   Add test for pickle after .save()   Add entry in changelog ,1
Remove deprecated master_params attributes in PrecisionPlugin (#10372),0.86699605,Deprecated PrecisionPlugin.master_params() in favor of PrecisionPlugin.main_params() (#10105),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Only import PostLocalSGD related modules when it's needed (#10359),0.5193358,Module imports are now catching ModuleNotFoundError instead of ImportError (#9867),  Only import PostLocalSGD related modules when it's needed   Only import PostLocalSGD related modules when it's needed   Only import PostLocalSGD related modules when it's needed ,0
Fix DataLoader inspection and re-instantiation in Lite (#10334),0.7104672,"Did not always create a DataLoader during reinstantiation, but the same type as before (if a subclass of DataLoader) (#1346)",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Remove every_n_val_epochs from ModelCheckpoint (#10366),0.80105376,Deprecated ModelCheckpoint(every_n_val_epochs) in favor of ModelCheckpoint(every_n_epochs) (#8383),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
Remove deprecated property configure_slurm_dpp from accelerator connector (#10370),0.8540655,Deprecated access to the AcceleratorConnector.configure_slurm_ddp method and marked it as protected (#10101),  Remove deprecated configure_slurm_ddp   Update CHANGELOG   Remove deprecated tests from test suite ,1
Move device parser utility function (#10230),0.7536388,"device parser (#3400, #3405)","  move parser function to utils   fix types   keep static method   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",1
Remove deprecated sync_batchnorm and num_nodes attributes in DDP plugins (#10357),0.8217561,Deprecated num_nodes and sync_batchnorm arguments in DDPPlugin and DDPSpawnPlugin (#7026)," Remove deprecated sync_batchnorm and num_nodes attributes in DDPPlugin  Part of #10312 test_v1_6_0_ddp_num_nodes() test_v1_6_0_ddp_sync_batchnorm()  Remove deprecated sync_batchnorm and num_nodes attributes in DDPPlugin  Part of #10312 test_v1_6_0_ddp_num_nodes() test_v1_6_0_ddp_sync_batchnorm()   remove deprecation warnings   apply removal to spawn plugin   update changelog   remove num_nodes in deepspeed   remove unused imports   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",1
Update Trainer(precision) docs (#10368),0.7212956,"Based on feedback, we decided to make the names for precision backends in Trainer(precision=...) clearer and less ambiguous. For example, the previous notation Trainer(precision=16) (which is still allowed to be used) suggested to some users that all of the weights and tensors would be stored in a float16 format, which is not true. To solve this misunderstanding, we now distinguish these modes with ""true"" and ""mixed"" suffixes in the names:",  Update Trainer(precision) docs   Update ,1
Add more trainer config tests (#10319),0.6867388,- Full tests that test specific functionality in trainer.,  Add more trainer config tests   Add more trainer config and ttp register tests   Add more trainer config and ttp register tests ,0
Remove deprecated property is_slurm_managing_tasks from accelerator connector (#10353),0.8589785,Deprecated access to the AcceleratorConnector.is_slurm_managing_tasks attribute and marked it as protected (#10101),"  Remove deprecated property _slurm_managing_tasks from accelerator connector   Update CHANGELOG   Update Changelog   Removed is_slurm_managing_tasks from AcceleratorConnector   resolve merge conflict   add back accidentally removed lines   remove test   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",1
Remove deprecated datamodule lifecycle properties (#10350),0.6299363,"Deprecated DataModule properties: has_prepared_data, has_setup_fit, has_setup_validate, has_setup_test, has_setup_predict, has_teardown_fit, has_teardown_validate, has_teardown_test, has_teardown_predict (#7657)",Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
Update Lite docs (#10347),0.65852356,There were two different ways of importing Lite in <= 1.9.0,Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Roger Shieh sh.rog@protonmail.ch,0
Update Python testing (#10269),0.5795728,python script.py test,,0
Remove deprecated method ClusterEnvironment.creates_children (#10339),0.8826815,Deprecated ClusterEnvironment.creates_children() in favor of ClusterEnvironment.creates_processes_externally (property) (#10106),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Fix failure when DataLoader(batch_size=None) is passed (#10345),0.70681506,Reset val_dataloader in tuner/batch_size_scaling (#9857),"  add test, + add change to data loading batch sample method   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  Refactor and CHANGELOG  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com",1
Update bfloat16 docs (#10330),0.72212076,BFloat16 Support,,1
Remove deprecated dataloader arguments in Trainer methods (#10325),0.80361545,Removed trainer.reset_*_dataloader() methods (#16726),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
Remove deprecated loaded_optimizer_states_dict property (#10346),0.79635775,Deprecated LightningModule.loaded_optimizer_states_dict (#8229),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
Update recommendation on dataloader_idx (#10318),0.6253104,"Accessing dataloaders (#16726, #16800)",,0
Update issue template ads (#10310),0.3713717,"In addition, we fixed:",,0
Remove TrainerModelHooksMixin (#10322),0.6686931,Removed obsolete self._device in Trainer (#1849),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
add minimum codeowners for lite (#10298),0.43249682,class Lite(LightningLite):,,0
Update dev branch to continue with 1.6 (#10332),0.45090628,The point of this release is more bug fixes ahead of v 1.0.0. We now have CI tests on TPU thanks to @zcain117 from Google! :slightly_smiling_face:,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Update signature of LightningModule.training_step_end (#10094),0.7461993,"- Removed the `training_step_end`, `validation_step_end`, and `test_step_end` hooks from the `LightningModule` in favor of the `*_batch_end` hooks ([#16791](https://github.com/Lightning-AI/lightning/pull/16791))",Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
Fix apply_to_collection(defaultdict) (#10316),0.58264744,Minor improvements to apply_to_collection and type signature of log_dict (#7851),,0
Add note for PyCharm users on configuration for RichProgressBar (#10307),0.60310817,- Added `console_kwargs` for `RichProgressBar` to initialize inner Console ([#10875](https://github.com/PyTorchLightning/pytorch-lightning/pull/10875)),,0
update ref to 1.5 as stable (#10311),0.56534046,Release LAI docs as stable (#14250),  update ref to 1.5 as stable   Fix CHANGELOG   Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
Lightning 1.5 release (#10306),0.67795277,"Lightning 1.5 marks our biggest release yet. Over 60 contributors have worked on features, bugfixes and documentation improvements for a total of 640 commits since v1.4. Here are some highlights:",  update changelog   update version in about   update references to 1.5 in readme   Co-authored-by: thomas chaton thomas@grid.ai,0
introduce has_len_all_ranks() to check the length of dataloader across ranks (#9827),0.7046093,"Raised MisconfigurationException when total length of dataloader across ranks is zero, and give warning when total length is non-zero, but only local rank length is zero. (#9827)","  introduce , udpate tests   update CHANGELOG.md   change staticmethod and hook attribute naming   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   fix typo   remove non-essential comment   fix merge error and comment format   try to fix test_tpu.py failure   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci   update on comments   chlog   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   chlog   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   try fix   Revert back TPUSpawn changes   Update test   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Kaushik B kaushikbokka@gmail.com",1
Add leave argument to RichProgressBar (#10301),0.6202605,New Rich Progress Bar,  Add display_every_n_epochs argument to RichProgressBar   Add tests   Update test   Update test   Update changelog   use leave argument instead   Update pytorch_lightning/callbacks/progress/rich_progress.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Lite Docs and Example Improvements (#10303),0.56196743,Docs improvements,Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Only enable pulse for infinite datasets (#10305),0.4679699,- Disable loading dataloades if corresponding `limit_batches=0` ([#11576](https://github.com/PyTorchLightning/pytorch-lightning/pull/11576)),,0
Fix yielding from iterator in LiteDataLoader (#10304),0.56384206,Make the _LiteDataLoader an iterator and add supports for custom dataloader (#10279),"  fix yielding form iterator   update description   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  remove unused code  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
protect ByteCounter (#10300),0.39761263,Changed the model size calculation using ByteCounter (#10123),,0
Lightning Lite Examples (#9987),0.67947996,class Lite(LightningLite):,Co-authored-by: Kaushik B kaushikbokka@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: SeanNaren sean@grid.ai Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: four4fish 88516121+four4fish@users.noreply.github.com Co-authored-by: Nicki Skafte Detlefsen skaftenicki@gmail.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: Pietro Lesci 61748653+pietrolesci@users.noreply.github.com,0
Add warning if multiple batch_sizes are found from ambiguous batch (#10247),0.60352147,"    batch_size=32,",,0
Update init_ddp_connection's name and log (#10295),0.70984507,        Override to init DDP in a different way or use your own wrapper.,,1
Update docs for devices flag (#10293),0.60437214,We cleaned up the properties related to device indices (#14829).,Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
Add missing deprecation notes for deprecated Trainer flags (#10296),0.6440396,Deprecated Trainer argument print_nan_grads (#1097),,0
Add support for empty gpus list to run on CPU (#10246),0.7432844,Support auto_select_gpus with the accelerator and devices API (#12608),Co-authored-by: rohitgr7 rohitgr1998@gmail.com,1
Add custom dataloader support with Lite  (#10279),0.7659513,        # Let Lite setup your dataloader(s),,1
Allow separate config files for parameters with class type when LightningCLI is in subclass_mode=False (#10286),0.986915,Allowed separate config files for parameters with class type when LightningCLI is in subclass_mode=False (#10286),,1
Add configure_columns method to RichProgressBar (#10288),0.5729948,The Rich progress bar now correctly shows the on_epoch logged values on train epoch end (#11689),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
update outdated links in examples (#10127),0.510049,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",Co-authored-by: tchaton thomas@grid.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
Remove unnecessary NotImplementedError used in Training Type Plugin (#10235),0.5976979,"As a result, an optional method is introduced training_end",Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Changed the model size calculation using ByteCounter (#10123),0.99999994,Changed the model size calculation using ByteCounter (#10123),,1
Exclude docs/ from default pytest dirs (#10078),0.46558273,Remove pytest as a requirement to run app by @manskx in https://github.com/Lightning-AI/lightning/pull/15449,Co-authored-by: tchaton thomas@grid.ai,0
add mypy ci check for Lite (#10289),0.43985844,"Lite(accelerator=""gpu"", devices=""auto"").run()",,0
enable on_load_checkpoint for datamodule for all trainer_fn (#10238),0.7772087,Enabled on_load_checkpoint for LightningDataModule for all trainer_fn (#10238),,1
Deprecate ProgressBar and rename it to TQDMProgressBar (#10134),0.84211814,Deprecated ProgressBar callback in favor of TQDMProgressBar (#10134),Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
switch azure pool (#10266),0.42730817,You can find a migration guide for this change in this PR's description.,,0
Fix distrib_type not being set when Plugin instances being passed to Trainer (#10251),0.5824423,Removed TrainerProperties mixin and moved property definitions directly into Trainer (#9495),,0
attach model parallelism video inside docs (#10274),0.40857854,model reference not provided,,0
Remove reinit_schedulers_properties (#10271),0.589628,Disabled lr_scheduler.step() in manual optimization  (#6825),,0
Changelog fixes for Lightning 1.5 release (#10281),0.66762406,"Lightning 1.5 marks our biggest release yet. Over 60 contributors have worked on features, bugfixes and documentation improvements for a total of 640 commits since v1.4. Here are some highlights:",,0
Link to arxiv summary instead of pdf in pruning docs (#10282),0.38971168,"From now on, outputs are no longer reduced except for the loss tensor, unless you implement training_step_end, in which case the loss won't get reduced either.",,0
Remove debugging_connector.py (#10113),0.653916,Removed logger_connector legacy code (#6733),,0
Simplify LightningOptimizer (#10224),0.5767836,Extend LightningOptimizer to exposure underlying Optimizer attributes + update doc (#5095),,0
Add support for devices='auto' (#10264),0.5264996,  * `pl.utilities.device_parser` ([#16412](https://github.com/Lightning-AI/lightning/pull/16412)),,0
Clip before step (#10248),0.5521403,    gradient_clip_algorithm,,0
Lightning Lite docs (#10176),0.7040803,Update the Lightning App docs (#13537),,1
Lightning Lite core and tests (#10175),0.54351795,Updated app testing (#16000),,0
Update docs for sync_dist logging option (#10186),0.76770896,Support number for logging with sync_dist=True (#5080),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Add auto_device_count method to Accelerators (#10222),0.7588632,"Custom Accelerator implementations must now implement two new abstract methods: is_available() (#11797) and auto_device_count() (#10222). The latter determines how many devices get used by default when specifying Trainer(accelerator=..., devices=""auto"").",Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
Remove training_tricks_connector.py (#10112),0.62400085,Moved accelerator_connector.py to the connectors subfolder (#6033),  deprecate training tricks connector   fixes ,0
del iterator on_run_end() (#9915),0.5164665,"  * max_size_cycle: stops after the longest iterable (the one with most items) is done, while cycling through the rest of the iterables.",,0
Raise exception for strategy=ddp_cpu|tpu_spawn (#10185),0.5607271,"When using multiple devices, the strategy now defaults to ""ddp"" instead of ""ddp_spawn"" when none is set (#16388)",,0
Implement double optimizer closure for hook structure consistency (#10167),0.6030597,Made optimization steps for hooks (#2363),,0
Mark callback_connector as protected (#10121),0.5277504,Removed deprecated callbacks (#3979),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Resolve batch_size in ResultCollection not resetted to 1 on epoch end (#10242),0.6572261,"    batch_size=32,",,0
"Fix log(sync_dist=True, on_epoch=True, on_step=True) not reducing on step (#10227)",0.7161286,Deprecated self.log(sync_dist_op) in favor of self.log(reduce_fx). (#7891),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
Fix Imagenet example (#10179),0.47611892,Updated semantic segmentation example with custom u-net and logging (#1371),,0
Avoid deprecated usage in accelerator connector tests (#10184),0.6021359,Removed deprecated connect_precision_plugin and connect_training_type_plugin from Accelerator (#9019),,0
"Revert ""Add support for len(datamodule) (#9895)"" (#10072)",0.6532352,Replaced _DataModuleWrapper with __new__ (#7289),This reverts commit 6429de8944b27beeeb25dcff5b423ecd26fd3265.,0
Replace _TORCH_GREATER_EQUAL_DEV_1_10 with _TORCH_GREATER_EQUAL_1_10 (#10240),0.7027325,Corrected call to torch.no_grad (#5124),,1
Added advise for num_workers=0 in docs/speed (#10215),0.70728254,"    num_workers=10,",,1
cleanup (#10081),0.6683514,"Cleaning (#5948, #5949, #5950)",,0
Update syntax errors in docs code snippets (#10036),0.5844752,Syntax changes are: ,Co-authored-by: tchaton thomas@grid.ai,0
Make codecov patch threshold 5%,0.40552872,val_percent_check in favour of limit_val_batches,,0
Fix iterating over a DummyLogger when fast_dev_run > 0 (#10232),0.66423297,"Disabled checkpointing, earlystopping and logging with fast_dev_run (#5277)",,0
Minimize the number of docker jobs (#10202),0.5513664,Remove unnecessary intermediate layers in Dockerfiles (#5697),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Add missing test for testing custom registered training plugin (#10225),0.6084632,Refactored setup_training and remove test_mode (#5388),,0
Fix sigterm signal handling (#10189),0.50553596,- Added support for cascading a SIGTERM signal to launched processes after the launching process (rank 0) receives it ([#16525](https://github.com/Lightning-AI/lightning/pull/16525)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
resolve failing test (#10191),0.55699223,- Code coverage (99%),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Fix gradient norm tracking and gradient clipping (#9287),0.8405016,Gradient norm tracking (#16745),"  WIP   Progress   Undo test change   Fix plugin closure execution order   Update CHANGELOG   Fix manual optimization on AMP and skipping backward   Fix for deepspeed   Typo   Hook test for manual closure   Add skipping test with AMP   You are hideous, apex   Add deepspeed test   Update CHANGELOG   Fix for broken master   Add RunIf   FIXMEs   Rename   Fix grad norm   add a simple test   update test   update  test   update test   fix merge conflicts   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Sea of changes   Undo change   Introduce TPUPrecisionPlugin   Undo changes   Undo changes   Resolve FIXME   Undo change   Undo change   Undo change   Fix FIXMEs   Fix FIXME   Correct value   Bad merge   Fix circular imports   WIP   Fixing clipping   Fixes   Bad merge   Move optimizer step and clipping into the PrecisionPlugin   Fix AMP   Update CHANGELOG   Fix tests   Underscore   Progress   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Remove pre_optimizer_step   Missed one   Progress   Progress   Fix test   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Update FIXMEs   Fix test   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Fix test   DeepSpeed warning. mypy   Rename   Finish tests   Update CHANGELOG   Dumb fixes   accelerator=auto   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   Update on comments   Use ClassifModule   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com",1
Pass the scaler as an input to NativeMixedPrecisionPlugin (#10055),0.7096613,NativeMixedPrecisionPlugin and its subclasses now take an optional GradScaler instance (#10055),Co-authored-by: thomas chaton thomas@grid.ai,1
Fix reset_seed() converting the PL_SEED_WORKERS environment variable str read to bool (#10099),0.6438517,seed_everything now fails when an invalid seed value is passed instead of selecting a random seed (#8787),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: tchaton thomas@grid.ai,0
Deprecate lr_sch_names from LearningRateMonitor (#10066),0.962947,Deprecated lr_sch_names from LearningRateMonitor (#10066),,1
Docs: fix mistakes in New Project docs (#10137),0.5622337,Docs improvements,Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
initialize poptorch_models based on trainer_fn (#10149),0.64621085,"Removed the deprecated Trainer.reset_train_val_dataloaders() in favor of Trainer.reset_{train,val}_dataloader (#16131)",,0
Add Plugins Registry to docs (#10181),0.4863637,Enabled plugins (#4041),,0
Let DDPSpawnPlugin.spawn return a result from rank 0 (#10162),0.6613493,"decoupled DDP, DDP spawn (#3733, #3766, #3767, #3774, #3802, #3806)",Co-authored-by: Kaushik B kaushikbokka@gmail.com,0
"Fix setting device when creating ""inf"" monitor value in ModelCheckpoint (#10118)",0.6680827,Allow ModelCheckpoint monitor to be None (#3633),Co-authored-by: thomas chaton thomas@grid.ai,0
Update deepspeed precision plugin for Lite (#10164),0.72462183,Updated precision attributes in DeepSpeedPlugin (#10164),,1
Pin fairscale version (#10200),0.46929854,"For reference, FairScale's implementation can be used with",,0
Replace _TORCH_GREATER_EQUAL_DEV_1_10 with _TORCH_GREATER_EQUAL_1_10 (#10157),0.6990541,Corrected call to torch.no_grad (#5124),,0
update type (#10163),0.49866372,Made type hints public (#17100),,0
Set dataset attribute to MpDeviceLoader used in TPU Spawn (#10151),0.51348364,Ensured process_dataloader is called when tpu_cores > 1 to use Parallel DataLoader (#6015),,0
Update README page in pl_examples folder (#10114),0.49619386,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",Co-authored-by: tchaton thomas@grid.ai,0
Update docutils package version in requirements.txt (#10158),0.48925346,Parsed local package versions (#13933),,0
Move optimizer step and clipping into the PrecisionPlugin (#10143),0.78587675,- PrecisionPlugin.{post_optimizer_step},,1
Avoid deprecated warnings from accelerator and checkpoint connector #10142,0.5979396,all the checkpoint issues should be gone now (including backward support for old checkpoints),,0
Some minor CI cleanup (#10088),0.57736844,some minor cleaning,,0
Small code simplification in training_epoch_loop.py (#10146),0.66244227,trainer = pl.Trainer(max_epochs=2),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Make sure file and folder exists in Profiler (#10073),0.62677467,Moved profilers to their own file (#7822),Co-authored-by: tchaton thomas@grid.ai,0
add 'sanity_checking' to datamodule 'on_after_batch_transfer' docs (#10067),0.59747064,Disabled batch transfer in DP mode (#6098),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
Remove model_connector.py (#10111),0.6633983,Moved accelerator_connector.py to the connectors subfolder (#6033),,0
Rename master_params to main_params (#10105),0.5313109,Override some of the params with new values,Co-authored-by: tchaton thomas@grid.ai Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
Raise MisconfigurationException if trainer.eval is missing required methods (#10016),0.8396318,Trainer is now raising a MisconfigurationException instead of a warning if Trainer.{validate/test} is missing required methods (#10016),,1
Remove optimizer_connector.py (#10120),0.67974067,Moved accelerator_connector.py to the connectors subfolder (#6033),,0
Rename ClusterEnvironment.creates_processes (#10106),0.77970934,Deprecated ClusterEnvironment.creates_children() in favor of ClusterEnvironment.creates_processes_externally (property) (#10106),Co-authored-by: tchaton thomas@grid.ai,1
Add example table to loop docs (#10058),0.4725781,Loop customization:,Co-authored-by: tchaton thomas@grid.ai Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
Fix Enums parsing in generated hparms yaml (#9170),0.66877496,    hparams_file='/path/to/hparams_file.yaml',Co-authored-by: tchaton thomas@grid.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
docker Conda timeout (#10087),0.3862954,"Show a message when BuildConfig(dockerfile=""..."") is passed but a Dockerfile file is already present in the Work (#15799)",,0
Change default value of the max_steps Trainer argument from None to -1 (#9460),0.97334456,Changed default value of the max_steps Trainer argument from None to -1 (#9460),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: rohitgr7 rohitgr1998@gmail.com,1
fix tests (#10138),0.69671434,- fixed all the .test() calls,,0
Mark accelerator connector as protected (#10032),0.62129325,Remove hardcoding of local rank in accelerator connector (#6878),,0
Unify checkpoint load paths [redo #9693] (#10061),0.70645255,Removed deprecated checkpoint argument filepath (#5321),,1
Mark SLURM detection methods in AcceleratorConnector as protected (#10101),0.78921354,Deprecated access to the AcceleratorConnector.is_slurm_managing_tasks attribute and marked it as protected (#10101),Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,1
Use torch.autocast (#10053),0.57619995,        scheduler = torch.optim.lr_scheduler.OneCycleLR(,,0
Fix optimizers overloads typing annotation (#10069),0.61719674,Refactored optimizer (#4658),,0
Minor fixes related to clipping (#10130),0.47292876,Resolve bug with Finetuning (#5744),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
Restrict setup methods to accept a single model (#10064),0.58173966,- Added model configuration checking before it runs,,0
Disable quantization aware training observers (#8540),0.81276995,Quantization aware training observers are now disabled by default during validating/testing/predicting stages (#8540),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: tchaton thomas@grid.ai Co-authored-by: rohitgr7 rohitgr1998@gmail.com,1
Add Yield loop example (#9983),0.5409355,Loop customization:,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: tchaton thomas@grid.ai,0
Remove dead code in accelerator connector (#10100),0.6422837,Remove hardcoding of local rank in accelerator connector (#6878),"  remove dead code in accelerator connector   remove slurm ""fake_slurm_managing_tasks"" dead code ",0
rename set_random_master_port (#10104),0.5734662,    default_port = int(default_port) + 15000,Co-authored-by: tchaton thomas@grid.ai,0
Add method to TPUSpawn plugin to override how models are setup (#10039),0.5808772,"Removed process_idx from the {DDPSpawnPlugin,TPUSpawnPlugin}.new_process methods (#10022)",,0
Mark trainer.data_connector as protected (#10031),0.85894096,Changed Trainer connectors to be protected attributes:,Co-authored-by: tchaton thomas@grid.ai,1
Do not use the base version by default in _compare_version (#10051),0.5659747,Set version as today (#13906),,0
update links in callback examples pointing to bolts (#10117),0.48891228,Removed deprecated callbacks (#3979),,0
fix noqa appearing in docs (#10116),0.3969035,Silenced some warnings. verified ddp refactors (#3483),,0
Add warning section for checkpoint_callback in Trainer docs (#10041),0.8304017,Deprecated checkpoint_callback from the Trainer constructor in favor of enable_checkpointing (#9754),,1
[CI] Comment flaky tests (#10084),0.4732051,"As we continue to strengthen the codebase with more tests, we’re finally getting rid of annoying bugs that have been around for a bit now. Mostly around the inconsistent checkpoint and early stopping behaviour (amazing work @awaelchli  @jeremyjordan )",,0
Prepare v1.5.0rc1 (#10068),0.5001534,[1.1.5] - 2021-01-19,,0
Remove redundant require_backward_grad_sync=False in sharded plugins (#10065),0.5641031,Removed call_configure_sharded_model_hook property from Accelerator and TrainingTypePlugin (#9612),,0
Fix: skip importing DistributedOptimizer for Windows (#10071),0.5255482,- No longer set a `DistributedSampler` to the `poptorch.DataLoader` when IPUs are used ([#12114](https://github.com/PyTorchLightning/pytorch-lightning/pull/12114)),,0
"Add support for init_meta_context, materialize_module (#9920)",0.38198715,Implemented ready for components (#16129),,0
Update setup logic in training type plugins (sharded) [4 / 4] (#10028),0.7445953,"- TrainingTypePlugin.{call_configure_sharded_model_hook, on_reset_*_dataloader, on_save, post_optimizer_step, update_global_step}",Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: thomas chaton thomas@grid.ai,1
Remove the DeepSource integration (#10056),0.41504487,"Updated DeepSpeed ZeRO (#6546, #6752, #6142, #6321)",,0
"Revert ""Support serialized checkpoint loading (#9605)"" (#10057)",0.67513543,Enable None model checkpoint default (#3669),This reverts commit f0e6f1b58aeb81f461491fd4818dd487a507ad42.,0
Add XLACheckpointIO (#9972),0.55397755,  * Deprecated the internal `XLADeviceUtils.xla_available` staticmethod,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Update optimizer_step methods in accelerator and plugins (#10023),0.7264907,Executing the optimizer_closure is now required when overriding the optimizer_step hook (#9360),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
Update API references (#10040),0.60950774,Removed deprecated API (#2073),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
Fix LearningRateMonitor logging with multiple param groups optimizer with no scheduler (#10044),0.64232814,Changed LearningRateLogger to LearningRateMonitor (#3251),,0
Remove unnecessary dependency available checks (#10050),0.5974445,Improved support for running apps when dependencies aren't installed (#15711),,0
Group all the logged gradients under the same sub-folder (#7756),0.5983158,Changed gradient logging to use parameter names instead of indexes (#660),,0
Isolate optimizer step logic to the PrecisionPlugin (#10029),0.7787707,- PrecisionPlugin.{post_optimizer_step},,1
Update strategy flag in docs (#10000),0.507885,"All TrainingTypePlugins have been renamed to Strategy (#11120). Strategy is a more appropriate name because it encompasses more than simply training communcation. This change is now aligned with the changes we implemented in 1.5, which introduced the new strategy and devices flags to the Trainer.",Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
Update Trainer flag docs for strategy (#10042),0.6657591,"All TrainingTypePlugins have been renamed to Strategy (#11120). Strategy is a more appropriate name because it encompasses more than simply training communcation. This change is now aligned with the changes we implemented in 1.5, which introduced the new strategy and devices flags to the Trainer.",Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
Rerun flaky profiler tests on failure (#10035),0.5984392,Moved profilers to their own file (#7822),,0
Default to precision=bf16 on CPU when precision=16 is passed (#10033),0.99999976,Default to precision=bf16 on CPU when precision=16 is passed (#10033),,1
remove dataloader patching on the LightningModule (#9764),0.76218534,"Removed automatic patching of {train,val,test,predict}_dataloader() on the LightningModule (#9764)",Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: rohitgr7 rohitgr1998@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Add typing to callbacks (#10001),0.62915325,Support for user-defined callbacks (#889 and #950),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Support serialized checkpoint loading (#9605),0.71529204,Checkpoint saving and loading extensibility:,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Constrain IPU precision choices (#10030),0.5431479,Mixed precision overhaul (#16783),,0
[CLI] Shorthand notation to instantiate datamodules (#10011),0.92627454,Support shorthand notation to instantiate datamodules (#10011),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Update TPUSpawnPlugin spawn methods (#10022),0.6839019,"Removed process_idx from the {DDPSpawnPlugin,TPUSpawnPlugin}.new_process methods (#10022)",,0
"Fix self.log(on_epoch=True, reduce_fx=sum) on_batch_start (#9791)",0.6247291,Allow passing self.log(batch_size=...) (#7891),,0
Rename TPUHalfPrecisionPlugin to TPUBf16PrecisionPlugin (#10026),0.96299446,Renamed TPUHalfPrecisionPlugin to TPUBf16PrecisionPlugin (#10026),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Remove reset_train_val_dataloaders from Trainer and move data reloading logic to loop (#9671),0.8758484,"Removed the deprecated Trainer.reset_train_val_dataloaders() in favor of Trainer.reset_{train,val}_dataloader (#16131)",Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com,1
Update setup logic in training type plugins (deepspeed) [2 / n] (#10009),0.6669442,"Implemented DeepSpeedPlugin._setup_model_and_optimizers (#10009, #10064)",Co-authored-by: Sean Naren sean@grid.ai Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Add TPUPrecisionPlugin (#10020),0.68233144,Renamed TPUHalfPrecisionPlugin to TPUBf16PrecisionPlugin (#10026),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Update setup logic in training type plugins (data-parallel) [3 / n] (#10010),0.6437801,Automatically set sync_batchnorm for training_type_plugin (#6536),Co-authored-by: thomas chaton thomas@grid.ai,0
Update setup logic in training type plugins [1 / n] (#9994),0.6836559,"- TrainingTypePlugin.{call_configure_sharded_model_hook, on_reset_*_dataloader, on_save, post_optimizer_step, update_global_step}",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Set the optimization output result class as a class attribute (#9977),0.5998135,Changed automatic_optimization to be a model attribute (#4602),,0
Update kfold example to avoid ci failures (#10019),0.40283456,"trainer.ckpt_path = ""/path/to/checkpoint.ckpt""",,0
Add check for callable with datamodule len (#10003),0.52493066,Support shorthand notation to instantiate datamodules (#10011),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
Fix _compare_version and add _TORCH_GREATER_EQUAL_DEV_1_10 (#9960),0.7027739,- Fixed an issue with comparing torch versions when using a version of torch built from source ([#17030](https://github.com/Lightning-AI/lightning/pull/17030)),,1
Add DDPSpawnPlugin.spawn() (#10018),0.68730974,- Removed argument `return_result` from the `DDPSpawnPlugin.spawn()` method ([#10867](https://github.com/PyTorchLightning/pytorch-lightning/pull/10867)),,0
Remove deprecated distributed_backend from Trainer (#10017),1.0,Remove deprecated distributed_backend from Trainer (#10017),  rm distributed_backend from Trainer   unused   chlog   internal distributed_backend   Docstring   Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
Simplify track grad norm condition (#9992),0.67363185,Gradient norm tracking (#16745),,0
Update backward hook for PrecisionPlugin  (#10008),0.8642757,The PrecisionPlugin.backward hooks no longer returns a value (#8328),Co-authored-by: thomas chaton thomas@grid.ai,1
Make pytest not run .github/* (#10012),0.58133733,Remove pytest as a requirement to run app by @manskx in https://github.com/Lightning-AI/lightning/pull/15449,,0
Make verify_loop_configurations a utility function (#9976),0.5097827,Enabling val/test loop disabling (#2692),,0
Don't raise DeprecationWarning for LoggerConnector.gpus_metrics (#9959),0.6889718,"Deprecated log_gpu_memory, gpu_metrics, and util funcs in favor of DeviceStatsMonitor callback (#9921)",,0
Update docs for base Loop class with examples (#9993),0.5151316,"Refactored internal loop interface; added new classes FitLoop, TrainingEpochLoop, TrainingBatchLoop (#7871, #8077)",Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
Add typing for LightningOptimizer (#9990),0.56868184,class MyLightningModule(pl.LightningModule):,,0
Add KFold Loop example (#9965),0.47228113,Loop customization:,,0
Add unit tests for pl.utilities.grads (#9765),0.4912206,"def on_test_start(self, trainer, pl_module):",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Fix logic to check for spawn in worker_check (#9902),0.5425093,Skip broadcast if distributed not initialized for the spawn plugins (#8017),  fix   update tests   chlog   skip windows ,0
Introduce PrecisionPlugin.forward_context() (#9988),0.74447775,Deprecated PrecisionPlugin.master_params() in favor of PrecisionPlugin.main_params() (#10105),Co-authored-by: thomas chaton thomas@grid.ai,1
Remove manual tracking of optimizer steps (#9957),0.6352382,Executing the optimizer_closure is now required when overriding the optimizer_step hook (#9360),,0
Update resume_from_checkpoint docs (#9952),0.7960818,Changed resume_from_checkpoint warning to an error when the checkpoint file does not exist (#7075),,1
Remove deprecated DataModule.dims usage in tests (#9948),0.6502142,"Deprecated DataModule properties: has_prepared_data, has_setup_fit, has_setup_validate, has_setup_test, has_setup_predict, has_teardown_fit, has_teardown_validate, has_teardown_test, has_teardown_predict (#7657)",,0
Fix self.log(on_epoch=True) on_batch_start (#9780),0.6725296,Allow passing self.log(batch_size=...) (#7891),,0
reset val dataloader for binsearch (#9975),0.8157079,Reset val_dataloader in tuner/batch_size_scaling for binsearch (#9975),,1
loop customization docs  (#9609),0.8624034,Loop customization improvements,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com,1
Update accelerator connector messages after the addition of strategy (#9937),0.55457515,Removed deprecated connect_precision_plugin and connect_training_type_plugin from Accelerator (#9019),,0
Fix LightningOptimizer step and toggling logic (#9958),0.61072063,LightningOptimizer manual optimizer is more flexible and expose toggle_model (#5771),,0
Fix issue with no-init dataclass fields in move_to_device (#9963),0.52435637,Removed obsolete self._device in Trainer (#1849),Co-authored-by: ronif ronif@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Avoid deprecation warning after #9901 (#9951),0.8228565,Deprecation warning (#3844),,1
Fixed use of LightningCLI in computer_vision_fine_tuning.py example (#9934),0.6393972,- Added `args` parameter to `LightningCLI` to ease running from within Python ([#14596](https://github.com/Lightning-AI/lightning/pull/14596)),,0
(1/n) tests: Use strategy flag instead of accelerator for training strategies (#9931),0.6391349,"In this release, we've made some large changes to achieve that goal. Not to worry, though! The only users affected by these changes are those who use custom implementations of Accelerator and Strategy (TrainingTypePlugin) as well as certain Plugins. In particular, we want to highlight the following changes:",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Use non-deprecated options in tests (#9949),0.53884935,Deprecated the TestTubeLogger (#9065),,0
Validate the precision input earlier (#9763),0.6585587,    precision=16,,0
Add support for len(datamodule) (#9895),0.6130985,Support shorthand notation to instantiate datamodules (#10011),Co-authored-by: tchaton thomas@grid.ai,0
"Deprecate log_gpu_memory, gpu_metrics, and util funcs in favor of DeviceStatsMonitor callback (#9921)",0.98383594,"Deprecated log_gpu_memory, gpu_metrics, and util funcs in favor of DeviceStatsMonitor callback (#9921)",Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
Single-process multi-node CPU training (#9603),0.6486831,Speed up single-core TPU training by loading data using ParallelLoader (#2033),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: thomas chaton thomas@grid.ai,0
Refactor tests for TPU Accelerator (#9718),0.6083199,Refactored into accelerator module:,Co-authored-by: tchaton thomas@grid.ai,0
Deprecate GPUStatsMonitor and XLAStatsMonitor in favor of DeviceStatsMonitor (#9924),0.93953127,Deprecated GPUStatsMonitor and XLAStatsMonitor in favor of DeviceStatsMonitor callback (#9924),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Nicki Skafte Detlefsen skaftenicki@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
[2/n] Directly call TrainingTypePlugin APIs instead of going through the Accelerator (#9901),0.8365207,"The Trainer now calls TrainingTypePlugin collective APIs directly instead of going through the Accelerator reference (#9677, #9901)",Co-authored-by: tchaton thomas@grid.ai,1
Log LR using LearningRateMonitor even when LR Scheduler is not defined.  (#9786),0.69383913,Disabled lr_scheduler.step() in manual optimization  (#6825),"  LR logging works even with no lr scheduler, wrote few extra tests as well   updated changelog   modified code as suggested by DeepSource   added helper functions   opt with no scheduler   rename   chlog   update test   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: rohitgr7 rohitgr1998@gmail.com",0
[2/4] Add DeviceStatsMonitor callback (#9712),0.6363466,Deprecated GPUStatsMonitor and XLAStatsMonitor in favor of DeviceStatsMonitor callback (#9924),Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Kaushik B kaushikbokka@gmail.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,0
Add configure_gradient_clipping hook in LightningModule (#9584),0.79704285,"By overriding the LightningModule.configure_gradient_clipping hook, you can customize gradient clipping to your needs:",  init hook   docs   dep train args   update tests   doc   doc   .gitignore   not dep   add trainer args   add & update tests   fix tests   pre-commit   docs   add docs   add exception   code review   deepspeed   update tests   not   try fix   Apply suggestions from code review   update deepspeed   disable some tests   disable some tests   enable all tests ,1
Add strategy argument to Trainer (#8597),0.75700086,"trainer = Trainer(strategy=""colossalai"")",Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
Add enable_model_summary flag and deprecate weights_summary (#9699),0.69048065,Changed the default value of the Trainer argument weights_summary from full to top (#2029),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Kaushik B kaushikbokka@gmail.com,0
Remove should_rank_save_checkpoint property from Trainer (#9433),0.9578277,Removed should_rank_save_checkpoint property from Trainer (#9433),,1
Remove epoch from trainer.logged_metrics (#9904),1.0000002,Remove epoch from trainer.logged_metrics (#9904),,1
Mark Trainer.terminate_on_nan protected and deprecate public property (#9849),0.8826673,Deprecated Trainer.terminate_on_nan public attribute access (#9849),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Remove type error handling in _configure_checkpoint_callbacks (#9823),0.7053813,Saved checkpoints will no longer use the type of a Callback as the key to avoid issues with unpickling (#6886),  remove type error handling in _configure_checkpoint_callbacks   rm test   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
CombinedLoader example fix (#9906),0.638969,"        return CombinedLoader(iterables, mode=""min_size"")",,0
guard against None in pytorch get_xla_supported_devices (#9572),0.59849274,Drop PyTorch 1.9 support (#15347),Co-authored-by: Chris Chow cchow@nianticlabs.com Co-authored-by: thomas chaton thomas@grid.ai,0
update tests to not rely on patched dataloaders (#9905),0.562709,"Redesigned multi-dataloader support (#16743, #16784, #16939)",,0
Update docs for GradientAccumulationScheduler (#9891),0.60474455,from lightning.pytorch.callbacks import GradientAccumulationScheduler,  update docs and add tests   update docs and add tests   Update pytorch_lightning/callbacks/gradient_accumulation_scheduler.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Apply suggestions from code review  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
update docs (#9903),0.55672264,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",,0
"Raise a MisconfigurationException when trainer functions are called with ckpt_path=""best"" but checkpoint_callback isn't configured (#9841)",0.95433545,"Trainer now raises a MisconfigurationException when its methods are called with ckpt_path=""best"" but a checkpoint callback isn't configured (#9841)",  add check   chlog   Apply suggestions from code review   Co-authored-by: ananthsub ananth.subramaniam@gmail.com  Apply suggestions from code review  Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: thomas chaton thomas@grid.ai,1
Update error message for interactive incompatible plugins (#9896),0.932931,Updated error message for interactive incompatible plugins (#9896),Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,1
DeepSpeed support for device IDs (#9847),0.58894235,Support for manual optimization with DeepSpeed (#7970),,0
Mark trainer.config_validator as protected (#9779),0.72995377,Trainer is now raising a MisconfigurationException instead of a warning if Trainer.{validate/test} is missing required methods (#10016),,1
Deprecate checkpoint_callback from the Trainer constructor in favour of enable_checkpointing (#9754),0.98179865,Deprecated checkpoint_callback from the Trainer constructor in favor of enable_checkpointing (#9754),"  enable_chekpointing   update codebase   chlog   update tests   fix warning   Apply suggestions from code review   Co-authored-by: ananthsub ananth.subramaniam@gmail.com  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci  Apply suggestions from code review  Co-authored-by: ananthsub ananth.subramaniam@gmail.com  Apply suggestions from code review  Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",1
Fix deprecation test version for accelerator collective (#9892),0.6473185,"Deprecated Accelerator collective API barrier, broadcast, and all_gather in favor of calling the TrainingTypePlugin collective API directly (#9677)",,0
Clarify lr scheduler frequency (#9843),0.63236314,lr_scheduler now activated after epoch    ,,0
"Update DeepSpeed version, fix failing tests (#9898)",0.65159845,"Updated DeepSpeed ZeRO (#6546, #6752, #6142, #6321)",,0
fix qconfig import for pytorch 1.10 (#9899),0.6800133,Enable PyTorch 1.7 compatibility (#3541),,0
Prepare v1.5.0rc0 (#9893),0.49018365,[1.5.4] - 2021-11-30,,0
Deprecate terminate_on_nan Trainer argument in favor of detect_anomaly (#9175),0.9729917,Deprecated Trainer argument terminate_on_nan in favor of detect_anomaly(#9175),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
remove redundant accumulation normalization in manual optimization (#9769),0.66492325,Refactored the logic around manual and automatic optimization inside the optimizer loop (#7526),,0
Removed a redundant warning with ModelCheckpoint(monitor=None) callback (#9875),1.0000002,Removed a redundant warning with ModelCheckpoint(monitor=None) callback (#9875),  Update README.md   Update README.md   Create evaluation.py   Update README.md   Update evaluation.py   Create evaluation.py   Create evaluation.py   Update evaluation.py   Create nlp.py   Update evaluation.py   Create evaluation.py   Update nlp.py   Update nlp.py   Update evaluation.py   Create evaluation.py   Update nlp.py   Update nlp.py   Update requirements.txt   Update evaluation.py   Create data_loader.py   Update nlp.py   Update evaluation.py   Update data_loader.py   Update nlp.py   Update data_loader.py   Update requirements.txt   Update model_checkpoint.py   Delete evaluation.py   Delete data_loader.py   Delete nlp.py   Update requirements.txt   Update model_checkpoint.py   Update README.md   Update pytorch_lightning/callbacks/model_checkpoint.py   Update CHANGELOG.md   Update test_model_checkpoint.py   Update model_checkpoint.py   update   update   chlog update   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
use existing logic to configure optimizers in lr_finder (#9789),0.61694384,Refactored optimizer (#4658),  use predefined logic   patch init_optimizers   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
[docs] Add Torch Distributed Run (#9890),0.7545144,Let TorchCollective works on the torch.distributed WORLD process group by default (#16995),,1
Updated quantization imports in PyTorch 1.10 (#9878),0.70269835,"Following our 4 PyTorch release window, this release supports PyTorch 1.8 to 1.11. Support for PyTorch 1.7 has been removed.",Co-authored-by: tchaton thomas@grid.ai Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
disable_logger (#9837),0.7024712,Change default logger to a dedicated one (#1064),,1
feat(wandb): support media logging (#9545),0.59075713,Changed WandbLogger(log_model={True/'all'}) to log models as artifacts (#6231),,0
use public format checkpoint method (#9818),0.54897547,    # put all logic related to saving a checkpoint here,  use public method   document   Apply suggestions from code review ,0
Reset val_dataloader in tuner/batch_size_scaling (#9857),0.9999999,Reset val_dataloader in tuner/batch_size_scaling (#9857),  reset val   chlog ,1
Fix typo in _validate_scheduler_optimizer() (#9886),0.6298521,Disabled lr_scheduler.step() in manual optimization  (#6825),,0
use ModuleNotFoundError instead of ImportError (#9867),0.84398174,Module imports are now catching ModuleNotFoundError instead of ImportError (#9867),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Correct function name (#9859),0.42611718,- Removed the deprecated code in:,,0
Add support for torch.set_detect_anomaly (#9848),0.70152175,Corrected call to torch.no_grad (#5124),  Add support for detect_anomaly   Update CHANGELOG.md ,1
Deprecate dataloader_idx from on_train_batch_start/end (#9816),0.70040286,"Overriding the on_train_batch_{start,end} hooks in conjunction with taking a dataloader_iter in the training_step no longer errors out and instead shows a warning (#16062)",  deprecate hooks   dep todo   explicit   Apply suggestions from code review   Apply suggestions from code review   code review   base ,1
Fix test_quantization with Pytorch 1.10 (#9808),0.7083705,Skipped testing with PyTorch 1.7 and Python 3.9 on Ubuntu (#11217),,1
Update the logic to check for accumulation steps with deepspeed (#9826),1.0000002,Update the logic to check for accumulation steps with deepspeed (#9826),  support_dict   chlog   fix test   epochs ,1
Fix restoring training state during trainer.fit only (#9413),0.7176781,Removed trainer.fit() return value of 1. It has no return now (#7237),  reload state on fit   trainer.state   add test   chlog   revert   review   review   rev and ammend   fix test and logic   update   code review   Apply suggestions from code review   better assertions   better assertions   Apply suggestions from code review   add loop test   Apply suggestions from code review   Split for typing   review comments   review comments   use if_else   code review   code review   code review   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   Remove unnecessary pieces from the test   move test   Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
rename callback FineTune arg round (#9711),0.5006504,Removed deprecated callbacks (#3979), rename CB Tune arg round  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Enable auto parameters tying for TPUs (#9525),0.74022186,Enabled manual optimization for TPUs (#8458),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
Fix missing arguments when saving hyperparams from parent class only (#9800),0.6919771,Add ignore param to save_hyperparameters (#6056),  Fix missing arguments when saving hyperparams from parent class only   fix antipattern ,0
Resolved wrong mv usage for extracted directory (#9678),0.43554807,Rename failed -> error in tables (#15608),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
Fix broken test_is_picklable with PT1.10 (#9810),0.6872994,Make the is_picklable function more robust (#17270),,0
[pre-commit.ci] pre-commit suggestions (#9819),0.58897907,@akihironitta @awaelchli @Borda @carmocca @kaushikb11 @krshrimali @mauvilsa @otaj @pre-commit-ci @rohitgr7 @semaphore-egg @tkonopka @wayi1,,0
Make DDP and Horovod batch_size scaling examples explicit (#9813),0.5981478,"    self.scale_batch_size(trainer, pl_module)",Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
Fix broken test_cpu_amp_precision_context_manager (#9809),0.514617,"  * The `PrecisionPlugin.optimizer_step` signature changed: The `model`, `optimizer_idx` and `closure` arguments need to be passed as keyword arguments now",  @RunIf(min_gpus=1)   dtype -> fast_dtype ,0
Add warnings regarding unsupported keys in optim config and OneCycleLR (#9666),0.5250721,Improved error messages for invalid configure_optimizers returns (#3587),"  Add warnings regarding unsupported keys in optim config and OneCycleLR   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Fix docstring   Update CHANGELOG.md   Split  into two parts   Use difference operator to find extra keys   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
[Feat] Add auto_restart for fault tolerant training (#9722),0.735908,PL_FAULT_TOLERANT_TRAINING=1 python train.py,,1
Remove return value from the backward closure (#9770),0.5410538,    return None,,0
Fix Deepspeed and lightning calling scheduler (#9788),0.6679551,"- Configure native Deepspeed schedulers with interval='step' ([#11788](https://github.com/PyTorchLightning/pytorch-lightning/pull/11788)), ([#12031](https://github.com/PyTorchLightning/pytorch-lightning/pull/12031))",,0
Raise an exception if using amp_level with native amp_backend (#9755),0.70239115,Trainer now raises an exception when requesting amp_level with native amp_backend (#9755),  add exception   chlog   code review   Apply suggestions from code review   Co-authored-by: thomas chaton thomas@grid.ai,1
Fix some flaky tests in tuner/lr_finder (#9766),0.63488686,tuner.lr_find(...),  update tests   fix more tests ,0
update changelog after 1.4.9 release (#9762),0.62668383,Full Changelog,,0
Merge pull request #9690 from PyTorchLightning/feature/codeowners-rohit,0.62044656,Full commit list: https://github.com/PyTorchLightning/pytorch-lightning/compare/1.5.0...1.6.0,add Rohit Gupta to default codeowners,0
disable warnings summary for pytest (#9743),0.68175113,Do not override PYTHONWARNINGS (#4700),,0
Add support for torch.use_deterministic_algorithms (#9121),0.7545248,Corrected call to torch.no_grad (#5124),  re-add changes   Update test_data_parallel.py   Update CHANGELOG.md   Update test_legacy_checkpoints.py   Update test_horovod.py   Update test_horovod.py   Update accelerator_connector.py   update tests ,1
Refactor grad_norm function (#9742),0.75402814,"def log_grad_norm(self, grad_norm_dict):",,1
Remove unnecessary pytest.param usage (#9760),0.5655304,            find_unused_parameters=True,,0
[3/n] add additional rich version check (#9757),0.6375279,Improved exception message if rich version is less than 10.2.2 (#10839),,0
[2/n] Fix rich model summary for tuples (#9756),0.47302222,Model summary: add 1 decimal place (#4745),,0
Remove legacy pytest markers (#9761),0.47617728,Avoid metadata.entry_points deprecation warning on Python 3.10 (#14052),,0
IPU hotfix for #9721 (#9759),0.4696238,Moveed HPU broadcast override to the HPU strategy file (#17011),,0
Remove _NATIVE_AMP_AVAILABLE checks (#9747),0.6533486,enable_pl_optimizer=False by default to temporarily fix AMP issues (#5163),,0
Remove duplicated native AMP + LBFGS check (#9748),0.45618838,Drop duplicate metrics (#5014),,0
Remove unused post_optimizer_step (#9746),0.65284413,Executing the optimizer_closure is now required when overriding the optimizer_step hook (#9360),,0
Update allowlist for leaked env variables set by XLA (#9753),0.45581123,Renamed xxx_AVAILABLE as protected (#5082),,0
[1/n] Add support for iterable datasets to Rich ProgressBar (#9734),0.6678418,New Rich Progress Bar,,0
Convert error to warning for logging without a Trainer (#9733),0.7674521,self.log-ing without a Trainer reference now raises a warning instead of an exception (#9733),Co-authored-by: thomas chaton thomas@grid.ai,1
Fix docstring in saving.py (#9738),0.489963,Moved save_function to accelerator (#6689),,0
[Refactor] Simplify data loading logic around replacing sampler to prevent confusion (#9721),0.72228026,Improved error messages in replace_sampler when the DataLoader attributes are not included in the signature or the signature is missing optional arguments (#8519),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
Remove training_epoch_end outputs check (#9719),0.79962707,"def training_epoch_end(self, outputs):",Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
reduce loop structure leakage into the TrainingEpochLoop (#9490),0.67785275,"Refactored TrainingBatchLoop and extracted OptimizerLoop, splitting off automatic optimization into its own loop (#9191)",Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Removed deprecated property is_using_torchelastic from AcceleratorConnector (#9729),0.8867951,Removed deprecated property AcceleratorConnector.is_using_torchelastic in favor of TorchElasticEnvironment.is_using_torchelastic() (#9729),,1
[bugfix] Prevent on_before_batch_transfer to be called twice (#9715),0.6412572,Disabled batch transfer in DP mode (#6098),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
[bugfix] Resolve metrics not being properly resetted on validation epoch end (#9717),0.5958611,Validation is now always run inside the training epoch scope (#7357),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
Make HorovodPlugin.all_gather return a tensor (#9696),0.8840996,- HorovodPlugin.all_gather now returns a torch.Tensor instead of a list., Make HorovodPlugin.all_gather consistent with other plugins,1
remove InternalDebugger (#9680),0.52923167,Removed pytorch_lightning.utilities.debugging.InternalDebugger (#9680),"  wip   reset _notebooks   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   reset _notebooks   testing with mock   update test with mock   update test   update tests   update test   remove track_load_dataloader_calls   update last test   remove unused imports   remove InternalDebugger   update changelog   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com",0
Call TrainingTypePlugin collective functions directly instead of going through the Accelerator  (#9677),0.7793868,"The Trainer now calls TrainingTypePlugin collective APIs directly instead of going through the Accelerator reference (#9677, #9901)",Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
[1/4] Add get_device_stats to accelerator interface (#9586),0.6823385,"To simplify this process, we've deprecated the per-accelerator properties to have accelerator agnostic properties. For example:",,0
Fix lr_find to generate same results on multiple calls (#9704),0.5852244,tuner.lr_find(...),  dump global_step   add test   chlog ,0
Deprecate stochastic_weight_avg from the Trainer constructor (#8989),0.8939984,Deprecated passing stochastic_weight_avg to the Trainer constructor in favor of adding the StochasticWeightAveraging callback directly to the list of callbacks (#8989),  Deprecate stochastic_weight_avg from the Trainer constructor   Update CHANGELOG.md   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
Update warnings in TrainingTricksConnector (#9595),0.6971299,Removed the deprecated TrainerTrainingTricksMixin class (#8679),  update warnings   add tests   comments   Apply suggestions from code review   Apply suggestions from code review ,0
Deprecate LightningDistributed and keep logic in ddp/ddpSpawn directly (#9691),0.7749261,Deprecated LightningDistributed and moved the broadcast logic to DDPPlugin and DDPSpawnPlugin directly (#9691),  Deprecate LightningDistributed and keep logic in ddp/ddpSpawn directly   Deprecate LightningDistributed and keep logic in ddp/ddpSpawn directly   Deprecate LightningDistributed and keep logic in ddp/ddpSpawn directly   Deprecate LightningDistributed and keep logic in ddp/ddpSpawn directly   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Apply suggestions from code review  Co-authored-by: ananthsub ananth.subramaniam@gmail.com   Deprecate LightningDistributed and keep logic in ddp/ddpSpawn directly   Update pytorch_lightning/distributed/dist.py   Co-authored-by: ananthsub ananth.subramaniam@gmail.com   Deprecate LightningDistributed and keep logic in ddp/ddpSpawn directly   Apply suggestions from code review   Co-authored-by: ananthsub ananth.subramaniam@gmail.com   Apply suggestions from code review   Apply suggestions from code review   Deprecate LightningDistributed and keep logic in ddp/ddpSpawn directly   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com,1
Optimize non-empty directory warning check in model checkpoint callback (#9615),0.62053174,Skipped best_model_path if checkpoint_callback is None (#2962),"  pt1 dir empty check   clean imports   bring back resolve mkdir?   original doc   warningcache   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   cp callback after resolve   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  move global_zero check outside warn fn  Co-authored-by: ananthsub ananth.subramaniam@gmail.com  move global_zero check outside warn fn 2  Co-authored-by: ananthsub ananth.subramaniam@gmail.com  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com",0
Use a unique filename to save temp ckpt in tuner (#9682),0.70339024,The tuner now usees a unique filename to save a temporary checkpoint (#9682),  unique filename   chlog   update tests ,1
move get_active_optimizers to utilities (#9581),0.6434264,Disabled optimizers setup during testing (#3059),,0
Add enable_progress_bar to Trainer constructor (#9664),0.87998235, - Progress bar: `Trainer(enable_progress_bar=True)`,,1
Rename reset_on_epoch to reset_on_run (#9658),0.9072336,Renamed reset_on_epoch to reset_on_run (#9658),,1
Fix accumulate_grad_batches on init (#9652),0.8325161,The default value of accumulate_grad_batches has changed from 1 to None (#9652).,  fix accumuate_grad_batches on init   chlog   update error   move to callback connector   add test with callback   fix tests   Update pytorch_lightning/trainer/connectors/callback_connector.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   update ipu logic   rev   rev   rev   pls work   code review   Co-authored-by: Rohit Gupta goku@rmac.local Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Enable DataLoader state restoration for the evaluation loop  (#9563),0.6768564,"Refactored evaluation loop interface; added new classes DataLoaderLoop, EvaluationLoop, EvaluationEpochLoop (#7990, #8077)",Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
Support skipping to validation (#9681),0.6370352,"Simplified ""should run validation"" logic (#7682)",Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
remove InternalDebugger.track_load_dataloader_call (#9675),0.6000186,Removed InterBatchProcessor in favor of DataLoaderIterDataFetcher (#9052),"  wip   reset _notebooks   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   reset _notebooks   testing with mock   update test with mock   update test   update tests   update test   remove track_load_dataloader_calls   update last test   remove unused imports   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
Report leaking environment variables in tests (#5872),0.4860512,Resolve memory leak for evaluation (#6326),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
Add torch v1.11.0 to the list of versions in adjust_versions.py (#9679),0.6851567,- Fixed an issue with comparing torch versions when using a version of torch built from source ([#17030](https://github.com/Lightning-AI/lightning/pull/17030)),,0
Remove call_configure_sharded_model lifecycle property (#9612),0.6690157,Removed call_configure_sharded_model_hook property from Accelerator and TrainingTypePlugin (#9612),,0
Use searchsorted over argmax (#9670),0.4440749,Refactored optimizer (#4658),,0
Disallow invalid seed string values (#8787),0.6683589,seed_everything now fails when an invalid seed value is passed instead of selecting a random seed (#8787),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Add is_last_batch to progress tracking (#9657),0.60488904,Do not reset the progress tracking dataclasses total counters (#8475),,0
Remove InternalDebugger.track_event (#9654),0.5602128,Removed pytorch_lightning.utilities.debugging.InternalDebugger (#9680),,0
Use completed over processed in reset_on_restart (#9656),0.9999997,Use completed over processed in reset_on_restart (#9656),,1
add legacy load utility (#9166),0.55004156,remove weight loading hack for ddp_cpu (#3808),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Deprecate progress_bar_refresh_rate from Trainer constructor (#9616),0.8804126,"Deprecated passing progress_bar_refresh_rate to the Trainer constructor in favor of adding the ProgressBar callback with refresh_rate directly to the list of callbacks, or passing enable_progress_bar=False to disable the progress bar (#9616)",,1
Reset metrics before each task starts (#9410),1.0,Reset metrics before each task starts (#9410), reset metrics,1
Remove InternalDebugger.track_lr_schedulers_update (#9653),0.6022783,Disabled lr_scheduler.step() in manual optimization  (#6825),,0
[CLI] Add option to enable/disable config save to preserve multiple files structure,0.76525235,  * `save_config_multifile`,,1
update changelog after 1.4.8 release (#9650),0.62827665,Full Changelog,,0
fix modify _DDPSinkBackward view inplace error for pytorch nightly 1.10 (#9649),0.67258835,  * Removed the `pytorch_lightning.strategies.sharded.DDPShardedStrategy` (ddp_sharded) class,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
revert #9125 / fix back-compatibility with saving hparams as a whole container (#9642),0.53405195,Support **DictConfig for hparam serialization (#2519),,0
Improvements for rich progress (#9579),0.5657605,New Rich Progress Bar,,0
[CLI] Shorthand notation to instantiate models (#9588),0.9296398,Support shorthand notation to instantiate models (#9588),,1
Fix ResultCollection._get_cache with multielement tensors (#9582),0.6084656,Changed HorovodPlugin.all_gather to return a torch.Tensor instead of a list (#9696),,0
Fix broken links to PyTorch Lightning Bolts (#9634),0.60064983,- Rewrote `accelerator_connector` ([#11448](https://github.com/PyTorchLightning/pytorch-lightning/pull/11448)),,0
Fix gradient accumulation for ShardedDataParallel (#9122),0.51812077,- Enable gradient accumulation using Horovod's `backward_passes_per_step` ([#11911](https://github.com/PyTorchLightning/pytorch-lightning/pull/11911)),  Fix gradient accumulation for ShardedDataParallel   Update changelog   Update pytorch_lightning/plugins/training_type/sharded.py   add test   Update test_sharded_plugin.py   Update test_sharded_plugin.py   Update test_sharded_plugin.py ,0
Fixing order of operations bug in Qnet (#9621),0.4386428,Resolved a bug about a race condition when sending the work state through the caller_queue (#14074),,0
Update versions after recent PyTorch releases (#9623),0.8039457,Dropped official support/testing for older PyTorch versions <1.3 (#1917),,1
Prune DeprecatedTrainerAttributes (#9598),0.6781711,Removed deprecated TrainResult (#5323),,0
[CLI] Avoid warning when configure_optimizers will not be overridden (#9583),0.7822418,Improved error messages for invalid configure_optimizers returns (#3587),,1
[CLI] Fix registry decorator return value (#9587),0.47166097,Enabled manual returns (#4089),,0
Deprecate LightningLoggerBase.close (#9422),0.89641404,"Deprecated LightningLoggerBase.close, LoggerCollection.close in favor of LightningLoggerBase.finalize, LoggerCollection.finalize (#9422)","  deprecate loggerbase.close   deprecate warning   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   add to changelog   fix import   fix import alphabetize   spacing?   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   copy-paste avoid pre-commit.ci?   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   literally match the other comment   unindent   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   suggest finalize instead of save   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Update tests/loggers/test_base.py   format but to be formatted   Update pytorch_lightning/loggers/base.py   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Update pytorch_lightning/loggers/base.py   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  Update pytorch_lightning/loggers/base.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",1
Fix missing url (#9602),0.5285708,Updated app URLs to the latest format (#16568),,0
Put back initialization of properties in trainer (#9594),0.7611393,Removed TrainerProperties mixin and moved property definitions directly into Trainer (#9495),,1
Fix typo in LightningModule.configure_callbacks() (#9591),0.6604874,- Added support for returning a single Callback from `LightningModule.configure_callbacks` without wrapping it into a list ([#11060](https://github.com/PyTorchLightning/pytorch-lightning/pull/11060)),,0
Deprecate TrainerProperties Mixin and move property definitions directly into trainer.py (#9495),0.87487936,Removed TrainerProperties mixin and moved property definitions directly into Trainer (#9495),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: thomas chaton thomas@grid.ai,1
Document exceptions in accelerators (#9558),0.58042365,Refactored Accelerators and Plugins (#5743),  Document exceptions in ipu.py   Document exceptions in tpu.py   Document exceptions in gpu.py ,0
[Feat] Add graceful detection of signal to exit + SignalConnector and merge SlurmConnector. (#9566),0.5618967,trainer = Trainer(plugins=[SLURMEnvironment(requeue_signal=signal.SIGHUP)]),Co-authored-by: Sean Naren sean@grid.ai,0
Improve collision check on hparams between LightningModule and DataModule  (#9496),0.6276046,Changed logging of LightningModule and LightningDataModule hyperparameters to raise an exception only if there are colliding keys with different values (#9496), fix hyperparameter logging between LightningModule and DataModule,0
[CLI] Shorthand notation to instantiate callbacks [3/3] (#8815),0.60020477,Support passing lists of callbacks via command line (#8815),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
[CLI] Shorthand notation to instantiate optimizers and lr schedulers [2/3] (#9565),0.8241011,Support shorthand notation to instantiate optimizers and learning rate schedulers (#9565),,1
Fix missing deepspeed distributed call (#9540),0.5615458,"Removed deprecated properties DeepSpeedPlugin.cpu_offload* in favor of offload_optimizer, offload_parameters and pin_memory (#9244)",,0
[Refactor]  1/2 Move reset_on_restart within the loop reset (#9561),0.7085667,Call reset_on_restart in the loop's reset hook instead of when loading a checkpoint (#9561),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
Add submodule update to contributing (#9578),0.36323598,1. Override LightningModule hook,,0
[CI] Make the bot less aggressive (#9575),0.42208934,(#16002),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Add RichModelSummary callback (#9546),0.6211985,ModelSummary Callback,,0
Fix add_argparse_args raising TypeError with Python 3.6 (#9554),0.70520806,"Alternatively, you can add the argparse arguments you want manually:python", Add test Accept TypeError for arg_type.args being None  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
Improvements for rich progress bar (#9559),0.89094496,New Rich Progress Bar,,1
Remove ABC from LightningModule (#9517),0.5508326,1. Override LightningModule hook,,0
Update notebooks submodule and add tutorial view to docs (#9420),0.39622605,Docs improvements,Co-authored-by: Ethan Harris ethanwharris@gmail.com,0
Minor CLI improvements [1/3] (#9553),0.6429309,Introducing CLI commands for apps (#13602)!,,0
multiple optimizer restart with fault-tolerant training (#9537),0.6223118,Fault-tolerant training:,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
ci (#9522),0.40321788,[0.5.4] - 2022-08-01,,0
mark several methods in evaluation loops as protected (#9516),0.65146756,"Marked several methods in EvaluationEpochLoop as protected: on_evaluation_batch_start, evaluation_step, evaluation_step_end (#9516)",Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
mark FitLoop.should_accumulate as protected (#9515),0.9551284,Marked FitLoop.should_accumulate as protected (#9515),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
mark OptimizerLoop.backward method protected (#9514),0.91211987,Marked OptimizerLoop.backward as protected (#9514),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Add custom logic to each OutputResult subclass [2/2] (#9424),0.505312,Completely overhaul the Result object in favor of ResultMetric (#7882),,0
Update CHANGELOG.md following patch releases (#9536),0.5796001,Full Changelog,,0
Skip reconciliate_processes if used within a cluster environment that creates processes externally (#9389),0.63793916,Deprecated ClusterEnvironment.creates_children() in favor of ClusterEnvironment.creates_processes_externally (property) (#10106), [RFC] Skip reconciliate_processes if used within a cluster environment that creates processes externally,0
[bugfix] Resolve example after LightningCLI update (#9520),0.7507346,- Fixed ``LightningCLI`` signature parameter resolving for some lightning classes ([#13283](https://github.com/Lightning-AI/lightning/pull/13283)),,1
Reset datafetcher references in teardown (#9387),0.5659151,Replaced old prefetch iterator with new DataFetcher in training loop (#8953), Free references to data fetcher in data connector teardown,0
Type trainer.connectors.checkpoint_connector (#9419),0.67774415,- Marked `trainer.checkpoint_connector` as protected ([#11550](https://github.com/PyTorchLightning/pytorch-lightning/pull/11550)),Co-authored-by: ananthsub ananth.subramaniam@gmail.com,0
update GH templates' labels (#9295),0.36417025,Changed gradient logging to use parameter names instead of indexes (#660),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
fix optimizer loop with frequencies (#9507),0.6305054,- Extended optimizer support with particular frequency,,0
Add a png image for dark mode support (#9518),0.38135856,- Update `RichProgressBarTheme` styles after detecting light theme on colab ([#10993](https://github.com/PyTorchLightning/pytorch-lightning/pull/10993)),,0
[docs] updated docs for min_delta parameter in early stopping callback (#9513),0.6478009,Deprecate early_stop_callback Trainer argument (#3845),,0
move state extraction for CaptureMapDataset (#9484),0.3873759,"With DPStrategy, the batch is not explicitly moved to the device (#11780)",,0
Introduce parameter to fix deepspeed crash for RNNS (#9489),0.65219927,Expose DeepSpeed loss parameters to allow users to fix loss instability (#6115),,0
[bugfix] Always return batch indices to prevent duplicated logic for the users (#9432),0.49897248,"Removed output argument from *_batch_end hooks (#3965, #3966)",Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Keep global step update in the loop (#8856),0.63608325,+ global_step += 1,,0
Add OutputResult [1/2] (#9437),0.513879,Completely overhaul the Result object in favor of ResultMetric (#7882),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
move deprecated keys check sooner in checkpoint loading (#9494),0.5968096,all the checkpoint issues should be gone now (including backward support for old checkpoints),,0
Handle collision of user argument when using ShardedDDP (#9512),0.508533,Pass init args to ShardedDataParallel (#9483),  Handle collision of user argument   Add CHANGELOG.md ,0
deprecate flush_logs_every_n_steps on Trainer (#9366),0.8525371,"Deprecated passing flush_logs_every_n_steps as a Trainer argument, instead pass it to the logger init if supported (#9366)",,1
Pass args to ShardedDataParallel (#9483),0.8408556,Pass init args to ShardedDataParallel (#9483),,1
add pytorch profiler codeowner (#9479),0.68226606,| Import pytorch_lightning.profiler                                                                          | 1.9             | pytorch_lightning.profilers                   |,,0
add test for model weights equality when fault-tolerant training (#9481),0.5544236,PL_FAULT_TOLERANT_TRAINING=1 python train.py,,0
make model checkpointing test deterministic (#9457),0.6998379,Enable None model checkpoint default (#3669),,0
fix mypy typing for model summary (#9447),0.5499597,import my_code.models,,0
update to _has_prepared_data (#9411),0.50894946,group prepare data hook (#3212),,0
Add init_optimizers test for predict (#9440),0.6033458,    def configure_optimizers(self):,  add predict   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Add docs for datamodule hparams (#9428),0.6171231,    hparams_file='/path/to/hparams_file.yaml',Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
update rank_zero condition for logging summary (#9461),0.61380595,"Defines shared proc. rank, remove rank from instances (e.g. loggers) (#1408)",,0
Add on_exception hook to documentation (#9365),0.7443319,An on_exception Callback hook has been added which allows the user to perform custom exception handling.,  Small doc fixes   remove space   small fix   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Fix type hint for filepath (#9434),0.59904784,Deprecated filepath in ModelCheckpoint (#4213),,0
[bugfix] Revert inference mode support from #8813 (#9443),0.64958936,Inference mode support,Fixes #9431,0
Move add_to_queue/get_from_queue to DDPSpawnPlugin (#9118),0.756721,"Deprecated add_to_queue, get_from_queue from LightningModule in favor of corresponding methods in the DDPSpawnPlugin (#9118)",Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: tchaton thomas@grid.ai Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
Update torch_xla wheels installation link (#9436),0.5161265,Removed dependency on torchvision (#797),,0
fix resuming from checkpoint for fault-tolerant in case of no failure (#9371),0.7528416,Resuming from checkpoints (#16167),Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,1
Merge pull request #9438 from PyTorchLightning/feature/neptune-code-owners,0.57540524,Full commit list: https://github.com/PyTorchLightning/pytorch-lightning/compare/1.7.0...1.8.0,add neptune staff as code owners for neptune.py,0
Adapt NeptuneLogger to new neptune-client api (#6867),0.87537897,"It now supports neptune-client 0.16.16 and neptune >=1.0, and we have replaced the log() method with append() and extend().","  Initial split to NeptuneLegacyLogger and NeptuneLogger   Adapt NeptuneLogger to neptune-pytorch-lightning repo version   Fix stylecheck and tests   Fix style and PR suggestions   Expect Run object in NeptuneLogger.init   Model checkpoint support and restructured tests   Reformat code - use "" instead of '   Fix logging INTEGRATION_VERSION_KEY   Update CHANGELOG.md   Fix stylecheck   Remove NeptuneLegacyLogger   updated neptune-related docstrings   PR suggestions   update CODEOWERS file  move import logic to imports.py  minor neptune.py improvements   formatting fixes and minor updates   Fix generation of docs   formatting fixes and minor updates   fix   PR fixes vol. 2   define return type of _dict_paths method   bump required version of neptune-client   Enable log_* functions   Update pytorch_lightning/loggers/neptune.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Revert ""Enable log_* functions""  This reverts commit 050a436899b7f3582c0455dc27b171335b85a3a5.   Make global helper lists internal   Logger's name and version methods return proper results   Initialize Run and its name and id at logger init level   Make _init_run_instance static   Add pre-commit hook code changes.   Fix blacken-docs check   Fix neptune doctests and test_all   added docs comment about neptune-specific syntax   added docs comment about neptune-specific syntax in the loggers.rst   fix   Add pickling test   added myself to neptune codeowners   Enable some of deprecated log_* functions   Restore _run_instance for unpickled logger   Add step parameter to log_* functions   Fix stylecheck   Fix checkstyle   Fix checkstyle   Update pytorch_lightning/loggers/neptune.py   Co-authored-by: thomas chaton thomas@grid.ai   Fix tests   Fix stylecheck   fixed project name   Fix windows tests   Fix stylechecks   Fix neptune docs tests   docformatter fixes   De-duplicate legacy_kwargs_msg   Update .github/CODEOWNERS   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: kamil-kaczmarek kamil.kaczmarek@neptune.ml Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: thomas chaton thomas@grid.ai",1
[Refactor] Improve auto-encoder example (#9402),0.51953197,Refactored training loop (#2336),,0
"[docs] Clear up default logging, showing you don't need to pass a logger (#9408)",0.7078309,Change default logger to a dedicated one (#1064),,1
Loop and test restructuring (#9383),0.6648233,Refactored Loops,,0
feat: Add ModelSummary Callback (#9344),0.81044066,ModelSummary Callback,Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
Type the Loop base class as generic (#9418),0.4766609,"Refactored internal loop interface; added new classes FitLoop, TrainingEpochLoop, TrainingBatchLoop (#7871, #8077)",,0
Share the training step output data via ClosureResult (#9349),0.6060451,"Deprecated TrainResult and EvalResult, use self.log and self.write from the LightningModule to log metrics and write predictions. training_step can now only return a scalar (for the loss) or a dictionary with anything you want. (#3681)",,0
Disable benchmark ci on PRs (#9430),0.5483461,Disabled optimizers setup during testing (#3059),Co-authored-by: Jirka jirka.borovec@seznam.cz,0
Add remove_checkpoint to CheckpointIO plugin to simplify ModelCheckpo… (#9373),0.7400575,Refactored CheckpointConnector to offload validation logic to the CheckpointIO plugin (#9045),Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
[loops] Reset reference to dataloader iterator on run end (#9386),0.6504812,"Did not always create a DataLoader during reinstantiation, but the same type as before (if a subclass of DataLoader) (#1346)", [loops] Reset reference to dataloader iterator on run end,0
Fix name order in CITATION.cff (#9423),0.3585307,Re-enabled naming metrics in ckpt name (#3060),,0
Deprecate LightningModule.get_progress_bar_dict (#8985),0.87492085,- Removed deprecated `get_progress_bar_dict` property from `LightningModule` ([#12839](https://github.com/Lightning-AI/lightning/pull/12839)), Move get_progress_bar_dict from lightning module to progress bar callback,1
convert state to tuple explicitly when setting python random state (#9401),0.97388184,Converted state to tuple explicitly when setting Python random state (#9401),  convert state to tuple explicitly   update changelog ,1
Remove metrics & cluster_environments from CODEOWNERS (#9376),0.58137834,"Removed experimental Metric API (#3868, #3943, #3949, #3946), listed changes before final removal:",,0
CODEOWNERS: add wandb (#9374),0.4881633,Removed wandb logger's finalize method (#1193),,0
Remove redundant quotes in an error message (#9392),0.4325505,- all the file path errors with loggers (txs @awaelchli),,0
Fix hiddens type annotation (#9377),0.6225202,    # 6. Remove the return of `hiddens`,,0
Fix logging of nan parameters (#9364),0.5641288,Raised ValueError when a None value is self.log-ed (#7771), Fix logging of nan parameters,0
Enable inference mode for testing and predicting (#8813),0.66473013,Inference mode support,Co-authored-by: ananthsub ananth.subramaniam@gmail.com,0
rewrite and improve tests for truncated back-propagation (#9369),0.58484113,Truncated backpropagation through time (TBPTT) (#16172),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: thomas chaton thomas@grid.ai,0
Keep hidden state in the optimization loops (#9368),0.6609888,    hiddens = ...  # 3. Choose the initial hidden state,,0
Fix replace_sampler missing the batch size under specific conditions (#9367),0.64086634,Allow setting replace_sampler_ddp=True with a distributed sampler already added (#4273),,0
Enforce that the optimizer closure is executed when optimizer_step is overridden (#9360),0.84029126,Executing the optimizer_closure is now required when overriding the optimizer_step hook (#9360),,1
Remove checkpoint tracking from internal debugger (#9326),0.612248,"Disabled checkpointing, earlystopping and logging with fast_dev_run (#5277)",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Add ManualOptimization loop (#9266),0.59429926,Extracted ManualOptimization logic from TrainingBatchLoop into its own separate loop class (#9266),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: thomas chaton thomas@grid.ai,0
Add a warning to deepspeed when inferring batch size (#9221),0.82519436,Do not fail if batch size could not be inferred for logging when using DeepSpeed (#10438),,1
Run plugin closure before on_before_optimizer_step [1/2] (#9288),0.73323494,Executing the optimizer_closure is now required when overriding the optimizer_step hook (#9360),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
"Remove todo, ensure we only check rank 0 for deepspeed warning (#9311)",0.54979676,- Removed deprecated support for passing the `rank_zero_warn` warning category positionally ([#14470](https://github.com/Lightning-AI/lightning/pull/14470)),,0
Remove some incorrect comments in ddp.py (#9319),0.54890275,Changed the default of find_unused_parameters to False in DDP (#5185),,0
Add CITATION.cff (#9139),0.40629047,model reference not provided,Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Add test assertion (#9309),0.5497724,Updated app testing (#16000),,0
Clear reference to training loss at the end of train step (#9336),0.6495079,Move training_output validation to after train_step_end (#7868),"Without clearing this reference, the loss tensor stays live through the next training step. This can be a problem for memory intensive models that produce very deep backward graphs such as neural ODEs. For these models, keeping the backward graph of the previous loss in memory can lead to OOM errors in the next training step even though the step might have succeeded if we had cleared (and thus GC'd) the previous backward graph. Co-authored-by: tchaton thomas@grid.ai Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com",0
CI: precommit - docformatter (#8584),0.5033692,@akihironitta @awaelchli @Borda @carmocca @kaushikb11 @krshrimali @mauvilsa @otaj @pre-commit-ci @rohitgr7 @semaphore-egg @tkonopka @wayi1, CI: precommit - docformatter fix deprecated  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
"Update requirements, update test (#9345)",0.5929593,Updated app testing (#16000),,0
Remove unnecessary TrainingEpochLoop return (#9298),0.6381838,"Refactored TrainingBatchLoop and extracted OptimizerLoop, splitting off automatic optimization into its own loop (#9191)",,0
Fix mypy typing errors in optimizer loop (#9317),0.55926555,    def configure_optimizers(self):,,0
[bugfix] Resolve PyTorch Profiling for Manual Optimization (#9316),0.75905573,"Resolved schedule step bug for PyTorch Profiler (#6674, #6681)",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Fix inspection of unspecified args for container hparams (#9125),0.5657931,Fixing critical bugs in newly added hooks and hparams assignment.,"  Update parsing.py   add todo (for single arg)   unblock non container single arg   init test   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Update CHANGELOG.md   pep8 line length   Update pytorch_lightning/utilities/parsing.py   remove dict namespace conversion   add omegaconf support   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   add dict test   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   add omegaconf test   Update CHANGELOG.md   Update pytorch_lightning/utilities/parsing.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/utilities/parsing.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",0
Add typing for ResultCollection [3/3] (#9271),0.5899871,Improved string conversion for ResultCollection (#8622),,0
fix progress bar restart with fault-tolerant training enabled (#9310),0.6153145,Changed progress_bar_refresh_rate trainer flag to disable progress bar when setting to 0. (#1108), reset progress updates update docs add test  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
remove early stopping tracking from internal debugger (#9327),0.57920516,"Disabled checkpointing, earlystopping and logging with fast_dev_run (#5277)",  replace dev debugger in early stopping   remove unused imports ,0
Remove deprecation warnings being called for on_{task}_dataloader (#9279),0.97559184,Removed deprecation warnings being called for on_{task}_dataloader (#9279), Avoid deprecation warnings being called when hooks are not implemented Update tests & changelog Apply suggestions from code review  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
Deprecate on_keyboard_interrupt callback hook (#9260),0.88933605,Deprecated on_keyboard_interrupt callback hook in favor of new on_exception hook (#9260),  add on_exception callback hook   deprecate on_keyboard_interrupt   Apply suggestions from code review   raise keyboard interrupt   Delete cluster   update changelog   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Avoid optional Tracker attributes and enable mypy (#9320),0.74692845,Avoid optional Tracker attributes (#9320),,1
Update ci badge (#9339),0.4084602,"Updated DeepSpeed ZeRO (#6546, #6752, #6142, #6321)",,0
Support infinite training (#8877),0.7594626,Infinite Training,Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Update call to amp.autocast from fast_dtype to dtype (#9211),0.53367,"  * Removed the `DeepSpeedPrecisionPlugin(amp_type=..., amp_level=...)` arguments",Co-authored-by: SeanNaren sean@grid.ai Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Renamed lr_dict to lr_scheduler_config (#9313),0.70835626,Do not warn when the name key is used in the lr_scheduler dict (#5057),,1
Prevent loss to be moved to the cpu before backward call. (#9308),0.52475536,    # Returning loss in manual optimization is not needed,,0
Fix TPU cleaning job (#9301),0.59685624,Resolve TPU miss rendezvous (#6781),,0
Typing tuner.auto_gpu_select (#9292),0.64648473,  * Removed the `Trainer(auto_select_gpus=...)` argument,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com,0
"Disable {save,check}_on_train_epoch_end with check_val_every_n_epoch>1 (#9156)",0.70473737,"Removed the on_epoch guard from the ""should stop"" validation check (#7701)",,1
Add typing for _FxValidator [1/3] (#9269),0.4266008,remove _evaluate fx (#3197),,0
Move tracking epoch end outputs logic to the EvaluationEpochLoop (#9261),0.633565,Log epoch metrics before the on_evaluation_end hook (#7272),,0
remove backward from training batch loop (#9265),0.6620651,Refactored training loop (#2336),,0
Improve progress.py docstrings (#9284),0.6663091,"Removed legacy code to log or include metrics in the progress bar by returning them in a dict with the ""log""/""progress_bar"" magic keys. Use self.log instead (#6734)",,0
Add missing callbacks to callbacks.rst (#9223),0.6147465,Removed deprecated callbacks (#3979),,0
Add typing for LoggerConnector [2/3] (#9270),0.64808524,Dramatically simplify the LoggerConnector (#7882),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Allow easy CLI trainer re-instantiation (#9241),0.8228491,Allow easy trainer re-instantiation (#7508),  Allow easy CLI trainer re-instantiation   Update CHANGELOG   Allow passing any trainer argument   Do not modify the previous config ,1
Added doc strings to base logger file (#9232),0.6163378,Change default logger to a dedicated one (#1064),  added doc strings to base logger   updated docs ,0
scheduled removal of auto_move_data decorator (#9231),0.8081191,Removed deprecated auto_move_data decorator (#9231),  scheduled removal of auto_move_data decorator   update CHANGELOG.md   remove unused import   remove test_decorators.py   fix missed merge conflict   Co-authored-by: thomas chaton thomas@grid.ai,1
DataModule compatiblity with Python dataclass (#9039),0.59433186,Replaced _DataModuleWrapper with __new__ (#7289), added support and checks required for use of datamodule as python dataclass made changes required for dataclass support for LightningDataModule and required tests made the code compliant with future releases edited tests - removed training call. left dataclass decorator to defaults. added tests to check for multilevel inheritence and make sure init isn't called on the parent of defined class modified new to ensure calling of init on LightningDataModule impliciltly added relevant tests for multilevel inheritence cases removed default values from tests  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: tchaton thomas@grid.ai Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
Add check for uninitialized _sync_dir in DDP Plugin to avoid errors during error handling (#9267),0.75488615,Ensure the existence of DDPPlugin._sync_dir in reconciliate_processes (#8939),,1
Fix LightningOptimizer.step signature (#9289),0.68387556,  * Removed `optimizer_idx` argument from `LightningModule.optimizer_step`,,0
fix state extraction from batch when fault-tolerant training (#9281),0.6329599,Fault-tolerant training:,,0
Remove redundant optimizer_idx reset on epoch (#9285),0.65222484,Removed the deprecated optimizer_idx from training_step as an accepted argument in manual optimization (#8576),,0
Tighten the checks for Trainer.terminate_on_nan (#9190),0.7747201,Deprecated Trainer argument terminate_on_nan in favor of detect_anomaly(#9175),,1
Add docstring example to use dataloader_idx in transfer_batch_to_device (#8982),0.61900777,Don't convert namedtuple to tuple when transferring the batch to target device (#1589),Co-authored-by: tchaton thomas@grid.ai,0
extract optimizer loop (#9191),0.66529393,Refactored optimizer (#4658),,0
[doc] Add Fault Tolerant Documentation Page (#9256),0.60374767,- Fault Tolerant Manual,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Sean Naren sean@grid.ai Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk,0
Add docstring to HyperparametersMixin (#9253),0.60056204,Deprecated the old way of assigning hyper-parameters through self.hparams = ... (#4813),,0
Avoid wrapping LightningModule in DDP plugins when not fitting (#9096),0.7737695,"- The LightningModule no longer gets wrapped with data-parallel modules when not fitting in DDPPlugin, DDPSpawnPlugin, DDPShardedPlugin, DDPSpawnShardedPlugin.",  Avoid wrapping LightningModule in DDP plugins when not fitting   Avoid wrapping LightningModule in DDP plugins when not fitting ,1
Allow exporting to onnx when input is tuple (#8800),0.54918265,Allow any input in to_onnx and to_torchscript (#4378),Fixes #8799,0
remove lightning module datamodule property (#9233),0.55388784,Deprecated the LightningModule.datamodule getter and setter methods; access them through Trainer.datamodule instead (#7168),Co-authored-by: thomas chaton thomas@grid.ai,0
Update CHANGELOG following patch release (#9255),0.6478132,Full Changelog,,0
Add doc strings to CSV logger (#9112),0.5001392,- Added support for writing logs remote file systems on `CSVLoggers`. ([#16880](https://github.com/Lightning-AI/lightning/pull/16880)),Co-authored-by: tchaton thomas@grid.ai,0
avoid suppressing exception in FitLoop (#9206),0.5495009,Marked FitLoop.should_accumulate as protected (#9515),,0
scheduled removal of DeepSpeedPlugin.cpu_offload* parameters (#9244),0.7625594,"Removed deprecated properties DeepSpeedPlugin.cpu_offload* in favor of offload_optimizer, offload_parameters and pin_memory (#9244)",,1
Add on_exception callback hook (#9183),0.807755,An on_exception Callback hook has been added which allows the user to perform custom exception handling.,,1
[bugfix] Prevent a DDP failure using copy (#9239),0.60127527,Changed warnings and recommendations for dataloaders in ddp_spawn (#6762),,0
Remove shown_warnings dead code from Trainer (#9230),0.8091617,Removed Warning from trainer loop (#1634),,1
Minor improvement to deepspeed utility comments (#9224),0.5164293,Support for manual optimization with DeepSpeed (#7970),,0
Some fixes to the trainer docstring (#9227),0.6967206,API changes to the trainer,,0
Deprecate process_position from the Trainer constructor (#9222),0.85126483,Deprecated passing process_position to the Trainer constructor in favor of adding the ProgressBar callback with process_position directly to the list of callbacks (#9222),,1
scheduled removal of BaseProfiler.output_filename in favor of dirpath… (#9214),0.86872345,Removed deprecated BaseProfiler.output_filename arg from it and its descendants in favor of dirpath and filename (#9214),,1
[bugfix] Changed CometLogger to stop modifying metrics in place (#9150),0.63028765,Using .comet.config file for CometLogger (#1913),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Remove deprecated property ModelCheckpoint.period in favor of ModelCheckpoint.every_n_epochs (#9213),0.9927403,Removed deprecated property ModelCheckpoint.period in favor of ModelCheckpoint.every_n_epochs (#9213),,1
Remove deprecated Trainer.running_sanity_check (#9209),0.9060684,Deprecated trainer.running_sanity_check in favor of trainer.sanity_checking (#4945),,1
Add mocked function assert to test_error_handling_all_stages (#9182),0.5170258,- fixed all the .test() calls,,0
Move TrainingBatchLoop.build_kwargs to utilities (#9198),0.6048556,move run_pretrain_routine -> setup_training (#3294),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
Fix callback instantiation with CLI subcommands (#9203),0.68372554,Support passing lists of callbacks via command line (#8815),,0
Remove calls to internal dev debugger in training- and eval loop (#9188),0.5787666,moved eval loop logging to loggers (#3408),,0
move block_ddp_sync_behaviour to utilities (#9192),0.7813361,Moved block_ddp_sync_behaviour out of TrainingBatchLoop to loop utilities (#9192),,1
removing legacy profiler arg (#9178),0.64326745,Moved profilers to their own file (#7822),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Make device and dtype required (#9168),0.6099378,"device parser (#3400, #3405)",,0
Fix a typo (#9169),0.5117327,"In addition, we fixed:",,0
deprecate on_{train/val/test/predict}_dataloader() from DataHooks (#9098),0.78153867,Deprecated on_{train/val/test/predict}_dataloader() from LightningModule and LightningDataModule (#9098),Co-authored-by: Sean Naren sean@grid.ai Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: thomas chaton thomas@grid.ai,1
Make unimplemented dataloader hooks raise NotImplementedError (#9161),0.6821912,"To use this new feature, return any iterable (or collection of iterables) from the dataloader hooks. ",,0
[docs] Add activation checkpointing information (#9165),0.5766906,Enable None model checkpoint default (#3669),,0
Call any trainer function from the LightningCLI (#7508),0.75324154,LightningCLI.instantiate_trainer now takes a config and a list of callbacks (#8721),,1
"Fix self.log(sync_dist=True, reduce_fx={mean,max}) (#9142)",0.7733863,Deprecated self.log(sync_dist_op) in favor of self.log(reduce_fx). (#7891),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
Add tutorial extension to the docs (#9167),0.492092,"For a full tutorial and running example, visit our docs. TODO: add to docs",,0
Check max_time when setting defaults for min/max epochs (#9072),0.6268532,"max_nb_epochs to max_epochs,",Co-authored-by: tchaton thomas@grid.ai,0
Update removal version of argparse_utils.py from 1.4 to 2.0 for backwards compatibility (#9162),0.71660095,argparse_utils >> argparse,,1
Add doc strings for Neptune logger (#9111),0.67362314,"It now supports neptune-client 0.16.16 and neptune >=1.0, and we have replaced the log() method with append() and extend().",Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Fix typo in API evolution (#9160),0.53776944,Syntax changes are: ,,0
add note (#9141),0.48203552,NOTE,,0
Remove reference to DistributedDataParallel from parallel plugin teardown (#8943),0.69609994,Removed teardown from ParallelPlugin (#8943),,0
Fix mypy typing for utilities.apply_func (#8781),0.48538768,Updated references to self.forward() to instead use the __call__ interface. (#1211),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
Move predictions to CPU before accumulating (#9085),0.5111145,"Refactored prediction loop interface; added new classes PredictionLoop, PredictionEpochLoop (#7700, #8077)",Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Added doc strings to wandb logger (#9109),0.6890044,Removed wandb logger's finalize method (#1193),,0
add fault-tolerance for global random state in map-style datasets (#8950),0.559098,Fault-tolerance improvements,Co-authored-by: tchaton thomas@grid.ai Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Added doc strings for Comet logger (#9114),0.720063,Using .comet.config file for CometLogger (#1913),,1
deprecate the TestTubeLogger (#9065),0.9409449,Deprecated the TestTubeLogger (#9065),Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,1
Remove ci tpu test from github workflows (#8965),0.4796962,moved TPU xxx_step to backend (#3118),Co-authored-by: Jirka jirka.borovec@seznam.cz,0
remove redundant iterator call to data fetcher in loops (#9117),0.5637731,"        iterables = [DataLoader(), DataLoader()]",Co-authored-by: tchaton thomas@grid.ai,0
generalize closure api in Lightning (#8642),0.6546766,support for LBFGS. If you pass in LBFGS Lightning handles the closure for you automatically.,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,0
[3 / 3] improvements to saving and loading callback state (#7161),0.5975515,"    # previously, only the state for this callback was passed in as argument",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Support post-localSGD in Lightning DDP plugin (#8967),0.5907384,- Added support for DDP Fork ([#13405](https://github.com/Lightning-AI/lightning/pull/13405)),Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Fix typing in hparams methods (#9116),0.57584935,Fixing critical bugs in newly added hooks and hparams assignment.,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Add docstrings to Test Tube logger (#9110),0.60204136,support for latest test-tube logger optimized for PT 1.2.0.   ,,0
Remove BasePlugin (#9066),0.5461104,- The base Plugin class has been removed. , Remove BasePlugin  Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
[docs] Add Mixed Precision detailed docs (#9104),0.6510458,Mixed precision overhaul (#16783),,0
Rename test file from log_dir to test_log_dir (#9105),0.42372712,Return the default_root_dir as the log_dir when the logger is a LoggerCollection (#8187),,0
Add support for CPU AMP autocast (#9084),0.44844455,enable_pl_optimizer=False by default to temporarily fix AMP issues (#5163),,0
Add doc strings to tensorboard logger class (#9093),0.7341666,We have added own custom Tensorboard logger as default logger. ,,1
Fix old_loader attribute in name _PatchDataLoader.__init__ (#9087),0.6245875,Deprecated @data_loader decorator  (#926),,0
Add validate logic for precision (#9080),0.57304233,    precision=16,,0
Fix torch bfloat import version (#9089),0.6988654,import torch,,0
3/n inter batch parallelism (#9052),0.81220555,Inter Batch Parallelism,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
[2 / 3] improvements to saving and loading callback state (#7187),0.6088693,"    # previously, only the state for this callback was passed in as argument",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Fix accumulated_grad_batches typehint (#9071),0.8013259,The default value of accumulate_grad_batches has changed from 1 to None (#9652)., Fix accumulated_grad_batches typehint,1
[Blocking CI] Fix pep8 error about unused imports (#9090),0.5016423,Module imports are now catching ModuleNotFoundError instead of ImportError (#9867),,0
Fig logging with log_gpu_memory='min_max' (#9013),0.6298593,Changed min-max GPU memory to be on their own plots (#1358),,0
Disable coverage annotations in GitHub (#9077),0.3806715,Refactored setup_training and remove test_mode (#5388),,0
fix typos in datamodule (#8949),0.6324035,Replaced _DataModuleWrapper with __new__ (#7289),Co-authored-by: Knarik Mheryan knarik@codics.am,0
sanitize arrays when logging as hyperparameters in TensorBoardLogger (#9031),0.6382831,Parsing of enums type hyperparameters to be saved in the haprams.yaml file by TensorBoard and CSV loggers has been fixed and made in line with how OmegaConf parses it (#9170),Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: thomas chaton thomas@grid.ai,0
Add bfloat16 support to Lightning Trainer (#9049),0.691015,BFloat16 Support,,0
update warning in docs regarding support of tuner features in DDP (#9011),0.620316,You can find more documentation about the tuner here.,Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com,0
feat: Add Rich Progress Bar (#8929),0.78720593,New Rich Progress Bar,Co-authored-by: thomas chaton thomas@grid.ai,1
Simplify checkpoint connector loading after Checkpoint IO plugin's introduction (#9045),0.83426666,Refactor load in checkpoint connector (#4593), Simplify checkpoint connector loading after Checkpoint IO plugins introduction  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Add ShardedTensor support in LightningModule (#8944),0.6827211,- Removed registration of `ShardedTensor` state dict hooks in `LightningModule.__init__` with `torch>=2.1` ([#16892](https://github.com/Lightning-AI/lightning/pull/16892)), Add ShardedTensor support in LightningModule  Co-authored-by: Yifu Wang yifuwang@2012@gmail.com Co-authored-by: Sean Naren sean@grid.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com,0
"Update correct var name, from step to stage (#9055)",0.60958815,Renames model steps (#1051),,0
2/n inter batch parallelism (#9047),0.843778,Inter Batch Parallelism,Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
Update Changelog with weekly releases (#9056),0.63699067,Full Changelog,,0
Fixes colab links for lightning in 2steps documentation (#9038),0.58268493,- Deprecated the `on_colab_kaggle` function ([#14247](https://github.com/Lightning-AI/lightning/pull/14247)),Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
Add docstrings in mlflow logger class (#9030),0.6705593,MlflowLogger limit parameter value length to 250 char (#5893),Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk,0
Remove TrainingTypePlugin.on_save and Accelerator.on_save (#9023),0.98077834,Removed TrainingTypePlugin.on_save and Accelerator.on_save (#9023), Remove TrainingTypePlugin.on_save and Accelerator.on_save,1
FIx mypy for init_ddp_connection (#9051),0.69200194,"def init_ddp_connection(self, proc_rank, world_size):",,0
update an outdated error message in DDPPlugin (#9005),0.60577816,Silenced some warnings. verified ddp refactors (#3483),,0
governance updates in documentation (#9025),0.8720329,Updated governance docs,Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,1
1/n inter batch parallelism (#9020),0.85206926,Inter Batch Parallelism,Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
Deprecate prepare_data_per_node flag on Trainer and set it as a property for DataHooks (#8958),0.90896654,"Deprecated prepare_data_per_node flag on Trainer and set it as a property of DataHooks, accessible in the LightningModule and LightningDataModule (#8958)",Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Move init_ddp_connection to distributed utilities (#9044),0.7064067,        Override to init DDP in a different way or use your own wrapper.,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
Remove unused on_train_epoch_end hook in accelerator (#9035),0.86676586,Removed on_train_epoch_end from Accelerator (#9035),,1
Remove unused rank_zero_deprecation in WandB logger (#9034),0.679811,Removed wandb logger's finalize method (#1193), Remove unused imports in WandB logger and corresponding test,0
"Remove deprecated on_{save,load}_checkpoint signature (#8697)",0.77036405,The signature and behavior of the on_load_checkpoint and on_save_checkpoint callback hooks have changed (#14835):,Co-authored-by: Yifu Wang yifuwang2012@gmail.com,1
Remove deprecated functions from accelerator.py (#9019),0.70310086,- Removed deprecated passthrough methods and properties from `Accelerator` base class:,,1
Add an issue template for Code Improvement (#8960),0.47603327,Docs improvements,,0
Handle the case with no queries in GPUStatsMonitor (#9014),0.5115489,Deprecated GPUStatsMonitor and XLAStatsMonitor in favor of DeviceStatsMonitor callback (#9924),Co-authored-by: Michele Sanna {ID}+{username}@users.noreply.github.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
Always use trainer.call_hook (#8498),0.7205075,"  * Removed class methods from Trainer: `default_attributes()`, `from_argparse_args()`, `parse_argparser()`, `match_env_arguments()`, `add_argparse_args()`",,1
convert warning cache usage to rank_zero_only in WandbLogger (#8764),0.5717774,Prevent WandbLogger from dropping values (#5931),,0
"Remove GradInformation module, including from LightningModule hierarchy (#8831)",0.6445297,Removed deprecated GradInformation module in favor of pytorch_lightning.utilities.grads (#8831), Remove GradInformation module from LightningModule hierarchy,0
Fix governance references in docs (#8984),0.6660445,Updated governance docs,,0
Add DeepSpeed Stage 1 + doc improvements for model parallel (#8974),0.66752666,DeepSpeed Stage 1,  Add stage 1 support + small doc improvements   Add CHANGELOG.md ,0
add docs (#8952),0.59073937,Docs improvements,,0
fix batch auto scaling when init_val causes OOM (#8954),0.6570184,Reset val_dataloader in tuner/batch_size_scaling (#9857),  fix batch auto scaling when init_val causes OOM   Update CHANGELOG.md   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
"Fix ""line too long"" PEP8 complaint (#8957)",0.4000795,"In addition, we fixed:",,0
Add error handling for all trainer entry points (#8819),0.6550542,"    def on_exception(self, trainer, pl_module, exception):", [lightning] Ensure error handling works different trainer entry points,0
3/n integrate new LightningDataFetcher into loop (#8953),0.5425491,- The top-level loops now own the data sources and combined dataloaders ([#16726](https://github.com/Lightning-AI/lightning/pull/16726)),Co-authored-by: tchaton thomas@grid.ai Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,0
Replace instances of self.lightning_module.trainer with trainer directly in ddp_spawn and tpu_spawn (#8942),0.69255924,The trainer.lightning_module reference is now properly set at the very beginning of a run (#8536), Replace instances of self.lightning_module.trainer with trainer directly in ddp_spawn and tpu_spawn,0
Update governance.rst (#8956),0.64421636,Updated governance docs,,0
[Feat] 2/n Add Fault Tolerant Training to LightningFetcher (#8891),0.62355775,    * Enable Fault Tolerant Manual Training ([#10707](https://github.com/PyTorchLightning/pytorch-lightning/pull/10707)),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,0
restrict deepspeed version in CI (#8951),0.5349714,Do not fail if batch size could not be inferred for logging when using DeepSpeed (#10438),,0
Update contributing docs with pointers for legacy checkpoint testing (#8892),0.5942581,all the checkpoint issues should be gone now (including backward support for old checkpoints),,0
Ensure the existence of DDPPlugin._sync_dir in reconciliate_processes (#8939),1.0000001,Ensure the existence of DDPPlugin._sync_dir in reconciliate_processes (#8939),Co-authored-by: Yifu Wang yifuwang@2012@gmail.com,1
Add a flavor of training_step that takes dataloader_iter as an argument (#8807),0.77109134,def train_dataloader(...):, Add a flavor of training_step that takes dataloader_iter as an argument,1
Drop breaking sphinx docs build (#8934),0.6479853,Pinned sphinx-autodoc-typehints with <v1.15 (#11400),,0
[bugfix] Resolve lost reference to meta object in ResultMetricCollection (#8932),0.60534555,Completely overhaul the Result object in favor of ResultMetric (#7882),,0
[1/n] Add LightningFetcher (#8890),0.50262916,Introduce lightning connect (#14452),,0
Delete TrainingEpochLoop._dataloader_idx which always equals 0 (#8911),0.78435963,Removed trainer.reset_*_dataloader() methods (#16726),,1
Fix CheckpointIO doc annotations (#8931),0.6279947,CheckpointIO Plugins,,0
Replace DataLoader sampler once for IPUs (#8858),0.62266624,Improved error messages in replace_sampler when the DataLoader attributes are not included in the signature or the signature is missing optional arguments (#8519),,0
[Bugfix] Detach Loaders after running entrypoint (#8885),0.70471096,Restore original loaders if replaced by entrypoint (#8885),detach loaders after run,1
A minor syntax correction (#8925),0.66484594,Syntax changes are: ,"Removed an extra quote - """,0
Integrate total_batch_idx with progress tracking (#8598),0.64888966,Integrated TrainingEpochLoop.total_batch_idx (#8598),,0
Smart handling of EarlyStopping.check_on_train_epoch_end (#8888),0.7790244,def on_train_epoch_end(self):,  Smart handling of EarlyStopping.check_on_train_epoch_end   dummy value   Extra flag ,1
Fix SWA with a list of learning rates (#8747),0.57012,Learning Rate Finder (#13802),  Fix swa lrs - needs test   Add test   Update CHANGELOG ,0
Remove write_predictions from LightningModule (#8850),0.7929843,Deprecated LightningModule.write_predictions and LightningModule.write_predictions_dict (#7066), Remove write_predictions from LightningModule,1
[Bug] Add SharedCycleIteratorState (#8889),0.50697577,"- Removed the unused `lightning.pytorch.utilities.supporters.{SharedCycleIteratorState,CombinedLoaderIterator}` classes ([#16714](https://github.com/Lightning-AI/lightning/pull/16714))",,0
Introduce CheckpointIO Plugin (#8743),0.8778238,CheckpointIO Plugins,,1
Automatic string fixes (#8886),0.42011476,Fixing critical bugs in newly added hooks and hparams assignment.,,0
simplify grad clip tests (#8883),0.47776878,"Refactored training_batch + tests to verify correctness (#2327, #2328)",Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Remove call to deprecated fit_loop (#8873),0.55242294,Deprecated the Trainer.train_loop property in favor of Trainer.fit_loop (#8025),,0
fix plateau scheduler stepping on incomplete epoch (#8861),0.65472865,- Do not update on-plateau schedulers when reloading from an end-of-epoch checkpoint ([#14702](https://github.com/Lightning-AI/lightning/pull/14702)),,0
Update DataModule docs following property deprecations (#8864),0.67647326,"Deprecated DataModule properties: has_prepared_data, has_setup_fit, has_setup_validate, has_setup_test, has_setup_predict, has_teardown_fit, has_teardown_validate, has_teardown_test, has_teardown_predict (#7657)",,0
Reduce flakiness of memory test (#8651),0.4985512,Resolve memory leak for evaluation (#6326),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Fix on_train_batch_end signature and call in ProgressBarBase example (#8836),0.6732375,"Removed unintended Trainer argument progress_bar_callback, the callback should be passed in by Trainer(callbacks=[...]) instead (#1855)",,0
"Deprecate DataModule properties: train_transforms, val_transforms, test_transforms, dims, and size (#8851)",0.9703791,"Deprecated DataModule properties: train_transforms, val_transforms, test_transforms, size, dims (#8851)"," Deprecate DataModule properties: train_transforms, val_transforms, test_transforms, dims, and size",1
Remove truncated_bptt_steps from Trainer constructor (#8825),0.8684975,Trainer(truncated_bptt_steps=2),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Add LightningCLI(run=False|True) (#8751),0.71264726,- Removed the deprecated `LightningCLI` arguments ([#16380](https://github.com/Lightning-AI/lightning/pull/16380)),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Tests: fix deprecated TM mape (#8830),0.5495776,The point of this release is more bug fixes ahead of v 1.0.0. We now have CI tests on TPU thanks to @zcain117 from Google! :slightly_smiling_face:,,0
Add warning when wandb.run already exists (#8714),0.6819522,WandbLogger does not force wandb reinit arg to True anymore and creates a run only when needed (#4648),Co-authored-by: thomas chaton thomas@grid.ai,0
Legacy: simple classif training (#8535),0.6198454,Refactored training loop (#2336), simple_classif_training fix test pt1.6 automate  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: thomas chaton thomas@grid.ai,0
Add tests for functions in utilities/data.py (#8785),0.5130875,python script.py test," Add tests for utilities/data.py: test_has_iterable_dataset, test_has_len, test_get_len",0
Fix truncated backprop through time when set on LightningModule and not Trainer (#8804),0.5941615,- Removed special support for truncated backpropagation through time (TBPTT) ([#16172](https://github.com/Lightning-AI/lightning/pull/16172)), Fix truncated backprop through time set on LightningModule and not Trainer,0
Update callback_connector.py (#8805),0.61342335,A setup.py file for a callbacks plugin package could look something like this:,,0
Restructure parsing flow in the LightningCLI (#8721),0.58124363,LightningCLI.init_parser now returns the parser instance (#8721),,0
is-instance check to determine the type of a plugin for teardown decision (#8741),0.59487,Changed teardown() in Accelerator to allow training_type_plugin to customize teardown logic (#7579),,0
rename on_expection -> on_exception (#8750),0.5887102,"    def on_exception(self, trainer, pl_module, exception):",,0
Fix tests for new tensorboard >= 2.6 (#8789),0.6885006,Improved the error message for installing tensorboard or tensorboardx (#17053),,0
Allow modifying run_examples.sh defaults (#8769),0.43393847,The params argument in TracerPythonScript.run no longer prepends -- automatically to parameters (#15518),,0
remove deprecated sync step argument from WandbLogger (#8763),0.9499483,Removed the deprecated sync_step argument from WandbLogger (#8763),  remove deprecated sync step   update chlog ,1
Fix comments for metrics_to_scalars (#8782),0.7155965,Changed metrics_to_scalars to work with any collection or value (#7888),"metrics_to_scalars can return non-float values, such as int or complex, depending on the dtype of the tensor.",1
remove deprecation of gpu string parsing behavior (#8770),0.6227925,nvidia/apex deprecation (#16039),,0
Fix mypy for utilities.xla_device (#8755),0.58767146,"Removed deprecated utils modules model_utils, warning_utils, xla_device_utils and partially argparse_utils (#7503)",  Fix mypy for utilities.xla_device   Add explicit type hint for _XLA_AVAILABLE in utilities.imports ,0
lift tensorboard version restriction (#8765),0.67568064,Improved the error message for installing tensorboard or tensorboardx (#17053),,0
change backbone to self (#8762),0.58829385,- Removed `BackboneFinetuning.on_save_checkpoint` and `BackboneFinetuning.on_load_checkpoint` in favor of `BackboneFinetuning.state_dict` and `BackboneFinetuning.load_state_dict` ([#11887](https://github.com/PyTorchLightning/pytorch-lightning/pull/11887)),No need to use backbone. Calling self proceeds forward.,0
Fix mypy for utilities.memory (#8744),0.43933803,Resolve memory leak for evaluation (#6326),  Fix the majority of mypy issues   Apply @carmocca's suggestion   Handle the exception when nvidia-smi not found   Update get_gpu_memory_map's docstring   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Minor code health fix for Trainer (#8761),0.70191574,Removed Warning from trainer loop (#1634),,1
Remove rank 0 restrictions from logger (#8608),0.7595649,Removed restrictions in the Trainer that loggers can only log from rank 0; the existing logger behavior has not changed (#8608),,1
Improve SWA docs (#8717),0.685012,Docs improvements,,0
Remove deprecated on_save_checkpoint argument (#8688),0.84797955,Removed support for the deprecated on_save_checkpoint signature. The hook now takes a checkpoint positional parameter (#8697),,1
fix typo error in docstring of LightningLoggerBase.after_save_checkpoint (#8737),0.65991545,"Deprecated LightningLoggerBase.close, LoggerCollection.close in favor of LightningLoggerBase.finalize, LoggerCollection.finalize (#9422)",,0
rtfd: drop building pdf (#8706),0.34211463,# Create model here and wrap the large layers for sharding,,0
Fix mypy typing for utilities.debugging (#8672),0.5245465,Removed support for self.log()ing a dictionary (#16389),,0
Fix mypy in utilities.distributed (#8201),0.4186913,Removed dependency on pandas (#736),,0
Move logger and profiler finalization to trainer's teardown (#8685),0.5640037,"All trainers now have a default logger, early stopping and checkpoint object. To modify the behavior, pass in your own versions of those. ",Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
fix recursive call for apply_to_collection(include_none=False) (#8719),0.51803213,Minor improvements to apply_to_collection and type signature of log_dict (#7851),,0
Connect the model to the training type plugin at the start of run (#8536),0.67796695,"- TrainingTypePlugin.{call_configure_sharded_model_hook, on_reset_*_dataloader, on_save, post_optimizer_step, update_global_step}",,0
[docs] Update FSDP instructions and add DeepSpeed evaluate/predict example (#8713),0.6144216,"Implemented DeepSpeedPlugin._setup_model_and_optimizers (#10009, #10064)",,0
WandbLogger to log model topology by default (#8662),0.7166333,Changed WandbLogger(log_model={True/'all'}) to log models as artifacts (#6231),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Add functions to collate deepspeed zero 3 checkpoints (#8701),0.54916704,"Updated DeepSpeed ZeRO (#6546, #6752, #6142, #6321)",,0
Fix an import deprecation warning (#8687),0.74802965,Deprecation warning (#3844),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Deprecate LightningModule.summarize() in favor of pl.utilities.model_summary.summarize() (#8513),0.9060448,Deprecated LightningModule.summarize() in favor of pytorch_lightning.utilities.model_summary.summarize() (#8513),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
"Add batch_size, rank_zero_only arguments for log_dict to match log (#8628)",0.679991,Allow passing self.log(batch_size=...) (#7891),,0
Reduce title length (#8709),0.36910927,"        # 5. ""Truncate""",,0
CI: add mdformat (#8673),0.3677739,"Renamed step_idx to step, epoch_idx to epoch, max_num_epochs to max_epochs and min_num_epochs to min_epochs (#589)", add mdformat exclude chlog fix ***  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
"[docs] Update deepspeed docs, add some more information and link to streamlit (#8691)",0.7054475,"Implemented DeepSpeedPlugin._setup_model_and_optimizers (#10009, #10064)",,1
"Expand the use cases, move them up for discoverability (#8692)",0.4720953,The main focus of this release was on adding flexibility and generalization to support broad research cases.,,0
Fix mypy typing for utilities.cloud_io.py (#8671),0.48951742,  * `pl.utilities.cloud_io` ([#16438](https://github.com/Lightning-AI/lightning/pull/16438)),Co-authored-by: tchaton thomas@grid.ai,0
Add check for unique device ids  (#8666),0.64247644,"            device_ids=device_ids,",Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,0
Fix save/load/resume from checkpoint for DeepSpeed Plugin (#8397),0.6508887,The signature and behavior of the on_load_checkpoint and on_save_checkpoint callback hooks have changed (#14835):,,0
Fix ddp accelerator choice for cpu (#8645),0.60286635,Fix hanging in DDP HPC accelerators (#5157), Fix ddp accelerator choice for cpu,0
Save the ResultCollection in the loops state dict (#8641),0.5714811,Improve Loop API to better handle children state_dict and progress (#8334),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
update logic to inject FastForwardSampler / CaptureIterableDataset 2/n (#8366),0.47999707,refactored dataloader process hook (#3139),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Justus Schock justus.schock@posteo.de Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Test metric_attribute for different children module structures (#8675),0.47622713,Changed metrics_to_scalars to work with any collection or value (#7888),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Torch Elastic DDP DeadLock bug fix (#8655),0.5908175,Swaped torch.load for fsspec load in DDP spawn backend (#3787),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Disable recurrent events on forks (#8668),0.44552025,Enable None model checkpoint default (#3669),,0
black: magic trailing comma (#8560),0.35674456,Args should come after the last positional argument (#1807),,0
Delete deprecated save function (#8680),0.6582632,Removed the deprecated save_function property in ModelCheckpoint (#8680),,0
CI: yesqa (#8564),0.3911007,@akihironitta @awaelchli @Borda @carmocca @dependabot @krshrimali @mauvilsa @pierocor @rohitgr7 @wangraying, add yesqa fix flake8  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
update NGC (#8652),0.47535688,Update the Lightning App docs (#13537), update NGC  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Delete deprecated TrainerTrainingTricksMixin (#8679),0.91700864,Removed the deprecated TrainerTrainingTricksMixin class (#8679),,1
Un-skip some Horovod tests (#8676),0.5393276,horovod deprecation (#16141),,0
Fix distributed types support for CPUs (#8667),0.55688655,"Removed experimental fault-tolerance support (#16516, #16533)",,0
update (#8674),0.4838581,"Updated DeepSpeed ZeRO (#6546, #6752, #6142, #6321)",,0
Add property to skip restoring optimizers and schedulers via plugin (#8644),0.54809403,Avoid redundant callback restore warning while tuning (#13026),,0
Fix mypy in utilities.argparse (#8124),0.67206204,import argparse,Co-authored-by: tchaton thomas@grid.ai Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
Update links for zero_grad to PyTorch docs (#8618),0.56624687,Dropped official support/testing for PyTorch <1.6 (#8288),,0
"Reverse width, height to height, width in docs (#8612)",0.31724387,Docs,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Deprecate LightningModule.model_size (#8495),0.9637096,Deprecated LightningModule.model_size (#8343),Co-authored-by: Caleb Robinson calebrob6@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
fix collecting training_step outputs (#8613),0.72487116,# 3. Remove the `optimizer_idx` argument from `training_step`,,1
Fix reference issues during epoch end result collection (#8621),0.6579074,Changed epoch indexing from 1 instead of 0 (#2206),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Fix pre-commit blacken-docs failures (#8624),0.4588793,Doing this minor release because correct validation metrics logging is critical.,,0
Remove dead code in eval loop output tracking (#8625),0.68738014,moved eval loop (#3412[#3408),,0
Docs improvements around hparams (#8577),0.62578607,Fixing critical bugs in newly added hooks and hparams assignment.,Co-authored-by: thomas chaton thomas@grid.ai,0
Fix references for ResultCollection.extra and improve str and repr (#8622),0.7169348,Improved string conversion for ResultCollection (#8622),,1
"[Fix] Add delay property for checkpointing, refactor loading checkpoint (DeepSpeed Checkpointing Fix 1/n)  (#8627)",0.6281031,Call reset_on_restart in the loop's reset hook instead of when loading a checkpoint (#9561),"  Add property to delay checkpointing, move loading checkpoint file into the run function to allow deepspeed engine to be loaded   Add a small test   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update pytorch_lightning/accelerators/accelerator.py  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   Address review   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
exclude mpi run from auto-detection of horovod (#8610),0.46819574,Don't raise a warning when nn.Module is not saved under hparams (#12669),,0
Test Callback.on_load_checkpoint order (#8588),0.6347781,"-def on_load_checkpoint(self, checkpoint):",,0
remove support for optimizer_idx in the training_step for manual optimization (#8576),0.9298123,Removed the deprecated optimizer_idx from training_step as an accepted argument in manual optimization (#8576),,1
Add ddp_cpu to DistributedType Enum (#8596),0.5542759,DDP custom implementation support (override these hooks):,,0
Fix trainer.fit_loop.split_idx reference (#8601),0.7961727,Deprecated the Trainer.train_loop property in favor of Trainer.fit_loop (#8025),  Fix split idx reference   Update CHANGELOG   Add comment ,1
Use class name in SWA info message (#8602),0.37830657,Enforce an epoch scheduler interval when using SWA (#6588),,0
Delete deprecated TrainerLoggingMixin (#8609),0.9182247,Removed the deprecated TrainerLoggingMixin class (#8609), Delete deprecated TrainerLoggingMixin Update CHANGELOG Delete from Trainer,1
[1 / 3] improvements to saving and loading callback state (#6886),0.607474,"    # Now, the state for this callback gets passed to this new method",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Prune deprecated metrics (#8586),0.7812787,Removed deprecated metrics (#8586),  drop metrics   drop tests   fix imports   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
Remove outputs in on_train_epoch_end hooks (#8587),0.756178,Pass batch outputs to on_train_batch_end instead of epoch_end outputs (#4369),,1
CI: validate JSON & fix benchmark (#8567),0.43852937,Removed callback metrics from test results obj (#2994),  CI: validate JSON   as GHA   PT1.8   32g   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
CI: black docs (#8566),0.44968104,Docs, black docs  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Load ckpt path when model provided in validate/test/predict (#8352),0.7261374,"trainer.test(model, ckpt_path=""my_path"") # load path","  Change trainer loading behaviour for validate/test/predict   Fix   Fix/add tests   remove   Cleanups   Space   cleanups   Add CHANGELOG.md   Move after setup   Cleanups on logic   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Remve   fix test   feedback   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  Update pytorch_lightning/trainer/properties.py  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   Feedback   Same fix   Same fix   Add test for behaviour, modify based on feedback   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Wording   Apply suggestions from code review   Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   Cleanup docs   Update pytorch_lightning/trainer/trainer.py   Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com   feedback   Fixes to test API   Add carlos description   Move logic further   Move checkpoint connector logic   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com",1
Avoid unnecessary list creation (#8595),0.5368497,    # 2. Add the outputs to the list,,0
bump version 1.3 to 1.4 in README (#8582),0.5128767,Set version as today (#13906),,0
Update version to 1.5.0dev (#8585),0.62882066,"    version=""0.0.1"",",  dev + chlog   Add placeholders   Clean previous entry   Add CHANGELOG fix   Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
Replace iteration_count and other index attributes in the loops with progress dataclasses (#8477),0.8654154,iteration_count and other index attributes in the loops has been replaced with progress dataclasses (#8477),  Delete iteration_count and batches_seen   Update CHANGELOG   Protect should accumulate   Update pytorch_lightning/loops/epoch/training_epoch_loop.py ,1
[bugfix] DeepSpeed with no schedulers (#8580),0.68530023,- Fixed the lr-scheduler state not being dumped to checkpoint when using the deepspeed strategy ([#11307](https://github.com/PyTorchLightning/pytorch-lightning/pull/11307)),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Lightning Release v1.4 (#8579),0.72911686,0.4.0 is the first public release after a short period testing with public users. Thanks for all the help ironing out bugs to get Lightning to run on everything from notebooks to local to server machines.,  Update Lightning version to v1.4   update notebooks   Update release date in Changelog   docs   Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
Update cloud docs (#8569),0.5418148,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ","  amp   amp   docs   add guides   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   amp   amp   docs   add guides   speed guides   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Delete ds.txt   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Update conf.py   Update docs.txt   remove 16 bit   remove finetune from speed guide   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   speed   speed   speed   speed   speed   speed   speed   speed   speed   speed   speed   speed   remove early stopping from speed guide   remove early stopping from speed guide   remove early stopping from speed guide   fix label   fix sync   reviews   Update trainer.rst   Update trainer.rst   Update speed.rst   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   managing data   managing data   amp   amp   docs   sync   sync   amp   amp   add data guide   from review   Apply suggestions from code review   Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci  Apply suggestions from code review  Co-authored-by: ananthsub ananth.subramaniam@gmail.com   from review   from review   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   add data guide   add data guide   add data guide   sync issues   from reviw   Update docs/source/guides/data.rst   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com   add info if import fails   fix cross referencing   Add Datamodule motivation   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   grid docs   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  Update cloud_training.rst  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com",0
Add deprecation warning & test for distributed_backend flag (#8575),0.65365875,Remove deprecated distributed_backend from Trainer (#10017),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Add missing highlighting for Python snippets (#8411),0.43577695,Use correct python version in lightning component template (#13790),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Bugfix: Scheduler monitor for manual optimization (#7643),0.7071667,Disabled lr_scheduler.step() in manual optimization  (#6825),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Kaushik B kaushikbokka@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
docs: explain how Lightning uses closures for automatic optimization (#8551),0.6129145,support for LBFGS. If you pass in LBFGS Lightning handles the closure for you automatically.,,0
v1.4.0rc2 (#8553),0.6318088,[1.7.4] - 2022-08-31,"  v1.4.0rc2   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   Apply suggestions from code review   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
Raise exception for ddp_cpu not supported for TPUs (#8530),0.62317455,Enabled manual optimization for TPUs (#8458),,0
Fix profiler test on Windows minimal (#8556),0.5791196,Moved profilers to their own file (#7822),,0
Add pyupgrade to pre-commit (#8557),0.4559292,@akihironitta @awaelchli @Borda @carmocca @kaushikb11 @krshrimali @mauvilsa @otaj @pre-commit-ci @rohitgr7 @semaphore-egg @tkonopka @wayi1,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Replace yapf with black (#7783),0.33015496,"| ""bf16-mixed""                      | ""bf16""     |",Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Update issue and PR templates (#8528),0.38874775,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Add support for functions to be parsed by the Lightning CLI in addition to Types (#8400),0.6112007,- `LightningCLI`'s shorthand notation changed to use jsonargparse native feature ([#12614](https://github.com/Lightning-AI/lightning/pull/12614)),  Initial commit   Update docstrings   Update CHANGELOG.md   Fix mypy   Fixes   Fixes   Update to comments   Fix   mypy   Update on comments   Update   Fix mypy   protected   Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
Remove undefined name from __all__ (#8468),0.4000747,Do not warn when the name key is used in the lr_scheduler dict (#5057), Remove undefined name from __all__  Co-authored-by: deepsource-autofix[bot] 62050782+deepsource-autofix[bot]@users.noreply.github.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Avoid partial for apply to collection (#8529),0.43544286,Improved string conversion for ResultCollection (#8622),,0
Support DataLoaders with missing arguments in replace_sampler (#8519),0.881379,Improved error messages in replace_sampler when the DataLoader attributes are not included in the signature or the signature is missing optional arguments (#8519),  Support DataLoaders with missing arguments in replace_sampler   Fix for multiprocessing context   Fixes and test improvements   Fixes and test improvements   Fixes and test improvements   Test any variadic name   Update CHANGELOG   Make sure extra attributes can be present   Skip on old Windows   Update pytorch_lightning/trainer/data_loading.py   Update pytorch_lightning/trainer/data_loading.py   Check is dataloader   Typo ,1
docs: clarify closure usage in gan example (#8521),0.4317733,We have fixed GAN training - supporting multiple optimizers.,"  clarify closure usage in gan example   Update docs/source/common/optimizers.rst   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   remove empty line   Update docs/source/common/optimizers.rst   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  do not capitalize if not a sentence  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com",0
docs: fix return type description of Trainer.validate/test (#8522),0.80421835,Trainer is now raising a MisconfigurationException instead of a warning if Trainer.{validate/test} is missing required methods (#10016),  update docs   update typehints   fix indentation ,1
Add ddp_*_find_unused_parameters_false to Plugins Registry. (#8483),0.79982424,Changed the default of find_unused_parameters to False in DDP (#5185),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
fix restoring finetune callbacks after accelerator setup on training resume (#8501),0.59938806,Removed TrainingTypePlugin.on_save and Accelerator.on_save (#9023),Co-authored-by: tchaton thomas@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
fix CI for PT 1.10 (#8526),0.5236293,The point of this release is more bug fixes ahead of v 1.0.0. We now have CI tests on TPU thanks to @zcain117 from Google! :slightly_smiling_face:, fix CI for PT 1.10 Apply suggestions from code review  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
[Typo] update some out-dated links from pytorch clip_grad_value_ (#8533),0.6490879,"Removed PyTorch 1.6 support (#10367, #10738)",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Update governance.rst,0.65651655,Updated governance docs,,0
Fix DeepSpeed lr scheduler logic (#8527),0.6781791,Changed lr schedule step interval behavior to update every backwards pass instead of every forwards pass (#1477),  Fix deepspeed scheduler logic   Fix tests   Minor changes   Improve tests   inference fix   CHANGELOG   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Fix log_dir tracking in case of multiple Trainer instances + DDP (#7403),0.6004459,"Deprecated passing flush_logs_every_n_steps as a Trainer argument, instead pass it to the logger init if supported (#9366)",Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Remove torch >= 1.6 checks (#8523),0.69864666,    * Some configuration errors that were previously raised as `MisconfigurationException`s will now be raised as `ProcessRaisedException` (torch>=1.8) or as `Exception` (torch<1.8),,0
Quant as optional step (#8464),0.40304756,Removed no return warning from val/test step (#6139),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
fix: Enable manual optimization for TPUs (#8458),0.8666134,Enabled manual optimization for TPUs (#8458),,1
[Docs revamp 2/N] New doc for managing data (#8034),0.64634895,Docs improvements,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk Co-authored-by: Ethan Harris ethanwharris@gmail.com,0
Merge pull request #8497 from PyTorchLightning/v1.4.0rc1,0.69480205,Full commit list: https://github.com/PyTorchLightning/pytorch-lightning/compare/1.7.0...1.8.0,v1.4.0rc1 & chlog,0
Maintain Backward compatibility for DeviceDtypeModuleMixin (#8474),0.67758864,Moved DeviceDtypeModuleMixin and HyperparametersMixin mixin to core (#8396),  Maintain Backward compatibility for DeviceDtypeModuleMixin   Update message   Add deprecation test   Update test ,0
[bugfix] Re-compute accumulated_grad_batches (#8493),0.8407953,The default value of accumulate_grad_batches has changed from 1 to None (#9652).,  resolve resolution   update changelog   typo   optimize test   update on comments   resolve comments   update ,1
Add the bound instance as method parameter (#8466),0.4209882,Changed callbacks argument in Trainer to allow Callback input (#5446),Co-authored-by: deepsource-autofix[bot] 62050782+deepsource-autofix[bot]@users.noreply.github.com,0
[bugfix] Reduce memory leaks (#8490),0.7437545,Resolve memory leak for evaluation (#6326),"  reduce memory leak   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update changelog   Apply suggestions from code review   Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk   resolve flake8   update on comments   resolve bug   update   Undo whitespace changes   remove bug   resolve flake8   revert change   update on comments   delete the ddp wrapper as it hold memory   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   resolve flake8   update on comments   update changelog   resolve test   Update CHANGELOG   Refactor teardown   Fix comment   Do it for non-gpu too   remove ref when the model is not a lightning_module   Fix import error   move down   resolve bug   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   resolve assignement   update   move above   Fix device calls to support tpu training   Updat todo   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: Kaushik B kaushikbokka@gmail.com",1
Bugfix: horovod optimizer missing 2 required positional arguments (#7840),0.609908,Changed HorovodPlugin.all_gather to return a torch.Tensor instead of a list (#9696),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Parity test (#7832),0.44377637,- Code coverage (99%),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
[bugfix] Prevent deepcopy of dataloaders / Trainer in SWA Callback (#8472),0.5357745,- Moved the warning about saving nn.Module in `save_hyperparameters()` to before the deepcopy ([#15132](https://github.com/Lightning-AI/lightning/pull/15132)),  resolve deepcopy   update changelog   move private   update on comments   Update CHANGELOG   Set skipped attributes to None   Simplify test   update   update changelog   update   update on comments   typo   update   Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
Add Windows Support for DeepSpeed (#8488),0.63987374,Removed DeepSpeed FP16 Exception as FP32 is now supported (#8462),  Modify deepspeed distributed to support windows   Add weak test   Cleanups   Capture more in tests   Add comment   Cleaner asserts ,0
Add Ready to Go label only after two approvals (#8484),0.38073984,Show a message when BuildConfig(requirements=[...]) is passed but a requirements.txt file is already present in the Work (#15799),,0
Adding appends to some of the pseudocode blocks (#8427),0.4041018,Moved result teardown to the loops (#8245),,0
Add support for devices flag to Trainer (#8440),0.6441917,- Removed legacy device arguments in Trainer ([#16171](https://github.com/Lightning-AI/lightning/pull/16171)), Support devices flag to Trainer,0
Use default_root_dir as the log_dir with LoggerCollections (#8187),0.9193693,Return the default_root_dir as the log_dir when the logger is a LoggerCollection (#8187),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Do not reset Loops total counters (#8475),0.63437074,Do not reset the progress tracking dataclasses total counters (#8475),,0
Merge isinstance calls (#8469),0.40883654,Check if optimizer supports closure (#4981),Co-authored-by: deepsource-autofix[bot] 62050782+deepsource-autofix[bot]@users.noreply.github.com,0
[Feat] Improve TPU CI (#6078),0.5977001,TPU training (#2708),  i   i   i   i   i   i   i   i   i   i   i   i   i   i   i   i   i   i   i   i   i   i   i   i   i   i   i   i   i   i   i   i   i   i   i   update   update ci   i   i   i   i ,0
Remove unnecessary generator (#8470),0.5340977,Remove MetricsHolder (#7909),Co-authored-by: deepsource-autofix[bot] 62050782+deepsource-autofix[bot]@users.noreply.github.com,0
Remove unnecessary use of comprehension (#8471),0.5641755,refactored inner eval loop (#3141),Co-authored-by: deepsource-autofix[bot] 62050782+deepsource-autofix[bot]@users.noreply.github.com,0
Loop specialization (#8226),0.6882924,Loop customization improvements,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: tchaton thomas@grid.ai Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: Justus Schock justus.schock@posteo.de Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Hash values in LightningEnum instead of name. (#8421),0.46517318,"- Changed `seed_everything_default` argument in the `LightningCLI` to type `Union[bool, int]`. If set to `True` a seed is automatically generated for the parser argument `--seed_everything`. ([#12822](https://github.com/Lightning-AI/lightning/pull/12822), [#13110](https://github.com/Lightning-AI/lightning/pull/13110))",Co-authored-by: Ramon Emiliani ramon@mbp-de-ana.lan Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk,0
Only output IPU report on request (#8340),0.4363011,Log just the GPU stats,These reports can be quite large and involve some processing to produce. It means on larger models there's a noticeable performance hit to produce the cycles/memory reports.,0
Refactor log_dir usage in the CLI (#8419),0.6536197,Return the default_root_dir as the log_dir when the logger is a LoggerCollection (#8187),,0
[DDP] Remove the outdated limitations of DDP communication hook since 1.9 (#8346),0.7343297,DDP custom implementation support (override these hooks):,"  [DDP] Remove the outdated limitations of DDP communication hook since 1.9   DDP communication hook can work on multiple backends since 1.9.   SPMD in DDP is completely retired in 1.9, and SPSD is the only option.   Update ddp.py   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",1
Fix: handle logical CUDA device IDs for GPUStatsMonitor if CUDA_VISIBLE_DEVICES set (#8260),0.6705061,Moved torch.cuda.set_device() to enable collective calls earlier in setup (#8312),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
update mergify rules (#8443),0.41497698,Syntax changes are: ,  update mergify rules   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Remove check_checkpoint_callback (#7724),0.71105355,Changed resume_from_checkpoint warning to an error when the checkpoint file does not exist (#7075),Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
update links for collect_env_details.py script (#8436),0.45939085,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",,0
Delete legacy dataloader processing utility (#8439),0.62699544,"Accessing dataloaders (#16726, #16800)",,0
Use is to compare type of objects (#8404),0.4892974,Changed type checker with explicit cast of ref_model object (#4457),,0
[Fix] Remove DeepSpeed Plugin FP16 exception (#8462),0.8087009,Removed DeepSpeed FP16 Exception as FP32 is now supported (#8462),"  Remove error, add mixed to check   Add test   Remove test   Add changelog   Add test for mixed   Update tests/plugins/test_deepspeed_plugin.py   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  Add special  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",1
move deprecation test to correct 1.6 test file (#8446),0.48198196,Deprecated the TestTubeLogger (#9065),"  move deprecation test to correct 1.6 test file   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  Update tests/deprecated_api/test_remove_1-6.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",0
Clear dataloader references before attaching new dataloaders to Trainer (#8442),0.7237773,"| Method Trainer.reset_train_val_dataloaders()                                                               | 1.9             | Trainer.reset_{train,val}_dataloader          |",  regression test   apply fix   simplify test and docs   update changlog ,1
[Feat] Add utilities for CombinedLoader state dict and dataloader state dict 1/n (#8364),0.6018046,"Add {,load_}state_dict to the progress tracking dataclasses (#8140)",Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Justus Schock justus.schock@posteo.de Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Add support for missing return obj from to function on custom batch objects (#8433),0.5836214,"Removed output argument from *_batch_end hooks (#3965, #3966)",  resolve bug   update   add changelog   Update tests/utilities/test_apply_func.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Add progress tracking on Loops - 2/n (#8362),0.73115015,"Connect the progress tracking dataclasses to the loops (#8244, #8362)","  resolve issues   update   update   update   add more exceptions   resolve bug   update   update   update changelog   resolve bug   resolve comments   update   update   update changelog   update   update   remove space   update   add progress tracking to loops   validate json   update   convert to dict for better readability   validate reload   update   update   update on comments   remove deadcode   clean changelog   clean changelog   update   update on comments   CHANGELOG   CHANGELOG   Update pytorch_lightning/loops/base.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   whitespace suggestions   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   make fault_tolerant_enabled protected   whitespace fixes around Args   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   typo it's -> its   fix copy-paste typo in progress docstring   Delete classes   Minor change   docs   protected get_loops_state   merge restore_loops with restore_progress   Fix tests after removals   explicit save with trainer.save_checkpoint()   handle optimization restart based on optimizer_idx   update increments   update val batch progress and remove iteration count   update progress tracking for dataloader loops   remove self.dataloader_idx from eval_epoch_loop   add batch progress to predict loop   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   incorporate progress tracking for current_epoch   Fix test   Actually remove it   Remove unused TrainingEpochProgress   Fix optimization progress - missing scheduler   Restarting changes   Scheduler progress   Unused property, reset on epoch   Resolve FIXME   Remove FIXME   fix test_progress (wip)   fix batch_progress.current.reset   Hold off on split progress. Out of scope of this PR   Unnecessary if   fix structure in test_progress   structure   clean up unused variables in test_progress   refactor naming and organization in test_progress   Unnecessary variable   Remove unnecessary diff   Improve comment   Undo typing change to avoid polluting everything with mypy fixes   Fix and improve test_loops.py   Fix and organize test_loop_state_dict   Remove unnecessary checks in test   Update test after disallowing updates on None attributes   Typing   Minor test cleanup   Fix and move loop test   Move test from progress to loops   Reset the scheduler progress   SchedulerProgress fix   Consistent whitespace   Fix final test   Minor test changes   One test to rule them all   Formatting   Rename and clean variables   Shorter names   Shorter scheduler name   Fix optimizer step calculation for stop_batch=2   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Remove empty connects   Update CHANGELOG   Holy shit finally got the formula right   Fix final thing!!!   Do not check state dicts   parametrize multiple_dataloader progress test   Update CHANGELOG.md   Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Justus Schock justus.schock@posteo.de",1
Remove unnecessary comprehension (#8405),0.5229287,refactored inner eval loop (#3141),,0
Move mixin to core (#8396),0.63647616,Removed unused mixin attributes (#6487),  Move mixin to core   Move mixin to core ,0
Unblock GPU CI (#8456),0.61317027,Enable non-blocking for device transfers to GPU (#1843),  Debug   Increase SHM size   Debug   Refactor MNIST imports   Undo debugging   Prints ,0
fix internal call to deprecated train_loop (#8434),0.7298043,Removed deprecated TrainResult (#5323),,1
prepare RC0 (#8399),0.4079105,"Updated DeepSpeed ZeRO (#6546, #6752, #6142, #6321)",  RC0   Update changelog   Co-authored-by: Kaushik B kaushikbokka@gmail.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,0
Mark evaluation epoch loops attributes as protected (#8420),0.6445318,"Marked several methods in EvaluationLoop as protected: get_max_batches, on_evaluation_model_eval, on_evaluation_model_train, on_evaluation_start, on_evaluation_epoch_start, on_evaluation_epoch_end, on_evaluation_end, reload_evaluation_dataloaders (#9516)",  Mark evaluation epoch loops attributes as protected   Fix pre-commit ,0
Move plateau schedulers epoch update to the training epoch loop (#8424),0.60184246,- Do not update on-plateau schedulers when reloading from an end-of-epoch checkpoint ([#14702](https://github.com/Lightning-AI/lightning/pull/14702)),,0
Add LSFEnvironment to API reference (#8423),0.42329526,Changed LSFEnvironment to use LSB_DJOB_RANKFILE environment variable instead of LSB_HOSTS for determining node rank and main address (#10825),,0
CI: support PT 1.10 (#8133),0.5331473,The point of this release is more bug fixes ahead of v 1.0.0. We now have CI tests on TPU thanks to @zcain117 from Google! :slightly_smiling_face:,  prepare PT 1.10   dockers   fixes   readme ,0
Refactor unnecessary else / elif when if block has a raise statement (#8403),0.50396055,refactored inner eval loop (#3141),Co-authored-by: deepsource-autofix[bot] 62050782+deepsource-autofix[bot]@users.noreply.github.com,0
Tidy up IPU documentation (#8401),0.55735564,Docs improvements,,0
support launching Lightning ddp with traditional command (#7480),0.61291456,- Added support for DDP Fork ([#13405](https://github.com/Lightning-AI/lightning/pull/13405)),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Use literal syntax instead of function calls to create data structure (#8406),0.45620537,Support shorthand notation to instantiate datamodules (#10011),,0
Set minimum PyTorch version to 1.6 (#8288),0.8367456,Drop PyTorch 1.7 support,Co-authored-by: Jirka jirka.borovec@seznam.cz,1
Remove pep8speaks (#8392),0.54109466,Remove MetricsHolder (#7909),,0
Add ModelCheckpoint(save_on_train_epoch_end) (#8389),0.76220024,ModelCheckpoint now runs at the end of the training epoch by default (#8389),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Expose extract_batch_size method and add corresponding tests. (#8357),0.6751945,Run batch size finder for validate/test/predict.,  expose extract_batch and make public   first pass   early return   add changelog   move to utilities/data.py   add test_data.py   tests are passing   precommit hook   address pep8 failure   Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
fix logger type hint (#8359),0.69336945,Change default logger to a dedicated one (#1064),,0
Add logger flag to save_hyperparameters (#7960),0.7714976,Add ignore param to save_hyperparameters (#6056),  Add log flag to save_hyperparameters   FIx setter   Add test & Update changelog   Address comments   Fix conflicts   Update trainer   Update CHANGELOG.md   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   Fix datamodule hparams fix   Fix datamodule hparams fix   Update test with patch   Update pytorch_lightning/utilities/hparams_mixin.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   Move log_hyperparams to mixin   Update hparams mixin   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
[pre-commit.ci] pre-commit autoupdate (#8388),0.6457205,@akihironitta @awaelchli @Borda @carmocca @kaushikb11 @krshrimali @mauvilsa @otaj @pre-commit-ci @rohitgr7 @semaphore-egg @tkonopka @wayi1,updates: - github.com/PyCQA/isort: 5.9.1 → 5.9.2 Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Update to Mypy>0.9 (#8386),0.46933925,"This release aims at fixing particular issues and improving the user development experience via extending docs, adding typing and supporting python 3.8. In particular, some of the release highlights are:",,0
every_n_val_epochs -> every_n_epochs (#8383),0.631966,(checks val every 100 train epochs),,0
Clean code formatting CI job (#8378),0.3940274,clean up data reset (#3161),,0
Fix mypy in utilities.device_dtype_mixin (#8127),0.7204704,| Import pl.core.mixins.DeviceDtypeModuleMixin                                                           | 1.10             | No longer supported |,,1
Remove Vulture (#8381),0.5588632,Remove MetricsHolder (#7909),,0
Add troubleshooting section for tpus (#8277),0.708032,Enabled manual optimization for TPUs (#8458),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Exclude lightning.py for vulture (#8379),0.49986202,- Removed support for Python 3.7 ([#16579](https://github.com/Lightning-AI/lightning/pull/16579)),,0
[Refactor] Improve loops API 1/n (#8334),0.75572896,Refactored Loops,"  resolve issues   update   update   update   add more exceptions   resolve bug   update   update   update changelog   resolve bug   resolve comments   update   update   update changelog   update   update   remove space   update   re-order protected trainer attr   move public method up   add docs to state dict methods   combine __load with load_state_dict   rename shadowed variable   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   move changelog entry to refactor section   refactor loop_progress property for test helper function   update trainer setter docstring   Update CHANGELOG.md   Update pytorch_lightning/loops/base.py   remove trainer check   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com",1
Temporarily pin sphinx version (#8377),0.69133097,Pinned sphinx-autodoc-typehints with <v1.15 (#11400),,0
Add support for (accelerator='cpu'|'gpu'|'tpu'|'ipu'|'auto') (#7808),0.72963965,"    accelerator=""gpu"", ",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: SeanNaren sean@grid.ai,1
Hyperparameters for datamodule (#3792),0.5414227,Allow passing model hyperparameters as complete kwarg list (#1896) ,Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk Co-authored-by: Tilman Krokotsch tilman.krokotsch@iav.de Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Ethan Harris ethanwharris@gmail.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Kaushik B kaushikbokka@gmail.com,0
Add LSF support (#5102),0.40641907,Changed LSFEnvironment to use LSB_DJOB_RANKFILE environment variable instead of LSB_HOSTS for determining node rank and main address (#10825),"  add ClusterEnvironment for LSF systems   update init file   add available cluster environments   clean up LSFEnvironment   add ddp_hpc as a distributed backend   clean up SLURMEnvironment   remove extra blank line   init device for DDPHPCAccelerator   We need to do this so we don't send the model to the same device from multiple ranks   committing current state   add additional methods to ClusterEnvironments   add NVIDIA mixin for setting up CUDA envars   remove troubleshooting prints   cleanup SLURMEnvironment   fix docstring   cleanup TorchElasticEnvironment and add documentation   PEP8 puts a cork in it   add set_ranks_to_trainer   remove unused import   move to new location   update LSF environment   remove mixin   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   changelog   reset slurm env   add tests   add licence   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   test node_rank   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   add lsf env to docs   add auto detection for lsf environment   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   fix is_using_lsf() and test   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
Add the on_before_optimizer_step hook (#8048),0.76825976,Executing the optimizer_closure is now required when overriding the optimizer_step hook (#9360),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
[docs] Add NCCL environment variable docs (#8345),0.54600537,"The default remains ""nccl"", and you should choose ""gloo"" only for debugging purposes.",  Add nccl env variable docs   Wording   Update docs/source/guides/speed.rst   Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,0
Parametrize fit hook test with manual optimization (#8071),0.5752403,Fit using tuned settings,,0
Add the on_before_backward hook (#7865),0.57415843,The on_after_backward hook is now called on accumulating iterations. Use the on_before_optimizer_step hook to mimic the old behaviour (#8328),"  Add callback to hook tests and add predict test   Fix lambda callback test   Simplify lambda call test   Use LambdaCallback   Dynamically append to called for the model   Remove print   Consistency   Consistency   Prepare args/kwargs testing   yapf doesn't like dict literals   Add arguments for fit no val test   Add arguments for fit no val test   add before_backward_hook   add test   resolve flake8   resolve tests   update changelog   add on_before_backward to LightningModule   update on comments   Test arguments   Datamodule refactor   Fix eval test   remove extra file   resolve bug   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   move to hooks   update   resolve flake8   update on comments   Update full fit + val test   Update test   Remove FIXME   Remove FIXME   Undo change   Fix   Parametrize fit hook test   Comment   Parametrize fit hook test with different precision plugins   Fix tests   Parametrize fit hook test with manual optimization   Unnecessary parenthesis   WIP   Comments   Fix message   Test CI error   Revert ""Test CI error""   This reverts commit 39c4a85a83cf32081b721f939ff83500b93f2dd3.   Add ddp training type teardown   Update CHANGELOG   Adrian's fix   Use destructor   Update CHANGELOG.md   RPC destructor   Update pytorch_lightning/plugins/training_type/ddp.py   Why do you not work :(   Missing condition   Fix deepspeed test   GC collect in conftest   Do not show warnings for special tests   Needs to run on 1.8   To avoid: ""RuntimeError: NCCL error in: /pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp:32, unhandled cuda error, NCCL version 2.4.8""   Run torch 1.8   Skip test due to 'Python bus error'   Debug NCCL   shm size   Disable warnings for special tests   Remove NCCL_DEBUG statement   Try smaller shm size   Revert ""Skip test due to 'Python bus error'""   This reverts commit e0a3e8785d2fecd63667da433a648f958d60ef89.   README and adjust versions   Avoid self.on_gpu call   empty cache cleanup   More garbage collection   Unroll parametrizations   Do not reuse mock   Undo changes   Undo notebooks modification   resolve test   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   delete file   Undo   Fix test   Revert ""WIP""   This reverts commit f5828a8c426ff44275f560aec8d898f56da2cbfe.   Rename   Remove optimizers   Fix bug with LightningOptimizer   Add optimizers   update   update   Update CHANGELOG   On after backward refactor   Do not call super   Fixes   Remove should_accumulate   pre/post backward refactor   Call the LM backward hook   Update tests   Remove dev debug patch   Fix test   Remove optimizer arguments and typing   Docs fixes   Fix comment   Undo changes   Split manual and auto   Undo change   Deepsource   Remove optimizers   Undo changes   Call the hook   Docs   Docs   Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com",0
Unpin PyYAML<=5.4.1 (#8329),0.51579773,- Fixed security vulnerabilities CVE-2020-1747 and CVE-2020-14343 caused by the `PyYAML` dependency ([#11099](https://github.com/PyTorchLightning/pytorch-lightning/pull/11099)),,0
Refactor plugins backward (#8328),0.6032866,Refactor Model backward (#2276),,0
Unpin Pillow after the 8.3.1 release (#8324),0.3837229,Removed deprecated API (#2073),,0
LightningCLI documentation improvements (#8303),0.76621056,LightningCLI Improvements,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Use accelerator instead of dist backend for missing horovod warning (#8319),0.6028226,Renamed all backends to Accelerator (#4066),,0
Fix mypy in utilities.parsing (#8132),0.50828433,```python,,0
Add auto_insert_metric_name to ModelCheckpoint docstring. (#8310),0.7026903,"Removed deprecated ModelCheckpoint arguments prefix, mode=""auto"" (#6162)",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Fix broadcast for Windows minimal (#8331),0.4987288,Moveed HPU broadcast override to the HPU strategy file (#17011),,0
[Feat] Add FastForwardSampler 2/n - Fault Tolerant Training (#8307),0.64151704,Fault-tolerant training:,  wip   update   resolve bug   wip   wip   wip   resolved tests   update on comments   update   update   Update pytorch_lightning/utilities/auto_restart.py   Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com   update on comments   Update pytorch_lightning/utilities/auto_restart.py   Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com   resolve bug   update   move properties to top   update docs for fast forward sampler   move public attribute to top   add missing super call   update docs for state_dict   fix merge conflict   add missing super() call   move property to top   update on comments   update   resolve bug   update   update on comments   activate coverage for CaptureIterableDataset   update on comments   Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Remove dev_debugger.call_count (#8317),0.5040064,Removed deprecated early_stop_callback (#3982),,0
[CLI] Drop ArgumentParser when pickling and save before spawning (#8017),0.6193415,Dropped the LightningCLI ArgumentParser when pickling (#8017),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Remove magic monitor support for ModelCheckpoint (#8293),0.76361704,Allow ModelCheckpoint monitor to be None (#3633),,1
Add logo_light.svg (#8327),0.33766863,"This project moved to new org PyTorchLightning, so no longer the root sits on WilliamFalcon/PyTorchLightning. ",,0
Fix self.optimizers() not returning a single LightningOptimizer (#8326),0.70737374,  * Removed `optimizer_idx` argument from `LightningModule.on_before_optimizer_step`,,1
Delete checkpoint_connector.has_trained (#8292),0.70150733,Removed passing a ModelCheckpoint instance to Trainer(checkpoint_callback) (#6166),,1
Add quick docs for deepspeed infinity (#8323),0.58658457,"Updated DeepSpeed ZeRO (#6546, #6752, #6142, #6321)",,0
Remove RC candidate install (#8322),0.43776262,Removed dependency on torchvision (#797),,0
Simplify logger connector access (#8318),0.810382,Dramatically simplify the LoggerConnector (#7882),,1
move torch.cuda.set_device() to enable collective calls earlier in setup (#8312),0.9869768,Moved torch.cuda.set_device() to enable collective calls earlier in setup (#8312),,1
Enables reload of dataloaders on every n epochs from every epoch (#5043),0.6188069,Deprecated reload_dataloaders_every_epoch argument of Trainer in favor of reload_dataloaders_every_n_epochs (#5043),"  edit arg to reload_dataloaders_every_n_epoch   init reload_dataloaders_every_n_epoch   edit logic to reload dl   update arg to test datamodule   update arg test dataloader   edit reload dl logic in eval loop   fix var name in reset_train_val_dataloaders   fix error, use current_epoch attribute   edit every_n_epoch to every_n_epochs   edit every_n_epoch to every_n_epochs   edit every_n_epoch to every_n_epochs   edit every_n_epoch to every_n_epochs   edit every_n_epoch to every_n_epochs   edit every_n_epoch to every_n_epochs   assert reload_dataloaders_every_n_epochs positive   assert reload_dataloaders_every_n_epochs positive   add trainer property should reload dl   update should reload dl in train loop   condition on should reload dl in eval loop   pep8   fix update should reload dl in train loop   add test case   replace assertion with misconfig exception   remove unused variable   remove unnecessary checks   replace to BoringModel   remove unrequired comment   deprecate _every_epoch   add deprecated argument to trainer   test case for deprecated arg   remove unrequired assertion in train loop   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  modify misconfig exception for int  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  conv bool to int of depreciated _every_epoch  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  update description of deprecated param  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  update deprecation warning  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   modify argument to int only   fix deprecated test function name   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   merge tests for reload dls   add propery should reload dl   removed and added to trainer property   use property in train loop   remove deprecated test   add deprecated test to new file   test case for exception   update test datamodule every_n_epochs   update trainer docs   update hooks with every_n_epochs   edit format if statement   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   Update CHANGELOG.md   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   typo in exception   pytest check only misconfig exception   remove unnecessary code in test   remove unnecessary code in deprec test   added match in test   typo in comment   revert to prev, keep only req in context manager   Apply suggestions from code review   docs   rebase   Apply suggestions from code review   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   fix import: model_helpers instead of model_utils   fix, add reload_dataloaders_every_n_epochs argument to data connector   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   add required imports   move deprecated log   add missing import rank_zero_warn   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  update varname in should_reload_dl_epoch  suggestion from code review   Fix CHANGELOG. Update deprecation versions   Minor change   change property name, mark protected   update property name   update property name   Remove deprecated *_loop.py files   Rename test func   Update CHANGELOG.md   use rank_zero_deprecation   update deprecation message in trainer api docs   test deprecation with real arg name in message   fix typo in trainer docs   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com",0
Decouple device parsing logic from Acc connector to Trainer (#8180),0.8241571,Decoupled device parsing logic from Accelerator connector to Trainer (#8180),,1
Update contributing templates (#8256),0.5022657,Included app templates to the lightning and app packages (#13731),  update   add a link   update   remove star   update   Update .github/PULL_REQUEST_TEMPLATE.md   Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk   Update .github/PULL_REQUEST_TEMPLATE.md   update   update   Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk,0
Fix PyTorchProfiler prefix typo (#8308),0.70610595,Removed deprecated profiled_functions argument from PyTorchProfiler (#9178),,1
Refactor GPU examples tests (#8294),0.5829212,refactored GPU backend __step (#3120),,0
Bump IPU version (#8290),0.49226367,Stopped optimizer_zero_grad from being called after IPU execution (#12913),,0
fix best score on wrong device in EarlyStopping callback (#8295),0.4924156,Removed callback metrics from test results obj (#2994),,0
Add functools.wraps support for is_overridden (#8296),0.6329246,Deprecated is_overridden(model=...) in favor of is_overridden(instance=...) (#7918),,0
Fix mypy in utilities.device_parser (#8136),0.61188793,"device parser (#3400, #3405)","  Fix mypy for utilities.device_parser   Fix remaining mypy issues + disable ignoring mypy errors   Return one Optional type annotation back   Fix annotation for the parse_tpu_cores method   Remove unused import   Include carmocca's suggestion and fix mypy issue   include carmocca's suggestion   add else statement to parse_gpu_ids to inform mypy gpus is a type of List[int]   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
Remove deprecated optimizer argument from manual_backward (#8287),0.70392805,Removed the deprecated optimizer_idx from training_step as an accepted argument in manual optimization (#8576),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
clean up unused attributes in LightningModule (#8259),0.5026159,- DDP's `find_unused_parameters` now defaults to `False` ([#16611](https://github.com/Lightning-AI/lightning/pull/16611)),,0
move profiler.step from training_step_and_backward to optimizer_step_… (#8224),0.67679095,# 3. Remove the `optimizer_idx` argument from `training_step`,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Deprecate trainer.disable_validation (#8291),0.9274657,Deprecated the Trainer.disable_validation property in favor of not Trainer.enable_validation (#8291),,1
Default EarlyStopping.check_on_train_epoch_end=True (#8286),0.8435941,EarlyStopping now runs at the end of the training epoch by default (#8286),,1
[IPU] Allow poptorch.Options to override Trainer (#8233),0.60805243,  * Removed the `Trainer(ipus=...)` argument,"  Add test for poptorch Options   Hacks to get manual plugin support   Revert changes   Fix tests + ensure logic follow suit   Update pytorch_lightning/plugins/training_type/ipu.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci   Cleaner   Cleaner   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
Add tests for GCS filesystem (#7946),0.50293076,Used fsspec instead of gfile for all IO (#3320),,0
fix missing call to untoggle_optimizer when accumulating gradients  (#8284),0.7032528,Changed calling of untoggle_optimizer(opt_idx) out of the closure function (#7563),  add fix   toggle test   re-structure   update changelog   update comment   remove debugging assertion ,1
prevent internal call to has_prepared_data showing a warning (#8271),0.4521189,group prepare data hook (#3212),,0
refresh LightningModule API docs (#8276),0.624439,Removed deprecated LightningModule hparams setter (#6207),"  optimization docs   example input array docs   step and rank   on_gpu   auto-opt   truncated backprop   fixes   manual backward   backward   all gather   fix configure optimizers html   whitespace   toggle, untoggle   scripting   save hyperparameters   optimizer step   step functions   hparams   model size   summarize   header   rm model summary   Update pytorch_lightning/core/lightning.py   Update pytorch_lightning/core/lightning.py ",0
Add XLAStatsMonitor Callback (#8235),0.63553447,- Removed the deprecated `XLAStatsMonitor` callback ([#12688](https://github.com/Lightning-AI/lightning/pull/12688)),,0
Connect progress tracking dataclasses to loops (#8244),0.94009614,"Connect the progress tracking dataclasses to the loops (#8244, #8362)",Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Test deepcopy for progress tracking dataclasses (#8265),0.5290036,"Connect the progress tracking dataclasses to the loops (#8244, #8362)",,0
Parametrize fit hook test with different precision plugins (#8070),0.60469526,precision plugins (#3504),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Fix progress bar updates for Pod Training (#8258),0.65982175,"trainer.{logged,progress_bar,callback}_metrics are now updated on-demand (#7882)",  Fix progress bar updates for Pod Training   Fix progress bar updates for Pod Training   Add _pod_progress_bar_force_stdout ,0
move batch to device before sending it to hooks (#7378),0.558714,"With DPStrategy, the batch is not explicitly moved to the device (#11780)","  update train step   test   x   limits   val   typeo   x   x   step   min gpus   run all loops   x   limit test   profiler   clean up accelerator code   move files   rename   move tests   changelog   reorder callbacks and model hooks   add test description   replace unneccessary method   fix chlog   adjust batch_to_device for DP Plugin   update tests for dataloader idx   unused imports   hook change   switch None   clear memory   change to None   None   None   memory savings   remove redundant todo   hack   cheat   Revert ""cheat""   This reverts commit a8433bd0b4bd35f218993335f7d4ff18977ae423.  Revert ""hack""  This reverts commit 43a6d1edeb62a15ac69ef69ef2352581ba1947a5.   update new epoch loop   remove from old loop code   update chlog   update hook test   changelog   teardown   integrate changes in new eval loop   fix hook calls   add prediction step   bad merge   Revert ""bad merge""   This reverts commit 488080863cf012dcf04446be3b7d973b7340687e.   fix train batch hook test   rm -rf _notebooks   update chlog   release memory   fix type   notebooks mess   debug   Revert ""debug""   This reverts commit eec4ee2f77b5eb39965211a250598ed5d2320e88.   teardown   fix teardown bug   debug   x   debug   Revert ""debug""   This reverts commit a6e61019462b80d09d31b65bed289fa6e4dd15f6. Revert ""debug"" This reverts commit 5ddeaec06911e96730aade1be6ee71d097b46b9a. debug debug Revert ""debug"" This reverts commit 605be746f7daedf265b2c05a1c153ce543394435. Revert ""Revert ""debug"""" This reverts commit a7612d5410409ed886cfb609457349ecf44cbfa8. debug x x x s tol x tol  Fix changelog  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com",0
Add periods to the documentation (#8252),0.46557075,Syntax changes are: ,,0
Move result teardown to loops (#8245),0.88728714,Moved result teardown to the loops (#8245),  Move result teardown to loops   Update CHANGELOG   Remove teardown from run   Move previous teardown to on_run_end   Add comment   Merge 8250   Remove stage set to None where it shouldnt ,1
[feat] Add restore to base loop (#8247),0.4819374,Moved result teardown to the loops (#8245),  add loop restart   update ,0
ngc (#8242),0.4331614,[1.4.4] - 2021-08-24,,0
Detach hiddens and add test (#8249),0.67655253,    # 6. Remove the return of `hiddens`,,0
[IPU] Fix Custom Poptorch options to IPUPlugin (#8241),0.6025803,- No longer set a `DistributedSampler` to the `poptorch.DataLoader` when IPUs are used ([#12114](https://github.com/PyTorchLightning/pytorch-lightning/pull/12114)),  Fixes to ensure ipu options are respected   Better setter   Add test for poptorch Options   Fix test   fix ipu test   Update pytorch_lightning/plugins/training_type/ipu.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Support DDPPlugin to be used on CPU (#6208),0.5212578,DDP custom implementation support (override these hooks):,"  Skip test due to 'Python bus error'   Debug NCCL   Remove NCCL_DEBUG statement   Revert ""Skip test due to 'Python bus error'""   This reverts commit e0a3e8785d2fecd63667da433a648f958d60ef89.   fix   add test   changelog   yapf   patch os environ   make a special test   destroy pg   debug   revert   revert   problematic test   skip   try the fixture   test   update sensitive test   update changelog   remove comment   update wrong test   update test name   parameterization   Revert ""parameterization""   This reverts commit b0542f43f59c5ce66800883b5e2f0c66a97408cc.   remove conftest   ignore test   teardown   fix merge   deep speed parameterization   uncomment test   update chlog   update changelog   split tests   update test   update test update test update test   update test comments   unroll test   unroll test   unroll test   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   increase shm   sudo   unroll ipu   Revert ""sudo""   This reverts commit 6cc68c1478c151caee86b8a0f8ded16b62a9c8ea.  Revert ""increase shm""  This reverts commit 8c2716348330dbc3c6b3685647f435127b870d79.   x   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   find guilty test   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   POPTORCH_WAIT_FOR_IPU=1   move test   redo parameterize for ipu   de-comment test   move chlog   Update tests/accelerators/test_accelerator_connector.py   Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com  Update tests/accelerators/test_accelerator_connector.py  Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com",0
Mark some loop attributes as protected (#8250),0.60677016,  * The loop classes are now marked as protected ([#16445](https://github.com/Lightning-AI/lightning/pull/16445)),,0
Remove methods with unnecessary super delegation. (#8148),0.47387928,Add function to remove checkpoint to allow override for extended classes (#16067),  Remove methods with unnecessary super delegation.   Update fully_sharded.py   replace init in test   Co-authored-by: deepsource-autofix[bot] 62050782+deepsource-autofix[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Ethan Harris ethanwharris@gmail.com,0
Update Torch Elastic documentation (#8248),0.61870104,- Fixed an issue with comparing torch versions when using a version of torch built from source ([#17030](https://github.com/Lightning-AI/lightning/pull/17030)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
update changelog after 1.3.8 patch release (#8239),0.60787797,Full Changelog,Co-authored-by: Jirka jirka.borovec@seznam.cz,0
Fix truncated_bptt_steps hiddens detach() and improve docs (#8145),0.6213875,    self.truncated_bptt_steps = 10, Fix truncated_bptt_steps hiddens detach() Improve truncated_bptt_docs Add missing import Improve documentation wordings pep8 detach typo Update test Implement comments parametrize test Apply suggestions from code review  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Signed-off-by: Guillaume Tauzin guillaumetauzin.ut@gmail.com  Remove import  Signed-off-by: Guillaume Tauzin guillaumetauzin.ut@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Deprecate LightningModule.loaded_optimizer_states_dict (#8229),0.97714025,Deprecated LightningModule.loaded_optimizer_states_dict (#8229),,1
Add state_dict to loops (#8197),0.7912708,Improve Loop API to better handle children state_dict and progress (#8334),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
Update docs for new template (#8232),0.553264,Docs improvements,  Update docs for new template   Fixes   Fixes   Drop links ,0
Avoid Pillow 8.3.0 due to errors with numpy (#8234),0.45806488,Improved error messages for invalid configure_optimizers returns (#3587),  Avoid Pillow 8.3.0   Move it to last ,0
Add ModelSummary.max_depth (#8062),0.71084726,Deprecated mode parameter in ModelSummary in favor of max_depth (#8062),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Add support for optimizers and learning rate schedulers to LightningCLI (#8093),0.70491517,"    def configure_optimizers(lightning_module, optimizer, lr_scheduler=None):",Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Logging Non-matching keys when loading from checkpoint in non-strict … (#8152),0.60251486,    # put all logic related to loading a checkpoint here,Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
[Refactor] Remove should_raise_exception (#8202),0.63929456,Used raise .. from .. to explicitly chain exceptions (#3750),Co-authored-by: Ethan Harris ethanwharris@gmail.com,0
Remove unnecessary generator (#8154),0.5508895,Remove MetricsHolder (#7909),Co-authored-by: deepsource-autofix[bot] 62050782+deepsource-autofix[bot]@users.noreply.github.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Clean cuda.empty_cache usage (#8199),0.5962615,Removed deprecated pytorch_lightning.utilities.memory.get_gpu_memory_map in favor of pytorch_lightning.accelerators.cuda.get_nvidia_gpu_stats (#15617),,0
Fix double precision casting complex buffers (#8208),0.58897805,Apex mixed precision gets replaced with AMP (#16149),  Fix double precision casting complex buffers   Update CHANGELOG.md   Fixes   Fixes   Fix   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
update bug report issue template - include PL version (#8209),0.47358924,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",  update github template   Update .github/ISSUE_TEMPLATE/bug_report.md   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update .github/ISSUE_TEMPLATE/bug_report.md  Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com,0
fix: training_step_end doesn't work as stated in docs (#8188),0.76307464,def training_step(...):,,1
Deprecate DDPPlugin.task_idx (#8203),0.77152574,Deprecated DDPPlugin.task_idx in favor of DDPPlugin.local_rank (#8203),,1
Add missing logging tests (#8195),0.69378936,Cleaning up stale logger tests (#3490),,0
Use full torch.distributed import (#8200),0.78056556,import torch,,1
Sync our torchmetrics wrappers after the 0.4 release (#8205),0.6595595,Avoided false positive warning about using sync_dist when using torchmetrics (#14143),Co-authored-by: Jirka jirka.borovec@seznam.cz,0
Fix Deprecation warning in DDPSpawn (#8193),0.7685543,Merged the implementation of DDPSpawnStrategy into DDPStrategy and removed DDPSpawnStrategy (#14952),,1
Add CODEOWNERS for progress dataclasses (#8196),0.5681432,"Connect the progress tracking dataclasses to the loops (#8244, #8362)",,0
[refactor] Add should_raise_exception for gpus / tpus utilities (#8194),0.61483467,"Deprecated log_gpu_memory, gpu_metrics, and util funcs in favor of DeviceStatsMonitor callback (#9921)",  add should_raise   update changelog   Update pytorch_lightning/utilities/device_parser.py   Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com   add to tpu_cores parser   add should_raise description   update on comments   update changelog   Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,0
add how to contribute (#8129),0.4495698,Made type hints public (#17100),  add how to contribute   docs   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Apply suggestions from code review  Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Roger Shieh sh.rog@protonmail.ch  Update README.md  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Roger Shieh sh.rog@protonmail.ch,0
Improvements and changes to progress tracking dataclasses (#8140),0.7033018,Progress tracking,  Improvements to progress dataclasses   Update CHANGELOG   Rename function   Undo CODEOWNERS update ,1
Avoid passing unnecessary params from TPUSpawn to DDPSpawn (#8192),0.62242544,"Removed process_idx from the {DDPSpawnPlugin,TPUSpawnPlugin}.new_process methods (#10022)",,0
Update dataloaders params in example (#8191),0.62349427,"Working with multiple dataloaders (#16800, #16753)",,0
training loop refactor - move val loop  (#8120),0.8075011,Refactored training loop (#2336),"  EvaluationDataLoaderLoop -> EvaluationLoop   proposed rename files   imports   bad merge   update init files   glue imports together   rename fit_loop.validation_loop to fit_loop.val_loop   move loop   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Group imports   Resolve circular import   Comment   fix test   try to resolve circ import   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com",1
remove deadcode in trainer (#8121),0.66763484,Removed trainer.fit() return value of 1. It has no return now (#7237),,0
Make Plugins Proxies after transfering ownership (#8117),0.41077977,Initial plugin server (#16523),"  Update accelerator_connector.py   Update accelerator_connector.py   Update accelerator_connector.py   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Update accelerator_connector.py   Update accelerator_connector.py   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com",0
Bugfix/swa iterable dset (#8172),0.5245542,Disabled sampler replacement when using IterableDataset (#11507),  add test   add fix   Update CHANGELOG.md ,0
Fix metric attribute lookup (#8181),0.6680622,Changed metrics_to_scalars to work with any collection or value (#7888),  Fix metric attribute lookup   Update CHANGELOG.md   Split tests ,0
fix NCCL error with non-consecutive trainer gpus (#8165),0.6901689,"Deprecated the use of Trainer(gpus=""i"") (string) for selecting the i-th GPU; from v1.5 this will set the number of GPUs instead of the index (#6388)", device ids in barrier  x x s same fix for spawn fix non-nccl  x   add changelog   get nccl backend   get backend   Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,0
XLA Profiler integration (#8014),0.58537185,Split profilers module (#6261),,0
[bugfix] Add mechanism to prevent deadlock for DDP on Exception Trigger (#8167),0.5846759,DDP custom implementation support (override these hooks):,"  add mechanism to prevent deadlock   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   resolve flake8 + update changelog   update on comments   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   remove space   resolve bugs   overwrite config   update on comments   update on comments   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   update test with comments   Update pytorch_lightning/plugins/training_type/parallel.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  update on comments  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",0
[bugfix] Clean Validation Sanity Checking metrics (#8171),0.67293876,Doing this minor release because correct validation metrics logging is critical.,  resolve logging issue   update changelog   remove breakpoint   resolve bugs   remove pass ,0
Merge pull request #8174 from PyTorchLightning/bugfix/8159_log_gpu_memory_on_step,0.6553167,Removed deprecated pytorch_lightning.utilities.memory.get_gpu_memory_map in favor of pytorch_lightning.accelerators.cuda.get_nvidia_gpu_stats (#15617),[bugfix] Resolve memory not logged when missing metrics,0
Fix module dict in base finetuning (#8170),0.5012846,Module imports are now catching ModuleNotFoundError instead of ImportError (#9867),  Fix module dict in base finetuning   Update CHANGELOG.md ,0
remove message (#8163),0.51374954,Remove MetricsHolder (#7909),,0
Refactor unnecessary else / elif when if block has a return statement (#8156),0.5215136,refactored inner eval loop (#3141),Co-authored-by: deepsource-autofix[bot] 62050782+deepsource-autofix[bot]@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz,0
Fix dangerous default argument (#8164),0.5720079,Args should come after the last positional argument (#1807),Co-authored-by: deepsource-autofix[bot] 62050782+deepsource-autofix[bot]@users.noreply.github.com,0
Remove unnecessary use of comprehension (#8147),0.55493546,refactored inner eval loop (#3141),Co-authored-by: deepsource-autofix[bot] 62050782+deepsource-autofix[bot]@users.noreply.github.com,0
Iterate dictionary directly (#8155),0.60977197,    # Return a list of dictionaries with commands:,Co-authored-by: deepsource-autofix[bot] 62050782+deepsource-autofix[bot]@users.noreply.github.com,0
resurface lost ddp info message (#8111),0.60821956,DDP(2) backend (#2796),,0
ignore tests in DeepSource analyses (#8151),0.39921355,Removed no return warning from val/test step (#6139),  ignore tests   . ,0
Remove unnecessary use of comprehension (#8149),0.55719143,refactored inner eval loop (#3141),Co-authored-by: deepsource-autofix[bot] 62050782+deepsource-autofix[bot]@users.noreply.github.com,0
add .deepsource.toml (#8144),0.39949033,"Updated DeepSpeed ZeRO (#6546, #6752, #6142, #6321)",,0
Support state restoration of logged results 2/2(#7966),0.5002543,Un-balanced logging properly supported (#5119),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
document exceptions in utilities (#8122),0.48203155,Used raise .. from .. to explicitly chain exceptions (#3750),Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
"pytorch_lightning.loops file structure: group by dataloader, epoch, and batch loop  (#8077)",0.60847825,| Import pytorch_lightning.loops.base.Loop                                                                   | 1.9             | pytorch_lightning.loops.loop.Loop             |,,0
GPU CI - run torch 1.8 (LTS) (#8116),0.6362106,Moved torch.cuda.set_device() to enable collective calls earlier in setup (#8312),,0
Fix notebook links (#8089),0.40683797,Updated app URLs to the latest format (#16568),  Fix notebook links   update   BERT   docs   Update README.md   Apply suggestions from code review   Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Nuke RPC (#8101),0.42376673,Remove MetricsHolder (#7909),,0
Add torchelastic check when sanitizing GPUs (#8095),0.5385322,"Deprecated log_gpu_memory, gpu_metrics, and util funcs in favor of DeviceStatsMonitor callback (#9921)",  Add torchelastic check   Add changelog   Address review   fix ,0
Loop Refactor 6/N - Remove Old Predict Loop (#8094),0.73647827,"Refactored prediction loop interface; added new classes PredictionLoop, PredictionEpochLoop (#7700, #8077)",,1
restrict public interface of training loop (#8024),0.5288103,"Refactored internal loop interface; added new classes FitLoop, TrainingEpochLoop, TrainingBatchLoop (#7871, #8077)","  active optimizers   check checkpoint callback   epoch loop properties   epoch loop methods   training_batch_loop   changelog   update chlog   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   unused imports   yapf   backward   fix missing string reference   is_last_batch remains public   remove dead code   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com",0
Loop Refactor 5/N - Prediction Loop (#7700),0.714269,Refactored training loop (#2336),"  integrate d180bb2   Minor changes   Refactor loop logic into logger connector   Refactor test   Tighter fx validator   Add back split idx   Typing   update   Conflict   Fix tests   resolve grad_norm   update   move to train loop   Bye grad_norm_dict parameter   Fix sync test   update   Fix bug when validation is run mid epoch   fix grad_norm_dict test   Fix fx_validator test   fix grad_norm_dict test   Fix order bug   Detach tensors in test   resolve some tests   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   remove pdb   resolve flake8   Update test   more tests   Revert last thomas' changes   resolve 1 test   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Refactor context restoration   integrate latest changes from logger connector refactor poc   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   integrate latest changes from logger connector refactor poc   Minor changes   update changelog   Remove unused argument   Update CHANGELOG   Copy call_hook changes   Docs   Fix ref   move to cpu   Bad merge   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   remove pdb   remove pdb   Refactor to   Avoid partial   trigger ci   Bad merge   integrate latest logger connector changes   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   remove grad norm dicts list   Diff   properties first   Bad merge   Reuse metrics_to_scalars   Use active loop   Move to device   resolve test   integrate latest changes from logger connector poc   define union   define union   Update logger connector   Update result   Update imports   Update after rename   Refactor reduce_fx and op   Fix test after rename   mypy   integrate latest logger connector refactor poc changes   Fix test   Refactor test   Deprecate self.log(sync_dist_op) in favor of self.log(reduce_fx)   Undo field   add redundant return   rename   rename files and classes  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci   rename   Replace code   Fix names and imports   Remove metric_attribute   imports   loop hygiene   yapf on loops   protected new loop trigger   rename NEW LOOP guard   integrate latest logger connector changes   integrate latest logger connector changes (eval loop)   resolve todo dataloading reset   re-add notebooks   add missing init   bad merge   remove NEW_LOOP guard   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   flake8   exclude coverage   coverage   integrate #7917, remove teardown from training loop   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  update ""accumulated_batches_reached"" condition  based on if iter count was updated  or not   remove public loop properties   make skip backward protected again   typing base loop   typing fit loop   typing training_batch_loop   typing evaluation loop   typing prediction loop   typing training epoch loop   dataloader_loop   evaluation_dataloader_loop   prediction_dataloader_loop   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   integrate train loop changes from master   integrate eval loop changes from master   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   fix tpipes moving model to cpu and leaving it there.   don't reset fit loop   don't reset fit loop   fix test iteration count <-> batch_idx reset   replace torch.Tensor -> Tensor   fix attribute error to block_ddp_sync_behaviour   fix flake8 and yapf conflict   remove redundant override   add classes   Co-authored-by: Justus Schock justus.schock@rwth-aachen.de Co-authored-by: Justus Schock justus.schock@posteo.de Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com   trainer changes   connect   clean up   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update test renaming   rename evaluation loop to evaluation epoch loop   minor docstring improvements   update chlog   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   try ci fix   update code owners for pl/loops   update mock path   re-order   simplify dataloader reset   simplify get_dataloaders()   save predictions on_run_end()   improve skip condition re-routing   re-order   remove unused type import   check which assert is failing   pig   hobbit   teardown for evaluation   Revert ""hobbit""   This reverts commit e81b0dbee31da813ba6ad58f74d236863c86d18e.  Revert ""pig""  This reverts commit 33d89e0720ce7380af80917b15a79362d9416ae7.  Revert ""check which assert is failing""  This reverts commit b7483b425cab95290eb2cbf354ccb0a77004df83.   free memory in fit loop teardown   update docstring   period   remove dead code   else carlos   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update pytorch_lightning/loops/dataloader/evaluation_dataloader_loop.py  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   update chlog   unused imp   move default construction in run_evaluation   add something for lawyer to read   switch typehint for eval loop trainer property   add missing imports   remove a todo that needs more discussion   combine _get_num_dataloaders with the property   Update pytorch_lightning/loops/dataloader/dataloader_loop.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   black + yapf   avoid coverage on old unused eval loop   empty space in docstring   Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk   resolve todo for args forwarding   weekproxy trainer   fix check for num dataloaders kwargs   clean up num prediction dataloaders property   free memory   rm notebooks folder   rm old file   revert changes to old eval loop   bad merge   undo teardown   setup signature   remove file for notes   free memory   chlog   Revert ""weekproxy trainer""   This reverts commit d4e6969170b80db4c9e6111fa9af507c740cde4a.   connect trainer   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   clean up max batches and dataloaders   max batches handling   no grad handling   unused argument   protected attrs   unused imports   undo unintentional rename   consistent naming   capitalization in docstring   list all args   Update pytorch_lightning/loops/prediction_epoch_loop.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update pytorch_lightning/loops/prediction_epoch_loop.py  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update pytorch_lightning/loops/dataloader/prediction_dataloader_loop.py  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update pytorch_lightning/loops/dataloader/prediction_dataloader_loop.py  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update pytorch_lightning/loops/prediction_epoch_loop.py  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: tchaton thomas@grid.ai Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Justus Schock justus.schock@posteo.de Co-authored-by: Justus Schock justus.schock@rwth-aachen.de Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk",1
Add log_device_info to Trainer (#8079),0.7056722,Removed obsolete self._device in Trainer (#1849),,1
Update fit with no validation hook test (#7738),0.5559106,"Changed setup() and teardown() stage argument to take any of {fit,validate,test,predict} (#6386)",  Add callback to hook tests and add predict test   Fix lambda callback test   Simplify lambda call test   Use LambdaCallback   Dynamically append to called for the model   Remove print   Consistency   Consistency   Prepare args/kwargs testing   yapf doesn't like dict literals   Add arguments for fit no val test   Add arguments for fit no val test   Test arguments   Datamodule refactor   Fix eval test   Update full fit + val test   Update test   Update resume test   Remove changes   Fix ,0
"Support calling fit and test scripts using ""python -m"" module syntax with DDP (#8073)",0.5681181,python script.py fit,Co-authored-by: Nisheeth Lahoti nisheeth@rephrase.ai,0
Add add_to_queue/get_from_queue for DDP spawn(#7916),0.703938,"decoupled DDP, DDP spawn (#3733, #3766, #3767, #3774, #3802, #3806)",Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Deprecate moved warning functions (#8085),0.7329755,Deprecation warning (#3844),,1
Use XLA utility API to move data to CPU (Single TPU core) (#8078),0.9758271,Used XLA utility API to move data to CPU (Single TPU core) (#8078),,1
[Refactor] Remove _run_evaluation + 3 EvaluationLoop (#8065),0.6950393,clean up hooks in run_evaluation (#3156),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Specify packaging version to be more than 17.0 (#8030),0.6014462,Parsed local package versions (#13933),,0
Add forgotten colon (#8076),0.41112536,Syntax changes are: ,Very small typo correction: add forgotten : in finetuning callbacks docs.,0
Loop Refactor 4/N - Remove Old Evaluation Loop (#8056),0.7548777,Refactored Loops,,1
rename old Trainer.train_loop -> Trainer.fit_loop  (#8025),0.8659705,Deprecated the Trainer.train_loop property in favor of Trainer.fit_loop (#8025),,1
update changelog after 1.3.7 (#8075),0.6454395,Full Changelog,,0
[pre-commit.ci] pre-commit autoupdate (#8067),0.63821346,@akihironitta @awaelchli @Borda @carmocca @kaushikb11 @krshrimali @mauvilsa @otaj @pre-commit-ci @rohitgr7 @semaphore-egg @tkonopka @wayi1,updates: - github.com/PyCQA/isort: 5.8.0 → 5.9.1 Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Update fit with val hook test (#8060),0.5206241,Removed no return warning from val/test step (#6139),,0
Actually show deprecation warnings and their line level [2/2] (#8002),0.6868256,Deprecation warning (#3844),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Add LightningCLI(save_config_overwrite=False|True) (#8059),0.8358952,"- `SaveConfigCallback` instances should only save the config once to allow having the `overwrite=False` safeguard when using `LightningCLI(..., run=False)` ([#14927](https://github.com/Lightning-AI/lightning/pull/14927))",,1
Fix checkpointed state for lr_schedulers with step interval (#7877),0.7284585,Disabled lr_scheduler.step() in manual optimization  (#6825),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
Fix(Early Stopping): move best score to device (#7959),0.5104035,We cleaned up the properties related to device indices (#14829).,,0
fix formatting typo in seed_everything docs (#8052),0.59418285,seed_everything now fails when an invalid seed value is passed instead of selecting a random seed (#8787),,0
prevent memory test failing due to earlier test leaking memory (#8029),0.6681996,Resolve memory leak for evaluation (#6326),,0
[bugfix] Properly name PyTorchProfiler traces (#8009),0.8154599,Improved PyTorchProfiler chrome traces names (#8009),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
Update evaluation hook tests (#8013),0.628077,Updated app testing (#16000),,0
Merge pull request #7990 from PyTorchLightning/refactor/loops/loops_everywhere_eval,0.65264654,- Added `Loop.replace` to easily switch one loop for another ([#10324](https://github.com/PyTorchLightning/pytorch-lightning/pull/10324)),Loop Refactor 3/N - Evaluation Loop,0
Use DistributedSampler when running with custom accelerator (#7814),0.5990188,It is now possible to use custom samplers in a distributed environment without the need to set replace_ddp_sampler=False and wrap your sampler manually with the DistributedSampler.,Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Deprecate returning extras with grads (#7994),0.87403214,Deprecated automatically detaching returned extras with grads (#7994),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Add warning_cache.deprecation and set warning stacklevel [1/2] (#8005),0.6654549,warning_utils >> warnings,Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
Fix Grid run commands (#8021),0.41876823,Improved the error message when the LightningWork is missing the run method (#14759),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk,0
CHANGELOG update after v1.3.6 release (#7988),0.6651999,Full Changelog,,0
Ipynb update (#8004),0.41769317,Raised a MisconfigurationException if batch transfer hooks are overriden with IPUAccelerator (13961),  git submodule update --remote   update notebooks in docs   prune   _notebooks   docs   path   path   ignore   head ,0
[feat] Allow overriding optimizer_zero_grad and/or optimizer_step when using accumulate_grad_batches (#7980),0.741974,The default value of accumulate_grad_batches has changed from 1 to None (#9652).,,1
progressive restoring of trainer state (#7652),0.7113388,Allow easy trainer re-instantiation (#7508),,1
"[feat] Add {,load_}state_dict to ResultCollection 1/n (#7948)",0.7177871,"+def load_state_dict(self, state):","  add metric reload   add tests   update changelog   udpate   remove print   remove attribute_name   update   update   resolve test   update on comments   bypass typing bug   update on comments   Update CHANGELOG   Update tests   Update code   Check if TODO persists   Remove unrelated changes   Fixes   Revert ""Check if TODO persists""   This reverts commit 68dac4ae693764b9fca4c8f4cef4fce66ce92122.   Do not serialize dataclasses   Avoid recostructing meta twice   Keep previous sync_fn   Move to device and map_location   Fix bug   Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com",1
[feat] Named Parameter Groups in LearningRateMonitor (#7987),0.57063574,Deprecated lr_sch_names from LearningRateMonitor (#10066),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
New speed documentation (#7665),0.5626435,"many speed improvements (how we move data, adjusted some flags & PL now adds 300ms overhead per epoch only!)","  amp   amp   docs   add guides   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   amp   amp   docs   add guides   speed guides   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Delete ds.txt   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Update conf.py   Update docs.txt   remove 16 bit   remove finetune from speed guide   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   speed   speed   speed   speed   speed   speed   speed   speed   speed   speed   speed   speed   remove early stopping from speed guide   remove early stopping from speed guide   remove early stopping from speed guide   fix label   fix sync   reviews   Update trainer.rst   Update trainer.rst   Update speed.rst   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
Fix Special Tests (#7841),0.6256788,- fixed all the .test() calls,  Remove port setting   Drop one of the params to see what happens   Split tests into two   Try using port setting ,0
Do not override the logged epoch in logged_metrics (#7982),0.92260647,Do not override the existing epoch value in logged_metrics when already logged by the user (#7982),,1
Change WarningCache to subclass set (#7995),0.5323177,warning_utils >> warnings,,0
Add predict hook test (#7973),0.560254,"Refactored prediction loop interface; added new classes PredictionLoop, PredictionEpochLoop (#7700, #8077)",,0
[doc] Add more reference around predict_step (#7997),0.64181906,"Refactored prediction loop interface; added new classes PredictionLoop, PredictionEpochLoop (#7700, #8077)",  add predict examples   update on comments ,0
[fix] Enable manual optimization DeepSpeed (#7970),0.70238936,"Implemented DeepSpeedPlugin._setup_model_and_optimizers (#10009, #10064)","  resolve manual optimization   resolve manual optimization   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update changelog   Simplify message   Move from deprecated   Split model parallel/manual model   Use property   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: SeanNaren sean@grid.ai",1
Support save_hyperparameters() in LightningModule dataclass (#7992),0.62645257,- Moved the warning about saving nn.Module in `save_hyperparameters()` to before the deepcopy ([#15132](https://github.com/Lightning-AI/lightning/pull/15132)),Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
Loop Refactor 2/N - Remove Old Training Loop (#7985),0.8519188,Refactored training loop (#2336),Co-authored-by: Justus Schock justus.schock@rwth-aachen.de Co-authored-by: Justus Schock justus.schock@posteo.de Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,1
Make optimizers skippable when using amp (#7975),0.6424197,enable_pl_optimizer=False by default to temporarily fix AMP issues (#5163),Co-authored-by: Yifu Wang yifuwang@2012@gmail.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
Easier configurability of callbacks that should always be present in LightningCLI (#7964),0.6516437,"LightningCLI now supports registries for callbacks, optimizers, learning rate schedulers, LightningModules and LightningDataModules. This greatly improves the command line experience as only the class names and arguments are required as follows:",Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
Add tpu_spawn_debug to plugin registry (#7933),0.58766735,- Renamed `strategy='tpu_spawn'` to `strategy='xla'` and `strategy='tpu_spawn_debug'` to `strategy='xla_debug'` ([#16781](https://github.com/Lightning-AI/lightning/pull/16781)),,0
Pt 1.9 breaking fix: iter type hint (#7993),0.49620867,Made type hints public (#17100),,0
Improvements related to save of config file by LightningCLI (#7963),0.7317453,"- `SaveConfigCallback` instances should only save the config once to allow having the `overwrite=False` safeguard when using `LightningCLI(..., run=False)` ([#14927](https://github.com/Lightning-AI/lightning/pull/14927))","   Exclude SaveConfigCallback for fast_dev_run=True.    SaveConfigCallback give a clearer message if config file already exists.   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci    Added unit test   Added entry in changelog  Improved save config docstring   Fix log line   Fixes   Fix changelog entry   Update pytorch_lightning/utilities/cli.py   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com  Apply suggestions from code review  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Suggested fixed change  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com",1
Loop Refactor 1/N - Training Loop (#7871),0.8822988,Refactored training loop (#2336),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: tchaton thomas@grid.ai Co-authored-by: Justus Schock justus.schock@rwth-aachen.de Co-authored-by: Justus Schock justus.schock@posteo.de Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,1
Standardize positional datamodule and argument names (#7431),0.65363985,Support shorthand notation to instantiate datamodules (#10011),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Add docs for IPUs (#7923),0.51431113,Docs improvements,  Added base docs for IPUs   Fix   Add details around poptorch profiler and model parallelism   more description   Add image   Clearer messaging   Cleanup   Better name   Add note   Add some details around device iterations and model parallelism   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   Add a small install comment   Add clip gradients not supported   Update docs/source/advanced/ipu.rst   Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com  Add note  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,0
"Remove convert_to_half, suggest using model.half (#7974)",0.5135387,Model summary: add 1 decimal place (#4745),,0
DeepSpeed Infinity Update (#7234),0.7033183,"Updated DeepSpeed ZeRO (#6546, #6752, #6142, #6321)","  Update configs to match latest API   Ensure we move the entire model to device before configure optimizer is called   Add missing param   Expose parameters   Update references, drop local rank as it's now infered from the environment variable   Fix ref   Force install deepspeed 0.3.16   Add guard for init   Update pytorch_lightning/plugins/training_type/deepspeed.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   Revert type checking   Install master for CI for testing purposes   Update CI   Fix tests   Add check   Update versions   Set precision   Fix   See if i can force upgrade   Attempt to fix   Drop   Add changelog   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com",1
Improve LightningModule hook tests (#7944),0.68886036,1. Override LightningModule hook,,0
Properly handle parent modules w/ parameters in BaseFinetuning callback (#7931),0.5174385,  * Removed `opt_idx` argument from `BaseFinetuning.finetune_function` callback method,Co-authored-by: Daniel Dale dan@distributedinsight.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Handle errors due to uninitailized parameters (#7642),0.53761727,Used raise .. from .. to explicitly chain exceptions (#3750),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
remove parsing comments (#7958),0.40137213,The brittle argument parsing utilities (#16708), remove parsing comments \s  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
added on_test_start() documentation (#7962),0.613285,"def on_test_start(self, trainer, pl_module):",Co-authored-by: ehuang68 <> Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Seed all workers when using DDP (#7942),0.59629345,"decoupled DDP, DDP spawn (#3733, #3766, #3767, #3774, #3802, #3806)",  Seed all workers when using DDP   Fix to dataloader seeding   Make argument name explicit   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   Use f-strings when logging   Removed a redundant log message   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Improve LightningDataModule hook test and fix dataloader_idx argument (#7941),0.6495646,- Added profiling of `LightningDataModule` hooks ([#12971](https://github.com/Lightning-AI/lightning/pull/12971)),Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,0
deprecate hpc_load() and integrate it with restore() (#7955),0.7191871,Deprecated the use of CheckpointConnector.hpc_load() in favor of CheckpointConnector.restore() (#7652),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
fix myst-parser warning blocking docs ci (#7967),0.4244504,  * `pytorch_lightning.utilities.warnings.rank_zero_warn` in favor of `pytorch_lightning.utilities.rank_zero.rank_zero_warn`,,0
update chlog + legacy chpt (#7954),0.4943058,Removed logger_connector legacy code (#6733),  update chlog   legacy ,0
Add dataclass support to apply_to_collection (#7935),0.6894278,- Added support for dataclasses in `apply_to_collections` ([#11889](https://github.com/PyTorchLightning/pytorch-lightning/pull/11889)),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
LightningCLI support for argument links applied on instantiation (#7895),0.68204486,"LightningCLI.add_core_arguments_to_parser, LightningCLI.parse_arguments now take a parser argument (#8721)",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Remove rank_zero_only on DataModule prepare_data (#7945),0.60100615,"Changed rank_zero_warn to NotImplementedError in the {train, val, test, predict}_dataloader hooks that Lightning(Data)Module uses (#9161)",Signed-off-by: Max Ehrlich max.ehr@gmail.com,0
IPU Integration 5/5 (#7867),0.5128518,Graphcore IPU devices,"  Initial changes   Add broken example for now   Fix reference   Fix format   Code runs   Fixes   Clear up files   Add tests, helpers, fixes   Small cleanups   Refactors based on review   Swap to special tests   Add special tests   Add source   Cleanups   Add logic to attach/detach model from devices   Fixes for tests   Fixes for tests   Move earlier   Cleanups   Add check for nvcc   Add tests, cleanups   Fix errors   fix   Try condition   Add missing annotation   Clearer   Clearer message   Fix variable   Cleanups   Add comment   CHANGELOG.md   Add simple selection test   Remove special=True to see what happens   Fix test   Update tests/accelerators/test_ipu.py   Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com   Convert ipu_cores -> ipus   Add typing, fail earlier   simplify precision   Add test, add helper   fix accum   Update pytorch_lightning/plugins/training_type/ipu.py   Co-authored-by: thomas chaton thomas@grid.ai   Use stages   Make sure warning message returned   thorw error   Add more tests, use fs   add comment   Clean   Address feedback, add IPU tests   Fixes   Fix signature   Add types   Remove autoround   Add docstring   ipu_cores -> ipus   Add test, remove unnecessary precision set   Add optimizer test   Add precision back with test   Address code review   Change to probs   Move some of the asserts earlier   Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: thomas chaton thomas@grid.ai",0
refactor checkpoint loading for training type plugins (#7928),0.68131155,Refactor load in checkpoint connector (#4593),"  plugin loading logic   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   integrate loading for test   fix   fix   unused iport   Update pytorch_lightning/trainer/connectors/checkpoint_connector.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com",0
is_overridden improvements (#7918),0.7085264,modular is_overridden (#3290),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Delete on_after_backward unused argument (#7925),0.48675877,Args should come after the last positional argument (#1807),,0
Deprecate the default EarlyStopping callback monitor value (#7907),0.7798002,Deprecated default value of monitor argument in EarlyStopping callback to enforce monitor as a required argument (#7907),"  removed monitor default value and added depreceation message   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   format change   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   requested changes   added test   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   format changes   typehint change   Update CHANGELOG.md   requested changes   regex   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com",1
split restore_training_state into logical parts [2 / 2] (#7900),0.5312195,def training_step(...):,,0
split restore_training_state into logical parts [1 / 2] (#7901),0.54144335,def training_step(...):,Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
add pre-commit hooks (#7906),0.551648,New Hooks,,0
Remove legacy teardown check in train loop (#7917),0.6175443,Changed teardown() in Accelerator to allow training_type_plugin to customize teardown logic (#7579),,0
Clean-up after logger connector redesign 2/2 (#7631),0.68414813,Dramatically simplify the LoggerConnector (#7882),Co-authored-by: tchaton thomas@grid.ai Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Remove fn check for ipu output (#7915),0.5650729,Changed IoU remove_bg bool to ignore_index optional int (#3098),,0
Remove dead code (#7910),0.51938593,clean up data reset (#3161),,0
Clean-up after logger connector redesign 1/2 (#7909),0.68581724,Dramatically simplify the LoggerConnector (#7882),,0
Enable logger connector re-design (#7891),0.7696151,Removed logger_connector legacy code (#6733),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
add logger to all (#6854),0.7419153,Change default logger to a dedicated one (#1064),,1
Deprecate LightningDataModule lifecycle properties (#7657),0.7358781,- Added profiling of `LightningDataModule` hooks ([#12971](https://github.com/Lightning-AI/lightning/pull/12971)),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
refactor CheckpointConnector.restore_weights  (#7862),0.68117285,Refactor load in checkpoint connector (#4593),,0
Fix logs overwriting issue for remote fs (#7889),0.49900532,- Added support for writing logs to remote file systems with the `CSVLogger` ([#16880](https://github.com/Lightning-AI/lightning/pull/16880)),  Fix logs overwriting issue for remote fs   Add test ,0
Logger connector re-design _Metadata.reduce_fx fixes. (#7890),0.7057361,Removed logger_connector legacy code (#6733),,1
New logger connector code (#7882),0.76262903,Removed logger_connector legacy code (#6733),  New logger connector code   Update CHANGELOG   Update requirements   Fix import path   Add new suffix   Tests   Minor changes   Rename and reorder   Fix import   Formatting   Fix with seed_everything?   Fix test?   Fix test?   Minor change   Minor changes   Minor changes   Force float   Fix minimal bug   Fix minimal bug   Update with latest changes   Fix import   bad merge   update typing   Co-authored-by: tchaton thomas@grid.ai,1
Use apply_to_collection in metrics_to_scalars (#7888),0.8951121,Changed metrics_to_scalars to work with any collection or value (#7888),  Use apply_to_collection in metrics_to_scalars   Typing   Update CHANGELOG   Update pytorch_lightning/utilities/metrics.py   Whitespace ,1
Refactor notebooks (#7752),0.58454376,Refactoring,"  drop notebooks   add submodule   copy notebooks   docs include ipynb   fix headers   CI   readthedocs   manifest   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   req   workdir   pandoc   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   pandoc   manifest   Apply suggestions from code review   fix versions   checkout   git submodule update --init --recursive --remote   notebooks @docs   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
better use of void (#7809),0.34962222,Resolve memory leak for evaluation (#6326),  use void   format ,0
Merge pull request #7872 from PyTorchLightning/refactor/logger-poc-changes,0.63349104,- Deprecated `LightningLoggerBase.agg_and_log_metrics` in favor of `LightningLoggerBase.log_metrics` ([#11832](https://github.com/PyTorchLightning/pytorch-lightning/pull/11832)),Random fixes for logger connector PoC,0
Only track dev debugger events if enabled (#7875),0.4809065,"Disabled checkpointing, earlystopping and logging with fast_dev_run (#5277)",,0
Add log_grad_norm hook to LightningModule (#7873),0.8355762,- Removed the `LightningModule.log_grad_norm()` hook method ([#16745](https://github.com/Lightning-AI/lightning/pull/16745)),,1
Move training_output validation to after train_step_end (#7868),0.9999999,Move training_output validation to after train_step_end (#7868),"  move validation to after aggregation   changelog   add test for training_step_end   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",1
Stricter FxValidator and add hooks (#7874),0.48331416,"Simplified ""should run validation"" logic (#7682)",  Stricter FxValidator and add hooks   Update CHANGELOG ,0
update fsspec to 2021.06.0 (#7869),0.5859066,Changed fsspec to tuner (#4458),,0
add warning when Trainer(log_every_n_steps) not well chosen (#7734),0.77454334,Rename Trainer arguments row_log_interval >> log_every_n_steps and log_save_interval >> flush_logs_every_n_steps (#3748),"  add warning   update changelog   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   logger check   add docstring for test   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com",1
[IPU] Add hooks for IPU lifecycle 4/5 (#7864),0.5112655,Raised a MisconfigurationException if batch transfer hooks are overriden with IPUAccelerator (13961),,0
[Test] Add extra test for val_check_interval in distributed scenario (#7863),0.5271559,Removed no return warning from val/test step (#6139),"  add extra test   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   add computation   Update docs/source/common/trainer.rst   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update docs/source/common/trainer.rst  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update tests/trainer/test_dataloaders.py  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   use tmpdir   update on comments   update   Update tests/callbacks/test_progress_bar.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com",0
[IPU] Add reset dataloader hooks to training type plugin 3/n (#7861),0.8132885,Removed on_reset_*_dataloader hooks in TrainingType Plugins and Accelerators (#8858),"  Add hooks   Add tests for hooks   Add changelog   Test changes, add typing ",1
[bugfix] Resolve LearningRateMonitor + BackboneFinetuning (#7835),0.67659384,Changed computer_vision_fine_tunning example to use BackboneLambdaFinetuningCallback (#5377),  add test + resolve bug   update changelog   resolve bug   resolve bug   Update pytorch_lightning/callbacks/lr_monitor.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/callbacks/lr_monitor.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   update on comments   resolve comments   update   Update tests/callbacks/test_lr_monitor.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update pytorch_lightning/callbacks/lr_monitor.py  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
make Trainer.resume_from_checkpoint a read-only property (#7857),0.82901204,1. Remove resume_from_checkpoint from the Trainer,,1
refactor on_gpu handling in checkpoint connector (#7860),0.67543876,Result - make monitor default to checkpoint_on to simplify (#3571),,0
update docs example with sharded eval step (#7748),0.5084877,def configure_sharded_model(self):,Co-authored-by: ananthsub ananth.subramaniam@gmail.com,0
[bugfix] Minor improvements to apply_to_collection and type signature of log_dict (#7851),0.9537417,Minor improvements to apply_to_collection and type signature of log_dict (#7851),"  minor fixeS   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",1
move amp checkpoint state management to precision plugin (#7831),0.58575964,- Added migration logic to warn about checkpoints with apex AMP state ([#16161](https://github.com/Lightning-AI/lightning/pull/16161)),,0
Fix an incorrect CHANGELOG link (#7850),0.541576,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",,0
Fix NVIDIA docker versions (#7834),0.46244064,nvidia/apex deprecation (#16039),,0
[docs] Fix truncated_bptt_steps docs (#7846),0.7166366,    self.truncated_bptt_steps = 10,,1
[IPU] Add special tests for IPUs 2/n (#7833),0.50845385,Changed IoU score behavior for classes absent in target and pred (#3098),"  Add special tests for IPUs, run nvprof only if cuda available   Add missing min_gpu ",0
[IPU] Call accelerator hooks regardless if LM hook overridden 1/n (#7826),0.64131016,Accelerator hooks are called regardless if LightningModule overrides the same hooks (#7826),  Modify API to ensure hooks defined in the accelerator are called as expected   handle step_end in dp   Add changelog   Update pytorch_lightning/trainer/trainer.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Add todo and explanation  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
[doc] Move each profiler to its own file + Add missing PyTorchProfiler to the doc (#7822),0.809815,Removed deprecated profiled_functions argument from PyTorchProfiler (#9178),,1
Add warning to trainstep output (#7779),0.61613077,"- Added info message when the `Trainer` arguments `limit_*_batches`, `overfit_batches`, or `val_check_interval` are set to `1` or `1.0` ([#11950](https://github.com/PyTorchLightning/pytorch-lightning/pull/11950))","  Update training_loop.py   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Update test_training_loop.py   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Update test_training_loop.py   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Update training_loop.py   Update pytorch_lightning/trainer/training_loop.py   Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk   Update test_training_loop.py   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Update training_loop.py   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  Update pytorch_lightning/trainer/training_loop.py  Co-authored-by: ananthsub ananth.subramaniam@gmail.com   Update training_loop.py   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Update test_training_loop.py   Update training_loop.py   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  escape regex  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk Co-authored-by: ananthsub ananth.subramaniam@gmail.com",0
[sharded plugin] Fix check for fp16 precision (#7825),0.5658978,Deprecated the pytorch_lightning.plugins.precision.sharded_native_amp.ShardedNativeMixedPrecisionPlugin class,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com,0
Fix support for torch Module type hints in LightningCLI (#7807),0.7307513,- Fixed issue where the CLI fails with certain torch objects ([#13153](https://github.com/Lightning-AI/lightning/pull/13153)),"  Fixed support for torch Module type hints in LightningCLI    Fix issue with serializing values when type hint is Any.    Run unit test only on newer torchvision versions in which the base class is Module.   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Minor change   Update CHANGELOG.md   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com",1
validate manual optimization and supported features before running training (#7788),0.5426017,Generalized Optimizer validation to accommodate both FSDP 1.x and 2.x (#16733),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Added Vulture dead code checker (#5654),0.43123472,"If we forgot somebody or you have a suggestion, find us on Discord :zap:","  integrated vulture CI   added vulture in workflows   added vulture in workflows   vulture logs verbose set false   Apply suggestions from code review   ignore name list and args to underscore naming   add ignore names   deadcode whitelist   deadcode whitelist   Apply suggestions from code review   Co-authored-by: Rahul Jha rahul722j@gmail.com  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci   Update whitelist   Sort   Updates   Updates   Apply suggestions from code review   Updates   Co-authored-by: Aniket Maurya aniket.maurya@gdn-commerce.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: chaton thomas@grid.ai Co-authored-by: Rahul Jha rahul722j@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk Co-authored-by: Ethan Harris ethanwharris@gmail.com",0
Add FSDP docs (#7791),0.669359,Native FSDP replaces Fairscale FSDP (#16400),  Add FSDP docs   Address reviews   Add note about how FSDP can replace pipe parallelism   Add import   Remove sentence ,0
Replace deprecated distributed_backend by acc in examples (#7795),0.6921436,Remove deprecated distributed_backend from Trainer (#10017),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Fix double precision + ddp_spawn (#6924),0.6128361,"decoupled DDP, DDP spawn (#3733, #3766, #3767, #3774, #3802, #3806)",  Initial fix   Initial fix   Initial fix   Updates   Updates   Update typing and docs   Undo accidental refactor   Remove unused imports   Add DDP double precision test   Remove unused variable   Update CHANGELOG.md   Fix test   Update tests   Formatting   Revert bad change   Add back changes   Correct wrapping order   Improve unwrapping   Correct wrapping order   Fix... finally   Respond to comments   Drop ddp test   Simplify ddp spawn test   Simplify ddp spawn test   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
apply_to_collection improvements and add apply_to_collections (#7769),0.64624774,Minor improvements to apply_to_collection and type signature of log_dict (#7851),  apply_to_collection improvements and add apply_to_collections   Update CHANGELOG   Minor fix   Minor fix   Remove attr   Swap is first is None   None test   OrderedDict support   flake8   Fix docstring ,0
Extend support for logging a collection (#7771),0.6216089,Minor improvements to apply_to_collection and type signature of log_dict (#7851),,0
update NGC docker (#7787),0.44617778,Pkg: fix parsing versions by @Borda in https://github.com/Lightning-AI/lightning/pull/15401,,0
Update pre-commit and add new hooks (#7781),0.5582293,New Hooks,  update precommit   Update .pre-commit-config.yaml   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Apply suggestions from code review  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Avoid adding None loss values in training_epoch_end (#7772),0.7181763,"def training_epoch_end(self, outputs):",,1
fix info message when max training time reached (#7780),0.6319958,"- Added info message when the `Trainer` arguments `limit_*_batches`, `overfit_batches`, or `val_check_interval` are set to `1` or `1.0` ([#11950](https://github.com/PyTorchLightning/pytorch-lightning/pull/11950))",  call time_elapsed   elapsed formatting   format   update test   changelog ,0
Use typing forward references (#7770),0.4088684,"This will compile forward and {training,validation,test,predict}_step",  Use typing forward references   Update pytorch_lightning/core/lightning.py ,0
Clean existing logging tests (#7760),0.85774946,Cleaning up stale logger tests (#3490),  Remove dev debugger metric tracking   Fix tests   Fix test   Import   Clean logging tests   flake8   Docstring ,1
Some test updates (#7761),0.67947006,Updated app testing (#16000),  Some test updates   flake8 ,0
Organize trainer properties (#7758),0.65429074,Removed TrainerProperties mixin and moved property definitions directly into Trainer (#9495),  Organize trainer properties   Single quote   Double quote ,0
Remove metric tracking from dev debugger (#7759),0.6426691,Change Metrics persistent default mode to False (#4685),  Remove dev debugger metric tracking   Fix tests   Fix test   Import   Fix tests   Fix test   flake8   Fix tests ,0
Add Test for memory consumption (#7733),0.5911937,Resolve memory leak for evaluation (#6326),  Add Test to ensure Training Batch is no longer in GPU memory when running validation   Add Test to ensure Training Batch is no longer in GPU memory when running validation   Add Test to ensure Training Batch is no longer in GPU memory when running validation   Temporary disable other tests   Verbose asserts   Verbose asserts   update tests + revert to original ci   Update test_evaluation_loop.py   Update test_evaluation_loop.py   Update tests/trainer/loops/test_evaluation_loop.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update tests/trainer/loops/test_evaluation_loop.py  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: justusschock justus.schock@psoteo.de Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Added save_config_filename init argument to LightningCLI (#7741),0.80671453,"- Deprecated duplicate `SaveConfigCallback` parameters in `LightningCLI.__init__`: `save_config_kwargs`, `save_config_overwrite` and `save_config_multifile`. New `save_config_kwargs` parameter should be used instead ([#14998](https://github.com/Lightning-AI/lightning/pull/14998))",,1
feat(wandb): log models as artifacts (#6231),0.6747437,Changed WandbLogger(log_model={True/'all'}) to log models as artifacts (#6231),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Rename and move Result (#7736),0.508007,Rename failed -> error in tables (#15608),Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,0
Update hooks pseudocode (#7713),0.6189097,Made optimization steps for hooks (#2363),,0
Add __len__ method to IndexBatchSamplerWrapper (#7681),0.6572739,- Removed deprecated `IndexBatchSamplerWrapper.batch_indices` ([#13565](https://github.com/Lightning-AI/lightning/pull/13565)),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Add tpuvm section in TPU docs (#7714),0.53367203,Enabled manual optimization for TPUs (#8458),,0
Always run validation inside the training loop epoch (#7357),0.83476,Validation is now always run inside the training epoch scope (#7357),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Update README.md (#7717),0.5347841,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",,0
Update sphinx version to 4.0 or later (#7716),0.67466074,Pinned sphinx-autodoc-typehints with <v1.15 (#11400),,0
Increase TPU Check timeout (#7706),0.9183628,Increased TPU check timeout from 20s to 100s (#5598),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Add ModelPruning(prune_on_train_epoch_end) to choose when to apply pruning (#7704),0.62753344,Changed EarlyStopping callback from by default running EarlyStopping.on_validation_end if only training is run. Set check_on_train_epoch_end to run the callback at the end of the train epoch instead of at the end of the validation epoch (#7069),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
CI: Reset cache weekly (#7686),0.4789809,clean up data reset (#3161),  Reset cache weekly   Update ci_test-base.yml   Update docs-checks.yml   Update ci_test-mnodes.yml   Update release-pypi.yml   Remove if latest ,0
fix: improve UserWarning message (#7685),0.5499264,Do not override PYTHONWARNINGS (#4700), fix: improve UserWarning message when both overfit and training dtaloader shuffling are enabled  fixes issue: #7656   chore: update changelog   Polish userwarning msg in pytorch_lightning/trainer/data_loading.py   Co-authored-by: ananthsub ananth.subramaniam@gmail.com   shuffling typo   Update CHANGELOG.md   Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Add should_rank_save_checkpoint property to Training Plugins (#7684),0.75358,Removed should_rank_save_checkpoint property from Trainer (#9433),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
Remove on epoch guard from the should stop validation check (#7701),0.8912735,"Removed the on_epoch guard from the ""should stop"" validation check (#7701)",,1
Refactor some loops code and hook tests (#7682),0.7108814,Refactored Loops,,1
Increment the total batch idx before the accumulation early exit (#7692),0.6441187,Integrated TrainingEpochLoop.total_batch_idx (#8598),  Increment the total batch idx before the accumulation early exit   Update CHANGELOG ,0
Move test_hooks.py code (#7689),0.5969615,moved ___step_end hooks (#3130),,0
MAINTAINER has been deprecated (#7683),0.50008184,Deprecation warning (#3844),,0
chlog for 1.3.2 + legacy test (#7676),0.5841676,support for latest test-tube logger optimized for PT 1.2.0.   ,,0
Fix global step update when the epoch is skipped (#7677),0.6555705,"To resolve fault-tolerance issues, we changed where the current epoch value gets increased.",  Fix global step update when the epoch is skipped   Update CHANGELOG   Move test ,0
Move parameter validation specific to TPU Training plugins (#7415),0.4989022,Move training_output validation to after train_step_end (#7868),  Move parameter validation specific to TPU Training plugins   update docstring ,0
Remove ProfilerConnector class (#7654),0.83885765,Removed ProfilerConnector (#7654),  Remove ProfilerConnector class   Update trainer.py   Update CHANGELOG.md   Update trainer.py   Update trainer.py   tests ,1
Fix progress bar print error when called before training (#7674),0.7238485,"Removed unintended Trainer argument progress_bar_callback, the callback should be passed in by Trainer(callbacks=[...]) instead (#1855)",  Check progress bar existence before printing   Add tests for predict_progres_bar   Add tests for progress_bar printing without training   Update changelog ,1
Move sync code from step result to lightning module [6/n] (#7651),0.50853395,"By extension, the LightningModule no longer has the training_step_end(), validation_step_end() and test_step_end() hooks because they were only used for reducing the outputs in DP. If you have other code that needs to run at the end of the step, you can migrate it to the corresponding *_batch_end hook for example.",,0
Fix dataloaders are not reset when tuning the model (#7566),0.7132517,Reset val_dataloader in tuner/batch_size_scaling (#9857),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
FSDP with full state dict (#7487),0.6046292,Native FSDP implementation," Fix some test errors Summary:  Test Plan: Reviewers: Subscribers: Tasks: Tags:   checkpoint consolidation   Update ddp_spawn.py   Update test_metric_result_integration.py   Update test_results.py   Update utils.py   Update utils.py   Update test_all_gather_grad.py   Update test_all_gather_grad.py   Update test_results.py   Revert ""Update test_results.py""   This reverts commit 9d4a2b891d2a4b37e21529a444bda1883d1b5ed1.  Revert ""Merge pull request #1 from shuyingsunshine21/shuyingsunshine21-checkpoint_consolidate""  This reverts commit c5053da789f9d04d2c967a65adf4fb026dc134b8, reversing changes made to 0d23d75bc91e4e0b7805712e394cb093fac22841.  Revert ""Update test_all_gather_grad.py""  This reverts commit 0d23d75bc91e4e0b7805712e394cb093fac22841.  Revert ""Update utils.py""  This reverts commit 70fe5da9c66ceff2fcf4be5b9efdd23a9af8389c.  Revert ""Update utils.py""  This reverts commit a9aae99f6ed6e9388ecf1d8a7bd79966176a65af.  Revert ""Update test_results.py""  This reverts commit ea749068785bbad689a12066544893b1605f20c5.  Revert ""Update test_metric_result_integration.py""  This reverts commit bf70e431b3ce4893de804e0f3b5d59e79346d6d7.  Revert ""Update ddp_spawn.py""  This reverts commit f17210183b84f90c9a62d1ff9b3e05e1fbe5f33b.  Revert ""checkpoint consolidation""  This reverts commit 536c1323b0e6715fb5919196ea48b0fcddddcd66.  Revert ""Revert ""checkpoint consolidation""""  This reverts commit 3a9fde915ad4c69620a6ccc411f5890cb38ba5ac.  Revert ""Revert ""Revert ""checkpoint consolidation""""""  This reverts commit 7a369f47e1a94d701fce48c994cc3f2da266dad0.  Revert ""Revert ""Update ddp_spawn.py""""  This reverts commit 8222dc98ead37d961a52b7366070aa10f66d92d1.  Revert ""Revert ""Update test_metric_result_integration.py""""  This reverts commit 6c095b2370a2afe9d24918a5798ce1ebffed7e0d.  Revert ""Revert ""Update test_results.py""""  This reverts commit 250d0aaaa2e6c6a6a3407bc6c8b83c0fe2479c0b.  Revert ""Revert ""Update utils.py""""  This reverts commit 8651d54d79396eaaba16d7eb1e769a1e91d5702e.  Revert ""Revert ""Update test_all_gather_grad.py""""  This reverts commit dcdcd29731061c919b15ab0b56669259817a81c4.   modify distributed environment to make test pass   fix version for ddp plugin test   fix   fix   changelog   Update CHANGELOG.md   fsdp with full state dict   fix missing import   modify unitest   fix   fix   fix typo   modify test and add changelog   fix   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   limit max_epoch to 1 for testing   test   fix   update   testing remove special for multi gpu   assert gpu   add assertion for gpu   fix   Re-enable special test, use ModelCheckpoint   Fix paths   Fix path passing   test   test   fix test   fix   pre-commit format   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: SeanNaren sean@grid.ai",0
Fix/mismatched toggle optimizer (#7563),0.68576795,"- The `LightningModule.{un}toggle_optimizer` methods no longer accept a `optimizer_idx` argument to select the relevant optimizer. Instead, the optimizer object can be passed in directly ([#16560](https://github.com/Lightning-AI/lightning/pull/16560))"," fix: avoid potential mismatched toggling of optimzier Refs #7405  chore: update CHANGELOG [pre-commit.ci] auto fixes from pre-commit.com hooks for more information, see https://pre-commit.ci fix: resolve a confict chore: update changelog   feat: add a test that fails in master   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  fix typo in tests/trainer/optimization/test_multiple_optimizers.py  Co-authored-by: ananthsub ananth.subramaniam@gmail.com  Polish tests/trainer/optimization/test_multiple_optimizers.py  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Polish tests/trainer/optimization/test_multiple_optimizers.py  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  fix: change placeholder in optimizer_step from positional args to keyword args  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com",0
refactor accelerator teardown -> training type plugin teardown (#7579),0.8888358,Changed teardown() in Accelerator to allow training_type_plugin to customize teardown logic (#7579),,1
Remove tbptt self.log flags and other dead code [5/n] (#7644),0.6315141,"Removed support for self.log(tbptt_reduce_fx) and self.log(tbptt_pad_token). Please, open a discussion explaining your use-case if you relied on these. (#7644)",,0
[2/N] Define dataclasses for progress tracking (#7574),0.707931,"Connect the progress tracking dataclasses to the loops (#8244, #8362)",Co-authored-by: ananthsub ananth.subramaniam@gmail.com,1
De-duplicate DistributedSampler mentions (#7636),0.4690618,Drop duplicate metrics (#5014),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com,0
"Improve argument validation for validate(), test(), and predict() (#7605)",0.5830522,"Refactored training_batch + tests to verify correctness (#2327, #2328)",Co-authored-by: Yifu Wang yifuwang@2012@gmail.com,0
CI code cleaning (#7615),0.55273664,clean up data reset (#3161),,0
[feat] Support custom filesystems in LightningModule.to_torchscript (#7617),0.5988519,- Fixed torchscript error with containers of LightningModules ([#14904](https://github.com/Lightning-AI/lightning/pull/14904)),  [feat] Support custom filesystems in LightningModule.to_torchscript   Update CHANGELOG.md   Update test_torchscript.py   Update test_torchscript.py   Update CHANGELOG.md   Update test_torchscript.py ,0
Remove Result(minimize) parameter [4/n] (#7628),0.5061885,Refactored optimizer (#4658),,0
Use trainer.call_hook in the evaluation loop (#7626),0.73048294,Made evaluate method private >> Trainer._evaluate(...). (#1260),,1
Replace CallbackHookNameValidator with FxValidator [3/n] (#7627),0.47978523,Removed deprecated callbacks (#3979),  Refactor FxValidator   Fix tests   Fix tests   Class attribute   Fix tests   Better error message   Fix tests   Update pytorch_lightning/trainer/connectors/logger_connector/fx_validator.py ,0
fix flag name to flush_logs_every_n_steps in logging doc (#7633),0.5737263,"Deprecated passing flush_logs_every_n_steps as a Trainer argument, instead pass it to the logger init if supported (#9366)",  fix method name to flush_logs_every_n_steps in logging doc   apply corrections in comments ,0
removed hparams assignment example (#7639),0.56156385,Fixing critical bugs in newly added hooks and hparams assignment.,,0
Add run_name argument to the MLFlowLogger constructor (#7622),0.90557986,MLFlowLogger now accepts run_name as an constructor argument (#7622),"  Add run_name argument to the MLFlowLogger   Update CHANGELOG   Fix unnecessary line   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Fix style by using yapf   Fix import error when mlflow is not installed   Update CHANGELOG.md   Update tests/loggers/test_mlflow.py   Co-authored-by: akiyuki ishikawa aki.y.ishikwa@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com",1
Update model_checkpoint.py (#7625),0.77655625,Removed the deprecated model argument from ModelCheckpoint.save_checkpoint (#8688),,1
Override broadcast_object_list for torch<1.8 (#7592),0.6564606,- Removed the `lightning.pytorch.overrides.torch_distributed.broadcast_object_list` function ([#17011](https://github.com/Lightning-AI/lightning/pull/17011)),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
Clear predict_progress_bar in ProgressBar.getstate (#7608),0.58929557,progress bar,Co-authored-by: Yifu Wang yifuwang@2012@gmail.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
[feat] Support time-based checkpointing during training (#7515),0.6653078, - Checkpointing: `Trainer(enable_checkpointing=True)`,Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Add link to TorchIO tutorial in PyTorch Ecosystem examples (#7612),0.5508424,"PL_TORCH_DISTRIBUTED_BACKEND=""gloo"" python train.py",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
[1/N] Define dataclasses for progress tracking (#6603),0.7119347,"Connect the progress tracking dataclasses to the loops (#8244, #8362)",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Unify current_fx_name and current_hook_fx_name [2/n] (#7594),0.5184997,remove _evaluate fx (#3197),  Minor loggger connector cleanup [1/n]   Missing line   Address comments   Rely on validator   Unify current_fx_name and current_hook_fx_name   Fix test ,0
Add typing to ModelPruning callback (#7529),0.5729984,ModelSummary Callback,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
TensorBoardLogger sub_dir parameter for grouping logs (#6195),0.67379206,"Improved flexibility for naming of TensorBoard logs, can now set version to a str to just save to that directory, and use name='' to prevent experiment-name directory (#804)","  fixed a small typo   cleaning up   added sub_dir argument to tensorboard and wrote test   sub dir arg exclusively for tensorboard, linted   resolving merge conflict   resolved merge conflict   resolved merge conflict   resolved merge conflict   resolve merge conflict before revert   resolving merge conflict   reverted to pre-lint   added tensorboard sub_dir test   pep8 formatting   removed sub_dir arg from test_all function:   updated feature description   typo in doc description   updated CHANGELOG   Update pytorch_lightning/loggers/tensorboard.py   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com   swapped argument position   added expandvars tests   added expandvars   removed model init   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   fix tests   fix failed test   Revert ""fix failed test""   This reverts commit 50b34c66dabbd8a0b5d84f905280a4ae6b9cdc55.   add env var to test   fix typo in tests   fix tests   for test consistency   fix typo   fix typo 2   Co-authored-by: Ubuntu azureuser@devhenrik.evuifrmjd4lepbj4relcwwu5va.ax.internal.cloudapp.net Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Roger Shieh sh.rog@protonmail.ch",0
docker use $(nproc) (#7606),0.41408676,Remove unnecessary intermediate layers in Dockerfiles (#5697),  docker use $(nproc)   Update typo   Co-authored-by: Roger Shieh sh.rog@protonmail.ch Co-authored-by: Roger Shieh sh.rog@protonmail.ch,0
[feat] Add stronger validation for checkpoint_callback argument (#7539),0.7240402,Changed the Trainer's checkpoint_callback argument to allow only boolean values (#7539),  [feat] Add stronger validation for checkpoint_callback configuration   chlog   Update callback_connector.py   Update test_model_checkpoint.py   Update pytorch_lightning/trainer/connectors/callback_connector.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   Update pytorch_lightning/trainer/connectors/callback_connector.py   Update tests/checkpointing/test_model_checkpoint.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update CHANGELOG.md  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
[pre-commit.ci] pre-commit autoupdate (#7577),0.63575333,@akihironitta @awaelchli @Borda @carmocca @kaushikb11 @krshrimali @mauvilsa @otaj @pre-commit-ci @rohitgr7 @semaphore-egg @tkonopka @wayi1,updates: - github.com/pre-commit/pre-commit-hooks: v3.4.0 → v4.0.1 Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Fix incorrect code-snippet in optimizers doc (#7598),0.73532003,Refactored optimizer (#4658),training_step(...) should take self as the first argument. It's a simple but necessary fix.,1
Minor logger connector cleanup [1/n] (#7590),0.8079108,Removed logger_connector legacy code (#6733),  Minor loggger connector cleanup [1/n]   Missing line   Address comments   Rely on validator ,1
make extra build for latest (#7593),0.5092342,Set version as today (#13906),,0
Fix the condition for calling update_learning_rates (#7032),0.6394754,Remove deprecated args to learning rate step function (#890),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
refactor optimizer loop logic for manual and automatic optimization (#7526),0.92689055,Refactored the logic around manual and automatic optimization inside the optimizer loop (#7526),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,1
Support TPU Pod Training (n/n) (#7296),0.7855266,TPU training (#2708),,1
Add kubeflow cluster environment (#7300),0.5724371,Deprecated ClusterEnvironment.creates_children() in favor of ClusterEnvironment.creates_processes_externally (property) (#10106),"  Add kubeflow cluster environment   Add KubeflowEnvironment to docs   Add KubeflowEnvironment to the changelog   break up a long line   Add method to detect kubeflow environment   Select Kubeflow environment when available   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Run pre-commit   task_idx == 0   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com",0
remove trainer hidden state | sanity refactor [2 / n] (#7507),0.6966754,Removed Warning from trainer loop (#1634),,0
Enable fsspec by default for cli config file (#7521),0.52814823," * `strategy=""fsdp_native""` is now `strategy=""fsdp""`",Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
add missing punctuation in lightning_cli.rst (#7554),0.55185807,- `LightningCLI`'s shorthand notation changed to use jsonargparse native feature ([#12614](https://github.com/Lightning-AI/lightning/pull/12614)),,0
Fix DistribType for ddp_cpu (spawn) (#7492),0.6002922,Changed warnings and recommendations for dataloaders in ddp_spawn (#6762),,0
update alumni (#7545),0.41463128,Renamed several Trainer atributes:  (#567),,0
update logo 48px (#7530),0.37580836,Minor changes,,0
Add dataloader_idx to batch transfer hooks (#6241),0.7293682,refactored dataloader process hook (#3139),  replace with kwargs   chlog   fix   add test   fix   device   deepspeed   pep   optional   docs   bc   comments   pep   mypy   pep   Apply suggestions from code review   kwargs   docs   .   .   1.3 -> 1.4   kwargs -> step_kwargs ,1
Default seed_everything(workers=True) in the LightningCLI (#7504),1.0000002,Default seed_everything(workers=True) in the LightningCLI (#7504),,1
Refactor result handling in training loop (#7506),0.9822005,Refactored result handling in training loop (#7506),"  refactor results   rename dic -> dict   simplify   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   changelog   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   fix None check   chlog wording   move process_closure_result to the end   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",1
Prune deprecated utils modules (#7503),0.6328162,Renamed utils modules (#5199),"  argparse_utils   model_utils   warning_utils   xla_device_utils   chlog   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
prune data parallel (#7510),0.57636356,The slow and clunky data-parallel strategy (#16748),,0
Add trainer.predict(ckpt_path) (#7430),0.81908864,"trainer.test(model, ckpt_path=None) # use provided model",Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
Prune deprecated trainer attributes (#7501),0.69610703,Deprecated Trainer.terminate_on_nan public attribute access (#9849),  use_single_gpu   use_horovod   use_ddp2   use_ddp   use_dp   on_gpu   use_tpu   on_tpu   on_cpu   cleaning   chlog   Apply suggestions from code review   Apply suggestions from code review ,0
Prune deprecated classif. metrics (#7499),0.6975194,Removed deprecated metrics (#6161),  stat_scores_multiple_classes   precision_recall   precision   recall   auc   auroc   multiclass_auroc   iou   clean-up   chlog   flake8   imports   prune ,0
Prune deprecated trainer attributes 2 (#7502),0.7101333,"Removed deprecated trainer attributes - on_cpu, on_tpu, use_tpu, on_gpu, use_dp, use_ddp, use_ddp2, use_horovod, use_single_gpu (#7501)",  accelerator_backend   get_model   clean   chlog   flake8 ,1
Fix yapf-isort conflict (#7500),0.33831084,Parsing of enums type hyperparameters to be saved in the haprams.yaml file by TensorBoard and CSV loggers has been fixed and made in line with how OmegaConf parses it (#9170),,0
Update README to 1.3 (#7489),0.521713,[1.3.7] - 2021-06-22,,0
MLFlow now uses env variable as default tracking uri (#7457),0.882277,MLflowLogger now uses the env variable MLFLOW_TRACKING_URI as default tracking URI (#7457), Clarify logger flag  Clarify behavior of boolean values on the logger flag for Trainer.   Update docs/source/common/trainer.rst   doc   MLFlow now uses env variable as default tracking uri   Solves https://github.com/PyTorchLightning/pytorch-lightning/issues/6894  Update pytorch_lightning/loggers/mlflow.py  Co-authored-by: thomas chaton thomas@grid.ai  changelog  Co-authored-by: SpontaneousDuck kennywitham4@gmail.com Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: jirka jirka.borovec@seznam.cz,1
added stage param to LightningDataModule.setup example (#7483),0.6161357,- Removed the deprecated `dim` and `size` arguments from the `LightningDataModule` constructor([#12780](https://github.com/Lightning-AI/lightning/pull/12780)),Co-authored-by: Sileadim christopher@omnius.com,0
Accelerator model state dict (#7474),0.58149767,reduced accelerator selection (#3211)," Fix some test errors Summary:  Test Plan: Reviewers: Subscribers: Tasks: Tags:   checkpoint consolidation   Update ddp_spawn.py   Update test_metric_result_integration.py   Update test_results.py   Update utils.py   Update utils.py   Update test_all_gather_grad.py   Update test_all_gather_grad.py   Update test_results.py   Revert ""Update test_results.py""   This reverts commit 9d4a2b891d2a4b37e21529a444bda1883d1b5ed1.  Revert ""Merge pull request #1 from shuyingsunshine21/shuyingsunshine21-checkpoint_consolidate""  This reverts commit c5053da789f9d04d2c967a65adf4fb026dc134b8, reversing changes made to 0d23d75bc91e4e0b7805712e394cb093fac22841.  Revert ""Update test_all_gather_grad.py""  This reverts commit 0d23d75bc91e4e0b7805712e394cb093fac22841.  Revert ""Update utils.py""  This reverts commit 70fe5da9c66ceff2fcf4be5b9efdd23a9af8389c.  Revert ""Update utils.py""  This reverts commit a9aae99f6ed6e9388ecf1d8a7bd79966176a65af.  Revert ""Update test_results.py""  This reverts commit ea749068785bbad689a12066544893b1605f20c5.  Revert ""Update test_metric_result_integration.py""  This reverts commit bf70e431b3ce4893de804e0f3b5d59e79346d6d7.  Revert ""Update ddp_spawn.py""  This reverts commit f17210183b84f90c9a62d1ff9b3e05e1fbe5f33b.  Revert ""checkpoint consolidation""  This reverts commit 536c1323b0e6715fb5919196ea48b0fcddddcd66.  Revert ""Revert ""checkpoint consolidation""""  This reverts commit 3a9fde915ad4c69620a6ccc411f5890cb38ba5ac.  Revert ""Revert ""Revert ""checkpoint consolidation""""""  This reverts commit 7a369f47e1a94d701fce48c994cc3f2da266dad0.  Revert ""Revert ""Update ddp_spawn.py""""  This reverts commit 8222dc98ead37d961a52b7366070aa10f66d92d1.  Revert ""Revert ""Update test_metric_result_integration.py""""  This reverts commit 6c095b2370a2afe9d24918a5798ce1ebffed7e0d.  Revert ""Revert ""Update test_results.py""""  This reverts commit 250d0aaaa2e6c6a6a3407bc6c8b83c0fe2479c0b.  Revert ""Revert ""Update utils.py""""  This reverts commit 8651d54d79396eaaba16d7eb1e769a1e91d5702e.  Revert ""Revert ""Update test_all_gather_grad.py""""  This reverts commit dcdcd29731061c919b15ab0b56669259817a81c4.   modify distributed environment to make test pass   modify model state dict to training type plugin   remove changes   add changelog   fixing isort for pre-commit failure   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  Address code review  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: SeanNaren sean@grid.ai",0
Reduce log output size in special tests (#7481),0.6092651,Cleaning up stale logger tests (#3490),,0
Bugfix/Multiple dataloaders (#7433),0.7806893,"Working with multiple dataloaders (#16800, #16753)","  Update supporters.py   Update apply_func.py   Update supporters.py   Update model_train_dataloaders.py   Update model_train_steps.py   Update test_dataloaders.py   Update CHANGELOG.md   Update model_train_steps.py   Update test_dataloaders.py   Update test_dataloaders.py   Update supporters.py   Update test_supporters.py   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci  Update tests/trainer/test_dataloaders.py  Co-authored-by: Akihiro Nitta nitta@akihironitta.com  Apply suggestions from code review  Co-authored-by: Edgar Riba edgar.riba@gmail.com   Update supporters.py   Update supporters.py   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Edgar Riba edgar.riba@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com",1
Docs: sync chlog 1.3.1 (#7478),0.55608577,Support number for logging with sync_dist=True (#5080),,0
Mark certain Trainer APIs as protected (#7420),0.69046324,Changed Trainer connectors to be protected attributes:,,0
remove trainer hidden state | sanity refactor [1 / n] (#7437),0.70510674,Removed Warning from trainer loop (#1634),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
Log epoch metrics before firing the on_evaluation_end hook (#7272),0.98200655,Log epoch metrics before the on_evaluation_end hook (#7272),  Log epoch metrics before firing the on_evaluation_end hook (addresses #7166)   test that epoch metrics are logged before on_evaluation_end hook   update CHANGELOG   Shorter test   Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
"Automatically check DataModule.has_{setup,teardown,prepare_data} [2/2] (#7238)",0.6633526,"DataModules now avoid duplicate {setup,teardown,prepare_data} calls for the same stage (#7238)","  Automatically check DataModule.has_{setup,teardown,prepare_data}   Use variable   Spacing   Docs   Update CHANGELOG   Remove _DataModuleWrapper   Add test   Update docs/source/extensions/datamodules.rst   Bad merge   add test for invalid name   Remove ValueError   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com",0
[pre-commit.ci] pre-commit autoupdate (#7475),0.62642425,@akihironitta @awaelchli @Borda @carmocca @kaushikb11 @krshrimali @mauvilsa @otaj @pre-commit-ci @rohitgr7 @semaphore-egg @tkonopka @wayi1,updates: - github.com/pre-commit/pre-commit-hooks: v2.3.0 → v3.4.0 - github.com/PyCQA/isort: 5.7.0 → 5.8.0 - github.com/pre-commit/mirrors-yapf: v0.30.0 → v0.31.0 Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
Fix Sphinx argument deprecation (#7464),0.6522281,Pinned sphinx-autodoc-typehints with <v1.15 (#11400),,0
Fix slack link (#7452),0.4428898,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",  Update README.md   Update Slack link   Co-authored-by: Akihiro Nitta nitta@akihironitta.com,0
Update introduction_guide.rst (#7453),0.41209906,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",,0
Pin Sphinx<4.0 (#7456),0.70203817,Pinned sphinx-autodoc-typehints with <v1.15 (#11400),  Dont use sphinx 4.0.0   Dont use sphinx 4.0.0   Update comment   Simple    There is no other release between 3.5 and 4.0 Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
fix display bug (#7395),0.4553656,Resolve bug with Finetuning (#5744),,0
fix 1.9 test (#7441),0.62293875,- fixed all the .test() calls,,0
Set num_nodes and sync_batchnorm From Trainer for Manually Passed Training Type Plugin (#7026),0.8534447,Changed resolve_training_type_plugins to allow setting num_nodes and sync_batchnorm from Trainer setting (#7026),Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
Restore trainer.current_epoch after tuning (#7434),0.7447101,The current_epoch and global_step attributes now get restored irrespective of the Trainer task (#9413),  Add a test   Save and restore current_epoch   Update CHANGELOG   alphabetical order ,1
Improve val step logging (#7351),0.5966191,bug fix with logging val epoch end + monitor (#3812),  Fix val step logging   Add a type   Fix   Update CHANGELOG.md ,0
Move DP warning suppression to the DataParallel Plugin (#7421),0.73161256,Moved ignore_scalar_return_in_dp warning suppression to the DataParallelPlugin class (#7421),,1
Deprecate TrainerModelHooksMixin (#7422),0.7487891,Removed the deprecated TrainerLoggingMixin class (#8609),  Deprecate TrainerModelHooksMixin   Update CHANGELOG.md   Update model_hooks.py   Update model_hooks.py ,1
Use torch.nn.utils.clip_grad_norm_  and add clip_grad_by_value support for TPU (#7025),0.8741078,Changed clip_grad_norm to use torch.nn.utils.clip_grad_norm_ (#7025),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,1
Unify configure_optimizers docs (#7399),0.660298,Improved error messages for invalid configure_optimizers returns (#3587),,0
Refactor tests to use BoringModel (#7401),0.49571833,- Full tests that run multiple models in different configs,,0
Add base IPU dockerfiles (#7252),0.49726474,Remove unnecessary intermediate layers in Dockerfiles (#5697),,0
update ngc for 1.3 (#7414),0.5035713,Update the Lightning App docs (#13537),,0
Fix DeepSpeedPlugin with IterableDataset (#7362),0.72326934,"Implemented DeepSpeedPlugin._setup_model_and_optimizers (#10009, #10064)","  deepspeed add train_micro_batch_size_per_gpu argument   Update naming and doc   Modify to use auto naming convention, add test   Add iterable tests   Fix tests, attempt by mocking   Import correct package   Fix comparison   Set as special test   Remove import   Add Changelog   Co-authored-by: SeanNaren sean@grid.ai",1
show mush go on (#7413),0.308042,@manskx,  chlog + version   readme   . ,0
update versions (#7409),0.63004375,Set version as today (#13906),  update versions   chlog   win   str ,0
release 1.3.0 (#7404),0.6384828,[1.3.7] - 2021-06-22,  v1.3.0   ci event   chlog   badge   formatting ,0
Call super().__init__() in MilestonesFinetuning example (#7398),0.6598964,"    def init(self, milestones, args, kwargs):",,0
[ci] Unpin pip==20.1 (#6375),0.46901911,[1.3.7post0] - 2021-06-23,Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: jirka jirka.borovec@seznam.cz,0
Add _gpus_arg_default in argparse_utils for backward compatibility (#7402),0.63227373,"Deprecated log_gpu_memory, gpu_metrics, and util funcs in favor of DeviceStatsMonitor callback (#9921)",,0
Advanced GPU Documentation (#7259),0.59131813,GPU training (#2704),"  Added advanced gpu section   Small changes   Better documentation   Address code review   Add warning about using trainer.model, clean up some of the examples   Add section for ddp, remove references and old sequential documentation   Remove Fully Sharded documentation for now   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com   Address code review   Address code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com",0
ci: adjust torch version requirements in IPU pipeline (#7383),0.6146941,- Fixed an issue with comparing torch versions when using a version of torch built from source ([#17030](https://github.com/Lightning-AI/lightning/pull/17030)),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
v0.1.3.0rc3 + changelogs (#7388),0.6403049,Full Changelog,  v0.1.3.0rc3   spaces   wip   wip   wip   wip   prune   wip   wip   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Updating docs and error message: half precision not available on CPU (#7384),0.51487947,    precision=16,  Updating docs and error message to specify that half precission not available on CPU   update messages   Co-authored-by: Martin Kristiansen martinkristiansen@sixgill.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: jirka jirka.borovec@seznam.cz,0
group all loop tests in a folder (#7394),0.49673295,refactor eval loop to use hooks - use test_mode for if so we can split later (#3129),  move files   rename ,0
Update configure_optimizers docs (#7390),0.72896665,Improved error messages for invalid configure_optimizers returns (#3587),  Update configure_optimizers docs   Update pytorch_lightning/core/lightning.py ,1
Add documentation for ways to access all batch outputs for on_train_epoch_end hook (#7389),0.7764173,Pass batch outputs to on_train_batch_end instead of epoch_end outputs (#4369),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
[2/2] Remove outputs from evaluation epoch end hooks (#7338),0.7761401,Removed output argument from *_epoch_end hooks (#3967),  Remove outputs from on_train_epoch_end   iterate   Update callback_hook.py   update   early stop?   fix   Update pytorch_lightning/trainer/training_loop.py   Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk   Update trainer.py   update   Update training_loop.py   early stop?   fix   Remove outputs from evaluation epoch end hooks   update   Update test_remove_1-5.py   fix lints   Update base.py   rm-outputs   Update evaluation_loop.py   try-save-more-memory   Update trainer.py   Update trainer.py   cache-at-start   Update evaluation_loop.py   Update training_loop.py   Update training_loop.py   Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk,1
Update changelog for recent releases (#7387),0.7396306,Full Changelog,,1
[1/2] Deprecate outputs in on_train_epoch_end hooks (#7339),0.75393784,"Changed the seq of on_train_batch_end, on_batch_end & on_train_epoch_end, on_epoch_end hooks (#5688)",  Remove outputs from on_train_epoch_end   iterate   Update callback_hook.py   update   Update training_loop.py   Update test_training_loop.py   early stop?   fix   update tests   Update test_hooks.py   Update pytorch_lightning/trainer/callback_hook.py   Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk  Update pytorch_lightning/trainer/training_loop.py  Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk   Update trainer.py   Update pytorch_lightning/trainer/trainer.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
CI/PT 1.9 (#7380),0.5221913,[1.3.7post0] - 2021-06-23,  add pt 1.9   pull ,0
update docs about transfer_batch_to_device hook while using DP (#7343),0.72533095,"With DPStrategy, the batch is not explicitly moved to the device (#11780)",  update   clarify   update ,1
enable Dockers for PT 1.9 (#7363),0.44018894,Remove unnecessary intermediate layers in Dockerfiles (#5697),  enable PT 1.9   fix versions   args   fix ,0
[bugfix] Resolve Kineto Profiler for Conda (#7376),0.5065239,"Resolved schedule step bug for PyTorch Profiler (#6674, #6681)",,0
Deprecatetruncated_bptt_steps flag on Trainer in favor of same setting on the LightningModule (#7323),0.86958516,Deprecated Trainer.truncated_bptt_steps in favor of LightningModule.truncated_bptt_steps (#7323),  deprecate-tbptt-trainer   Update CHANGELOG.md   Update lightning.py   test   Update lightning.py   Update training_loop.py   Update training_loop.py   Update lightning.py   Update training_loop.py   Update training_loop.py   update docs   Update accelerator.py   Update accelerator.py   more docs   tweaks   chlog   comments   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
update building latest XLA 1.8 (#7359),0.5269003,  * Deprecated the internal `XLADeviceUtils.xla_available` staticmethod,  wip   XLA   . ,0
Fix Namespace loading in PyYAML 5.4.x (#6673),0.46334863,- Fixed security vulnerabilities CVE-2020-1747 and CVE-2020-14343 caused by the `PyYAML` dependency ([#11099](https://github.com/PyTorchLightning/pytorch-lightning/pull/11099)),  Fix Namespace loading in PyYAML 5.4.x   Remove OmegaConf reference from PyYAML requirements   Max allowed version for pyyaml   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Add ddp_find_unused_parameters_false to Registry (#7224),0.7443502,Changed the default of find_unused_parameters to False in DDP (#5185),,1
set min PT version for legacy (#7358),0.6217181,Set version as today (#13906),,0
fix readme badges (#7354),0.4072404,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",  fix readme badges   Apply suggestions from code review   Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk,0
Pass current_epoch/global_step as monitor candidates [1/2] (#7344),0.65010566,Re-define the current_epoch boundary,  Pass current_epoch/global_step as monitor candidates   Formatting   Fix deprecated test   Update CHANGELOG ,0
temp suspend NVIDIA CI build (#7350),0.4310146,nvidia/apex deprecation (#16039),  temp suspend NVIDIA CI build   just skip   todo   if: false ,0
add CI event published (#7353),0.4061361,@akihironitta @awaelchli @Borda @carmocca @kaushikb11 @krshrimali @mauvilsa @otaj @pre-commit-ci @rohitgr7 @semaphore-egg @tkonopka @wayi1,,0
CI: move azure-pipelines config to separate directory (#7276),0.36385107,Add --secret option to CLI to allow binding Secrets to app environment variables when running in the cloud (#14612), CI: move azure pipelines to separate directory  This removes some extra clutter in the top level as we add more pipelines.  rename  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Fix support for dataloader with None batches (#7342),0.63866895,Allow dataloaders without sampler field present (#1907),  Fix Dataloader None batch   Fix Dataloader None batch   Update CHANGELOG.md   Fix breaking test   Address comments ,0
Fix auto scaling mode when calling tune method on trainer. (#7321),0.7505596,trainer.tune(model),"  Add test for non-existing mode, the test should fail if something different from power or binsearch is passed.   Add newline.   Apply fix   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   Update tests/tuner/test_scale_batch_size.py   Update pytorch_lightning/tuner/batch_size_scaling.py   Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com",1
Update trainer.py (#7340),0.77054834,adding Trainer.tune() (#3293),,1
TrainerState refactor [5/5] (#7173),0.6806569,"Refactor RunningStage and TrainerState usage (#4945, #7173)",  TrainerState refactor   flake8   Update finished check   Test cleanup   Fix tests   Fixes   Reorder   flake8   Update CHANGELOG   Better docs   Better docs   Remove default   Update tests   Bad merge ,0
make gpus=str in Trainer consistent with command line parsing of string (#6388),0.77756417,"Parsing of the gpus Trainer argument has changed: gpus=""n"" (str) no longer selects the GPU index n and instead selects the first n devices (#8770)","  string gpu input   update docs   deprecation warning   Revert ""update docs""   This reverts commit c5f38934133812280ae98f6489c008a39796dd4d.   deprecation   add changelog   update parser   update warning   implement v1.5 behavior ahead of time   formatting   set accelerator in test to avoid different warning   add warning   remove todo warn   Update pytorch_lightning/utilities/device_parser.py   Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com  resolve flake8  Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: tchaton thomas@grid.ai",1
fix(wandb): allow custom init args (#6989),0.6909224,WandbLogger does not force wandb reinit arg to True anymore and creates a run only when needed (#4648),  feat(wandb): allow custom init args   style: pep8   fix: get dict args   refactor: simplify init args   test: test init args   style: pep8   docs: update CHANGELOG   test: check default resume value   fix: default value of anonymous   fix: respect order of parameters   feat: use look-up table for anonymous   yapf formatting   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Update LR schedulers only when their corresponding Optimizer is being… (#4868),0.7517964,Disabled lr_scheduler.step() in manual optimization  (#6825)," Update LR schedulers only when their corresponding Optimizer is being used.  In the case when optimizer frequencies are specified, the LR scheduler corresponding to a particular optimizer is updated only when that optimizer is being used in the training loop or epoch.   pep8speak fixes   Fix failing tests   Add docs   PR Feedback   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   formatting fix   PR Feedback - part 2   More PR feedback   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   Add typing imports   Stronger tests and fixes related to that   Add more tests plus PR feedback   Make optimizer_freq_cumsum a cached property   @cached_property is only available after Python 3.8 so had to do it manually.   Fix tests   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   Avoid mutable defaults   Parametrize lr scheduling tests   PR feedback   Apply suggestions from code review   spell   Apply suggestions from code review   flake8   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: chaton thomas@grid.ai Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com",1
update test for resume_from_checkpoint on missing file (#7255),0.8004898,Changed resume_from_checkpoint warning to an error when the checkpoint file does not exist (#7075),,1
Add initial IPU CI job (#7251),0.45466492,Raised a MisconfigurationException if batch transfer hooks are overriden with IPUAccelerator (13961),"This adds an azure-pipelines job so we can verify the runners are connected correctly. Since the IPU branch isn't merged, it won't yet give any actual IPU test coverage.",0
Replace _DataModuleWrapper with __new__ [1/2] (#7289),0.8361829,Replaced _DataModuleWrapper with __new__ (#7289),  Remove _DataModuleWrapper   Update pytorch_lightning/core/datamodule.py   Update pytorch_lightning/core/datamodule.py   Replace __reduce__ with __getstate__ ,1
Fix Trainer.plugins type declaration (#7288),0.72533166,Removed TrainerProperties mixin and moved property definitions directly into Trainer (#9495),"  Fix trainer.plugins type declaration   Don't ClusterEnvironment(Plugin)   fix import error, yapf formatter   Add test   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com",1
Clarify logger flag (#7190),0.61184764,Removed logger_connector legacy code (#6733), Clarify logger flag  Clarify behavior of boolean values on the logger flag for Trainer.   Update docs/source/common/trainer.rst   doc   Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
Deprecate LightningModule.datamodule reference in favor of the trainer one (#6929) (#7168),0.89626706,Removed deprecated property LightningModule.datamodule in favor of Trainer.datamodule (#9233),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
add forgotten test in #7240 (#7283),0.47415185,- Code coverage (99%),^,0
Fix requirements/adjust_versions.py (#7149),0.58239394,Add missing python-multipart dependency (#17244),Co-authored-by: jirka jirka.borovec@seznam.cz,0
Update Accelerator Connector for Registry (#7214),0.5912542,Removed deprecated connect_precision_plugin and connect_training_type_plugin from Accelerator (#9019),,0
Remove model.trainer call inside of dataloading mixin (#7317),1.0000002,Remove model.trainer call inside of dataloading mixin (#7317),  Update data_loading.py   Update data_loading.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Example and documentation for LightningCLI linking model and data arguments (#7299),0.61398345,"    cli = LightningCLI(MyModel, ..., args=args)",Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
improve early stopping verbose logging (#6811),0.7182709,Improved verbose logging for EarlyStopping callback (#6811),,1
Update CODEOWNERS (#7302),0.45520657,"Updated DeepSpeed ZeRO (#6546, #6752, #6142, #6321)",  Update CODEOWNERS   @carmocca   @borda   Update CODEOWNERS   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: William Falcon waf2107@columbia.edu,0
[bugfix] Fix dataloading for iterable datasets and limit_train_batches (#7306),0.6679554,Deprecated reload_dataloaders_every_epoch argument of Trainer in favor of reload_dataloaders_every_n_epochs (#5043),  bugfix-dataloading   rm-logs   Update CHANGELOG.md   Update test_dataloaders.py   Update test_dataloaders.py   Update training_loop.py   Update test_dataloaders.py   Update CHANGELOG.md   Update CHANGELOG.md   Update test_dataloaders.py   Update training_loop.py   Update training_loop.py   comments   address comments   more tests   Update progress.py   Update test_dataloaders.py   Update test_dataloaders.py   Update training_loop.py   Update training_loop.py   test ckpt fix?   update again ,0
Update DeepSpeed version requirement in Dockerfile (#7326),0.54185474,"Implemented DeepSpeedPlugin._setup_model_and_optimizers (#10009, #10064)",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Update accelerator.py (#7318),0.70090604,Moved accelerator_connector.py to the connectors subfolder (#6033),,1
Move trainer functions (#7295),0.69216824,trainer = Trainer(),,0
Fix Adagrad optimizer not working with DDP/GPU (#7277),0.54865336,Made DDP the default if no backend specified with multiple GPUs (#1789),Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: thomas chaton thomas@grid.ai,0
Device updates for TPU Pod (#7243),0.44143322,The point of this release is more bug fixes ahead of v 1.0.0. We now have CI tests on TPU thanks to @zcain117 from Google! :slightly_smiling_face:,,0
[bugfix] Apex never instantiated. (#7274),0.5367487,Changed default apex level to 'O2' (#2362),  update   update   update apex   update   update   update   remove test.py   update   update   update on comments   update changelog   update   update   typo ,0
Move grad_norm to a dedicated utilities file  (#7292),0.5756736,"def log_grad_norm(self, grad_norm_dict):",  rm-grad-norm-mixin   Update grads.py   Update CHANGELOG.md   Apply suggestions from code review   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   Update docstrings   Update init.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
[fix] Attach train+val dataloaders to trainer in trainer loop (#7207),0.83161366,"Removed the deprecated Trainer.reset_train_val_dataloaders() in favor of Trainer.reset_{train,val}_dataloader (#16131)",  Update training_loop.py   Update test_dataloaders.py   changelog   delay reload   go back   comments   Update training_loop.py   Update test_dataloaders.py   Update tests/trainer/test_dataloaders.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
[bugfix] Add reloading support using BaseFinetuning (#7253),0.5319529,  * Removed `opt_idx` argument from `BaseFinetuning.finetune_function` callback method,  update   wip   udpate   update   update   update   resolve bug   update on comments   update on comments   update   update   formatting   add comments   update on comments   update   Update pytorch_lightning/callbacks/base.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   update   update   Typing and minor changes   Refactor   Fix deprecated test   Broken commit   Fix broken commit   flake8   Update CHANGELOG   update on comments   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
Attach data refactor and tuner bugs [4/n] (#7258),0.49653563,Replaced _DataModuleWrapper with __new__ (#7289),Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,0
update training type plugin docs regarding result caching (#7261),0.5747972,"The Trainer now calls TrainingTypePlugin collective APIs directly instead of going through the Accelerator reference (#9677, #9901)",  add docs   typo   update ,0
fix case where an IterableDataset doesn't produce a batch for an epoch (#7294),0.6195352,Disabled sampler replacement when using IterableDataset (#11507),  wip   fix   add test   refactor + test   rm   formatting   update changelog   doc   docstring   remove unused import   Update CHANGELOG.md   Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,0
Rename trainer._launch to trainer._run (#7265),0.7705346,Removed deprecated property Trainer.running_sanity_check in favor of Trainer.sanity_checking (#9209),  rename-run   fix ,1
fix save_hyperparameters(container) if container is empty  (#7268),0.7499945,Add ignore param to save_hyperparameters (#6056),  fix   add tests   changelog   fix test ,1
Updated docs to fix typo and update grid status (#7270),0.5200834,"In addition, we fixed:",  Updated docs to fix typo and update grid status   Update docs/source/starter/new-project.rst   Co-authored-by: ananthsub ananth.subramaniam@gmail.com   Update docs/source/starter/new-project.rst   Update docs/source/starter/new-project.rst   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,0
No need of warning when saved callback_states is None (#7293),0.6204088,"    # previously, only the state for this callback was passed in as argument",,0
Remove exp_save_path on the LightningModule (#7266),0.93115973,Removed the exp_save_path property from the LightningModule (#7266),  deprecate-exp-save-path   Update lightning.py   Update CHANGELOG.md   remove-not-deprecate ,1
fix fast_dev_run parsing from cli (#7240),0.6609596,Updated fast_dev_run to accept integer representing num_batches (#4629),,0
[2/2] Remove training loop force calling early stopping callback (#7069),0.70913994,-   --trainer.callbacks=EarlyStopping \,  rebase   doc   Update training_loop.py   Update CHANGELOG.md   Update CHANGELOG.md   Update CHANGELOG.md   Update CHANGELOG.md   Update CHANGELOG.md ,1
Code cleaning in preparation for #7258 [3/n] (#7262),0.54306424,"Cleaning (#5948, #5949, #5950)",,0
[warning] Add a warning with missing callback with resume_from_checkpoint (#7254),0.78207135,Changed resume_from_checkpoint warning to an error when the checkpoint file does not exist (#7075),  add a warning   add changelog ,1
Updated ModelCheckpoint documentation (#6873),0.7442055,Deprecated prefix argument in ModelCheckpoint (#4765),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Simplify backbone_image_classifier example (#7246),0.6297212,Changed computer_vision_fine_tunning example to use BackboneLambdaFinetuningCallback (#5377),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com,0
Fixes a potential bug on pip install (#7256),0.5150205,- Removed support for Python 3.7 ([#16579](https://github.com/Lightning-AI/lightning/pull/16579)),  Fix install bug   Better fix   Fix   Fix   Remove unused import   Update docs conf.py   Updates ,0
Reset current_fx properties on lightning module in teardown (#7247),1.0,Reset current_fx properties on lightning module in teardown (#7247),  Update trainer.py   cleanup module properties in teardown   Update test_trainer.py   Update lightning.py   Formatting   flake8   Update pytorch_lightning/trainer/trainer.py   Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
Remove trainer.fit return value [2/n] (#7237),0.8394561,Removed trainer.fit() return value of 1. It has no return now (#7237),  _fit_impl refactor and types   Fix return   Remove return docstring   Fixes   Fixes   Remove trainer.fit return value   Update CHANGELOG   flake8   Undo results change   Fix test   Revert changes for a separate PR   flake8 ,1
_launch refactor and types [1/n] (#7232),0.45677483,training forward refactor (#3134),,0
CI for pre-release (#7220),0.5219088,The point of this release is more bug fixes ahead of v 1.0.0. We now have CI tests on TPU thanks to @zcain117 from Google! :slightly_smiling_face:,  ci for pre-release   .   drop 3.7 ,0
[1/2] Add support for early stopping during training epoch end (#6944),0.72999233,EarlyStopping now runs at the end of the training epoch by default (#8286),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: jirka jirka.borovec@seznam.cz,1
Changes resume_from_checkpoint warning to error (#7075),0.8929427,Changed resume_from_checkpoint warning to an error when the checkpoint file does not exist (#7075),Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Fixed bug: replaced bce_loss_with_logits with bce_loss (#7096),0.61137563,  * The `PrecisionPlugin.backward` signature changed: The `closure_loss` argument was renamed to `tensor`,  Fixed bug: replaced bce_loss_with_logits with bec_loss   Fixed bug: removed sigmoid activation from forward pass   switched names for scores and logits   Co-authored-by: Alexey Misev amisev@fb.com Co-authored-by: Alexey Misev alexey@MacBook-Pro-Natalia.local Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Improve LightningCLI documentation and tests (#7156),0.75297624,LightningCLI Improvements,"   Added cli unit tests for help, print_config and submodules.   Added to cli documentation use of subclass help and print_config, submodules and other minor improvements.  Increased minimum jsonargparse version required for new documented features.   Improvements to lightning_cli.rst   Add check for all trainer parameters in test_lightning_cli_help   Increased minimum jsonargparse version   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com",1
Update fsspec dependency and remove un-needed code (#7210),0.5454276,Changed fsspec to tuner (#4458),  Update fsspec dep and remove un-needed code   Remove unused import ,0
warn about dp + manual optimization in docs (#7230),0.6137512,Silenced some warnings. verified ddp refactors (#3483),  add warning   typo   add link ,0
Replace 'step' with 'global_step' (#7244),0.6833829,+ global_step += 1,,0
Add debug flag to TPU Training Plugins (PT_XLA_DEBUG) (#7219),0.518658,"All trainers now have a default logger, early stopping and checkpoint object. To modify the behavior, pass in your own versions of those. ",,0
[feat] Add BasePredictionWriter 3/3 (#7127),0.44240934,Made type hints public (#17100),  wip   update   update   update   update   update   typo   update on comments   update   update   update   update   update changelog   update   Fix merge   Fix merge   move code   resolve test   add extra test   add an extra test   update on comments   add typing   resolve flake8   Refactor and Docs   Fix tests   Fix tests   Fix tests   Duplicate   Fix tests   resolve bug   update   update on comments   Update pytorch_lightning/utilities/imports.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update pytorch_lightning/utilities/device_parser.py  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   update   update   update   update on comments   resolve flkae8   update test   Apply suggestions from code review   update on comments   Update pytorch_lightning/callbacks/prediction_writer.py   Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk  Update pytorch_lightning/callbacks/prediction_writer.py  Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk  Update pytorch_lightning/callbacks/prediction_writer.py  Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk   update on comments   update   update on comment   Apply suggestions from code review   update   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Add a check for TPU Spawn barrrier (#7241),0.510091,Resolve TPU miss rendezvous (#6781),,0
Docker/nvidia (#7109),0.51655006,nvidia/apex deprecation (#16039),  version check   ... ,0
[bug/feat] Support parameters_to_ignore in DDP (#7239),0.7761657,Changed the default of find_unused_parameters to False in DDP (#5185),  update   update   update   update on comments   update ,1
Do not shuffle in LightningDataModule.from_datasets for IterableDataset (#7053),0.6585893,- Avoid enforcing `shuffle=False` for eval dataloaders ([#11575](https://github.com/PyTorchLightning/pytorch-lightning/pull/11575)),  Expose shuffle argument in LightningDataModule.from_datasets   Add test for DataModule initialization with iterable datasets   Add changelog   Remove trailing whitespace   Add more tests for coverage   Fix sequence dataset coverage   Fix Sequence dataset tests   Directly check whether each passed dataset is an IterableDataset   Expose shuffle argument in LightningDataModule.from_datasets   Add test for DataModule initialization with iterable datasets   Add changelog   Remove trailing whitespace   Add more tests for coverage   Fix sequence dataset coverage   Fix Sequence dataset tests   Directly check whether each passed dataset is an IterableDataset   Fix changelog to reflect review direction   Update CHANGELOG.md   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   Fix changelog to reflect review direction (2)   Add suggested braces   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   Reuse isinstance check   Merged tests with parametrize. Use mocks   Co-authored-by: Seongmin Park seongmin.park@actionpower.kr Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
[fix] Add barriers before and after setup hook is run (#7202),0.5108882,moved ___step_end hooks (#3130),  Update data_connector.py   move-barrier   Update trainer.py   Update ddp.py   changelog   Spacing   Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
[bugfix] Metric not logged properly in manual optimization (#7228),0.65604305,Change Metrics persistent default mode to False (#4685),  resolve bug   update changelog   typo   Update tests/trainer/optimization/test_manual_optimization.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Apply suggestions from code review  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
[feat] Add better support for predict + ddp 2/3 (#7215),0.5181565,"Refactored prediction loop interface; added new classes PredictionLoop, PredictionEpochLoop (#7700, #8077)",  wip   update   update   update   update   update   typo   update on comments   update   update   update   update   update changelog   update   Fix merge   Fix merge   move code   resolve test   add extra test   add an extra test   update on comments   add typing   resolve flake8   Refactor and Docs   Fix tests   Fix tests   Fix tests   Duplicate   Fix tests   resolve bug   update   update on comments   update   update changelog   update   update   remove tpu   resolve flake8   update on comments   update on comments   update on comment   resolve flake8   add a cpu test for predict   add None test   update   Update CHANGELOG.md   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  resolve tests  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Add back clip_gradients(model) (#7231),0.7723589,def configure_gradient_clipping(,,1
Fixed num_sanity_val_steps affecting reproducibility of training data shuffling (#7014),0.57572365, - Sanity checking: `Trainer(num_sanity_val_steps>0)`,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,0
Add fairscale install msg for Sharded Plugins (#7213),0.50188243,Removed support in LightningLite for FairScale's sharded training (strategy='ddp_sharded'|'ddp_sharded_spawn'). Use Fully-Sharded Data Parallel instead (strategy='fsdp') (#16329),,0
Set smarter default for DDP sharded for performance optimization (#6937),0.61200505,Ensure we check deepspeed/sharded in multinode DDP (#6297),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Deprecate save_function from model checkpoint callback (#7201),0.8098297,Deprecated the save_function property from the ModelCheckpoint callback (#7201),  Update model_checkpoint.py   Update CHANGELOG.md   fix-tests   deprecate not remove   Update model_checkpoint.py   Update test_remove_1-5.py ,1
Fix NeptuneLogger.log_text(step=None) (#7194),0.68404603,"It now supports neptune-client 0.16.16 and neptune >=1.0, and we have replaced the log() method with append() and extend().",,0
Update teardown for TPU acc (#7211),0.5370829,Moved device-specific teardown logic from training loop to accelerator (#5973),,0
[fix] Add barrier to accelerator's teardown (#6814),0.64091057,Moved device-specific teardown logic from training loop to accelerator (#5973),,0
Enforce Lightning module as source of truth for automatic optimization (#7130),0.6640608,Add automatic optimization property setter to lightning module (#5169),  make lightning module source of truth for automatic optimization   Update configuration_validator.py   Update model_connector.py   rm-references   Update CHANGELOG.md   Update CHANGELOG.md   Co-authored-by: jirka jirka.borovec@seznam.cz,0
Update Error message for ProfileConnector (#7204),0.661107,Removed ProfilerConnector (#7654),  Update Error message for ProfileConnector   Update test ,0
Deprecate write_predictions on the LightningModule (#7066),0.877667,Deprecated LightningModule.write_predictions and LightningModule.write_predictions_dict (#7066),  deprecate-write-predictions   Update CHANGELOG.md   Update test_remove_1-5.py   Co-authored-by: thomas chaton thomas@grid.ai,1
make bug_report_model minimal (#7191),0.46906188,Changed ModelCheckpoint version suffixes to start at 1 (#5008),  simple and boring script   simplify dataloader   replace bug report model ,0
Move metrics_to_scalars to a dedicated utilities file (#7180),0.6632267,Changed metrics_to_scalars to work with any collection or value (#7888),  rm-trainer-logging   Update CHANGELOG.md   Update metrics.py   Update logging.py   Update metrics.py ,0
Properly set LightningModule.device after model replacement (#7188),0.6908866,Move lightning module to correct device type when using LightningDistributedWrapper (#6070),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Update FairScale on CI (#7017),0.50070727,"    strategy=""fsdp_native"",  # or strategy=""fsdp"" for fairscale","  Try updating CI to latest fairscale   Update availability of imports.py   Remove some of the fairscale custom ci stuff   Update grad scaler within the new process as reference is incorrect for spawn   Remove fairscale from mocks   Install fairscale 0.3.4 into the base container, remove from extra.txt   Update docs/source/conf.py   Fix import issues   Mock fairscale for docs   Fix DeepSpeed and FairScale to specific versions   Swap back to greater than   extras   Revert ""extras""   This reverts commit 7353479f  ci  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: jirka jirka.borovec@seznam.cz",0
Fix lr_finder suggesting too high learning rates (#7076),0.63582927,- Fixed error handling in learning rate finder when not enough data points are available to give a good suggestion ([#13845](https://github.com/Lightning-AI/lightning/pull/13845)),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
add missing predict docs (#7150),0.5442999,"Refactored prediction loop interface; added new classes PredictionLoop, PredictionEpochLoop (#7700, #8077)",  update docs   add datamodule predict   fix docs   typo ,0
Fixed setting of _save_dir when run initiated externally (#7106),0.52080125,- Fixed wandb `save_dir` is overridden by `None` `dir` when using CLI ([#14878](https://github.com/Lightning-AI/lightning/pull/14878)),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
fix pip install (#7170),0.52086604,pip install rich,,0
Fix torchmetrics compatibility (#7131),0.6631788,Avoided false positive warning about using sync_dist when using torchmetrics (#14143),  get_num_classes   tmp   fix one test   fix deprecated tests   fix deprecate   pep8   deprecate 0.3   wip   wip   HaCK   brnch   brnch   format   Apply suggestions from code review   prune   rev   mltilabel   Apply suggestions from code review   master   rev   .   Co-authored-by: Nicki Skafte skaftenicki@gmail.com,0
fix version + yapf (#6999),0.48830906,Set version as today (#13906),,0
Ban tensorboard==2.5.0 and deepspeed==0.3.15 (#7159),0.6355347,Improved the error message for installing tensorboard or tensorboardx (#17053),  ban TB 2.5   note   push   Ban tb==2.5.0 and deepspeed==0.3.15   Fix pip command   pull   up   up   Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
"Add on_predict_{batch,epoch}_{start,end} and Callback.on_predict_{start,end} (#7141)",0.6693832,- Removed the `outputs` argument from the `on_predict_epoch_end` hook. You can access them via `trainer.predict_loop.predictions` ([#16655](https://github.com/Lightning-AI/lightning/pull/16655)),"  Update hooks typing and predict hooks   Update CHANGELOG   Progress   Progress   Add back on_predict_{start,end}   Typing and fix   Update tests/trainer/logging_/test_logger_connector.py   Update tests/callbacks/test_lambda_function.py ",0
Fix mypy checks for double precision plugin (#7151),0.6291969,Deprecated PrecisionPlugin.master_params() in favor of PrecisionPlugin.main_params() (#10105),,0
Delete unused CI scripts (#7152),0.42738283,Remove MetricsHolder (#7909),,0
[bugfix] Remove warning for distributed values (#7132),0.5477155,Moved ignore_scalar_return_in_dp warning suppression to the DataParallelPlugin class (#7421),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: jirka jirka.borovec@seznam.cz,0
Fix argparse docs (#7148),0.7789266,argparse_utils >> argparse,,1
[FSDP] Move on save checkpoint outside of zero check (#7134),0.657149,    # put all logic related to saving a checkpoint here,  Move on save checkpoint outside of zero check   Remove unnecessary override   Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
Update Grid details in README.md (#7139),0.3701771,Replace mata_tags.csv with hparams.yaml (#1271),,0
Update ClusterEnvironment docs (#7120),0.67360127,Deprecated ClusterEnvironment.creates_children() in favor of ClusterEnvironment.creates_processes_externally (property) (#10106),  update cluster   update index   update references   update grid docs   update duplicated title   Update docs/source/clouds/cloud_training.rst   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   fix doctest   Remove self-balancing section   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Fix apex version in Docker due to broken upstream (#7146),0.44142416,Pkg: fix parsing versions by @Borda in https://github.com/Lightning-AI/lightning/pull/15401,  Set Apex commit before introduction of new MLP extensions   Refactor install command   Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
temp freeze TM v0.2 (#7147),0.37873256,Freezed models hparams as Namespace property (#1029),,0
Broadcast dirpath for tighter consistency in model checkpoint callback (#6978),0.6571497,Skipped best_model_path if checkpoint_callback is None (#2962),  Update model_checkpoint.py   Update model_checkpoint.py   Update model_checkpoint.py ,0
Update EarlyStopping docs (#7121),0.6921052,EarlyStopping now runs at the end of the training epoch by default (#8286),,0
[bugfix] Add set_default_tensor_type to torch.DoubleTensor with precision=64 (#7108),0.74645793,Corrected call to torch.no_grad (#5124),  update   Update pytorch_lightning/plugins/precision/double.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update pytorch_lightning/plugins/precision/double.py  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update pytorch_lightning/plugins/precision/double.py  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  resolve tests  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Move save_hyperparameters to its own function (#7119),0.935186,Moved save_hyperparameters to its own function (#7119),  move hyper_parameters   Update pytorch_lightning/core/lightning.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update pytorch_lightning/utilities/parsing.py  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   resolve flake8   update   resolve tests   Update pytorch_lightning/core/lightning.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Update governance.rst (#7111),0.63337004,Updated governance docs,,0
Add MpModelWrapper in TPU Spawn (#7045),0.46902943,Support to tie weights after moving model to TPU via on_post_move_to_device hook,Co-authored-by: tchaton thomas@grid.ai Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
Disable lr_scheduler.step() in manual optimization (#6825),0.9691539,Disabled lr_scheduler.step() in manual optimization  (#6825),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: thomas chaton thomas@grid.ai,1
Update test for ckpt+val_check_interval (#7084),0.49818075,Trainer(val_check_interval=100),  update test   Apply suggestions from code review   Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
[bugfix] Add support for CombinedLoader in validation with ddp (#7102),0.66553444,enabled multiple dataloaders for validation.    ,  add test   add changelog   resolve flake8   remove print ,0
Fix attribute error for _gpus_arg_default loading checkpoint prior to 1.2.8 (#7043),0.56974196,"Deprecated log_gpu_memory, gpu_metrics, and util funcs in favor of DeviceStatsMonitor callback (#9921)",,0
Update community example links (#7087),0.5607602,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",Co-authored-by: tchaton thomas@grid.ai,0
[test] Add checks for gpus=1 (#7105),0.64180464,"Deprecated the use of Trainer(gpus=""i"") (string) for selecting the i-th GPU; from v1.5 this will set the number of GPUs instead of the index (#6388)",  update   remove cluster env ,0
fix logger experiment version in multiple run DDP (#7077),0.67384386,Removed collisions with logger versions by tying it to job id.,  fix   changelog ,0
more early stopping options (convergence and divergence threshold) (#6868),0.3951953,Squeeze the early stopping monitor to remove empty tensor dimensions (#10461),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Auto-set DataLoader.worker_init_fn with seed_everything (#6960),1.0000004,Auto-set DataLoader.worker_init_fn with seed_everything (#6960),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com,1
Optimization docs (#6907),0.6538396,        # 4. Perform the optimization in a loop,  .   .   Fix link to the section   Fix link to the section   Consistent indent   Update docs   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   Add note for optimizer.optimizer   .   Update hooks   Update closure docstring   Update optimizer methods   Update optimizer   Remove manopt + grad clipping (by @flukeskywalker)   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
fix docs rendering in datamodule (#7064),0.5106114,Replaced _DataModuleWrapper with __new__ (#7289),  [docs]: add newline to correctly render Example   whitespace   Co-authored-by: Matthew Sarmiento matthewcs@me.com,0
Deprecate @auto_move_data in favor of trainer.predict (#6993),0.98479545,Deprecated @auto_move_data in favor of trainer.predict (#6993),  Deprecated @auto_move_data in favor of trainer.predict   Update CHANGELOG ,1
Set DistributedSampler seed if seed_everything was called (#7024),0.8204857,pl.seed_everything will now also set the seed on the DistributedSampler (#7024),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
skipp drafts for full test (#7046),0.39003545,Updated app testing (#16000),,0
Correctly reset metric objects in self.log (#7055),0.6488539,Metric compute() method will no longer automatically call reset() (#5409),  reset   fix tests   fix tests   Apply suggestions from code review   Co-authored-by: ananthsub ananth.subramaniam@gmail.com   move logic   chglog   pep8   Add test   Improve test   Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk Co-authored-by: ananthsub ananth.subramaniam@gmail.com,0
Typo LightningMoule -> LightningModule (#7038),0.65656245,Removed deprecated LightningModule hparams setter (#6207),,0
Create pytorch_lightning/utilities/types.py (#7048),0.84706175,| Import pytorch_lightning.utilities.cli module                                                              | 1.9             | pytorch_lightning.cli                         |,,1
Update CI torch-xla version to 1.8 (#7019),0.62664276,    * Some configuration errors that were previously raised as `MisconfigurationException`s will now be raised as `ProcessRaisedException` (torch>=1.8) or as `Exception` (torch<1.8),  Update CI torch-xla version to 1.8   Update minimal to 1.6 ,0
TPU Spawn Rank & root device Error  (#7074),0.49413833,Resolve TPU miss rendezvous (#6781),  TPU Spawn Rank Error   Update tpu spawn   Fix root device property for tpu spawn   Update changelog ,0
Update default gym env version to CartPole-v1 (#7079),0.49810305,"Deprecated Trainer attribute ckpt_path, which will now be set by weights_save_path (#2681)","Version v1 generates a better baseline with higher max_episodes and reward_threshold attained. changed_params --> register(     id='CartPole-v1',     entry_point='gym.envs.classic_control:CartPoleEnv',     max_episode_steps=500,     reward_threshold=475.0, )",0
Better approach to register plugins (#7063),0.47574747,Initial plugin server (#16523),  Better approach to register plugins   Add ddp_with_find_unused_parameters_false   Remove unnecessary break   Revert back the ddp commit   Update register override logic   Update register override logic   fix mypy ,0
update (#7056),0.4933096,"Updated DeepSpeed ZeRO (#6546, #6752, #6142, #6321)",,0
[fix] Fix multi-node DDP launch by using local rank instead of global rank for main process (#7061),0.74555147,Enabled traditional/manual launching of DDP processes through LOCAL_RANK and NODE_RANK environment variable assignments (#7480),  Update ddp.py   Update CHANGELOG.md ,1
Fix mypy for plugins registry (#7062),0.42904857,Removed Plugin in base_plugin.py in favor of accessing TrainingTypePlugin and PrecisionPlugin directly instead (#9066),,0
rc2 (#7057),0.42642444,Barebones Trainer mode (#16854),,0
Add Training Type Plugins Registry (#6982),0.63102746,Removed Plugin in base_plugin.py in favor of accessing TrainingTypePlugin and PrecisionPlugin directly instead (#9066),Co-authored-by: Sean Naren sean@grid.ai Co-authored-by: thomas chaton thomas@grid.ai,0
Add Trainer max_time argument + Callback (#6823),0.7939596,    --trainer.callbacks.logging_interval=epoch \,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com,1
[docs]: pass parser to Trainer.add_argparse_args() (#7029),0.9433539,parser = Trainer.add_argparse_args(parser),,1
Use PyTorch API logging for Lightning Trainer (#6771),0.8359983,- Added a `loggers` property to `LightningModule` which retrieves the `loggers` property from `Trainer` ([#11683](https://github.com/PyTorchLightning/pytorch-lightning/pull/11683)),  Update trainer.py   Update trainer.py   Update trainer.py ,1
Typing for accelerators and plugins (#7022),0.6987899,Refactored Accelerators and Plugins (#5743),,0
[fix] Add a cluster environment teardown to clean up environment state (#6942),0.6411305,Cluster creation and deletion now waits by default [#15458,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Update DeepSpeed docs after single saving PR (#7036),0.6015427,DeepSpeed single file saving (#6900),,0
Changed basic_examples to use LightningCLI (#6862),0.7096308,- Removed the deprecated `LightningCLI` arguments ([#16380](https://github.com/Lightning-AI/lightning/pull/16380)),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
Add typings for evaluation_loop.py and remove some dead code (#7015),0.53168565,iteration_count and other index attributes in the loops has been replaced with progress dataclasses (#8477),,0
Bugfix/cuda oom detection and handling (#6934),0.6453054,"Deprecated log_gpu_memory, gpu_metrics, and util funcs in favor of DeviceStatsMonitor callback (#9921)",Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
rename about (#7002),0.4614436,Renamed several Trainer atributes:  (#567),  rename about   .   .. ,0
Remove lightning-dtrun installation (#7018),0.48986438,- Removed support for the `DDP2Strategy` ([#12705](https://github.com/Lightning-AI/lightning/pull/12705)),,0
Plugin Docs (#6952),0.6241243,apex plugin (#3502),Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: ananthsub ananth.subramaniam@gmail.com,0
Use PickleError base class to detect all pickle errors (#6917),0.63609654,- pickling errors with loggers (txs @awaelchli),  Use PickleError base class to detect all pickle errors   Update changelog with #6917   Add pickle test for torch ScriptModule   Co-authored-by: Ken Witham k.witham@kri.neu.edu Co-authored-by: Akihiro Nitta nitta@akihironitta.com,0
Train End Error Handling Fix (#6864),0.6514971,The trainer no longer tries to save a checkpoint on exception or run callback's on_train_end functions (#6864),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk,0
Bugfix for predict progressbar (#6884),0.7180347,- Fixed bug where progress bar was not being disabled when not in rank zero during predict ([#11377](https://github.com/PyTorchLightning/pytorch-lightning/pull/11377)),  gating   tests   pep8   changelog ,1
Fix the gradient_clip_algorithm has no effect issue. (#6928),0.78764653,"            gradient_clip_val=gradient_clip_val,",,1
Fixed wrong on_fit_start callback reference in documentation (#7001),0.6757544,"  * `Callback.on_pretrain_routine_{start,end}` in favor of `Callback.on_fit_start`",Co-authored-by: Daniele Acquaviva danieleacquaviva@Danieles-MacBook-Pro.local,0
update notebooks (#6968),0.4344519,"Updated DeepSpeed ZeRO (#6546, #6752, #6142, #6321)",,0
docs: lightning-bolts (#6967),0.52317107,LightningLite:,,0
Clean up environment access in plugins (#6941),0.49258193,"Prevent artefactual ""running from outside your current environment"" error (#15647)",Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,0
Fix Multi-GPU join for horovod (#6954),0.53534734,Moved torch.cuda.set_device() to enable collective calls earlier in setup (#8312),  fixjoin   fix join on cpu   fix typo   try to undo horovod skip   undo   Try removing skip   Update CHANGELOG   add back skip for test_horovod_multi_optimizer   Add back skip   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
Add SWA warning if not running every epoch (#6987),0.7863136,Enforce an epoch scheduler interval when using SWA (#6588),  Add SWA warning if not running every epoch   Typo ,1
Reuse _TORCHVISION_AVAILABLE (#6976),0.6066502,Removed dependency on torchvision (#797),,0
Fix inconsistent outputs in on_*_end and *_end (#6969),0.52229846,"Removed output argument from *_batch_end hooks (#3965, #3966)",Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,0
Remove evaluation loop legacy dict returns for *_epoch_end hooks (#6973),0.8426649,Removed evaluation loop legacy returns for *_epoch_end hooks (#6973),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
[CI] Drop brew update (#6985),0.4190876,Cleanup cluster waiting (#16054),,0
Fixing word tense (#6974),0.3552146,Refactor Model backward (#2276),"Changing ""defined"" to ""defines"" in keeping with the convention of using present tense in comments.",0
Fix Checkpoint issue when using Horovod distributed backend (PyTorchLightning#6947) (#6958),0.6629829,- Removed deprecated `CheckpointConnector.hpc_load` property in favor of `CheckpointConnector.restore` ([#10525](https://github.com/PyTorchLightning/pytorch-lightning/pull/10525)),Co-Authored-By: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
update docker base on PT 1.7 (#6931),0.51984614,Remove unnecessary intermediate layers in Dockerfiles (#5697),  update docker base on PT 1.7   fix path ,0
Updated iterable dataset with len warning message (#6972),0.5486568,Disabled sampler replacement when using IterableDataset (#11507),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
Fix sync_dist for tpus (#6950),0.54796124,Enabled manual optimization for TPUs (#8458),,0
fix self.device access in DataParallel (#6414),0.54194117,We cleaned up the properties related to device indices (#14829).,,0
"Fix failing profiler tests for CI: conda (3.7, 1.8) (#6765)",0.4929827,"Resolved schedule step bug for PyTorch Profiler (#6674, #6681)",,0
[Feat] DeepSpeed single file saving (#6900),0.8786889,DeepSpeed single file saving (#6900),"  Add single checkpoint capability   Fix checkpointing in test, few cleanups   Add comment   Change restore logic   Move vars around, add better explanation, make todo align with DeepSpeed team   Fix checkpointing   Remove deepspeed from extra, install in Dockerfile   push   pull   Split to two tests to see if it fixes Deepspeed error   Add comment ",1
Brew update to fix mac tests (#6970),0.41141504,Pkg: fix parsing versions by @Borda in https://github.com/Lightning-AI/lightning/pull/15401,"  Drop libomp to see what happens   Drop openmpi/horovod installs   Revert ""Drop libomp to see what happens""   This reverts commit cdd524f3   Update before install   Skip horovod failing test ",0
Fix ShardedDataParallel has no attribute require_backward_grad_sync (#6915),0.5362097,Removed call_configure_sharded_model_hook property from Accelerator and TrainingTypePlugin (#9612),Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,0
Accelerator API docs (#6936),0.71062076,Refactored into accelerator module:,Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
Run ddp_spawn dataloader checks on windows (#6930),0.9999999,Run ddp_spawn dataloader checks on Windows (#6930),,1
v1.3.0rc1 (#6925),0.6188654,[1.7.6] - 2022-09-13,  v1.3.0rc1   . ,0
Fix TPU Spawn gather (#6896),0.63740915,Resolve TPU miss rendezvous (#6781),,0
[fix] [easy] Update Model Checkpoint callback overrides to use base Callback signature (#6908),0.74096775,Saved checkpoints will no longer use the type of a Callback as the key to avoid issues with unpickling (#6886), Update model_checkpoint.py,1
Update DataLoader.persistent_workers warnings in ddp_spawn (#6762),0.84349567,Changed warnings and recommendations for dataloaders in ddp_spawn (#6762),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
[RFC] Add self.lr_schedulers() to LightningModule for manual optimization (#6567),0.77704006,"    def configure_optimizers(lightning_module, optimizer, lr_scheduler=None):",  Add test for lr_schedulers()   Add lr_schedulers to LightningModule   Update test comment   Update CHANGELOG ,1
fix gpus default for Trainer.add_argparse_args (#6898),0.84049404,  * Removed the `Trainer(auto_select_gpus=...)` argument,,1
Fix version check in DDP plugin test (#6906),0.54744756,Merged the implementation of DDPSpawnStrategy into DDPStrategy and removed DDPSpawnStrategy (#14952),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Move NaN/Inf detection to a separate utilities file (#6834),0.4617793,Remove nan loss in manual optimization (#5121),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
fix exception raising (#6901),0.61820084,Used raise .. from .. to explicitly chain exceptions (#3750),,0
[Fix] Ensure we set the eval/train flag correctly on accelerator model  (#6877),0.93103814,Ensure we set the eval/train flag correctly on accelerator model (#6877),"  Ensure we move the model to eval mode before running evaluation   Ensure we set the flag appropriately across all stages   Add test, move hooks logic   Apply same fix to the validate loop   Update pytorch_lightning/trainer/trainer.py   Fix function name   Fix order, add predict   Shorten the name   Fix input dm, drop duplicate on predict start hook call, as it's called in the setup function   Use hook, remove double call ",1
Merge pull request #6885 from PyTorchLightning/v1.3.0rc,0.69591415,Full commit list: https://github.com/PyTorchLightning/pytorch-lightning/compare/1.6.0...1.7.0,prepare v1.3.0rc,0
TPUSpawn + IterableDataset error message (#6875),0.55545086,"Removed process_idx from the {DDPSpawnPlugin,TPUSpawnPlugin}.new_process methods (#10022)",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Fix DDP_SPAWN compatibility with bug_report_model.py (#6892),0.6145078,Changed warnings and recommendations for dataloaders in ddp_spawn (#6762),,0
Update mlflow with using resolve_tags (#6746),0.95440483,Updated mlflow with using resolve_tags (#6746)," Update mlflow.py  6745 adds additional info about the run, as in the native API  Update mlflow.py  trying to fix some backward compatibility issues with resolve_tags  wip on backward compatibility  added a default for getattr in case the registry object exists, but has no proper attribute (weird case but who knows...)   fix pep   impoert   fix registry import   try fix failing tests   removed the first if statement, so that resolve_tags would be defined either case  fix formatting  Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz",1
Fix finetuning complex models correctly unfreezes. (#6880),0.6215589,Resolve bug with Finetuning (#5744),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
Remove hardcoding of rank_zero_only.rank in accelerator connector (#6878),0.8865169,Remove hardcoding of local rank in accelerator connector (#6878),,1
Add separators to performance docs (#6882),0.4548922,Allow metrics logged together with hparams (#1630),,0
Fix csv extension check (#6436),0.50309384,Deprecated tags_csv in favor of hparams_file (#1271),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
Update Changelog for v1.2.7 (#6874),0.655769,Full Changelog,  Update Changelog for v1.2.7   legacy   Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz,0
Docs fixes (#6870),0.64204633,Docs improvements,,0
Supporting Adding DDP Communication Hooks (#6736),0.80032563,DDP custom implementation support (override these hooks):," Fix some test errors Summary:  Test Plan: Reviewers: Subscribers: Tasks: Tags:   checkpoint consolidation   Update ddp_spawn.py   Update test_metric_result_integration.py   Update test_results.py   Update utils.py   Update utils.py   Update test_all_gather_grad.py   Update test_all_gather_grad.py   Update test_results.py   Revert ""Update test_results.py""   This reverts commit 9d4a2b891d2a4b37e21529a444bda1883d1b5ed1.  Revert ""Merge pull request #1 from shuyingsunshine21/shuyingsunshine21-checkpoint_consolidate""  This reverts commit c5053da789f9d04d2c967a65adf4fb026dc134b8, reversing changes made to 0d23d75bc91e4e0b7805712e394cb093fac22841.  Revert ""Update test_all_gather_grad.py""  This reverts commit 0d23d75bc91e4e0b7805712e394cb093fac22841.  Revert ""Update utils.py""  This reverts commit 70fe5da9c66ceff2fcf4be5b9efdd23a9af8389c.  Revert ""Update utils.py""  This reverts commit a9aae99f6ed6e9388ecf1d8a7bd79966176a65af.  Revert ""Update test_results.py""  This reverts commit ea749068785bbad689a12066544893b1605f20c5.  Revert ""Update test_metric_result_integration.py""  This reverts commit bf70e431b3ce4893de804e0f3b5d59e79346d6d7.  Revert ""Update ddp_spawn.py""  This reverts commit f17210183b84f90c9a62d1ff9b3e05e1fbe5f33b.  Revert ""checkpoint consolidation""  This reverts commit 536c1323b0e6715fb5919196ea48b0fcddddcd66.  Revert ""Revert ""checkpoint consolidation""""  This reverts commit 3a9fde915ad4c69620a6ccc411f5890cb38ba5ac.  Revert ""Revert ""Revert ""checkpoint consolidation""""""  This reverts commit 7a369f47e1a94d701fce48c994cc3f2da266dad0.  Revert ""Revert ""Update ddp_spawn.py""""  This reverts commit 8222dc98ead37d961a52b7366070aa10f66d92d1.  Revert ""Revert ""Update test_metric_result_integration.py""""  This reverts commit 6c095b2370a2afe9d24918a5798ce1ebffed7e0d.  Revert ""Revert ""Update test_results.py""""  This reverts commit 250d0aaaa2e6c6a6a3407bc6c8b83c0fe2479c0b.  Revert ""Revert ""Update utils.py""""  This reverts commit 8651d54d79396eaaba16d7eb1e769a1e91d5702e.  Revert ""Revert ""Update test_all_gather_grad.py""""  This reverts commit dcdcd29731061c919b15ab0b56669259817a81c4.   modify distributed environment to make test pass   add DDP communication hook   remove test related setting   remove more test related setting   fix ddp comm hook util import issue   comments   one more fix for test_custom_plugin   fix ddp spwan   fix sgd   address comments and add tests    add is gpu checking 2. modify test a bit 3. formatting    formatting nit   fix conda 3.7 1.7 issue for no torch.distributed.algorithms module   need at least 1.8.0   minor fix   modify changelog   changelog should link to PR number instead of issue number   refine a bit on doc for register_ddp_comm_hook function, like ddp_comm_wrapper explanation and add hyperparameter for power sgd states in example usge   move single device checking before call register_ddp_comm_hook   formatting   comments   typo   pre-commit formatting ",1
[fix] Better support for rank_zero_only setting for SLURM and torchelastic (#6802),0.7785455,Set better defaults for rank_zero_only.rank when training is launched with SLURM and torchelastic (#6802),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Update seed_everything() (#6843),0.65830404,seed_everything now fails when an invalid seed value is passed instead of selecting a random seed (#8787),  Update seed.py   Update pytorch_lightning/utilities/seed.py   Co-authored-by: thomas chaton thomas@grid.ai   Update seed.py   Update seed.py   Update seed.py   Co-authored-by: thomas chaton thomas@grid.ai,0
CI: fixture for global rank variable reset (#6839),0.51871216,"Defines shared proc. rank, remove rank from instances (e.g. loggers) (#1408)",,0
Update sync_dist warning for multiple processes (#6790),0.60341775,Ensure the existence of DDPPlugin._sync_dir in reconciliate_processes (#8939),,0
Add Trainer(gradient_clip_algorithm='value'|'norm') (#6123),0.72897995,    gradient_clip_algorithm,"  add changelog   add clip by value   fix bug in training tricks.rst   fix bug in trainer.rst   Update trainer.rst   Update trainer.rst   Update CHANGELOG.md   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/plugins/precision/deepspeed_precision.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/utilities/enums.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   yapf formatting   update training tricks   update based on comment   update based on comment   Update pytorch_lightning/trainer/trainer.py   Co-authored-by: ananthsub ananth.subramaniam@gmail.com   update based on comment   pep8   mypy   mypy   Update docs/source/advanced/training_tricks.rst   Co-authored-by: thomas chaton thomas@grid.ai   Update sharded_native_amp.py   Update test_sharded_parity.py   update test codes   Update test_tpu.py   Update pytorch_lightning/trainer/connectors/training_trick_connector.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   Update test_trainer.py   Update enums.py   Update enums.py   add super-class initialization to precision plugins.   add clip_grad horovod cpu test   add clip_grad horovod cpu test   use subprocess check_call   change order of horovod tests   set max_epochs 2 in horovod test   remove clip_grad_val test from horovod-cpu   remove ""type: ignore""   divide clip grad val test in horovod   update based on comments   add super-class initialization to precision plugins.   bugfix   bugfix   revert some changes   revert some changes   Update tests/models/test_horovod.py   merge master   Delete signature test   No point in testing a signature Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz",1
Simple reproducibility with minimum boilerplate CLI training with LightningCLI (#4492),0.7012594,"LightningCLI now supports registries for callbacks, optimizers, learning rate schedulers, LightningModules and LightningDataModules. This greatly improves the command line experience as only the class names and arguments are required as follows:",Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Fix EarlyStopping logic when min_epochs not met (#6705),0.68809885,EarlyStopping now runs at the end of the training epoch by default (#8286),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Fixed missing arguments in lr_find call (#6784),0.5809486,tuner.lr_find(...),There seem to be 3 arguments missing in the lr_find call in the tunining.py file.,0
Fix support for symlink save_dir in TensorBoardLogger (#6730),0.76853955,"Improved flexibility for naming of TensorBoard logs, can now set version to a str to just save to that directory, and use name='' to prevent experiment-name directory (#804)",  Add test for symlink support and initial fix   Respond to comment and add docstring   Update CHANGELOG.md   Simplify   Update pytorch_lightning/utilities/cloud_io.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Make LightningLocalFileSystem protected  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
[Fix] TPU Training Type Plugin (#6816),0.6541623,Removed the deprecated TrainerTrainingTricksMixin class (#8679),,0
Fix DPP + SyncBN (#6838),0.5645599,Ensure the existence of DDPPlugin._sync_dir in reconciliate_processes (#8939), Fix DPP + SyncBN   Ensure that model is already on correct GPU before applying SyncBN conversion  Fix order of SyncBN for ddp_spawn,0
Enforce an epoch scheduler interval when using SWA (#6588),1.0,Enforce an epoch scheduler interval when using SWA (#6588),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
Fix unfreeze_and_add_param_group expects modules rather than module (#6822),0.5348583,  * Removed the deprecated `pl_module` argument from the distributed module wrappers,,0
Sanitize None params during pruning (#6836),1.0000004,Sanitize None params during pruning (#6836),  sanitize none params during pruning   amend ,1
fix boolean check on iterable dataset when len not defined (#6828),0.5104618,    ds = IterableDataset(...),  fix iterable dataset len check   update predict and validate   add validate to test   add changelog   add predict ,0
Update TPU docs for installation (#6794),0.551844,Enabled manual optimization for TPUs (#8458),,0
[typing] Add typehint for broadcast in training type plugin (#6777),0.6078421,- Removed all deprecated training type plugins ([#14011](https://github.com/Lightning-AI/lightning/pull/14011)),  Update training_type_plugin.py   Update accelerator.py   Update pytorch_lightning/plugins/training_type/training_type_plugin.py   Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com,0
Remove extinct parameters from lightning_module.rst (#6801),0.5360108,Deprecated .get_model() with explicit .lightning_module property (#6035),Fixes  #6800,0
Fix validation progress counter with check_val_every_n_epoch > 1 (#5952),0.6824945,"Removed the on_epoch guard from the ""should stop"" validation check (#7701)",Co-authored-by: rohitgr7 rohitgr1998@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
less IDE complain about unused args (#6786),0.5565252,Args should come after the last positional argument (#1807),  less IDE complain about unused args   ... ,0
resolve bug (#6781),0.59434247,Resolve bug with Finetuning (#5744),,0
Update logic for checking TPUs availability (#6767),0.9536773,Updated logic for checking TPUs availability (#6767),  Update logic for checking TPUs availability   fix flake8   add fix ,1
Update clip gradients signature for precision plugins (#6764),0.63001883,"Moved the optimizer_step and clip_gradients hook from the Accelerator and TrainingTypePlugin into the PrecisionPlugin (#10143, #10029)",,0
Add 1.2.6 section to CHANGELOG (#6732),0.56372684,Full Changelog,  Add 1.2.6 sections to CHANGELOG   Update CHANGELOG.md   legacy   Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz,0
Remove legacy support for the magic log/progress_bar keys in dict returns (#6734),0.6094641,"Removed legacy code to log or include metrics in the progress bar by returning them in a dict with the ""log""/""progress_bar"" magic keys. Use self.log instead (#6734)",,0
DeepSpeed ZeRO Docs update (#6752),0.78580403,"Updated DeepSpeed ZeRO (#6546, #6752, #6142, #6321)",  Added base docs   Add more information   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
DeepSpeed ZeRO Update (#6546),0.8488699,"Updated DeepSpeed ZeRO (#6546, #6752, #6142, #6321)","  Add context to call hook to handle all modules defined within the hook   Expose some additional parameters   Added docs, exposed parameters   Make sure we only configure if necessary   Setup activation checkpointing regardless, saves the user having to do it manually   Add some tests that fail currently   update   update   update   add tests   change docstring   resolve accumulate_grad_batches   resolve flake8   Update DeepSpeed to use latest version, add some comments   add metrics   update   Small formatting fixes, clean up some code   Few cleanups   No need for default state   Fix tests, add some boilerplate that should move eventually   Add hook removal   Add a context manager to handle hook   Small naming cleanup   wip   move save_checkpoint responsability to accelerator   resolve flake8   add BC   Change recommended scale to 16   resolve flake8   update test   update install   update   update test   update   update   update test   resolve flake8   update   update   update on comments   Push   pull   Update pytorch_lightning/plugins/training_type/deepspeed.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update pytorch_lightning/plugins/training_type/deepspeed.py  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   update   Apply suggestions from code review   Swap to using world size defined by plugin   update   update todo   Remove deepspeed from extra, keep it in the base cuda docker install   Push   pull   update   update   update   update   Minor changes   duplicate   format   format2   Co-authored-by: SeanNaren sean@grid.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz",1
[docs] Update Bolts link (#6743),0.60486877,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",  Update Bolts link   Update Bolts link   formt   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
[bugfix] Add support for omegaconf and tpu (#6741),0.51920897,Enabled manual optimization for TPUs (#8458),  fix_hydra   update changelog   Co-authored-by: Your Name you@example.com,0
update chlog v1.2.5 (#6742),0.5213155,Removed logger_connector legacy code (#6733),  update chlog v1.2.5   legacy ,0
Remove logger_connector legacy code (#6733),0.96314883,Removed logger_connector legacy code (#6733),,1
update readme by v1.2.x (#6728),0.5551641,"Since 2.0 is a major release, we took the opportunity to take our APIs to the next level and make considerable changes to reduce the backwards incompatible changes in the future. To alleviate this, we will commit to continue supporting the 1.9.x line of releases by doing bug-fix releases with any important fixes that are necessary.",,0
[Model Parallel] Add configure sharded model hook (#6679),0.6875026,def configure_sharded_model(self):,"  Add base hook for model parallel   fix callback signature   Simplify hook   Add hook logic   add tests   add property setter   add logic for being called once   Update changelog   Fix   fix return type   fix lambda callback test   Fix tests   Apply code suggestions   add logic for setup_optimizers_predispatch   add common dummy model   Swap call order   Remove test that isn't needed anymore   Update tests   Add a bit more doc   Few code review fixes   Update pytorch_lightning/accelerators/accelerator.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   Change hook name   Fix test   Test setup hook, refactor names   Swap call order of callbacks and model initialization   Change name of context manager   Co-authored-by: SeanNaren sean@grid.ai Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com",0
[refactor] Move save_function to accelerator 1/n [DeepSpeed] (#6689),0.79901975,Moved save_function to accelerator (#6689),  move save_checkpoint responsability to accelerator   update ,1
[TPU] update is_tpu_exists utils internal logic to rely on xmp.spawn  (#6719),0.49359632,Ensured process_dataloader is called when tpu_cores > 1 to use Parallel DataLoader (#6015),  update_logic   update   Update tests/utilities/test_xla_device_utils.py   Update pytorch_lightning/utilities/xla_device.py   Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com  Update pytorch_lightning/utilities/xla_device.py  Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com   update test   Update tests/utilities/test_xla_device_utils.py   update   Apply fix   Docstring   flake8   update   Co-authored-by: Your Name you@example.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
support python 3.9 (#4944),0.79030526,Compatibility for Python 3.10,  support python 3.9   update CI   onnxruntime   .   .   onnxruntime   t 55   t 75   add script   use   onnx   onnx   onnx   whl   np   find   21   Apply suggestions from code review   Apply suggestions from code review   onnx   CI   req   ~ dockers   min   .   drop horovod   drop horovod   drop horovod   fix   fix   . ,1
More explicit exception message when testing with fast_dev_run=True (#6667),0.66678643,"Disabled checkpointing, earlystopping and logging with fast_dev_run (#5277)",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
remake nvidia docker (#6686),0.4911136,Remove unnecessary intermediate layers in Dockerfiles (#5697),  use latest   remake   examples ,0
Remove legacy Result parameters (#6016),0.6247774,Removed legacy references for magic keys in the Result object (#6016),,0
[warning] Add warning when values are not being reduced (#6417),0.54578483,Removed auto val reduce (#2462),  add warning non reduced   add test   update test   update changelog   Update pytorch_lightning/trainer/connectors/logger_connector/epoch_result_store.py   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com  update  Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,0
Automatically find and run special tests (#6669),0.56556594,- Full tests that test specific functionality in trainer.,,0
Do not describe when there's no summary (#6681),0.58632386,  * `description`,,0
Do not add return dict items to callback_metrics (#6682),0.7936155,Removed legacy code to include step dictionary returns in callback_metrics. Use self.log_dict instead. (#6682),,1
Add artifcact_location arg to MLFlow logger (#6677),0.6800028,MLflowLogger now uses the env variable MLFLOW_TRACKING_URI as default tracking URI (#7457),  Add artifcact_location arg to MLFlow logger   Add CHANGELOG URL   Update test ,0
Resolve schedule step bug for PyTorch Profiler (#6674),0.98754495,"Resolved schedule step bug for PyTorch Profiler (#6674, #6681)",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Simplify deprecations (#6620),0.7249665,Deprecation warning (#3844),  use external deprecate   simplify   simplify   simplify   flake8   .   others   . ,1
Add on_epoch_start to run at the beginning of every loop irrespective of train/val/test (#6498),0.77022046,Changed the behavior of on_epoch_start to run at the beginning of validation & test epoch (#6498),  update docs   add hook and update docs   update tests   chlog   Update CHANGELOG.md   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  chlog  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Support teardown hook on DataModule (#4673),0.6838618,"DataModules now avoid duplicate {setup,teardown,prepare_data} calls for the same stage (#7238)",Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: chaton thomas@grid.ai,0
Update CODEOWNERS (#6220),0.44929633,Renamed several Trainer atributes:  (#567),,0
Fix checkpoint callback & Trainer.test(_) issue for TPUs (#6654),0.77446324,Deprecated checkpoint_callback from the Trainer constructor in favor of enable_checkpointing (#9754),  Fix checkpoint callback issue for TPUs   update changelog   add barrier   apply code suggestions   update trainer test   remove spaces   fix tpu tests   Apply suggestions from code review   add comment   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
Match the number of outputs of backward with forward for AllGatherGrad (#6625),0.51731604,"Changed the order of backward, step, zero_grad to zero_grad, backward, step (#6147)",,0
MetricsHolder clean-up + typing (#6645),0.69873774,Remove MetricsHolder (#7909),  Metrics holder cleanup and better error message   Update pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py   _VALUE -> _METRIC_TYPE ,0
add copyr (#6661),0.41598025,"If we forgot somebody or you have a suggestion, find us on Discord :zap:",,0
Remove ModelSummary validation from train loop on_trainer_init (#6610),0.95875853,Removed ModelSummary validation from train loop on_trainer_init (#6610),,1
Follow E231 [flake8] (#6110),0.4332019,"@Borda, @EspenHa, @teddykoker",  Remove E231 from ignore list   Follow E231   Update pytorch_lightning/trainer/data_loading.py ,0
Feature/double precision (#6595),0.7011218,    precision=16,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,1
Docs/robots (#6658),0.501003,Docs,,0
fix: update example autoencoder.py to reflect args (#6638),0.5527644,Changed PyTorchProfiler to use torch.autograd.profiler.record_function to record functions (#6349),  fix: update example autoencoder.py to reflect args   Update pl_examples/basic_examples/autoencoder.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Prune metrics: others 11/DoNe (#6659),0.6644529,Drop duplicate metrics (#5014),  classif   grad_img   nlp   ssl   format ,0
Use pl.LightningModule in new-project docs (#6656),0.67255676,class MyLightningModule(pl.LightningModule):,,0
Fix disabled grads after call to predict (#6657),0.5381458,"Marked several methods in PredictionLoop as protected: on_predict_start, on_predict_epoch_end, on_predict_end, on_predict_model_eval (#9516)",,0
update coverage config (#6524),0.52575594,- Code coverage (99%),  update coverage config   parallel   parallel   Apply suggestions from code review   Apply suggestions from code review   paralel   paralel   paralel   combine   combine   .   ..   ..   ..   rev   cb   cb   drop   drop   .   ..   ...   ...   ...   . ,0
Add PyTorch 1.8 Profiler 5/5 (#6618),0.6679392,| Import pytorch_lightning.profiler                                                                          | 1.9             | pytorch_lightning.profilers                   |,  Refactor profilers   Update PassThrough   WIP - This is broken and will change   Update pytorch_lightning/profiler/pytorch.py   Co-authored-by: thomas chaton thomas@grid.ai   resolve tests   resolve tests   find output   try something   update   add support for test and predict   update   update   use getattr   test   test   update   tests   update   update   update   update   update   remove file   update   update   update   update   update   test   update#   update   update tests   update   add suport for 1.8   rename records   add support for 1.8   update   resolve flake8   resolve test   Refactor basic profilers   Fixes   Unused import   Introduce setup   Profile on all ranks. Print to stdout on 0   Introduce dirpath + filename   CHANGELOG   Add tests. Address comments   add on_run_stage_setup   add on_run_stage_setup function   update   add test for RegisterRecordFunction   update lightnng flow direction   move variable to private   remove trace   Undo code that should be in 3/4   Multi-stage multi-rank   2/5 changes   Pass stage in del   Remove TODOs   Describe on_evaluation_end. Add tests   Typo   Address comments   deepcopy tests   Advanced teardown   Fix teardown test   Fix tests   Minor change   Update CHANGELOG.md   Fix test   Quick fixes   Fix 6522   resolve ddp tests   resolve tests   resolve some tests   update tests   resolve tests   update   resolve tests   resolve some tests   Missed fixes from 3/5   Fixes   resolve some tests   resolve test for 1.7.1   Broken refactor   Missed stage   Minor changes   resolve tests   Update CHANGELOG   resolve bug   remove print   Typo   Cleanup   resolve ddp test   remove barrier   update profiler   update   Smaller model   update   resolve tests   update   Minor changes. CHANGELOG   Minimize diff   update to 1.8.1   RunIf. Extra code. Check segfault   resolve tests   Typo. Bad merge   Fixing a bad merge   replace for kineto   Update pytorch_lightning/profiler/pytorch.py   Co-authored-by: ananthsub ananth.subramaniam@gmail.com  Update pytorch_lightning/profiler/pytorch.py  Co-authored-by: ananthsub ananth.subramaniam@gmail.com   Minor changes   Bad merge   Use lists for flexibility   Use sets   predict_step   Ananth's suggestion   update   Docs   Update pl_examples/basic_examples/profiler_example.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   update example   update example   Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Refactor PyTorch profiler 4/5 (#6349),0.67232835,"Resolved schedule step bug for PyTorch Profiler (#6674, #6681)",Co-authored-by: thomas chaton thomas@grid.ai,0
fix back-compatibility for Accel (#6655),0.5014133,Renamed all backends to Accelerator (#4066),,0
Flash predict step (#6577),0.51512057,"Refactored prediction loop interface; added new classes PredictionLoop, PredictionEpochLoop (#7700, #8077)",  add predict_step   Update predict_loop.py   Update trainer.py   Update trainer.py   resolve bugs   update   update   update   resolve bug   resolve some failing tests   udpate tests   update   resolve tests   add a test   remove typo   add a test for attachement   update   changed to on_train_dataloader   remove flash_special_attr   resolve tests   update   update   update   update on comments   Update pytorch_lightning/trainer/data_loading.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
prune metrics: info retrieval (#6649),0.5741987,Classification metrics overhaul (#4837),,0
Refactor base profilers 3/5 (#6621),0.6438297,Split profilers module (#6261),Co-authored-by: tchaton thomas@grid.ai,0
Prune metyrics: regression 9/n (#6637),0.4855358,Regression metrics (#2221),  psnr   r2score   ssim   chlog ,0
Prune metrics: regression 8/n (#6636),0.74293625,Regression metrics (#2221),  explained_variance   tests   mean_absolute_error   mean_squared_error   mean_relative_error   mean_squared_log_error   chlog ,1
fix comparing versions (#6434),0.59942454,Parsed local package versions (#13933),  fix comparing versions   chlog   .   ...   datasets ,0
[refactor] Add setup to profilers + _run_stage_setup to trainer 2/5 (#6633),0.74075174, - Profiling: `Trainer(profiler=...)`,  add setup   update   updates on comment   Minor changes   Extra import   Docs   Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
hotfix: mock examples (#6632),0.5248796,Updated app testing (#16000),  mock examples   drop from GA ,0
refactoring setup (#6590),0.62630904,Refactoring,  refactoring setup   .   docs   flake8 ,0
Add teardown method to BaseProfiler. (#6370),0.5396822,Changed teardown() in Accelerator to allow training_type_plugin to customize teardown logic (#7579),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com,0
Allow training type plugin to delay optimizer creation (FSDP 2/n) (#6331),0.90083003,Allowed training type plugin to delay optimizer creation (#6331),"  Allow training_type_plugin to delay optimizer configure   Add missing references to trainer, add a CPU accelerator based test ",1
Clean utilities/argparse and add missing tests (#6607),0.6573354,argparse_utils >> argparse,,0
drop mypy from .pre-commit-config.yaml (#6542),0.4094497,  * `save_config_overwrite`,,0
Move profiler tests (#6619),0.6925866,Moved profilers to their own file (#7822),,0
Add DDP Spawn being default for Multi GPUs (#6292),0.7668354,Made DDP the default if no backend specified with multiple GPUs (#1789),,1
Add trainer.predict config validation (#6543),0.79352283,Deprecated the Trainer.disable_validation property in favor of not Trainer.enable_validation (#8291),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
"Add AMP for validation, prediction and testing (#6565)",0.5543417,  * Removed the `Trainer(amp_backend=...)` argument,  Add Tests for val and test-steps   Add native AMP   pep8 tests   pep8 plugin   changelog ,0
fixing examples (#6600),0.528556,Simplify the PL examples structure (shallower and more readable) (#1247),  try Azure   -e   path ,0
Prune metrics: other classification 7/n (#6584),0.722676,Classification metrics overhaul (#4837),  confusion_matrix   iou   f_beta   hamming_distance   stat_scores   tests   flake8   chlog ,1
Automatically set sync_batchnorm for training_type_plugin (#6536),0.99999994,Automatically set sync_batchnorm for training_type_plugin (#6536),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Roger Shieh sh.rog@protonmail.ch Co-authored-by: Kaushik Bokka kaushikbokka@gmail.com,1
NGC container PoC (#6187),0.38995367,- Removed deprecated ClusterEnvironment properties `master_address` and `master_port` in favor of `main_address` and `main_port` ([#13458](https://github.com/Lightning-AI/lightning/pull/13458)),  add NVIDIA flows   push   pull   ...   extras   ci prune   fix   tag   .   list ,0
Update Gradient Clipping for TPU Accelerator (#6576),0.97613335,Update Gradient Clipping for the TPU Accelerator (#6576),,1
Fix all_gather for tpu_cores=8 (#6587),0.67289037,  * Removed the `Trainer(tpu_cores=...)` argument,,0
[Fix] Move init dist connection into the setup function (#6506),0.5237944,moves init apex from LM to apex connector (#3923),"  Move connection setup into the setup function. Call setup hook after we set up the accelerator   Added CHANGELOG.md   fix setup order in callback test   fix input arguments in test   Mock distributed function, remove protection to turn into training type hook   Remove import   Add missing mock, ensure custom plugin does not create children process   Skip test on windows   Update deepspeed to init connection in setup   Do not initialize distributed module   Move DeepSpeed tests to special tests since dist communication is being set up   Special the test to see if this fixes CI   Delete accelerator connector test to see if its causing build to fail   Delete deepspeed test   Revert ""Delete accelerator connector test to see if its causing build to fail""   This reverts commit edde60b8  Revert ""Delete deepspeed test""  This reverts commit 9d317429   Reverse hook   Reverse setup hooks to debug again   Add todo so i know where i left off   For single device move in pre_dispatch after setup function   Add additional model to device hook if any additional parameters have been set   See if we can enable deepspeed tests   Revert ""See if we can enable deepspeed tests""   This reverts commit b5450def   See if this hook approach works   Introduce new granular hooks   Remove import, fix tpu spawn by moving the function to setup   Added missing special test   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com",0
Update Changelog for v1.2.4 (#6581),0.6311133,Full Changelog,  Update changelog for v1.2.4   lagacy v1.2.4   prune duplicates from changelog   Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz,0
Prune metrics: precision & recall 6/n (#6573),0.68610287,The Recall and Precision metrics (and their functional counterparts recall and precision) can now be generalized to Recall@K and Precision@K with the use of top_k parameter (#4842),  avg precision   precision   recall   curve   tests   chlog   isort   fix ,0
[doc] Update Dict Train Loader doc.  (#6579),0.63076234,Deprecated training_tqdm_dict in favor of progress_bar_dict (#1450).,  update doc   update example ,0
Prune metrics: AUC & AUROC (#6572),0.6360142,Removed reorder parameter of the auc metric (#5004),  class: AUC AUROC   func: auc auroc   format   tests ,0
prune metric: accuracy 4/n (#6515),0.57193655,Classification metrics overhaul (#4837),  prune accuracy   chlog   flake8   Apply suggestions from code review   Co-authored-by: Nicki Skafte skaftenicki@gmail.com   wrap   test   test   fix   Co-authored-by: Nicki Skafte skaftenicki@gmail.com,0
fix deprecation wrapper & tests (#6553),0.6801734,Deprecation warning (#3844),  fix deprecation wrapper & tests   flake8 ,0
[doc] Add Zero Grad set_to_none=True trick (#6548),0.63106304,            optimizer.zero_grad(),  add trick to doc   update   update path   Update docs/source/benchmarking/performance.rst   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
Add outputs param for on_val/test_epoch_end hooks (#6120),0.7835817,Removed output argument from *_epoch_end hooks (#3967),  add outputs param for on_val/test_epoch_end hooks   update changelog   fix warning message   add custom call hook   cache logged metrics   add args to docstrings   use warning cache   add utility method for param in sig check   Update CHANGELOG.md   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   update docstring   add test for eval epoch end hook   add types and replace model ref   add deprecation test   fix test fx name   add model hooks warning   add old signature model to tests   add clear warning cache   sopport args param   update tests   add tests for model hooks   code suggestions   add signature utils   fix pep8 issues   fix pep8 issues   fix outputs issue   fix tests   code fixes   fix validate test   test   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
prune warning & deprecation wrapper (#6540),0.6970217,Deprecation warning (#3844),  docs   wrapper   test   count   flake8 ,0
Prune metric: helpers and inputs 3/n (#6547),0.6179635,Metric reduction with Logging (#5150),  _basic_input_validation   _check_shape_and_type_consistency   _check_num_classes_binary   _check_num_classes_mc   _check_num_classes_ml   _check_top_k   _check_classification_inputs   _input_format_classification   _reduce_stat_scores   DataType   rest   flake8   chlog ,0
refactor reading env defaults (#6510),0.5873706,  * `env_parse`,  change tests   fix   test   _defaults_from_env_vars   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Custom Plugin is_distributed (#6537),0.5603437,Skip broadcast if distributed not initialized for the spawn plugins (#8017),  return from plugin   dont return for tpu ,0
Prune metrics base classes 2/n (#6530),0.620758,Sklearn metrics classes (#1327),  base class   extensions   chlog   _stable_1d_sort   _check_same_shape   _input_format_classification_one_hot   utils   to_onehot   select_topk   to_categorical   get_num_classes   reduce   class_reduce   tests ,0
Update hook lifecycle (#6538),0.6155889,Updated hooks arguments - breaking for setup and teardown (#2850),  Update hook lifecycle   Update docs/source/common/lightning_module.rst ,0
fix attribute access in LightningModule.toggle_optimizer (#6513),0.757221,"- The `LightningModule.{un}toggle_optimizer` methods no longer accept a `optimizer_idx` argument to select the relevant optimizer. Instead, the optimizer object can be passed in directly ([#16560](https://github.com/Lightning-AI/lightning/pull/16560))",,1
Update DeepSpeed docs (#6528),0.7947105,"Updated DeepSpeed ZeRO (#6546, #6752, #6142, #6321)",  Clean up docs and add some explicitness around stages   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
[test] lr_find with bs_scale (#6422),0.53354686,tuner.lr_find(...),  init test: test_lr_find_with_bs_scale   Update test_lr_finder.py   remove gpu req   try boring model   custom boring model   pep8   fix typo   Update test_lr_finder.py   typo   typo ,0
deprecate metrics pkg (#6505),0.85287327,Removed deprecated metrics (#6161),  deprecate metrics   examples   req   docs   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com  pep8  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com,1
CI: Azure publish results (#6514),0.44747666,Resolved a bug where the wrong client was passed to collect cloud logs (#14684),,0
Mean Average Precision metric for Information Retrieval (1/5) (#5032),0.5947759,The Recall and Precision metrics (and their functional counterparts recall and precision) can now be generalized to Recall@K and Precision@K with the use of top_k parameter (#4842),"  init information retrieval metrics   changed retrieval metrics names, expanded arguments and fixed typo   added 'Retrieval' prefix to metrics and fixed conflict with already-present 'average_precision' file   improved code formatting   pep8 code compatibility   features/implemented new Mean Average Precision metrics for Information Retrieval + doc   fixed pep8 compatibility   removed threshold parameter and fixed typo on types in RetrievalMAP and improved doc   improved doc, put first class-specific args in RetrievalMetric and transformed RetrievalMetric in abstract class   implemented tests for functional and class metric. fixed typo when input tensors are empty or when all targets are False   fixed typos in doc and changed torch.true_divide to torch.div   fixed typos pep8 compatibility   fixed types in long division in ir_average_precision and example in mean_average_precision   RetrievalMetric states are not lists and _metric method accepts predictions and targets for easier extension   updated CHANGELOG file   added '# noqa: F401' flag to not used imports   added double space before '# noqa: F401' flag   Update CHANGELOG.md   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   change get_mini_groups in get_group_indexes   added checks on target inputs   minor refactoring for code cleanness   split tests over exception raising in separate function && refactored test code into multiple functions   fixed pep8 compatibility   implemented suggestions of @SkafteNicki   fixed imports for isort and added types annontations to functions in test_map.py   isort on test_map and fixed typing   isort on retrieval and on init.py and utils.py in metrics package   fixed typo in pytorch_lightning/metrics/init.py regarding code style   fixed yapf compatibility   fixed yapf compatibility   fixed typo in doc   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com",0
document exceptions for metrics/functional (#6273),0.6327113,"Add deprecated metric utility functions back to functional (#5067, #5068)",  document exceptions for metrics/functional   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Apply suggestions from code review  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com,0
CI: resume testing with py3.8 (#6516),0.58129144,Skip restore from resume_from_checkpoint while testing (#5161),  testing on python 3.8   req ,0
Handle torch.jit scripted modules in layer summary (#6511),0.5331863,- Fixed an issue with comparing torch versions when using a version of torch built from source ([#17030](https://github.com/Lightning-AI/lightning/pull/17030)),,0
[bug] Update broadcast + reduce decision ModelCheckpoint] (#6410),0.52384496,Removed a redundant warning with ModelCheckpoint(monitor=None) callback (#9875),  resolve bug   update   update changelog   update PR   Update pytorch_lightning/trainer/connectors/logger_connector/epoch_result_store.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   add todo   resolve issues   resolve flake8   update   add coverage for reduce   wip   restore back to brodbact   remove test.py   resolve flake8   update   check world size   resolve test   update   use pytorch version when defined   update on comments   update on comments   flake8   resolve bugs   Update CHANGELOG.md   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   update   update   update   update   remove test   update   resolve flake8   update   update   update   proxy   update   update   resolve typo   prune   update parallel   update   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Update docs for limit_predict_batches (#6507),0.58838916,- Predict's custom BatchSampler that tracks the batch indices no longer consumes the entire batch sampler at the beginning ([#16826](https://github.com/Lightning-AI/lightning/pull/16826)),  add docs and minor updates   docs   fraction ,0
Fix tuner.scale_batch_size not finding batch size attribute when using datamodule (#5968),0.8087851,tuner.scale_batch_size(...),,1
[doc] Update the order of zero_grad and backward (#6478),0.6489847,"Changed the order of backward, step, zero_grad to zero_grad, backward, step (#6147)",  Fix zero_grad in docs   Fix zero_grad in docs ,0
Remove unused mixin attributes (#6487),0.9264266,Removed unused mixin attributes (#6487),  Remove unused mixing attributes   Missing import ,1
update xla version (#6464),0.63611543,  * Deprecated the internal `XLADeviceUtils.xla_available` staticmethod,,0
[feat] Support iteration-based checkpointing in model checkpoint callback (#6146),0.707628,Forced ModelCheckpoint callbacks to run after all others to guarantee all states are saved to the checkpoint (#5731),  Update model_checkpoint.py   add tests   Update model_checkpoint.py   Update test_model_checkpoint.py   fix tests   every_n_batches   Update test_model_checkpoint.py   defaults   rm tests   Update model_checkpoint.py   Update test_model_checkpoint.py   Prune deprecated metrics for 1.3 (#6161)   prune deprecated metrics for 1.3   isort / yapf   Update model_checkpoint.py   add tests   defaults   Update CHANGELOG.md   pre-commit   Update model_checkpoint.py   update defaults   Update test_remove_1-5.py   Update model_checkpoint.py   Update model_checkpoint.py   Update model_checkpoint.py   Update model_checkpoint.py   Update model_checkpoint.py   Update model_checkpoint.py   fix tests   Update test_model_checkpoint.py   Update model_checkpoint.py   Update model_checkpoint.py   Update model_checkpoint.py   Update test_model_checkpoint.py   ckpt-callback   Update test_model_checkpoint.py   Update model_checkpoint.py   Update model_checkpoint.py   validation-end   Update model_checkpoint.py   Update test_model_checkpoint.py   Update test_model_checkpoint.py   Update test_model_checkpoint.py   Update test_model_checkpoint.py   clarify-names   Make names explicit as to which hooks they apply to   Use step instead of batch for consistency with global step   Update model_checkpoint.py   Update model_checkpoint.py   Update model_checkpoint.py   Update model_checkpoint.py   Update model_checkpoint.py   mutual-exclusive   Make every_n_train_steps and every_n_val_epochs mutually exclusive   fix-default-0   Update CHANGELOG.md   formatting   make-private   make attributes private to the class  rebase  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
remove obsolete todo in pl_examples (#6475),0.4329857,Deprecated is_overridden(model=...) in favor of is_overridden(instance=...) (#7918),,0
Disable batch transfer in DP mode (#6098),0.9718715,Disabled batch transfer in DP mode (#6098),  add exceptions and test   hook   fix   clean up   clean up   regex   regex   docs   rev   comment and docs   chlog   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Apply suggestions from code review  Co-authored-by: chaton thomas@grid.ai   Monkey-patch device count   docs   pep   api_change   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: chaton thomas@grid.ai,1
argparse: Add use_argument_group=True (#6088),0.8105664,"Alternatively, you can add the argparse arguments you want manually:python"," argparse: Add inplace option  Replicate in GAN model   datamodule: Deduplicate logic w/ argparser utilities   Update pl_examples/domain_templates/generative_adversarial_net.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Apply suggestions from code review  Co-authored-by: Akihiro Nitta nitta@akihironitta.com   Keep docstrings   Correct name   Whitespace   Consistency   fix weird type stuff   try alt - use_argument_group   fix syntax + lint   fix ci errs   fix ci   change examples... still failing w/ ""unrecognized arguments: --batch_size""   address review   mnist_datamodule: add some docstrings   argparse: check cls or cls.init for param   didn't capture issue, but meh   fix lint   fix no-doc edge case   address review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com",1
cover subproc coverage (#6477),0.48788798,- Code coverage (99%),,0
Hotfix for torchvision (#6476),0.6735371,Removed dependency on torchvision (#797),,0
Allow user to disable the automatic formatting of checkpoint file names. (#6277),0.60693085,Removed deprecated checkpoint argument filepath (#5321),  cleaning SWA (#6259)   rename   if   test   chlog   Remove opt from manual_backward in docs (#6267)   switch agents pool (#6270)   Allow user to disable the automatic formatting of checkpoint file names.   Added changelog entry.   Made flake8 happy.   Applied review suggestion: quotes for special characters in docstring   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   Fixed example in docstring.   Fixed syntax error in docstring.   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Add Trainer.validate(…) method to run one validation epoch (#4948),0.77927446,Deprecated the Trainer.disable_validation property in favor of not Trainer.enable_validation (#8291),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: chaton thomas@grid.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Remove redundant test (#6466),0.5353608,Refactored setup_training and remove test_mode (#5388),,0
[Fix] Ensure we set the default device before initializing deepspeed (#6460),0.6580657,"Implemented DeepSpeedPlugin._setup_model_and_optimizers (#10009, #10064)",  Ensure we set the default device before initializing deepspeed   Add CHANGELOG.md   Update pytorch_lightning/plugins/training_type/deepspeed.py   Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,0
[bug] All_gather support tensor on cpu (#6416),0.6767012,Auto convert tensors to contiguous format when gather_all (#4907),  add test   update changelog   update   rename function ,0
Set find unused parameters to True by default to fix breaking compatibility (#6438),0.66431564,Changed the default of find_unused_parameters to False in DDP (#5185),"  Set find unused parameters to True by default to fix breaking models, add suggestion to re-enable   Add changelog ",0
Raise an exception if check_val_every_n_epoch is not an integer (#6411),0.56826484,(checks val every 100 train epochs),  raise an exception if check_val_every_n_epoch is not an integer   remove unused object   add type hints   add return type   update exception message   update exception message ,0
Improve DummyLogger (#6398),0.6387548,"Mocking loggers (#3596, #3617, #3851, #3859, #3884, #3853, #3910, #3889, #3926)","  fix dummy logger   docs   update docs   add changelog   add none return annotation   return empty string for name, version ",0
[changelog] Update Changelog on release v1.2.3 (#6444),0.65804434,Full Changelog,  update changelog   legacy 1.2.3   Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz,0
Typing for tests 1/n (#6313),0.4943335,Refactored setup for typing friendly (#6590),  typing   yapf   typing ,0
fix logger creating directory structure too early in DDP (#6380),0.59908867,Return the default_root_dir as the log_dir when the logger is a LoggerCollection (#8187),  fix   add simple test   fix imports   add changelog   tighter test with on_fit_start hook closer to the dispatch call   move class inside test f unction   add a comment ,0
update (#6403),0.4817846,"Updated DeepSpeed ZeRO (#6546, #6752, #6142, #6321)",,0
fixed bug where tuner would not tune lr if also tuning batch_size (#4688),0.7090954,tuner.scale_batch_size(...),  fixed bug where tuner would not tune lr if also tuning batch_size   added a '+1' to computing the smoothed loss. This maintains the behavior for the smoothed loss as before the bug fix   pep8 fix   add changelog   Co-authored-by: chaton thomas@grid.ai Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Add check for verbose attribute of ModelCheckpoint (#6419),0.66199905,"Removed deprecated ModelCheckpoint arguments prefix, mode=""auto"" (#6162)",Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
fix dp reduction test (#6404),0.61095756,fix result obj DP auto reduce (#3013),  fix   update   fix   move the class outside ,0
"Pass {fit,validate,test,predict} to setup() and teardown() (#6386)",0.9112792,"Changed setup() and teardown() stage argument to take any of {fit,validate,test,predict} (#6386)",,1
Run CI (#6402),0.43787524,"    'path/to/checkpoint.ckpt',",,0
Fix AttributeError: 'NoneType' object has no attribute 'finalize'  on TPU (#6221),0.50437367,Raised ValueError when a None value is self.log-ed (#7771), Fix bug  Fix AttributeError: 'NoneType' object has no attribute 'finalize'   Update CHANGELOG.md   deleted a period   Update CHANGELOG.md   Co-authored-by: Akihiro Nitta nitta@akihironitta.com   Update CHANGELOG.md   Update pytorch_lightning/plugins/training_type/tpu_spawn.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
update python version (#6399),0.63570106,"Following Python's end-of-life, support for Python 3.6 has been removed.",,0
Fix trainer not resetting lightning_optimizers (#6372),0.7965865,- Removed the deprecated `Trainer.lightning_optimizers` ([#14889](https://github.com/Lightning-AI/lightning/pull/14889)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Update TBLogger docs (#6315),0.5999304,Changed the default logger to TensorBoardLogger (#609),  Update tensorboard.py   Update logging.rst   pep8   Update logging.rst   Update logging.rst   Apply suggestions from code review   add code sample   Update logging.rst   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
"Fix ModelCheckpoint(monitor=None, save_last=True) not saving checkpoints (#6136)",0.8380283,Removed the deprecated model argument from ModelCheckpoint.save_checkpoint (#8688),Co-authored-by: ananthsub ananth.subramaniam@gmail.com,1
[doc] Fix closure in manual optimization (#6374),0.6691408,Check if optimizer supports closure (#4981),  Fix manual optimization docs   Fix typo. Thanks @import-antigravity ,0
[doc] Improve Multiple Val/Test Dataloaders with simultaneous batches option (#6320),0.6436809,Passing 1 dataloader per stage,  improve doc to describe how to combine batches of multiple test and val dataloaders simultaneously   fix typo   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  use paramref  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Remove optimizer_idx arg in manual optimization (#6093),0.74899364,Removed the deprecated optimizer_idx from training_step as an accepted argument in manual optimization (#8576),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: chaton thomas@grid.ai,1
Update Sharded test with RunIf (#6384),0.5149199,def configure_sharded_model(self):,,0
Fix manual optimization in pl_example (#6373),0.6524936,Changed deprecated enable_pl_optimizer=True (#5244),  Fix automatic_optimization   Fix automatic_optimization   Uncomment fairscale ,0
Remove no return warning from val/test step (#6139),0.98338735,Removed no return warning from val/test step (#6139),  remove warning   auto_opt   chlog   auto_opt   no_warning_call   rm old code   add warning for predict   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
"Use f-""""""-string in a Trainer comment (#6377)",0.50766826,trainer = Trainer(),"  Use f-""""""-string   Add r   Use Trainer.   r -> noqa: W605 ",0
require: adjust versions (#6363),0.5119423,Pkg: fix parsing versions by @Borda in https://github.com/Lightning-AI/lightning/pull/15401,  adjust versions   release   manifest   pep8   CI   fix   build ,0
Refactor RunningStage usage in advance of implementing Trainer.validate() (#4945),0.7887423,"Refactor RunningStage and TrainerState usage (#4945, #7173)"," Update code  Co-authored-by: EliaCereda   More property updates   Move properties. Introduce trainer._fitting   Use trainer.fitting   Fix reset dataloaders   Unused code   RunningStage.SANITY_CHECKING   Use setters   Fix bugs   Fix bugs   TrainerState.{FITTING,VALIDATING,TESTING,PREDICTING,TUNING}   Fix bugs   Fix bugs   Fix tests   Update CHANGELOG. Add deprecation warning. Fix tests   Unused imports   Optional trainer   More deprecation. More refactoring   Correct version   Use properties   Address comments   flake8   Missed renamings   Typo   is -> ==   It is recommended to use  for Enums since they are singletons, however, since the LightningEnum subclasses str, it's not a good idea in case a user sets the state/stage with a str   Also for tests   Typo   Address @tchaton's comments   PEP8   Correct property   Update CHANGELOG   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update pytorch_lightning/trainer/trainer.py  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Remove called sanity check  Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com",1
give a more complete GAN example (#6294),0.48211357,We have fixed GAN training - supporting multiple optimizers.,,0
fix importing torchtext batch (#6365),0.66643703,import torch,  copy torchtext batch   update   rev   rev ,0
[bug] Fix Pytorch profiler with emit_nvtx (#6260),0.76360583,"Resolved schedule step bug for PyTorch Profiler (#6674, #6681)",  resolve bug   update changelog   Update tests/trainer/test_trainer.py   Update pytorch_lightning/profiler/profilers.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   resolve comments   resolve flake8   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
CI: fix examples - patch download MNIST (#6357),0.46724153,Dropped torchvision dependency in tests and added own MNIST dataset class instead (#986),  patch download   CI   isort   extra ,0
Update changelog for v1.2.2 (#6325),0.6375513,Full Changelog,  update changelog for v1.2.2   ckpr 1.2.2   Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz,0
[bugfix] Resolve memory leak for evaluation (#6326),0.8949365,Resolve memory leak for evaluation (#6326),  resolve bug   resolve flake8   revert name ,1
introduce default cluster environment for lightning-specific ddp (#5915),0.709126,"- The selection `Fabric(strategy=""ddp_spawn"", ...)` no longer falls back to ""ddp"" when a cluster environment gets detected ([#16780](https://github.com/Lightning-AI/lightning/pull/16780))","  handle distributed_sampler_kwargs   move emptying cache to accelertor   fix a few tests   restoring the result from subprocess   fix queue.get() order for results   add missing ""block_backward_sync"" context manager   add missing ""block_backward_sync"" context manager   fix sync_batchnorm   fix supported gpu-ids for tuple   fix clip gradients and inf recursion   accelerator selection: added cluster_environment plugin   fix torchelastic test   fix reduce early stopping decision for DDP   fix tests: callbacks, conversion to lightning optimizer   fix lightning optimizer does not pickle   fix setting benchmark and deterministic option   fix slurm amp test   fix prepare_data test and determine node_rank   fix retrieving last path when testing   remove obsolete plugin argument   fix test: test_trainer_config   fix torchscript tests   fix trainer.model access   move properties   fix test_transfer_batch_hook   fix auto_select_gpus   fix omegaconf test   fix test that needs to simulate slurm ddp   add horovod plugin   fix test with named arguments   clean up whitespace   fix datamodules test   remove old accelerators   fix naming   move old plugins   move to plugins   create precision subpackage   create training_type subpackage   fix all new import errors   fix wrong arguments order passed to test   fix LR finder   Added sharded training type and amp plugin   Move clip grad to precision plugin   Added sharded spawn, select accelerators based on distributed_backend + enable custom fp16 plugin automatically   Fix import issue, attempting to fix tests   Fix initial test   Reflect hook logic from master, should wrap model after move to device   Optional state consolidation, since master has optimizers not wrapped   change attribute for instance test   reset optimizers   optimizers are not used in main process, so state would be wrong.   legacy   imports in accel   legacy2   trainer imports   fix import errors after rebase   move hook to new setup location   provide unwrapping logic   fix trainer callback system   added ddp2 implementation   fix imports .legacy   move plugins   restore legacy   drop test.py from root   add tpu accelerator and plugins   fixes   fix lightning optimizer merge   reset bugreportmodel   unwrapping   step routing forward   model access   unwrap   opt   integrate distrib_type   sync changes   sync   fixes   add forgotten generators   add missing logic   update   import   missed imports   import fixes   isort   mv f   changelog   format   move helper to parallel plugin   d   add world size   clean up   duplicate   activate ddp_sharded and tpu   set nvidia flags   remove unused colab var   use_tpu <-> on_tpu attrs   make some ddp_cpu and clusterplugin tests pass   Ref/accelerator connector (#5742)   final cleanup   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  connector cleanup  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  trainer cleanup  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  accelerator cleanup + missing logic in accelerator connector  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  add missing changes to callbacks  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  reflect accelerator changes to lightning module  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  clean cluster envs  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  cleanup plugins  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  add broadcasting  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   yapf   remove plugin connector   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   plugins   manual optimization   update optimizer routing   add rank to torchelastic   fix memory mixed precision   setstate on trainer for pickling in ddp spawn   add predict method   add back commented accelerator code   adapt test for sync_batch_norm to new plugin   fix deprecated tests   fix ddp cpu choice when no num_processes are given   yapf format   skip a memory test that cannot pass anymore   fix pickle error in spawn plugin   x   avoid   x   fix cyclic import in docs build   add support for sharded   update typing   add sharded and sharded_spawn to distributed types   make unwrap model default   refactor LightningShardedDataParallel similar to LightningDistributedDataParallel   update sharded spawn to reflect changes   update sharded to reflect changes   Merge 1.1.5 changes   fix merge   fix merge   yapf isort   fix merge   yapf isort   fix indentation in test   copy over reinit scheduler implementation from dev1.2   fix apex tracking calls with dev_debugger   reduce diff to dev1.2, clean up   fix trainer config test  when gpus>0 and num_processes >0 and ddp_cpu   sort plugin tests legacy/new   fix error handling for amp on cpu   fix merge   fix merge fix merge   [Feat] Resolve manual_backward (#5837)   resolve manual_backward   resolve flake8   update   resolve for ddp_spawn   resolve flake8   resolve flake8   resolve flake8   Co-authored-by: Ubuntu ubuntu@ip-172-31-88-60.ec2.internal   fix tests/accelerator tests on cpu   [BugFix] Resolve manual optimization (#5852)   resolve manual_optimization   update   update   Co-authored-by: Ubuntu ubuntu@ip-172-31-88-60.ec2.internal   Remove copy trainer parameters to happen earlier within the loop and add safe guard to get ref model (#5856)   resovle a bug   Accelerator refactor sharded rpc (#5854)   rpc branch   merge   update handling of rpc   make devices etc. Optional in RPC   set devices etc. later if necessary   remove devices from sequential   make devices optional in rpc   fix import   uncomment everything   fix cluster selection   Co-authored-by: Ubuntu ubuntu@ip-172-31-88-60.ec2.internal   resolve bug   fix assert in rpc test   resolve a test   fix docs compilation   accelerator refactor - fix for sharded parity test (#5866)   fix memory issue with ddp_spawn   x   x x x x x x x x   x   Remove DDP2 as this does not apply   Add missing pre optimizer hook to ensure lambda closure is called   fix apex docstring   [accelerator][BugFix] Resolve some test for 1 gpu (#5863)   update   revert init   resolve a bug   update   resolve flake8   update   update   update   revert init   resolve a bug   update   resolve flake8   update   update   update   update   update   revert init   resolve a bug   update   resolve flake8   update   update   update   revert init   update   resolve flake8   update   update   update   update   update   all_gather   update   make plugins work, add misconfig for RPC   update   update   remove breaking test   resolve some tests   resolve flake8   revert to ddp_spawn   Co-authored-by: root root@ip-172-31-88-60.ec2.internal Co-authored-by: Ubuntu ubuntu@ip-172-31-88-60.ec2.internal Co-authored-by: Justus Schock justus.schock@rwth-aachen.de   yapf isort   resolve flake8   fix apex doctests   fix apex doctests 2   resolve docs   update drone   clean env   update   update   update   update   merge   Fix RPC related tests, clean out old API, update for new accelerator API [skip ci] (#5881)   Fix RPC related tests, clean out old API, update for new accelerator API   Move tests out of legacy folder, update paths and names   Update test_remove_1-4.py   Expose properties for tpu cores/gpus/num_gpus   Add root GPU property   Move properties to properties.py   move tests that were previously in drone   Fix root GPU property (#5908)   Move root GPU to property, remove horovod set as this is handled in horovod plugin, ensure we mock correctly to set GPU accelerator   Add missing tests back   fix best model path transfer when no checkpoint callback available   Fix setup hook order [wip] (#5858)   Call trainer setup hook before accelerator setup   Add test case   add new test   typo   fix callback order in test   Co-authored-by: tchaton thomas@grid.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   rename ddp sequential -> rpc sequential for special test   revert   fix stupid merge problem   abstract the cluster plugins   default plugin   integrate default environment   fix property   adapt tests   adjust test   fix world size access   base cluster env   revert rebase errors   revert rebase errors   missing import   revert unrelated change   remove unused cluster local rank   remove unrelated changes   fix unrelated changes   fix pep8   remove unused var   reset permissions   ypaf   test default environment   test torchelastic environment   world  size as int   tests for slurm environment   changelog   test comments   remove unintended change   keep master port fixed after it is generated   test random master port   yapf   add missing default environment   move helper function   rename default environment   rename   rename   yapf   Update pytorch_lightning/plugins/environments/lightning_environment.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update CHANGELOG.md  Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com  spawn -> create  Co-authored-by: justusschock justus.schock@posteo.de Co-authored-by: SeanNaren sean@grid.ai Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz Co-authored-by: Justus Schock justus.schock@rwth-aachen.de Co-authored-by: chaton thomas@grid.ai Co-authored-by: Ubuntu ubuntu@ip-172-31-88-60.ec2.internal Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: root root@ip-172-31-88-60.ec2.internal Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com",1
[bugfix] Perform reduction for dict in training_step and DP (#6324),0.6400562,# 3. Remove the `optimizer_idx` argument from `training_step`,  fix   update   update   add changelog   Update CHANGELOG.md   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update tests/accelerators/test_dp.py  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  update changelog  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
docstring changes in accelerators (#6327),0.61242664,Refactored Accelerators and Plugins (#5743),  docstring changes in accelerators   docstrings moved   whitespaces removed   PEP8 correction[1] ,0
[bugfix] Check LightningOptimizer doesn't delete optimizer hooks (#6305),0.9368232,Check LightningOptimizer doesn't delete optimizer hooks (#6305),  update   resolve bug ,1
[Fix] Call clip gradients if clip val greater than 0 (#6330),0.89504766,Ensure that clip gradients is only called if the value is greater than 0 (#6330),  Call clip gradients if clip val greater than 0   format   Format   Move to top of file ,1
Document exception for metrics/classification (#6190),0.55866545,Sklearn metrics classes (#1327),  document exception for metrics/classification   minor formatting fixes   fix trailing whitespaces   document exception for metrics   Apply suggestions from code review   Co-authored-by: Nicki Skafte skaftenicki@gmail.com  Apply suggestions from code review  Co-authored-by: Nicki Skafte skaftenicki@gmail.com  Apply suggestions from code review  Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com,0
missing tests default_root_dir=tmpdir (#6314),0.4547487,"trainer.test(model, ckpt_path=""my_path"") # load path",  default_root_dir=tmpdir   miss ,0
Update docs on arg train_dataloader in fit (#6076),0.7533549,"Standardized the dataloaders arguments of trainer.{fit,valdiate,test,tune} (#7431)",  add to docs   update docs   Apply suggestions from code review   Update pytorch_lightning/core/hooks.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   nested loaders   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   shorten text length   Update pytorch_lightning/core/hooks.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Fix when _stable_1d_sort to work when n >= N (#6177),0.405061,Disabled val and test shuffling (#1600),  Fix when _stable_1d_sort to work when n >= N   Apply suggestions   Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
Add ignore param to save_hyperparameters (#6056),1.0,Add ignore param to save_hyperparameters (#6056),  add ignore param to save_hyperparameters   add docstring for ignore   add type for frame object   Update pytorch_lightning/core/lightning.py   Co-authored-by: Nicki Skafte skaftenicki@gmail.com  Update pytorch_lightning/core/lightning.py  Co-authored-by: Nicki Skafte skaftenicki@gmail.com   fix whitespace   Update pytorch_lightning/core/lightning.py   Co-authored-by: Nicki Skafte skaftenicki@gmail.com   Parametrize tests   Update pytorch_lightning/core/lightning.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update pytorch_lightning/core/lightning.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   seq   fix docs   Update lightning.py   Update lightning.py   fix docs errors   add example keyword   update docstring   Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
Add tests/utilities/test_parsing.py (#4460),0.5709539,parser = Trainer.add_argparse_args(parser),"  Create branch tests/4400_parsing   Rename test file for parsing.py   Fix lightning_hasattr   Fix lightning_hasattr   Fix lightning_setattr   Add empty lines and remove rubbish spaces   Raise AttributeError not ValueError   Use getattr in hasattr   Remove rubbish spaces   Fix getattr   Fix by flake8   Add tests for str_to_bool_or_str   Fix by flake8   Add tests for str_to_bool   Add tests for is_picklable   Add tests for clean_namespace   Fix typo   Fix lightning_getattr   Add tests for AttributeDict   Add tests for flatten_dict   Fix by flake8   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   Apply isort   Revert ""Apply suggestions from code review""   Define unpicklable_function outside   Add comment to test_clean_namespace   Add tests for parse_class_init_keys   Add tests for get_init_args and collect_init_args   Share objects across the tests   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk",0
leaving lezwon (#6347),0.43909064,"If we forgot somebody or you have a suggestion, find us on Discord :zap:",,0
[fix] Use training type plugin hook when saving (FSDP 1/n) (#6321),0.69622767,Removed TrainingTypePlugin.on_save and Accelerator.on_save (#9023),  Rely on training type plugin when saving   Add better typing to training type plugin ,0
hotfix for PT1.6 and torchtext (#6323),0.72354275,- Removed deprecated support for old torchtext versions ([#14375](https://github.com/Lightning-AI/lightning/pull/14375)),  ci: azure reinstall torchtext   move   todos   0.6.0   skip examples   formatter   skip   todo   Apply suggestions from code review ,1
drop unused variable in API (#6308),0.48396432,            find_unused_parameters=True,  drop unused pl model in ckpt   irelevant   on_evaluation_batch_start   evaluation_epoch_end   attach_datamodule ,0
[bugfix] TPU + all_gather + SingleTPU shouldn't call xm.all_gather (#6296),0.58914256,  * Removed the `Trainer(tpu_cores=...)` argument,  resolve an issue with TPU   update   add changelog ,0
Fix ModelPruning(make_pruning_permanent=True) buffers getting removed when saved during training (#6073),0.5796582,Disable saving checkpoints if not trained (#4372),Co-authored-by: chaton thomas@grid.ai,0
Simplify test for AMP plugins (#6311),0.5383859,        if use_amp:,  AMP   fuse   yapf ,0
prune duplicite test in optim (#6312),0.6283276,Pruned requirements duplicity (#13739),,0
[bugfix] TPU test hangs to barrier on 1 process (#6272),0.55115277,This means we fixed many TPU bugs we hadn’t caught before because we had no tests.,  update   resolve flake8   update   update   update changelog   update   resolve flake8   Co-authored-by: Your Name you@example.com,0
Add fairscale & deepspeed to skipif 4/n (#6281),0.47633296,Update the logic to check for accumulation steps with deepspeed (#9826),  add fairscale & windows to skipif   add deepspeed to runif   fairscale   deepspeed   flake8   Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz,0
Refactor: runif for spec 6/6 (#6307),0.4650466,Refactored trainer _run_* functions and separate evaluation loops (#8065),  special   rpc ,0
Refactor: Runif for TPU and Horovod 5/n (#6301),0.5109819,Refactored trainer _run_* functions and separate evaluation loops (#8065),  TPU   horovod   extra   fix   Apply suggestions from code review   Co-authored-by: Nicki Skafte skaftenicki@gmail.com  doc  Co-authored-by: Nicki Skafte skaftenicki@gmail.com,0
try to fix imports for parsing (#6256),0.47209048,Replace mata_tags.csv with hparams.yaml (#1271),  try to fix imports   legacy 1.2.1 ,0
Add possibility for custom naming when using multiple dataloaders (#6274),0.7649535,"Working with multiple dataloaders (#16800, #16753)",,1
unfreeze torchtext version (#6302),0.5495864,- Removed deprecated support for old torchtext versions ([#14375](https://github.com/Lightning-AI/lightning/pull/14375)),,0
[fix] Ensure we check deepspeed/sharded in multinode DDP (#6297),0.8827475,Ensure we check deepspeed/sharded in multinode DDP (#6297),"  Ensure we check deepspeed/sharded in multinode   Add CHANGELOG.md   Add CHANGELOG.md   Drop mock, use actual multi-gpu node ",1
Refactor: skipif for AMPs 3/n (#6293),0.5320112,        if use_amp:,  args   native   apex   isort ,0
fix duplicate console logging bug v2 (#6275),0.56322455,Un-balanced logging properly supported (#5119),Co-authored-by: chaton thomas@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Refactor: skipif for Windows 2/n (#6268),0.4108393,Improved exception message if rich version is less than 10.2.2 (#10839),  win   isort   flake8 ,0
Improved EarlyStopping.patience documentation (#6278),0.59154266,Improved verbose logging for EarlyStopping callback (#6811),  Improved early stopping documentation   Changed to 120 column format   doc   doc   doc   Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz,0
Refactor: skipif for multi - gpus 1/n (#6266),0.57171226,"Deprecated the use of Trainer(gpus=""i"") (string) for selecting the i-th GPU; from v1.5 this will set the number of GPUs instead of the index (#6388)",  ngpus   gpu   isort   pt   flake8 ,0
split profilers (#6261),0.88438237,Split profilers module (#6261),,1
Disable CPU Offload as default for DeepSpeed (#6262),0.8166789,"Changed default for DeepSpeed CPU Offload to False, due to prohibitively slow speeds at smaller scale (#6262)",  Change default for CPU offload to false for best throughput/memory efficiency   Add changelog   default   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
docstring changes in tuner (#6264),0.555733,The tuner now usees a unique filename to save a temporary checkpoint (#9682),  docstring changes in tuner   added full stop ,0
switch agents pool (#6270),0.42918307,"If we forgot somebody or you have a suggestion, find us on Discord :zap:",,0
Remove opt from manual_backward in docs (#6267),0.5586603,Marked OptimizerLoop.backward as protected (#9514),,0
cleaning SWA (#6259),0.69238496,"Cleaning (#5948, #5949, #5950)",  rename   if   test   chlog ,0
add skipif warpper (#6258),0.39835355,"Updated DeepSpeed ZeRO (#6546, #6752, #6142, #6321)",,0
"Fix for incorrect usage of detach(), cpu(), to() (#6216)",0.4944422,Resolve memory leak for evaluation (#6326),"  Fix for incorrect detach/cpu calls (#6214)   Fix incorrect use of detach(), to(), and cpu(), #6214   Fix incorrect use of detach() and cpu(), #6214   update pr   add typing   chlog   more...   revert on module   update on comments   revert changes on model   Co-authored-by: tchaton thomas@grid.ai Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz",0
Call optimizer.zero_grad() before backward inside closure in AutoOpt (#6147),0.6748295,"The on_before_optimizer_step hook previously ran before the entire optimization closure, including backward. This was unintended behavior and if you rely on this, move your code to the new on_before_backward` hook.",Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
Document Exceptions in profilers (#6229),0.5649371,Moved profilers to their own file (#7822),  docstring changes in profilers   minor changes in profilers.py ,0
update (#6237),0.48631346,"Updated DeepSpeed ZeRO (#6546, #6752, #6142, #6321)",,0
simplify skip-if tests >> 0/n (#5920),0.43543953,Simplify optimization Logic (#4984),  skipif + yapf + isort   tests   docs   pp ,0
document exceptions for metrics/regression (#6202),0.5406091,Removed deprecated metrics (#8586),Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Prajakta Phadke pphadke@iu.edu Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
Prune deprecated hparams setter (#6207),0.6202986,Deprecated the old way of assigning hyper-parameters through self.hparams = ... (#4813),,0
fix(wandb): prevent WandbLogger from dropping values (#5931),0.9053679,Prevent WandbLogger from dropping values (#5931),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: chaton thomas@grid.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
apply_func.py: from torchtext.legacy.data import Batch (#6211),0.596774,"  from torch.utils.data import DataLoader, Dataset"," Update apply_func.py  The name Batch is no longer located under torchtext.data --Error message-- File ""/home/daniel/py38/lib/python3.8/site-packages/pytorch_lightning/utilities/apply_func.py"", line 25, in       from torchtext.data import Batch                                                 ImportError: cannot import name 'Batch' from 'torchtext.data' (/home/daniel/py38/lib/p ython3.8/site-packages/torchtext/data/init.py) You can fix this by changing line line 28 to:     from torchtext.legacy.data import Batch   Update apply_func.py   Update apply_func.py   Update apply_func.py   Update apply_func.py   Update apply_func.py ",0
Add mypy typing to precision plugins. (#6149),0.65519786,Precision Plugins (#5718),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz Co-authored-by: Akihiro Nitta nitta@akihironitta.com,0
fix parallel devices return type & add copyright (#6215),0.60899085,Made parallel devices optional across all plugins (#6051),,0
Prune deprecated Trainer(checkpoint_callback=ModelCheckpoint()) (#6166),0.82089853,Deprecated passing ModelCheckpoint instance to checkpoint_callback Trainer argument (#4336),,1
Document exceptions in loggers (#6171),0.62567157,- all the file path errors with loggers (txs @awaelchli),  Document exceptions in loggers   minor formatting   docstring changed in comet.py   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
Add checkpoint parameter to on_save_checkpoint (#6072),0.8470151,"-def on_save_checkpoint(self, checkpoint):",Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,1
Fix for multiple callbacks (#6197),0.6992982,  callbacks:,"  Fix for multiple callbacks   Add CHANGELOG.md   Remove old params   Skip tests on windows using ddp   Change name of the variable to not clash with should stop, which is separate   Apply suggestions from code review   Fix params   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",0
type accelerators (#6148),0.69416285,reduced accelerator selection (#3211),,0
Update gpu warning (#6181),0.56848866,"Deprecated log_gpu_memory, gpu_metrics, and util funcs in favor of DeviceStatsMonitor callback (#9921)",Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Kaushik Bokka kaushikbokka@gmail.com,0
Update with GitHub Discussions (#6186),0.47714785,@akihironitta @awaelchli @Borda @carmocca @kaushikb11 @krshrimali @mauvilsa @otaj @pre-commit-ci @rohitgr7 @semaphore-egg @tkonopka @wayi1,,0
Update issue template to use discussions for questions (#6155),0.31168258,Updated LightningTemplateModel to look more like Colab example (#1577),  add issue config   remove question template   update URL   Update README.md   Update README.md   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update .github/ISSUE_TEMPLATE/config.yml  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
Fix typo (#6178),0.5228075,"In addition, we fixed:",,0
Prune deprecated EarlyStopping(mode='auto') (#6167),0.5506668,EarlyStopping now runs at the end of the training epoch by default (#8286),Co-authored-by: Roger Shieh sh.rog@protonmail.ch Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
Prune deprecated checkpoint arguments (#6162),0.7138573,Removed deprecated checkpoint argument filepath (#5321),  prune prefix   prune mode=auto   chlog ,1
[Bugfix] Fixed epoch level schedulers not being called when val_check_interval < 1.0 (#6075),0.6390116,lr_scheduler now activated after epoch    ,  fix bug   fix tests   changelog   fix pep8   fix tests   fix and add some tests   add test for rlop   chlog   Update CHANGELOG.md   Co-authored-by: rohitgr7 rohitgr1998@gmail.com,0
Prune deprecated metrics for 1.3 (#6161),0.75127995,Removed deprecated metrics (#6161),  prune deprecated metrics for 1.3   isort / yapf ,1
prune deprecated Trainer arg enable_pl_optimizer (#6163),0.87062454,Removed deprecated Trainer argument enable_pl_optimizer and automatic_optimization (#6163),  prune enable_pl_optimizer   prune automatic_optimization ,1
prune deprecated profiler as bool (#6164),0.6691707,Deprecated bool values in Trainer's profiler parameter (#3656),  prune profiler   chlog ,0
Update CHANGELOG (#6156),0.6337519,Full Changelog,,0
fixing miss-leading tested acc values (#5876),0.44672018,Removed reorder parameter of the auc metric (#5004),"  fixing tested values   .   tests   yapf   softmax   hvd   rename   lr   duplicate   drop   classif   rm EvalModel   Revert ""rm EvalModel""   This reverts commit 6c3fb39ebe0c4bfb52357bccfd050438f2c0f31c.   update tests   fix   azure   azure   self   cpu   Apply suggestions from code review   Co-authored-by: rohitgr7 rohitgr1998@gmail.com",0
Ensure accelerator is valid if running interactively (#5970),1.0000002,Ensure accelerator is valid if running interactively (#5970),Co-authored-by: chaton thomas@grid.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
Add specifics around DeepSpeed docs (#6142),0.68289185,Support for manual optimization with DeepSpeed (#7970),  Be more specific with DeepSpeed compatibility   Better wording ,0
mini refactor for _running_stage access (#5724),0.40556705,"DataModules now avoid duplicate {setup,teardown,prepare_data} calls for the same stage (#7238)","  running stage   circular import   running stage cleanup   fix unused import   fix running stage access   add return type   Revert ""add return type""   This reverts commit 65b0fe269c6547213e34b6a88b97bee31cdfe8c7.  try fix typing",0
Feature/5275 clean progress bar print (#5470),0.6595161,Better progress bar (#16695),  Trainer.test should return only test metrics (#5214)   resolve bug   merge tests   Fix metric state reset (#5273)   Fix metric state reset   Fix test   Improve formatting   Co-authored-by: Ananya Harsh Jha ananya@pytorchlightning.ai   print() method added to ProgressBar   printing alongside progress bar added to LightningModule.print()   LightningModule.print() method documentation updated   ProgressBarBase.print() stub added   stub   add progress bar tests   fix isort   Progress Callback fixes   test_metric.py duplicate DummyList removed   PEP and isort fixes   CHANGELOG updated   test_progress_bar_print win linesep fix   test_progress_bar.py remove whitespaces   Update CHANGELOG.md   Co-authored-by: chaton thomas@grid.ai Co-authored-by: Tadej Svetina tadej.svetina@gmail.com Co-authored-by: Ananya Harsh Jha ananya@pytorchlightning.ai Co-authored-by: Alexander Snorkin Alexander.Snorkin@acronis.com Co-authored-by: rohitgr7 rohitgr1998@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Avoid printing ModelCheckpoint log with monitor=None and verbose=True (#6109),0.8593235,Do not print top-k verbose log with ModelCheckpoint(monitor=None) (#6109),,1
Minor fixes/improvements in Metric docs (#6114),0.75897384,"docs for all Metrics (#2184, #2209)",  Fix wrong render   Improve classification metrics docs   Improve other domain metrics docs   Change the structure level in the docs ,1
Update Contributing Guide (#6118),0.5562086,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",  Update Contributing Guide   update docs ,0
fix amp/apex misconfiguration error for cpu (#6107),0.5886837,enable_pl_optimizer=False by default to temporarily fix AMP issues (#5163),  fix weird test   fix apex plugin test   fix raise   cpu test   fix type   add changelog ,0
Collapse 2 DeepSpeed tests (#6108),0.52806604,"Updated DeepSpeed ZeRO (#6546, #6752, #6142, #6321)",,0
Expose DeepSpeed FP16 parameters due to loss instability (#6115),0.798287,Expose DeepSpeed loss parameters to allow users to fix loss instability (#6115),"  Expose deepspeed config parameters to init function due to instability in parameters   See if tests can run on normal CI, without special tests   Add changelog   Update pytorch_lightning/plugins/training_type/deepspeed.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com",1
"Enable ZeRO tests for CI, fix to/half function calls (#6070)",0.5537468,Removed no return warning from val/test step (#6139),"  Enable ZeRO optimization, and make sure that the lightning module hook is called when we move to half precision   Added test, update to function ",0
"[Hot Fix] Give priority to plugins to set distributed mode, and then accelerator (#6089)",0.5785291,The argument distributed_backend has been removed from the Trainer in favor of the new accelerator and strategy arguments (#10017).,"  Give priority to plugins to set distributed mode, and then accelerator   Add CHANGELOG.md   Update CHANGELOG.md   Remove very scary line   Ensure we set cluster environment after slurm configured if necessary   Simplify the fix with a reset   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com",0
consistent behavior for reduce method across all Plugins (#6011),0.78840053,Made the Plugin.reduce method more consistent across all Plugins to reflect a mean-reduction by default (#6011),  reduction docs   docs for abstract base method   make mean the default   add preliminary chlog   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
add sanity check on nb available GPUs (#6092),0.5700555,nvidia/apex deprecation (#16039),,0
Fix amp autocast  (#6080),0.5589838,enable_pl_optimizer=False by default to temporarily fix AMP issues (#5163),  precision fixes   add amp test model   fix test   revert   move assert to training step   fix test   fix test   remove unrelated changes   add changelog   remove unused import ,0
continue towards 1.3 (#6069),0.53375363,[1.3.0] - 2021-05-06,,0
pypi azure badges - tags (#6068),0.35215327,Updated mlflow with using resolve_tags (#6746),  pypi azure badges - tags   pep8   id ,0
add Azure tags trigger (#6066),0.37660107,Updated mlflow with using resolve_tags (#6746),  add Azure tags trigger   fix   mnodes ,0
v1.2.0 (#6065),0.66593593,[1.6.2] - 2022-04-27,  v1.2.0   docs ,0
default sched (#6062),0.45717043,Enforce an epoch scheduler interval when using SWA (#6588),,0
Raise AttributeError in lightning_getattr and lightning_setattr when attribute not found (#6024),0.46214303,- The model wrapper returned by `LightningLite.setup()` now properly supports pass-through when looking up attributes ([#12597](https://github.com/Lightning-AI/lightning/pull/12597)),  Empty commit   Raise AttributeError instead of ValueError   Make functions private   Update tests   Add match string   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  lightning to Lightning  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Update auto-opt docs (#6037),0.46793625,Porting fixes to autoscaler component (#16249),  fix docs   update on comments   Apply suggestions from code review   Co-authored-by: Nicki Skafte skaftenicki@gmail.com  Apply suggestions from code review  Co-authored-by: Nicki Skafte skaftenicki@gmail.com  Apply suggestions from code review  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   rm comment   Update docs/source/common/lightning_module.rst   Co-authored-by: chaton thomas@grid.ai Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: chaton thomas@grid.ai,0
v1.2.0rc2 (#6063),0.63332045,[1.6.2] - 2022-04-27,  v1.2.0rc2   chlogs   chlogs   format   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
Add warnings to on_before/after_batch_transfer hooks (#6059),0.6012689,"Overriding the on_train_batch_{start,end} hooks in conjunction with taking a dataloader_iter in the training_step no longer errors out and instead shows a warning (#16062)","  Add warnings to hooks   Add default idx to prevent signature change in the future   Nothing to see here   Add default val to transfer_batch_to_device hook   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Revert ""Add default val to transfer_batch_to_device hook""  This reverts commit 5c6a68f2 Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",0
fix docs links (#6057),0.5447042,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",,0
fix flake8 for new plugins (#5951),0.4632445,Updated error message for interactive incompatible plugins (#9896),  flake8   fix cyclic import   isort ,0
rename accelerator_backend -> accelerator (#6034),0.80276537,Renamed all backends to Accelerator (#4066),  rename accelerator backend   rename new additions from master   add proper deprecation   pep8   warning match   add missing warning type ,1
Replace .get_model() with explicit .lightning_module (#6035),0.8198105,Deprecated .get_model() with explicit .lightning_module property (#6035),  rename get_model -> lightning_module   update references to get_model   pep8   add proper deprecation   remove outdated _get_reference_model   fix cyclic import ,1
"Docs for Pruning, Quantization, and SWA (#6041)",0.46241847,Support shorthand notation to instantiate optimizers and learning rate schedulers (#9565),Co-authored-by: chaton thomas@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Sean Naren sean.narenthiran@gmail.com,0
Fix docs typo (#6055),0.49729538,"In addition, we fixed:",Put .test() in  code blocks,0
clarify gpu / process (#6049),0.695246,Parsing of GPU Argument,,0
Make parallel devices optional across all plugins (#6051),0.94063675,Made parallel devices optional across all plugins (#6051),  Make parallel devices optional across all plugins so that they can be instantiated   Add any to types to capture vars passed in ,1
Add before_batch_transfer and after_batch_transfer hooks (#3671),0.5881336,Raised a MisconfigurationException if batch transfer hooks are overriden with IPUAccelerator (13961),  add hooks   comment   docs   add tests   make it private   fix tests   docs   chlog   testcode   codefactor   fix doctest   fix doctest   suggestions   is always overriden   pep and BoringModel   BoringModel   docs   docs   docs   fix   rebase   rebase   suggestions   docs   suggestions   try fix docs   docs   update name   yapf   docs   rebase   yapf ,0
Add descriptions to accelerator broadcast function/clean up all_gather (#6044),0.5998843,"To simplify this process, we've deprecated the per-accelerator properties to have accelerator agnostic properties. For example:",  Add descriptions to accelerator broadcast function/clean up all_gather   Remove todo ,0
fix/test quant (#6040),0.52066886,- fixed all the .test() calls,  fix/test quant   ...    ,0
[ModelPruning] Add missing attribute with use_global_unstructured=False and verbose (#6045),0.5479133,Changed automatic_optimization to be a model attribute (#4602),,0
et al. (#6050),0.40895143,The main focus of this release was on adding flexibility and generalization to support broad research cases.,  et al.   Apply suggestions from code review   Apply suggestions from code review   Co-authored-by: chaton thomas@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: chaton thomas@grid.ai,0
Fix: Allow hashing of metrics with lists in their state (#5939),0.6342367,Changed metrics_to_scalars to work with any collection or value (#7888),  Fix: Allow hashing of metrics with lists in their state   Add test case and modify semantics of Metric hash in order to be compatible with structural equality checks   Fix pep8 style issue   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,0
Delete tests.helpers.TrialMNISTDataModule (#5999),0.4999286,Refactored setup_training and remove test_mode (#5388),  Remove TrialMNISTDataModule   Allow using TrialMNIST in the MNISTDataModule   Update tests/helpers/datasets.py ,0
Add option for weight tying on TPU's (#5441),0.60746896,Support to tie weights after moving model to TPU via on_post_move_to_device hook,  added on_post_move_to_device   added tests   docs and refactors   Update tests/backends/test_tpu_backend.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update docs/source/tpu.rst  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update docs/source/tpu.rst  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/core/decorators.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/core/decorators.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update docs/source/tpu.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update pytorch_lightning/core/decorators.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update pytorch_lightning/core/decorators.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update pytorch_lightning/core/decorators.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update pytorch_lightning/core/decorators.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update pytorch_lightning/core/hooks.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  moved weight sharing module back to test  updated tpu available   add count to warning   fix doctest   import trainer in doctest   import trainer in doctest   do not test code as no TPU device   param count to layer count   formatting   update docs   update import   update   resolve tests   remove legacy accelerator   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: tchaton thomas@grid.ai Co-authored-by: Your Name you@example.com,0
drop deprecated result object 1/n (#5005),0.6719993,- Results objects are deprecated (we hated them too haha),  ro1   ro2 ,0
"[CI] Move DeepSpeed into CUDA image, remove DeepSpeed install from azure (#6043)",0.5157448,Removed DeepSpeed FP16 Exception as FP32 is now supported (#8462),"  Move to CUDA image   Remove deepspeed install as deepspeed now in the cuda image   Remove path setting, as ninja should be in the container now ",0
[feat] Add Trainer(stochastic_weight_avg=True/False) (#6038),0.76221675,Deprecated passing stochastic_weight_avg to the Trainer constructor in favor of adding the StochasticWeightAveraging callback directly to the list of callbacks (#8989),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Address code review for deepspeed (#6042),0.60718733,"Updated DeepSpeed ZeRO (#6546, #6752, #6142, #6321)",,0
Trainer only references accelerator (#6039),1.0000001,Trainer only references accelerator (#6039),"  Trainer only references accelerator where it can   Move teardown to the trainer, as it is reponsible for the accelerator ",1
DeepSpeed Integration (#5954),0.5107267,DeepSpeed Stage 1,"  Add initial deepspeed changes   Address code review   Move static method outside of function   Fixes   Add missing annotation   Remove seed setting   Doc changes   Doc changes, add address reviews   Fix docs   Try fixing issue by moving to torch adam   Clean up check   Changes, better APIs!   Add wrapper, swap to git install revision   Add special test   Add warning   Address review   Add better disclaimer   Turn off ZeRO for testing due to compilation   Add description on modifying parameters via the plugin   Doc strings clear   Small doc fixes   Fix hash, reduce test   Added CI change   Move to azure pipeline   Fix test name   Add missing flag   Remove sudo...   Try conda instead   Swap to conda base   Try suggested install   Apply suggestions from code review   Apply suggestions from code review   Revert ""Apply suggestions from code review""   This reverts commit 41cca05a  Revert ""Apply suggestions from code review""  This reverts commit e06ec29e   Remove setter   Address most review   Move out function, remove DeepSpeed from requirements   Install deepspeed/mpi4py within container   Use special tests, move to master commit for deepspeed   Export path   Force compile to happen first   Remove!   Debugging ninja   Fix error in optimizer step logic   Attempt to fix symbolic link   Reverse to aid debugging   Export path again   Clean up mess   var   Revert ""var""   This reverts commit 3450eaca   Address review, add todo   Add note about unsupported functionality   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: tchaton thomas@grid.ai Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz",0
remove outdated info (#6032),0.51662076,clean up data reset (#3161),,0
[Bugfix] Apply untoggle_optimizer when result is None (#5983),0.7863835,Changed calling of untoggle_optimizer(opt_idx) out of the closure function (#7563),  update changelog   apply untoggle_optimizer when result is None   update tests   still return loss sometimes   Update CHANGELOG.md   Co-authored-by: deng-cy dcy1996@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
Fix Wrapping optimizers upon assignment (#6006),0.6316376,Executing the optimizer_closure is now required when overriding the optimizer_step hook (#9360),  Update properties.py   pep8 ,0
Prevent flickering progress bar (#6009),0.6516245,Better progress bar (#16695),  add padding   fix   fix   Update pytorch_lightning/callbacks/progress.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   updated based on suggestion   changelog   add test   fix pep8   resolve test   fix code format   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: tchaton thomas@grid.ai,0
Add hint in docs for how to use shared memory (#6036),0.47381285,    # You should be aware of the implications on memory usage,,0
move accelerator_connector.py to the connectors subfolder (#6033),0.9716114,Moved accelerator_connector.py to the connectors subfolder (#6033),  move accelerator connector   rename BackendConnector -> AcceleratorConnector ,1
fix type for log_gpu_memory (#6031),0.6638715,"Deprecated log_gpu_memory, gpu_metrics, and util funcs in favor of DeviceStatsMonitor callback (#9921)",,0
[BugFix] Add on_epoch_end hook after on_test/validation_epoch_end hook (#5986),0.8340671,Changed the behavior of on_epoch_start to run at the beginning of validation & test epoch (#6498),  add hook   update changelog   resolve tests ,1
[HotFix] Resolve TPU Training (#6027),0.70259756,TPU training (#2708),  fix tpus   update   add back reduction in val_loss   resolve some bugs with TPUs   update changelog   update on comments   forgot status   Fix train_bn arg   resolve comments   update on comments   Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
[Feat] Add TORCH_DISTRIBUTED_BACKEND env variable  (#5981),0.79291004,Setting the torch-distributed backend,  add backend support   resolve flake8   update changelog   update   Apply suggestions from code review   Update docs/source/advanced/multi_gpu.rst   add patch as context manager   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Add openmpi to our base cuda container for MPI support (#6026),0.5143488,Moved torch.cuda.set_device() to enable collective calls earlier in setup (#8312),  Add openmpi to our base container for DeepSpeed MPI support   conda   Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz,0
update lr_finder to check for attribute if not running fast_dev_run (#5990),1.0000001,Update lr_finder to check for attribute if not running fast_dev_run (#5990),  ref lr_finder a bit   chlog   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,1
fix install dtrun (#6025),0.45570177,"Fixing missing packaging package in dependencies, which was affecting the only installation to a very blank system.",,0
Re-introduce fix for Hydra directory sync with multiple process (#5993),0.9913718,Re-introduced fix for Hydra directory sync with multiple process (#5993),  Add hydra fix that was missing from master   Remove error commas   Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,1
Add dim to pytorch_lightning.metrics.PSNR (#5957),0.7833397,- Removed the unused `lightning.pytorch.utilities.metrics.metrics_to_scalars` function ([#16681](https://github.com/Lightning-AI/lightning/pull/16681)),  Add dim to PSNR   Update CHANGELOG.md   Update CHANGELOG.md   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   Add reduction tests   Recover warnings on reduction and add tests   Add copyright texts   Refactor PSNR   Change warnings   Update pytorch_lightning/metrics/functional/psnr.py   Change functional.psnr dim doc Co-authored-by: Nicki Skafte skaftenicki@gmail.com   Change PSNR dim docs   Apply suggestions from code review   tests   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz,1
Add deprecation warning to ModelCheckpoint when logging val_loss with no monitor (#6012),0.7935371,Deprecated using 'val_loss' to set the ModelCheckpoint monitor (#6012),  Add deprecation warning when logging val_loss with no monitor   EOF   Update CHANGELOG   Clear warning cache before testing   pep8   Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz,1
Attempt slurm auto resume call when non-shell call fails (#6002),0.99294746,Attempted SLURM auto resume call when non-shell call fails (#6002),Signed-off-by: smajumdar titu1994@gmail.com,1
Fix typo and code rendering in docs (#5940),0.5352243,Syntax changes are: ,,0
hotfix: move process_dataloader to plugins (#6023),0.63684297,refactored dataloader process hook (#3139),,0
"changed spelling of ""licence"" (#5937)",0.4282974,Changed signatures:,"Normalized spelling to ""license"" as in the URL ""https://github.com/PytorchLightning/pytorch-lightning/blob/master/LICENSE""",0
Remove torch<=1.4.0 checks (#5998),0.6761882,    * Some configuration errors that were previously raised as `MisconfigurationException`s will now be raised as `ProcessRaisedException` (torch>=1.8) or as `Exception` (torch<1.8),  Remove torch<=1.4.0 checks   Update pytorch_lightning/utilities/data.py   Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,0
Add PredictLoop (#5752),0.61657506,"Refactored prediction loop interface; added new classes PredictionLoop, PredictionEpochLoop (#7700, #8077)","  integrate distrib_type   sync changes   sync   fixes   add forgotten generators   add missing logic   update   import   missed imports   import fixes   isort   mv f   changelog   format   move helper to parallel plugin   d   add world size   clean up   duplicate   activate ddp_sharded and tpu   set nvidia flags   remove unused colab var   use_tpu <-> on_tpu attrs   make some ddp_cpu and clusterplugin tests pass   Ref/accelerator connector (#5742)   final cleanup   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  connector cleanup  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  trainer cleanup  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  accelerator cleanup + missing logic in accelerator connector  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  add missing changes to callbacks  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  reflect accelerator changes to lightning module  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  clean cluster envs  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  cleanup plugins  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  add broadcasting  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   yapf   remove plugin connector   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   plugins   add predict_loop   manual optimization   clean predictloop   update optimizer routing   add predict loop on new accelerator   resolve a bug   add rank to torchelastic   add predict_loop   add predict loop on new accelerator   resolve a bug   fix memory mixed precision   update   setstate on trainer for pickling in ddp spawn   add predict_loop   clean predictloop   add predict loop on new accelerator   resolve a bug   add predict_loop   add predict loop on new accelerator   resolve a bug   add predict_loop   add predict loop on new accelerator   resolve a bug   add predict_loop   add predict loop on new accelerator   resolve a bug   add predict_loop   clean predictloop   add predict loop on new accelerator   resolve a bug   add predict_loop   add predict loop on new accelerator   resolve a bug   resolve tests   add predict method   add back commented accelerator code   adapt test for sync_batch_norm to new plugin   fix deprecated tests   fix ddp cpu choice when no num_processes are given   yapf format   skip a memory test that cannot pass anymore   remove sanetize   rename train to run_train   remove useless hooks   add misconfigurationException   remove wrong naming   resolve some legacy   udpate docstring   fix pickle error in spawn plugin   x   avoid   x   fix cyclic import in docs build   add support for sharded   update typing   add sharded and sharded_spawn to distributed types   make unwrap model default   refactor LightningShardedDataParallel similar to LightningDistributedDataParallel   update sharded spawn to reflect changes   update sharded to reflect changes   Merge 1.1.5 changes   fix merge   fix merge   yapf isort   fix merge   yapf isort   fix indentation in test   copy over reinit scheduler implementation from dev1.2   fix apex tracking calls with dev_debugger   reduce diff to dev1.2, clean up   fix trainer config test  when gpus>0 and num_processes >0 and ddp_cpu   sort plugin tests legacy/new   fix error handling for amp on cpu   fix merge   fix merge fix merge   [Feat] Resolve manual_backward (#5837)   resolve manual_backward   resolve flake8   update   resolve for ddp_spawn   resolve flake8   resolve flake8   resolve flake8   Co-authored-by: Ubuntu ubuntu@ip-172-31-88-60.ec2.internal   fix tests/accelerator tests on cpu   [BugFix] Resolve manual optimization (#5852)   resolve manual_optimization   update   update   Co-authored-by: Ubuntu ubuntu@ip-172-31-88-60.ec2.internal   Remove copy trainer parameters to happen earlier within the loop and add safe guard to get ref model (#5856)   resovle a bug   Accelerator refactor sharded rpc (#5854)   rpc branch   merge   update handling of rpc   make devices etc. Optional in RPC   set devices etc. later if necessary   remove devices from sequential   make devices optional in rpc   fix import   uncomment everything   fix cluster selection   Co-authored-by: Ubuntu ubuntu@ip-172-31-88-60.ec2.internal   resolve bug   fix assert in rpc test   resolve a test   fix docs compilation   accelerator refactor - fix for sharded parity test (#5866)   fix memory issue with ddp_spawn   x   x x x x x x x x   x   Remove DDP2 as this does not apply   Add missing pre optimizer hook to ensure lambda closure is called   fix apex docstring   [accelerator][BugFix] Resolve some test for 1 gpu (#5863)   update   revert init   resolve a bug   update   resolve flake8   update   update   update   revert init   resolve a bug   update   resolve flake8   update   update   update   update   update   revert init   resolve a bug   update   resolve flake8   update   update   update   revert init   update   resolve flake8   update   update   update   update   update   all_gather   update   make plugins work, add misconfig for RPC   update   update   remove breaking test   resolve some tests   resolve flake8   revert to ddp_spawn   Co-authored-by: root root@ip-172-31-88-60.ec2.internal Co-authored-by: Ubuntu ubuntu@ip-172-31-88-60.ec2.internal Co-authored-by: Justus Schock justus.schock@rwth-aachen.de   yapf isort   resolve flake8   fix apex doctests   fix apex doctests 2   resolve docs   update drone   clean env   update   update   update   update   merge   Fix RPC related tests, clean out old API, update for new accelerator API [skip ci] (#5881)   Fix RPC related tests, clean out old API, update for new accelerator API   Move tests out of legacy folder, update paths and names   Update test_remove_1-4.py   Expose properties for tpu cores/gpus/num_gpus   Add root GPU property   Move properties to properties.py   move tests that were previously in drone   Fix root GPU property (#5908)   Move root GPU to property, remove horovod set as this is handled in horovod plugin, ensure we mock correctly to set GPU accelerator   Add missing tests back   fix best model path transfer when no checkpoint callback available   Fix setup hook order [wip] (#5858)   Call trainer setup hook before accelerator setup   Add test case   add new test   typo   fix callback order in test   Co-authored-by: tchaton thomas@grid.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   rename ddp sequential -> rpc sequential for special test   revert   fix stupid merge problem   Use property in connector for sampler (#5913)   merge the import conflicts   fix spawning of processes in slurm   [wip] Fix some bugs for TPU [skip ci] (#5878)   fixed for single tpu   fixed spawn   fixed spawn   update   update   wip   resolve bugs   resolve bug   update on comment   removed decorator   resolve comments   set to 4   update   update   need cleaning   update   update   update   resolve flake8   resolve bugs   exclude broadcast   resolve bugs   change test   update   update   skip if meet fails   properly raise trace   update   add catch   wrap test   resolve typo   update   typo   Co-authored-by: Lezwon Castelino lezwon@gmail.com Co-authored-by: Your Name you@example.com   resolve some tests   update   fix imports   update   resolve flake8   update azure pipeline   skip a sharded test on cpu that requires a gpu   resolve tpus   resolve bug   resolve flake8   update   updat utils   revert permission change on files   suggestions from carlos   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   remove unrelated formatting changes   remove incomplete comment   Update pytorch_lightning/accelerators/init.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   remove unrelated formatting change   add types   warn 1.7 ddp manual backward only if ddp kwarg unset   yapf + isort   pep8 unused imports   fix cyclic import in docs   Apply suggestions from code review   typer in accelerator.py   typo   resolve flake8   update code   update   Update pytorch_lightning/trainer/predict_loop.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update pytorch_lightning/trainer/predict_loop.py  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   fix merge   fix merge   reset legacy accelerator   add missing rename dispatch   rename post traning   update code   resolved comments   typo   typo   add flow description   resolve comments   update on comments   update flow   add backticks   resolve tpu   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: justusschock justus.schock@posteo.de Co-authored-by: Justus Schock justus.schock@rwth-aachen.de Co-authored-by: Ubuntu ubuntu@ip-172-31-88-60.ec2.internal Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: SeanNaren sean@grid.ai Co-authored-by: root root@ip-172-31-88-60.ec2.internal Co-authored-by: Lezwon Castelino lezwon@gmail.com Co-authored-by: Your Name you@example.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com",0
[Hot Fix] Ensure process_dataloader is called when tpu_cores > 1 to use Parallel DataLoader (#6015),0.81870764,Ensured process_dataloader is called when tpu_cores > 1 to use Parallel DataLoader (#6015),  hotfix for tpu   update changelog   Update CHANGELOG.md   Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com Co-authored-by: Sean Naren sean.narenthiran@gmail.com,1
Make move_metrics_to_cpu work recursively (#6007),0.5726656,Removed the Trainer(move_metrics_to_cpu=True) argument (#16358),  Propagate to_cpu flag down the recursion chain   Refactor   Add test   Update CHANGELOG   Update tests/utilities/test_memory.py   Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
MlflowLogger limit parameter value length to 250 char (#5893),1.0,MlflowLogger limit parameter value length to 250 char (#5893),,1
[Docs] Explain metric internals (#5899),0.72555995,"docs for all Metrics (#2184, #2209)",  correct docs   fix levels ,1
[BugFix] Resolve bugs in computer_vision_fine_tuning.py example (#5985),0.55520016,Changed computer_vision_fine_tunning example to use BackboneLambdaFinetuningCallback (#5377),  update the script to use DataModule   add message at for the frozen parameters   add message about trainable parameters   resolve flake8 ,0
[accelerator][FeatBugFix] Improve manual optimization API (#5771),0.66581357,Support for manual optimization with DeepSpeed (#7970),"  fix trainer.model access   move properties   fix test_transfer_batch_hook   fix auto_select_gpus   fix omegaconf test   fix test that needs to simulate slurm ddp   add horovod plugin   fix test with named arguments   clean up whitespace   fix datamodules test   remove old accelerators   fix naming   move old plugins   move to plugins   create precision subpackage   create training_type subpackage   fix all new import errors   fix wrong arguments order passed to test   fix LR finder   Added sharded training type and amp plugin   Move clip grad to precision plugin   Added sharded spawn, select accelerators based on distributed_backend + enable custom fp16 plugin automatically   Fix import issue, attempting to fix tests   Fix initial test   Reflect hook logic from master, should wrap model after move to device   Optional state consolidation, since master has optimizers not wrapped   change attribute for instance test   reset optimizers   optimizers are not used in main process, so state would be wrong.   legacy   imports in accel   legacy2   trainer imports   fix import errors after rebase   move hook to new setup location   provide unwrapping logic   fix trainer callback system   added ddp2 implementation   fix imports .legacy   move plugins   restore legacy   drop test.py from root   add tpu accelerator and plugins   fixes   fix lightning optimizer merge   reset bugreportmodel   unwrapping   step routing forward   model access   unwrap   opt   integrate distrib_type   sync changes   sync   fixes   add forgotten generators   add missing logic   update   import   missed imports   import fixes   isort   mv f   changelog   format   move helper to parallel plugin   d   add world size   clean up   duplicate   activate ddp_sharded and tpu   set nvidia flags   remove unused colab var   use_tpu <-> on_tpu attrs   make some ddp_cpu and clusterplugin tests pass   Ref/accelerator connector (#5742)   final cleanup   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  connector cleanup  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  trainer cleanup  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  accelerator cleanup + missing logic in accelerator connector  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  add missing changes to callbacks  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  reflect accelerator changes to lightning module  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  clean cluster envs  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  cleanup plugins  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  add broadcasting  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   yapf   remove plugin connector   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   plugins   manual optimization   update optimizer routing   add rank to torchelastic   fix memory mixed precision   setstate on trainer for pickling in ddp spawn   add predict method   add back commented accelerator code   adapt test for sync_batch_norm to new plugin   fix deprecated tests   fix ddp cpu choice when no num_processes are given   yapf format   skip a memory test that cannot pass anymore   update on comments   fix pickle error in spawn plugin   x   avoid   x   fix cyclic import in docs build   add support for sharded   update typing   add sharded and sharded_spawn to distributed types   make unwrap model default   refactor LightningShardedDataParallel similar to LightningDistributedDataParallel   update sharded spawn to reflect changes   update sharded to reflect changes   Merge 1.1.5 changes   fix merge   fix merge   yapf isort   fix merge   yapf isort   fix indentation in test   copy over reinit scheduler implementation from dev1.2   fix apex tracking calls with dev_debugger   reduce diff to dev1.2, clean up   fix trainer config test  when gpus>0 and num_processes >0 and ddp_cpu   sort plugin tests legacy/new   fix error handling for amp on cpu   fix merge   fix merge fix merge   [Feat] Resolve manual_backward (#5837)   resolve manual_backward   resolve flake8   update   resolve for ddp_spawn   resolve flake8   resolve flake8   resolve flake8   Co-authored-by: Ubuntu ubuntu@ip-172-31-88-60.ec2.internal   fix tests/accelerator tests on cpu   [BugFix] Resolve manual optimization (#5852)   resolve manual_optimization   update   update   Co-authored-by: Ubuntu ubuntu@ip-172-31-88-60.ec2.internal   Remove copy trainer parameters to happen earlier within the loop and add safe guard to get ref model (#5856)   resovle a bug   Accelerator refactor sharded rpc (#5854)   rpc branch   merge   update handling of rpc   make devices etc. Optional in RPC   set devices etc. later if necessary   remove devices from sequential   make devices optional in rpc   fix import   uncomment everything   fix cluster selection   Co-authored-by: Ubuntu ubuntu@ip-172-31-88-60.ec2.internal   resolve bug   fix assert in rpc test   resolve a test   fix docs compilation   accelerator refactor - fix for sharded parity test (#5866)   fix memory issue with ddp_spawn   x   x x x x x x x x   x   Remove DDP2 as this does not apply   Add missing pre optimizer hook to ensure lambda closure is called   fix apex docstring   [accelerator][BugFix] Resolve some test for 1 gpu (#5863)   update   revert init   resolve a bug   update   resolve flake8   update   update   update   revert init   resolve a bug   update   resolve flake8   update   update   update   update   update   revert init   resolve a bug   update   resolve flake8   update   update   update   revert init   update   resolve flake8   update   update   update   update   update   all_gather   update   make plugins work, add misconfig for RPC   update   update   remove breaking test   resolve some tests   resolve flake8   revert to ddp_spawn   Co-authored-by: root root@ip-172-31-88-60.ec2.internal Co-authored-by: Ubuntu ubuntu@ip-172-31-88-60.ec2.internal Co-authored-by: Justus Schock justus.schock@rwth-aachen.de   yapf isort   resolve flake8   fix apex doctests   fix apex doctests 2   resolve docs   update drone   clean env   update   update   update   update   merge   Fix RPC related tests, clean out old API, update for new accelerator API [skip ci] (#5881)   Fix RPC related tests, clean out old API, update for new accelerator API   Move tests out of legacy folder, update paths and names   Update test_remove_1-4.py   Expose properties for tpu cores/gpus/num_gpus   Add root GPU property   Move properties to properties.py   move tests that were previously in drone   Fix root GPU property (#5908)   Move root GPU to property, remove horovod set as this is handled in horovod plugin, ensure we mock correctly to set GPU accelerator   Add missing tests back   fix best model path transfer when no checkpoint callback available   Fix setup hook order [wip] (#5858)   Call trainer setup hook before accelerator setup   Add test case   add new test   typo   fix callback order in test   Co-authored-by: tchaton thomas@grid.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   rename ddp sequential -> rpc sequential for special test   revert   fix stupid merge problem   Use property in connector for sampler (#5913)   merge the import conflicts   fix spawning of processes in slurm   [wip] Fix some bugs for TPU [skip ci] (#5878)   fixed for single tpu   fixed spawn   fixed spawn   update   update   wip   resolve bugs   resolve bug   update on comment   removed decorator   resolve comments   set to 4   update   update   need cleaning   update   update   update   resolve flake8   resolve bugs   exclude broadcast   resolve bugs   change test   update   update   skip if meet fails   properly raise trace   update   add catch   wrap test   resolve typo   update   typo   Co-authored-by: Lezwon Castelino lezwon@gmail.com Co-authored-by: Your Name you@example.com   resolve some tests   update   fix imports   update   resolve flake8   update azure pipeline   skip a sharded test on cpu that requires a gpu   resolve tpus   resolve bug   resolve flake8   update   updat utils   revert permission change on files   suggestions from carlos   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   remove unrelated formatting changes   remove incomplete comment   Update pytorch_lightning/accelerators/init.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   remove unrelated formatting change   add types   warn 1.7 ddp manual backward only if ddp kwarg unset   yapf + isort   pep8 unused imports   fix cyclic import in docs   Apply suggestions from code review   typer in accelerator.py   typo   Apply suggestions from code review   formatting   update on comments   update typo   Update pytorch_lightning/trainer/properties.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   update   update on comments   resolve some comments   update on comments   resolve test   add toggle_model   update   update on comments   update doc   typo   update   typo   remove space   update   update on comments   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: justusschock justus.schock@posteo.de Co-authored-by: SeanNaren sean@grid.ai Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz Co-authored-by: Justus Schock justus.schock@rwth-aachen.de Co-authored-by: Ubuntu ubuntu@ip-172-31-88-60.ec2.internal Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: root root@ip-172-31-88-60.ec2.internal Co-authored-by: Lezwon Castelino lezwon@gmail.com Co-authored-by: Your Name you@example.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com",0
Prune EvalModelTemplate from callbacks and utilities (#6018),0.5363226,remove obscure forward call in eval + CPU backend ___step (#3123),  boring   boring ,0
update XLA nightly version check (#5989),0.60200167,  * Deprecated the internal `XLADeviceUtils.xla_available` staticmethod,Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,0
doc: Add hint towards using ArgumentParser.add_argument_group (#5911),0.6882237,"Alternatively, you can add the argparse arguments you want manually:python"," doc: Add hint towards using ArgumentParser.add_argument_group  Since pl adds many arguments, it is nice to distinguish these arguments  fixup! address review  Co-authored-by: chaton thomas@grid.ai",0
fix fairscale compatible with PT 1.8 (#5996),0.589312,Avoided requiring the FairScale package to use precision with the fsdp native strategy (#14092),  try to extend fairscale available   1.2 ,0
Basic examples fixes (#5912),0.58058214,Simplify the PL examples structure (shallower and more readable) (#1247),"  Move pl_bolts assert to actually do something   Define val, test steps, use _DATASETS_PATH   Use DATASETS_PATH in DALI classifier   Fix incorrect paths and style in example READMEs   Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com",0
remove legacy plugins (#5950),0.5823303,Moved accelerators and plugins to its legacy pkg (#5645),  remove legacy plugins   imports   formatting   fix docs references   fix cluster environment inheritance   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
wandb: Fix example rendering for docs (#5905),0.5032075,WandbLogger does not force wandb reinit arg to True anymore and creates a run only when needed (#4648),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
clean AMP logic (#5994),0.49467874,"Cleaning (#5948, #5949, #5950)",  clean AMP logic   cleaning   ...   ...   Even apex ,0
Fix error in pre-optim logic (#5995),0.56677353,Refactored optimizer (#4658),,0
fix nightly releases & readme (#5922),0.5571671,Pkg: fix parsing versions by @Borda in https://github.com/Lightning-AI/lightning/pull/15401,  fix nightly releases   readme   cuda   doxker   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  revert  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
temporary suspend master update (#6017),0.47081512,Changed resume_from_checkpoint warning to an error when the checkpoint file does not exist (#7075),,0
move TPU cleaning to GH actions (#5991),0.60294074,moved TPU xxx_step to backend (#3118),  move TPU cleaning to GH actions   test   . ,0
try random TPU config (#5992),0.6027783,Enabled manual optimization for TPUs (#8458),  try random TPU config   random   Apply suggestions from code review   Co-authored-by: chaton thomas@grid.ai Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,0
Update PULL_REQUEST_TEMPLATE [skip ci] (#6000),0.38913882,Resolved a bug where the wrong client was passed to collect cloud logs (#14684),Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,0
CI: drop code formatters (#5971),0.4442856,- Removed the deprecated code in:,Co-authored-by: chaton thomas@grid.ai,0
fix deprecated call (#6005),0.76311064,Removed deprecated callbacks (#3979),,1
add missing typing to trainer properties (#5974),0.76713735,Removed TrainerProperties mixin and moved property definitions directly into Trainer (#9495),  add typing   clean up   isort   fix typing in log_dir ,1
move device-specific teardown logic from training loop to accelerator (#5973),0.9573879,Moved device-specific teardown logic from training loop to accelerator (#5973),  on train end   switch order ,1
Docs: fix failing make (#5988),0.4433914,Improved error messages for invalid configure_optimizers returns (#3587),,0
clean up unused distributed sampler logic in trainer (#5975),0.67994773,"We renamed the Trainer(replace_sampler_ddp=...) argument to Trainer(use_distributed_sampler=...) to communicate that the sampler gets created not only for the DDP strategies, but all distributed strategies that need it. Its function is still the same as before, and most users don't need to change its default value.",  clean up sampler unused logic   undo cached   imports ,0
"Makefile: Refer to CONTRIBUTING doc, reword test to avoid ""example"" (#5910)",0.36370397,"This will compile forward and {training,validation,test,predict}_step",CONTRIBUTING: Add concrete example for running single test,0
Fix: hparams.yaml saved twice when using TensorBoardLogger (#5953),0.7424482,Changed the default logger to TensorBoardLogger (#609),,1
enable testing DDP examples (#4995),0.6501645,        Override to init DDP in a different way or use your own wrapper.,  enable testing DDP examples   args   ddp_spawn   ddp as extra script   path   Conflicts: .drone.yml   install   -u   q ,0
fixed TPU docs (#5958),0.6168804,Enabled manual optimization for TPUs (#8458),,0
Document exceptions in callbacks (#5541),0.5975358,Used raise .. from .. to explicitly chain exceptions (#3750),  Add Raises: section to docstring   Add Raises section to the docs   Add raises section to the docs   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   fix   Remove unnecessary instance check   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Docs: Fix broken get-started link (#5960),0.5341854,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",,0
remove legacy accelerators (#5949),0.6692807,Moved accelerators and plugins to its legacy pkg (#5645),  remove legacy accelerators   update imports   formatting   Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz,0
move accelerator legacy tests (#5948),0.73167026,move specific accelerator code (#3457),Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,1
v1.2.0rc1 (#5946),0.6455419,[1.6.2] - 2022-04-27,  v1.2.0rc0   chlog   chlog   chlog   chlog   Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,0
Fix: Repeated .fit() calls ignore max_steps iteration bound  (#5936),0.57440305,"Changed setup() and teardown() stage argument to take any of {fit,validate,test,predict} (#6386)",  fix repeated fit calls ignoring max_steps   fix fast dev progress bar ,0
"new LightningModule hook ""configure_callbacks"" (#5621)",0.7107332,- Added support for returning a single Callback from `LightningModule.configure_callbacks` without wrapping it into a list ([#11060](https://github.com/PyTorchLightning/pytorch-lightning/pull/11060)),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: chaton thomas@grid.ai Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,1
PoC: Accelerator refactor (#5743),0.8372302,Refactored Accelerators and Plugins (#5743),"  restoring the result from subprocess   fix queue.get() order for results   add missing ""block_backward_sync"" context manager   add missing ""block_backward_sync"" context manager   fix sync_batchnorm   fix supported gpu-ids for tuple   fix clip gradients and inf recursion   accelerator selection: added cluster_environment plugin   fix torchelastic test   fix reduce early stopping decision for DDP   fix tests: callbacks, conversion to lightning optimizer   fix lightning optimizer does not pickle   fix setting benchmark and deterministic option   fix slurm amp test   fix prepare_data test and determine node_rank   fix retrieving last path when testing   remove obsolete plugin argument   fix test: test_trainer_config   fix torchscript tests   fix trainer.model access   move properties   fix test_transfer_batch_hook   fix auto_select_gpus   fix omegaconf test   fix test that needs to simulate slurm ddp   add horovod plugin   fix test with named arguments   clean up whitespace   fix datamodules test   remove old accelerators   fix naming   move old plugins   move to plugins   create precision subpackage   create training_type subpackage   fix all new import errors   fix wrong arguments order passed to test   fix LR finder   Added sharded training type and amp plugin   Move clip grad to precision plugin   Added sharded spawn, select accelerators based on distributed_backend + enable custom fp16 plugin automatically   Fix import issue, attempting to fix tests   Fix initial test   Reflect hook logic from master, should wrap model after move to device   Optional state consolidation, since master has optimizers not wrapped   change attribute for instance test   reset optimizers   optimizers are not used in main process, so state would be wrong.   legacy   imports in accel   legacy2   trainer imports   fix import errors after rebase   move hook to new setup location   provide unwrapping logic   fix trainer callback system   added ddp2 implementation   fix imports .legacy   move plugins   restore legacy   drop test.py from root   add tpu accelerator and plugins   fixes   fix lightning optimizer merge   reset bugreportmodel   unwrapping   step routing forward   model access   unwrap   opt   integrate distrib_type   sync changes   sync   fixes   add forgotten generators   add missing logic   update   import   missed imports   import fixes   isort   mv f   changelog   format   move helper to parallel plugin   d   add world size   clean up   duplicate   activate ddp_sharded and tpu   set nvidia flags   remove unused colab var   use_tpu <-> on_tpu attrs   make some ddp_cpu and clusterplugin tests pass   Ref/accelerator connector (#5742)   final cleanup   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  connector cleanup  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  trainer cleanup  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  accelerator cleanup + missing logic in accelerator connector  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  add missing changes to callbacks  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  reflect accelerator changes to lightning module  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  clean cluster envs  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  cleanup plugins  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  add broadcasting  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   yapf   remove plugin connector   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   plugins   manual optimization   update optimizer routing   add rank to torchelastic   fix memory mixed precision   setstate on trainer for pickling in ddp spawn   add predict method   add back commented accelerator code   adapt test for sync_batch_norm to new plugin   fix deprecated tests   fix ddp cpu choice when no num_processes are given   yapf format   skip a memory test that cannot pass anymore   fix pickle error in spawn plugin   x   avoid   x   fix cyclic import in docs build   add support for sharded   update typing   add sharded and sharded_spawn to distributed types   make unwrap model default   refactor LightningShardedDataParallel similar to LightningDistributedDataParallel   update sharded spawn to reflect changes   update sharded to reflect changes   Merge 1.1.5 changes   fix merge   fix merge   yapf isort   fix merge   yapf isort   fix indentation in test   copy over reinit scheduler implementation from dev1.2   fix apex tracking calls with dev_debugger   reduce diff to dev1.2, clean up   fix trainer config test  when gpus>0 and num_processes >0 and ddp_cpu   sort plugin tests legacy/new   fix error handling for amp on cpu   fix merge   fix merge fix merge   [Feat] Resolve manual_backward (#5837)   resolve manual_backward   resolve flake8   update   resolve for ddp_spawn   resolve flake8   resolve flake8   resolve flake8   Co-authored-by: Ubuntu ubuntu@ip-172-31-88-60.ec2.internal   fix tests/accelerator tests on cpu   [BugFix] Resolve manual optimization (#5852)   resolve manual_optimization   update   update   Co-authored-by: Ubuntu ubuntu@ip-172-31-88-60.ec2.internal   Remove copy trainer parameters to happen earlier within the loop and add safe guard to get ref model (#5856)   resovle a bug   Accelerator refactor sharded rpc (#5854)   rpc branch   merge   update handling of rpc   make devices etc. Optional in RPC   set devices etc. later if necessary   remove devices from sequential   make devices optional in rpc   fix import   uncomment everything   fix cluster selection   Co-authored-by: Ubuntu ubuntu@ip-172-31-88-60.ec2.internal   resolve bug   fix assert in rpc test   resolve a test   fix docs compilation   accelerator refactor - fix for sharded parity test (#5866)   fix memory issue with ddp_spawn   x   x x x x x x x x   x   Remove DDP2 as this does not apply   Add missing pre optimizer hook to ensure lambda closure is called   fix apex docstring   [accelerator][BugFix] Resolve some test for 1 gpu (#5863)   update   revert init   resolve a bug   update   resolve flake8   update   update   update   revert init   resolve a bug   update   resolve flake8   update   update   update   update   update   revert init   resolve a bug   update   resolve flake8   update   update   update   revert init   update   resolve flake8   update   update   update   update   update   all_gather   update   make plugins work, add misconfig for RPC   update   update   remove breaking test   resolve some tests   resolve flake8   revert to ddp_spawn   Co-authored-by: root root@ip-172-31-88-60.ec2.internal Co-authored-by: Ubuntu ubuntu@ip-172-31-88-60.ec2.internal Co-authored-by: Justus Schock justus.schock@rwth-aachen.de   yapf isort   resolve flake8   fix apex doctests   fix apex doctests 2   resolve docs   update drone   clean env   update   update   update   update   merge   Fix RPC related tests, clean out old API, update for new accelerator API [skip ci] (#5881)   Fix RPC related tests, clean out old API, update for new accelerator API   Move tests out of legacy folder, update paths and names   Update test_remove_1-4.py   Expose properties for tpu cores/gpus/num_gpus   Add root GPU property   Move properties to properties.py   move tests that were previously in drone   Fix root GPU property (#5908)   Move root GPU to property, remove horovod set as this is handled in horovod plugin, ensure we mock correctly to set GPU accelerator   Add missing tests back   fix best model path transfer when no checkpoint callback available   Fix setup hook order [wip] (#5858)   Call trainer setup hook before accelerator setup   Add test case   add new test   typo   fix callback order in test   Co-authored-by: tchaton thomas@grid.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   rename ddp sequential -> rpc sequential for special test   revert   fix stupid merge problem   Use property in connector for sampler (#5913)   merge the import conflicts   fix spawning of processes in slurm   [wip] Fix some bugs for TPU [skip ci] (#5878)   fixed for single tpu   fixed spawn   fixed spawn   update   update   wip   resolve bugs   resolve bug   update on comment   removed decorator   resolve comments   set to 4   update   update   need cleaning   update   update   update   resolve flake8   resolve bugs   exclude broadcast   resolve bugs   change test   update   update   skip if meet fails   properly raise trace   update   add catch   wrap test   resolve typo   update   typo   Co-authored-by: Lezwon Castelino lezwon@gmail.com Co-authored-by: Your Name you@example.com   resolve some tests   update   fix imports   update   resolve flake8   update azure pipeline   skip a sharded test on cpu that requires a gpu   resolve tpus   resolve bug   resolve flake8   update   updat utils   revert permission change on files   suggestions from carlos   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   remove unrelated formatting changes   remove incomplete comment   Update pytorch_lightning/accelerators/init.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   remove unrelated formatting change   add types   warn 1.7 ddp manual backward only if ddp kwarg unset   yapf + isort   pep8 unused imports   fix cyclic import in docs   Apply suggestions from code review   typer in accelerator.py   typo   Apply suggestions from code review   formatting   update on comments   update typo   Update pytorch_lightning/trainer/properties.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   update   suggestion from code review   suggestion from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: SeanNaren sean@grid.ai Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz Co-authored-by: chaton thomas@grid.ai Co-authored-by: Ubuntu ubuntu@ip-172-31-88-60.ec2.internal Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: root root@ip-172-31-88-60.ec2.internal Co-authored-by: Lezwon Castelino lezwon@gmail.com Co-authored-by: Your Name you@example.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com",1
Fix: passing wrong strings for scheduler interval doesn't throw an error (#5923),0.5589111,Changed lr schedule step interval behavior to update every backwards pass instead of every forwards pass (#1477),  Raise if scheduler interval not 'step' or 'epoch'   Add test for unknown 'interval' value in scheduler   Use BoringModel instead of EvalModelTemplate   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   Fix import order   Apply yapf in test_datamodules   Add missing imports to test_datamodules   Fix too long comment   Update pytorch_lightning/trainer/optimizers.py   Fix unused imports and exception message   Fix failing test   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
tests: Remove usage of --flake8 flag (#5909),0.4391548,Removed no return warning from val/test step (#6139),  tests: Remove usage of --flake8 flag   Remove commented line   Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
drop DDP CLI test (#5938),0.58725,Run ddp_spawn dataloader checks on Windows (#6930),  fix tests   =   Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz,0
remove executable bit on source files (#5929),0.40999854,Set version as today (#13906), 644,0
Fix: Failing test in data_modules(dp) (#5924),0.66407764,"Deprecated DataModule properties: has_prepared_data, has_setup_fit, has_setup_validate, has_setup_test, has_setup_predict, has_teardown_fit, has_teardown_validate, has_teardown_test, has_teardown_predict (#7657)",  Update test_datamodules.py   fix code format issue   fix test restore   fix code format issue ,0
Typing: callback base (#5919),0.57887065,  callbacks:, typing for callback base,0
More EpochResultStore refactors! :tada: (#5522),0.8702952,Refactored EpochResultStore (#5522),Co-authored-by: chaton thomas@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
Feature: LightningDataModule.from_datasets(...) (#5133),0.81063336,- Added support for passing extra init-parameters to the `LightningDataModule.from_datasets` ([#14185](https://github.com/Lightning-AI/lightning/pull/14185)),  add class method   add tests   docstring   pep   Add type annotations   Co-authored-by: Nicki Skafte skaftenicki@gmail.com   pep   fix import   remove num_workers inference   Update pytorch_lightning/core/datamodule.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update pytorch_lightning/core/datamodule.py  Co-authored-by: Nicki Skafte skaftenicki@gmail.com  Update pytorch_lightning/core/datamodule.py  Co-authored-by: Nicki Skafte skaftenicki@gmail.com   fix syntax   typing fix   list -> sequence   list -> sequence   missing import   fix test   Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
add docs (#5902),0.5688964,Docs improvements,,0
Delete unused autopep8 config (#5904),0.449098,Removed mode='auto' from EarlyStopping (#6167),,0
forward cache fix (#5895),0.49213052,refactored DDP backend forward (#3119),,0
fix metric docs (#5880),0.7375288,Removed deprecated metrics (#6161),,1
update example (#5753),0.41847262,"Updated DeepSpeed ZeRO (#6546, #6752, #6142, #6321)",,0
[tests/models] refactor with BoringModel (#5507),0.5882642,Refactor Model backward (#2276),  update with BoringModel   update with BoringModel   step   try TPU   TPU   update tests   update tpu tests   self   fix   dp   update tests   ref   update tests   fix tpu tests   fix dp and run_prediction   dp   only dp   Apply suggestions from code review   Apply suggestions from code review   Apply suggestions from code review   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Quantisation (#5706),0.39189383,"Epoch 8:  53%|█████    | 17/32 [5.13/s, v_num=2]",  empty   sq   obs   int   ts   helpers   chlog   yapf   avg   dupl   Apply suggestions from code review   Co-authored-by: Nicki Skafte skaftenicki@gmail.com  Apply suggestions from code review  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   fixes   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   fixes   note   warn   45   link   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Apply suggestions from code review  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   yapf   flake8   Apply suggestions from code review   Apply suggestions from code review   Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Docs/fixes (#5914),0.5936777,Docs improvements,  wip   ..   ...   Apply suggestions from code review   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,0
Convert progress bar metrics to float (#5692),0.73703146,Progress bar metrics tensors are now converted to float (#5692),  MetricsHolder(to_float=True)   Update CHANGELOG   Update tests/callbacks/test_progress_bar.py   flake8   Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz,1
[feat] Add StochasticWeightAveragingCallback (#5640),0.5282242,Deprecated passing stochastic_weight_avg to the Trainer constructor in favor of adding the StochasticWeightAveraging callback directly to the list of callbacks (#8989),  add swa callback   switch back to 1.6.0   remove optimizer_step   move super   update   forgot update_parameters   update on comments   works for ddp   resolve flake8   remove set_model   resolve flake8   resolve cpu   resolve flake8   resolve flake8   update   update on comments ,0
Refactor utilities/imports.py (#5874),0.55080616,Wrapped imports for traceability (#13924),Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz Co-authored-by: Nicki Skafte skaftenicki@gmail.com,0
add azure timeout (#5907),0.5010116,Increased TPU check timeout from 20s to 100s (#5598),  add azure timeout   rework ,0
Fix Pruning callback and add a few features (#5825),0.5479938,Sanitize None params during pruning (#6836),"  Remove pruning check because it was added in 1.4.0 and that is our minimal torch version   Fixing many bugs   Fix misconfig test   Fix tests   Improve error message   Reduce whitespace   WIP   TODOs   _MODULE_CONTAINERS   Add LTH test   Allow resampling   Iterative pruning   Log pruning percentage   Properly make pruning permanent   Fix docstring   Minor changes   Test loading non-permanent model   corrent bugs   Revert ""corrent bugs""   This reverts commit ffb8d4754710ce7473f7e14542af8f1594962b5d.   Add beta warning   Fix docs   2 verbosity levels   OCD   Co-authored-by: Your Name you@example.com",0
bye bye Drone (#5901),0.45063952,(#16002),,0
CI: Azure (#5882),0.40750647,"If we forgot someone due to not matching the commit email with GitHub account, let us know :]",  add base Azure pipeline   skip ,0
Enable purely iteration-based training (#5726),0.7828608,Extended support for purely iteration-based training (#5726),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Kaushik Bokka kaushikbokka@gmail.com,1
codeowner: update test/boring model (#5869),0.5764214,Refactor Model backward (#2276),  update test/boring model   reorder ,0
fixing some compatibility with PT 1.8 (#5864),0.5940941,Here is a summary of major deprecations introduced in 1.8:,  change default   .   p   0.21.2   .   fix   . ,0
create new Conda images (#5877),0.4010498,"fabric = Fabric(accelerator=""cuda"", devices=8, strategy=""ddp"")",  create new Conda images   .   . ,0
fix miss-leading imports in tests (#5873),0.5928939,Module imports are now catching ModuleNotFoundError instead of ImportError (#9867),  fix imorts   . ,0
try fix: Docker with Conda & PT 1.8 (#5842),0.47364366,Removed the deprecated pytorch_lightning.accelerators.GPUAccelerator in favor of pytorch_lightning.accelerators.CUDAAccelerator (#16050),  ci   ver   list   pt   nk   ch   4.9 ,0
prune Drone build (#5871),0.4002614,Refactored optimizer (#4658),,0
formatting 5/n: Core (#5721),0.40600792,Model summary: add 1 decimal place (#4745),  yapf core   Update pytorch_lightning/core/lightning.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
formatting 3/n: PL modules (#5716),0.4808085,Simplify the PL examples structure (shallower and more readable) (#1247),  cb   log   prof   tune   flake8 ,0
sync,0.48228326,moves sync bn to each backend (#3925),,0
Fix typo in docstring (#5835),0.5288899,Syntax changes are: ,Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com (cherry picked from commit 0a50bb406fa41dfa6a0e2be52f531a9c81c87d00),0
Separate epoch validation from step validation (#5208),1.0000002,Separate epoch validation from step validation (#5208),  Seperate epoch validaton from step validation   update system   test   baked logic in callbacks   unbake logic in callbacks   fix the call for scheduler   use property   pep   correct rebase   gitignore   ref   add tests   fix   add early stopping test   trigger   chlog   rev   1.3   log   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   Update pytorch_lightning/trainer/training_loop.py   Update CHANGELOG.md   Apply suggestions from code review   Co-authored-by: chaton thomas@grid.ai Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com (cherry picked from commit e429f97b670846bf0e0595074c4d3c4f152dcf27),1
prepare v1.1.8 (#5839),0.4837897,[1.1.8] - 2021-02-08,(cherry picked from commit 3b7afb932b838e51b3bf99bc4213053c63c277cb),0
Fix fine-tuning callback test (#5643),0.6685458,Removed callback metrics from test results obj (#2994),  fix   batch size ,0
Fix broken code in docs (#5859),0.5444493,- Removed the deprecated code in:,Co-authored-by: Roman Tezikov roman.tezikov@lamoda.ru,0
Refactor simplify tests (#5861),0.5424325,- fixed all the .test() calls,  add new   restructure   yapf   move   fix ,0
prune SimpleModel (#5862),0.5080638,Refactor Model backward (#2276),,0
prune unused methods (#5860),0.4828136,            find_unused_parameters=True,,0
Add an additional check to ensure pipe checks supported fairscale version (#5853),0.48210698,Avoided requiring the FairScale package to use precision with the fsdp native strategy (#14092),,0
yapf tests metrics (#5845),0.6044775,Regression metrics (#2221),,0
yapf tests trainer (#5844),0.5995996,trainer.test(...),,0
formatting tests1/n (#5843),0.41452116,- fixed all the .test() calls,  utils   tuner   base ,0
formatting tests: 5/5 (#5848),0.4567606,- fixed all the .test() calls,  cb   acc   plug   . ,0
formatting tests: 4/n (#5846),0.44477177,- fixed all the .test() calls,  models   ckpt   core   log ,0
fix s3 download for PT 1.8 (#5840),0.477778,Pkg: fix parsing versions by @Borda in https://github.com/Lightning-AI/lightning/pull/15401, fix s3 download for PT 1.8,0
formatting flake8 & isort (#5824),0.3235414,"| ""16-mixed""                        | ""16"", 16   |",  formatting   isort   make   yapf   isort ,0
Update README .gif (#5777),0.4756361,"Updated DeepSpeed ZeRO (#6546, #6752, #6142, #6321)",Co-authored-by: Jirka Borovec Borda@users.noreply.github.com (cherry picked from commit e8c17551291e61972297c12f2491d7b4e87bf511),0
Apply suggestions from code review,0.35956877,Refactoring,,0
resolve conflits,0.40635175,This conversation was marked as resolved by carmocca,resolve doc boring commit docs torchvision tpu Update dockers/tpu-tests/tpu_test_cases.jsonnet Update dockers/tpu-tests/tpu_test_cases.jsonnet,0
Fix toggle optimizer (#5775),0.68449837,"- The `LightningModule.{un}toggle_optimizer` methods no longer accept a `optimizer_idx` argument to select the relevant optimizer. Instead, the optimizer object can be passed in directly ([#16560](https://github.com/Lightning-AI/lightning/pull/16560))",  Update lightning.py   update changelog   add a 3 optimizer test   resolve flake8   remove extra code   typo   resolve typo   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: tchaton thomas@grid.ai Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,0
[docs] Add docs for non-SLURM cluster setup (#5754),0.6113745,Support running on multiple clusters (#16016),  Add docs for non-slurm cluster setup   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update docs/source/cluster.rst  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update docs/source/cluster.rst  Co-authored-by: Alexander alexander@reshytko.com Co-authored-by: chaton thomas@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
replace upload (#5765),0.3822947,Add support to upload files to the Drive through an asynchronous upload_file endpoint (#14703),,0
hotfix for GHA tpu (#5762),0.556773,Resolve TPU miss rendezvous (#6781),  -y   t   .   t ,0
release 1.1.7 (#5761),0.6908864,[1.7.6] - 2022-09-13,Co-authored-by: chaton thomas@grid.ai,0
Disable training with zero num_training_batches when insufficient limit_train_batches (#5703),0.9385972,Disabled training with zero num_training_batches when insufficient limit_train_batches (#5703),  disable training when zero num_train_batches with limit_train_batches   refactor train skip condition   fix formatting issues   fix formatting issues   ref: test error msg   fix tests for data loader calls   fix train dataloader condition   update limit_train_batches upper range in test comment   remove model state check test   Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,1
Fix log_dir property (#5537),0.688694,- all the file path errors with loggers (txs @awaelchli),  fix and update tests   update with ModelCheckpoint   chlog   wip wandb fix   all fixed   Co-authored-by: chaton thomas@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,0
link blog (#5740),0.39746392,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",,0
Fix typo in LightningOptimizer (#5736),0.60758775,- Removed the deprecated ([#14471](https://github.com/Lightning-AI/lightning/pull/14471)),,0
add make for docs (#5685),0.43715543,Docs improvements,  add make docs   docs ,0
Bugfix/5487 auto lr ordering (#5638),0.48276225,Deprecated reorder parameter of the auc metric (#4237),  started to write failing test. just getting into the framework...   started to write failing test. just getting into the framework...   added failing test for misconfiguration of lr finder   made test startup quickly. making sure without the fix it also fails slowly   improved test   fixed for linter   fixed for linter   yet another fix for the linter   yet another fix for the linter   fixed comment by @carmocca   fixed comment by @carmocca   Fix test   chlog   Apply suggestions from code review   Fix test   Update pytorch_lightning/tuner/lr_finder.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update pytorch_lightning/tuner/lr_finder.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   Update tests/trainer/test_lr_finder.py   Update pytorch_lightning/tuner/lr_finder.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update pytorch_lightning/tuner/lr_finder.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   Update pytorch_lightning/tuner/lr_finder.py   Update tests/trainer/test_lr_finder.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
Fix docs typo (#4930),0.4904125,"In addition, we fixed:",  Fix docs   typo   import   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: chaton thomas@grid.ai Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,0
Fix mypy 0.800 plus when prepending $PYTHONPATH to sys.path (#5698),0.4746445,Use correct python version in lightning component template (#13790),"  Fix mypy when prepending $PYTHONPATH to sys.path   attempt mypy fix   Revert ""attempt mypy fix""   This reverts commit fb7ed827d924452218ec23ddb2fd7484952a1294.  fix mypy  Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz",0
add contrib questions (#5691),0.4315924,Made type hints public (#17100),  add contrib questions   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Apply suggestions from code review  Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Roger Shieh sh.rog@protonmail.ch Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Roger Shieh sh.rog@protonmail.ch,0
Rename opt_idx to optimizer_idx in docs for complex training loops (#5712),0.72354877,Removed the deprecated optimizer_idx from training_step as an accepted argument in manual optimization (#8576),,1
CI: skip horovod in testing docs (#5702),0.47924158,Removed no return warning from val/test step (#6139),,0
Close SummaryWriter in TensorBoardLogger on finalize (#5696),0.69255924,Remove explicit flush from tensorboard logger (#2126),"Not entirely sure this is the ""right"" solution to this problem, but currently when model fitting is finished the TensorBoardLogger attribute _experiment (a SummaryWriter) is left with an open file handle. This causes issues in particular on Windows systems (and probably others), and also makes the files un-syncable on cloud-synced devices like OneDrive. This PR adds a close() to finalize to make sure this handle is closed upon fit completion. Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com",0
Remove unnecessary intermediate layers in base-conda Dockerfile (#5697),0.85092926,Remove unnecessary intermediate layers in Dockerfiles (#5697),  [docker][base-conda] Combine ENV+COPY instructions   [docker][base-cuda] Combine ENV+COPY instructions   [docker][base-xla] Combine ENV+COPY instructions   [docker][base-cuda] Fix COPY instruction   [docker][base-xla] Fix quote in ENV   [docker][base-xla] Fix $PATH in ENV   [docker][base-conda] Fix COPY instruction   chlog   Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz,1
Update CODEOWNERS (#5561),0.4613275,"Updated DeepSpeed ZeRO (#6546, #6752, #6142, #6321)",  Update CODEOWNERS   Update CODEOWNERS   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Apply suggestions from code review  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com,0
fix model arguements (#5653),0.6662767,Refactor Model backward (#2276),,0
update mergify - master only (#5682),0.39104858,Pkg: fix parsing versions by @Borda in https://github.com/Lightning-AI/lightning/pull/15401,,0
"Filter ""unsqueeze"" warning when using DP (#5622)",0.6357448,On DP and DDP2 unsqueeze is automated now (#1319),  filtering unsqueeze warning directly in DP   add changelog   constrain to module   Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
resolve conflict,0.41594103,This conversation was marked as resolved by carmocca,resolve failing test,0
Fix ModelCheckpoint race condition in file existence check (#5155),0.7864059,Fix saved filename in ModelCheckpoint if it already exists (#4861),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com,1
Fix num_classes arg in F1 metric (#5663),0.6317525,Renamed class metric Fbeta >> FBeta (#4656),  fix f1 metric   Apply suggestions from code review   chlog   Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz,0
move progress bar test to correct test folder (#5667),0.43214113,- Renamed `TQDMProgressBar.main_progress_bar` to `TQDMProgressBar.train_progress_bar` ([#16695](https://github.com/Lightning-AI/lightning/pull/16695)),,0
new section + legacy 1.1.6 (#5666),0.4845283,This section outlines notable changes that are not backward compatible with previous versions. The full list of changes and removals can be found in the Full Changelog below.,,0
[Feat-BugFix] Resolve custom DataLoader (#5745),0.6586083,"Did not always create a DataLoader during reinstantiation, but the same type as before (if a subclass of DataLoader) (#1346)",  resolve custom dataloader   update changelog   fix tests   update on comments   resolve comments   add support for custom batch_sampler   Update tests/trainer/test_data_loading.py   resolve test   resolve flake8   resolve yapf   Co-authored-by: Ubuntu ubuntu@ip-172-31-88-60.ec2.internal Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
fix tests,0.6285534,- fixed all the .test() calls,,0
flake8 + yapf,0.30989394,Updated model_checkpoint's to_yaml to use fsspec open (#3801),,0
Prepare 1.1.6 release (#5661),0.5265013,[1.1.6] - 2021-01-26,  Parepre 1.1.6 release   Remove headers   Apply suggestions from code review   docs: Apply docs suggestions for release   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com (cherry picked from commit c462b274ee193a40ab0172931e0c5102a7be51bc),0
Sync working directory with all processes when using Hydra (#5629),0.8242152,Re-introduced fix for Hydra directory sync with multiple process (#5993),  Append current work dir (hydra dir) to ensure all processes reference the same directory   Add changelog   Add different job name for DDP child processes to log to   Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com (cherry picked from commit ca68cac57ad8eefc9b477ee126eb42a483f27a39),1
passing batch outputs to on_train_batch_end (#4369),0.9154899,Pass batch outputs to on_train_batch_end instead of epoch_end outputs (#4369),  passing batch outputs to on_train_batch_end   styling   updating epoch end logic   also condition on on_train_epoch_end hooks   more readable   pep8   pep8   readability suggestion accepted   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   adding test_training_epoch_end_metrics_collection_on_override test   fix formatting   fix formatting   Co-authored-by: Swetha Mandava smandava@nvidia.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: Roger Shieh sh.rog@protonmail.ch (cherry picked from commit 5fcca4e43b243cd9fdb08050b285fb052856f13b),1
"Unify attribute finding logic, fix not using dataloader when hparams present (#4559)",0.46024072,Improved error messages in replace_sampler when the DataLoader attributes are not included in the signature or the signature is missing optional arguments (#8519),  Rebase onto master   indent fix   Remove duplicated logic   Use single return   Remove extra else   add __contains__ to TestHparamsNamespace to fix tests   Fix lightning_setattr to set all valid attributes   update doc   better names   fix holder order preference   tests for new behavior   Comment about using the last holder   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Sean Naren sean.narenthiran@gmail.com (cherry picked from commit eee3b1a284470b3eca6ebd1815c0ddd7f550c667),0
[bugfix] Resolve bug with multiple optimizers and toggle. (#5574),0.6590281,"- Fixed `LightningModule.{un,}toggle_model` when only 1 optimizer is used ([#12088](https://github.com/PyTorchLightning/pytorch-lightning/pull/12088))",  fix toggle_optimizer   update doc   resolve bug   update   Update pytorch_lightning/core/lightning.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   update on comments   update on comments   update   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com (cherry picked from commit c76cc23b3dc44cadd718b8b85469ace4b8cfb445),0
Fix Metric.state_dict (#5614),0.84582067,Metric states are no longer as default added to state_dict (#4685),  Fix Metric.state_dict   Update CHANGELOG.md   Update CHANGELOG.md   Detach tensors in a list if needed   Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com (cherry picked from commit e87424adfb0a4d1ddd496d97b6974f65172c3191),1
Ignore step param in Neptune logger's log_metric method (#5510),0.9544164,Ignored step param in Neptune logger's log_metric method (#5510)," Ignore step param in Neptune logger's log_metric method  The step parameter is ignored because Neptune requires strictly increasing step values, a condition which is sometimes violated in Lighting e.g. when fit() and test() are called one after another on some models. step could be enabled again once Lightning guarantees that step values are always strictly increasing. Also a minor bugfix: the log_text() method should use Neptune's log_text() method.   Update neptune.py   Update test_neptune.py   Update test_all.py   fix neptune tests   add chlog   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com (cherry picked from commit 5d76b31881398188556c186e967166d21eea9746)",1
fix error when logging to progress bar with reserved name (#5620),0.57186145,Truncated long version numbers in progress bar (#2594),  warn about duplicate metrics   update changelog   suggestions from rohit   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   multiple values in message   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,0
Fix tensor printing in trainer.test() (#5138),0.6748589,"- `Trainer.logged_metrics` now always contains scalar tensors, even when a Python scalar was logged ([#11270](https://github.com/PyTorchLightning/pytorch-lightning/pull/11270))",  Fix showing test results for tensors   Fix docs   Update pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py   Co-authored-by: Nicki Skafte skaftenicki@gmail.com  Fix lint issues  Co-authored-by: Nicki Skafte skaftenicki@gmail.com,0
mergify: less updates (#5639),0.42577213,"Updated DeepSpeed ZeRO (#6546, #6752, #6142, #6321)",  mergify: less updates   Drop label ,0
unify pep8 speaks (#5607),0.35684353,"@airglow, @akshaykvnit, @AljoSt, @AntixK, @awaelchli, @baeseongsu, @bobkemp, @Borda, @calclavia, @Calysto, @djbyrne, @ethanwharris, @fdelrio89, @hadim, @hanbyul-kim, @jeremyjordan, @kuynzereb, @luiscape, @MattPainter01, @neggert, @onkyo14taro, @peteriz, @shoarora, @SkafteNicki, @smallzzy, @srush, @theevann, @tullie, @williamFalcon, @xeTaiz, @xssChauhan, @yukw777",Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,0
Increase TPU check timeout (#5598),0.9378485,Increased TPU check timeout from 20s to 100s (#5598),  change timeout to 100   add to CHANGELOG.md   update test   updates   reduce TPU_TIMEOUT_CONSTANT during test   Update tests/utilities/test_xla_device_utils.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   patch TPU_TIMEOUT_CONSTANT   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,1
fix Neptune logger creating multiple experiments when gpus > 1 (#3256),0.6067264,- The `NeptuneLogger` now uses `neptune.init_run` instead of the deprecated `neptune.init` to initialize a run ([#15393](https://github.com/Lightning-AI/lightning/pull/15393)),  DP device fix   potential fix   fix merge   update tests   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,0
name typo (#5612),0.57058805,Renamed several Trainer atributes:  (#567),,0
update mergify (#5608),0.49941185,"Updated DeepSpeed ZeRO (#6546, #6752, #6142, #6321)",  try fix update and review   less spam ,0
Update help steps (#5564),0.53860706,Renames model steps (#1051),  Update help steps   Update README.md   Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com   Update README.md   Update README.md   Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com,0
hotfix: drop bad pyyaml version (#5606),0.5567157,- Fixed security vulnerabilities CVE-2020-1747 and CVE-2020-14343 caused by the `PyYAML` dependency ([#11099](https://github.com/PyTorchLightning/pytorch-lightning/pull/11099)),  drop potetionaly bad version   * ,0
fix PyPI releasing (#5605),0.4481755,Removed dependency on pandas (#736),,0
Mnodes (#5020),0.44151837,We cleaned up the properties related to device indices (#14829)., add a multi-nodesworkflow  Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz,0
Add note about returning None (#5578),0.7067295,    return None,Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
Add new CHANGELOG section (#5580),0.7158478,Full Changelog,,1
[BugOnFeat] Resolve bug with Finetuning (#5744),0.8768854,Resolve bug with Finetuning (#5744),  resolve bug + add doc   Update pytorch_lightning/callbacks/finetuning.py   resolve bug   start adding more test   add more tests for finetuning callback functions   rename to flatten_modules   resolve doc   Update pytorch_lightning/callbacks/finetuning.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   resolve comments   remove update on BoringModel   update on comments   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
Merge remote-tracking branch 'carmocca/sync-1.1.5' into release/1.2-dev,0.39128095,This conversation was marked as resolved by carmocca,,0
"Change the seq of on_train_batch_end, on_batch_end & on_train_epoch_end, on_epoch_end hooks (#5688)",0.9756477,"Changed the seq of on_train_batch_end, on_batch_end & on_train_epoch_end, on_epoch_end hooks (#5688)",,1
Force ModelCheckpoint callback to run last (#5731),0.77043605,Forced ModelCheckpoint callbacks to run after all others to guarantee all states are saved to the checkpoint (#5731),,1
drop deprecated docs (#5768),0.6292692,Deprecation warning (#3844),  drop deprecated docs   notes   . ,0
Fix sync,0.534425,Remove .item which causes sync issues (#1254),resolve wrong merge tpu yapf,0
Prepare 1.1.5 release (#5576),0.5308562,[1.5.1] - 2021-11-09,Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
fix argparse conflicting options error (#5569),0.7249187,argparse_utils >> argparse,Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
Fix root node resolution in slurm environment,0.5113651,Fix an issue with the SLURM srun detection causing permission errors (#15485),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: chaton thomas@grid.ai,0
Drop greetings comment (#5563),0.35842744,"@ananthsub, @awaelchli, @carmocca, @ddrevicky, @louis-she, @mauvilsa, @rohitgr7, @SeanNaren, @tchaton",Co-authored-by: chaton thomas@grid.ai,0
Fix command line run for refinforce_learn_qnet in pl_examples  (#5414),0.5575816,"All CLI commands now need to include the Trainer method to run as the first command, i.e., one of fit, validate, test, predict.",  fix wrong argument in argparse   remove wrong default arg in argparser   disable add help argparse ,0
Fix logging on_train_batch_end in a callback with multiple optimizers (#5521),0.7005172,  * `Callback.on_batch_end` in favor of `Callback.on_train_batch_end`,  Start with the failing test   Then fix the failing test   Update CHANGELOG ,1
Fix val_check_interval with fast_dev_run (#5540),0.65606314,Updated fast_dev_run to accept integer representing num_batches (#4629),  fix val_check_interval with fast_dev_run   chlog ,0
[bugfix] Fix signature mismatch in DDPCPUHPCAccelerator's model_to_device (#5505),0.58915985,We cleaned up the properties related to device indices (#14829).,  Update ddp_cpu_hpc_accelerator.py   Update CHANGELOG.md ,0
fix reinit_schedulers with correct optimizer (#5519),0.7218516,Disabled lr_scheduler.step() in manual optimization  (#6825),  update test   syntax   fix   update test   scheduler   only apex   fix   rev drone   chlog ,1
Tensorboard Docu about Hyperparams saving (#5158),0.60994804,Parsing of enums type hyperparameters to be saved in the haprams.yaml file by TensorBoard and CSV loggers has been fixed and made in line with how OmegaConf parses it (#9170),  Add documentation to tensorboard   Remove unnecessary whitespaces   Update pytorch_lightning/loggers/tensorboard.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   Add metrics to tensorboard logger   Whitespace removed   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Fix Wrong exception message (#5492),0.541366,Used raise .. from .. to explicitly chain exceptions (#3750),  Update accelerator_connector.py   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
reconfigure mergify (#5499),0.37474245,"Renamed step_idx to step, epoch_idx to epoch, max_num_epochs to max_epochs and min_num_epochs to min_epochs (#589)",  configure mergify   drop gha   drop commented section   Co-authored-by: github-actions[bot] 41898282+github-actions[bot]@users.noreply.github.com Co-authored-by: Roger Shieh sh.rog@protonmail.ch,0
Fix visual progress bar bug / properly reset progress bar (#4579),0.6700111,Better progress bar (#16695),  reset   fix reset   changelog   update chlog   typing   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: github-actions[bot] 41898282+github-actions[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
add section & add testing ckpt 1.1.4 (#5495),0.45992178,"trainer.ckpt_path = ""/path/to/checkpoint.ckpt""",  add section   test legacy   Co-authored-by: github-actions[bot] 41898282+github-actions[bot]@users.noreply.github.com,0
Refactor setup_training and remove test_mode (#5388),0.9716281,Refactored setup_training and remove test_mode (#5388),"  ref and fix call for on_pretrained_routine   avoid failing tests   unnecessary_call   unnecessary call in accelerators   tmpdir   rm test_mode   pep   updates   more ref   Revert ""more ref""   This reverts commit 5d9e95f87343a4d9853eb30ca883d1dbfba369c6.  more refac  Co-authored-by: github-actions[bot] 41898282+github-actions[bot]@users.noreply.github.com",1
pipeline release CI (#5494),0.4998545,The point of this release is more bug fixes ahead of v 1.0.0. We now have CI tests on TPU thanks to @zcain117 from Google! :slightly_smiling_face:,  pipeline release CI   trigger   trigger   .   t1   t2   t1   t2 ,0
fix typo in multi-gpu docs (#5402),0.54936856,"Deprecated the use of Trainer(gpus=""i"") (string) for selecting the i-th GPU; from v1.5 this will set the number of GPUs instead of the index (#6388)",Co-authored-by: github-actions[bot] 41898282+github-actions[bot]@users.noreply.github.com,0
[Docs] fix on_after_backward example (#5278),0.55795455,The on_after_backward hook is now called on accumulating iterations. Use the on_before_optimizer_step hook to mimic the old behaviour (#8328),  fix on_after_backward docs   doc fix ,0
update tests with new auto_opt api (#5466),0.5531387,We have upgrade Continues Integration to speed up the automatic testing. ,  update tests with new auto_opt api   Apply suggestions from code review   Co-authored-by: github-actions[bot] 41898282+github-actions[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
fix generate checkpoint (#5489),0.727417,Removed deprecated checkpoint argument filepath (#5321),,1
Docs: move images (#5756),0.48109844,move lr_finder (#3434),  move images   logo ,0
formatting 4/n: Trainer (#5720),0.62670124,  trainer:,  yapf trainer   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   .   fix   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
Remove psf/black references (#5739),0.42033485,Removed legacy references for magic keys in the Result object (#6016),  Update pyproject.toml   Update setup.cfg   Update test.txt   Update CONTRIBUTING.md   Update requirements/test.txt ,0
Add yapf to pre-commit (#5747),0.44108403,@akihironitta @awaelchli @Borda @carmocca @kaushikb11 @krshrimali @mauvilsa @otaj @pre-commit-ci @rohitgr7 @semaphore-egg @tkonopka @wayi1,,0
Update metrics to use Enum (#5689),0.6076593,Updated metrics to use LightningEnum (#5689)," Add DataType, AverageMethod and MDMCAverageMethod",0
add missing logic to new plugins and accelerator (#5734),0.7954362,Refactored Accelerators and Plugins (#5743),  add missing logic   missed imports   import fixes   isort   mv f   changelog   format   move helper to parallel plugin   d ,1
Refactor access to trainer attributes in LightningModule (#5730),0.7123139,Deprecated the LightningModule.datamodule getter and setter methods; access them through Trainer.datamodule instead (#7168),  rank access   tests for property   weekref   logger   changelog   torchscript   changelog   chlog   .   amp   yapf   flake8   Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz,1
Accelerator Refactor/RPC + Sharded (#5732),0.6594074,Refactored Accelerators and Plugins (#5743),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz,0
Hardware specific parts of Accelerator Refactoring (#5719),0.74501157,Refactored Accelerators and Plugins (#5743),  add basic accelerator class. Co-Authored with @awaelchi   pep8   Co-authored-by: @awaelchi  add cpu accelerator  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  add gpu accelerator  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  add tpu accelerator  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  add accelerator connector  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  add single device training  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  add single tpu  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  add tpu spawn  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   make on_colab_kaggle utility func   add basic accelerator class. Co-Authored with @awaelchi   pep8   Co-authored-by: @awaelchi  add cpu accelerator  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  add gpu accelerator  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  add tpu accelerator  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  add accelerator connector  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  add single device training  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  add single tpu  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  add tpu spawn  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   make on_colab_kaggle utility func   fixes   move   yapf   .   .   .   flake8   sync accelerator connector changes from dev1.2   changelog   fix tpu handling   tpu   aval   yapf   Update pytorch_lightning/plugins/training_type/tpu_spawn.py   Co-authored-by: chaton thomas@grid.ai  Update pytorch_lightning/accelerators/accelerator_connector.py  Co-authored-by: chaton thomas@grid.ai  Update pytorch_lightning/plugins/training_type/tpu_spawn.py  Co-authored-by: chaton thomas@grid.ai   Update tpu_spawn.py   Update pytorch_lightning/accelerators/accelerator_connector.py   Co-authored-by: chaton thomas@grid.ai  indentation  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz Co-authored-by: chaton thomas@grid.ai,1
formatting 6/n: metrics (#5722),0.5649407,Removed deprecated metrics (#6161),  yapf metrics   op ,0
Accelerator Refactor: Precision Plugins (#5718),0.8265931,Precision Plugins (#5718),  add basic accelerator class. Co-Authored with @awaelchi   add basic trainign type plugin. Co-Authored with @awaelchi   pep8   Co-authored-by: @awaelchi  update copyright  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  add apex_amp  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  add mixed base class  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  add native amp  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  add native amp sharded  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  add tpu bfloat  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  add inits  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update precision_plugin.py  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
accelerator refactor - add parallel plugins  (#5714),0.7513216,Refactored Accelerators and Plugins (#5743),Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,1
Refactor LightningDataParallel (#5670),0.7776947,Deprecated LightningDataParallel in favor of new wrapper module LightningParallelModule (#5670),  module   fix model access   scalar conversion   refactor   kwargs   auto unsqueeze   refactor code duplication   clean up   docs   update dp docs   changelog   generalize test   test   rename   warning cache   isort   unsqueezing test   device   device   scalar test   device   device   include coverage of overrides   clear   add deprecation test   docs   improve coverage   increase coverage   fix merge   extend test   rename base class   mention the predict method in docs   combine iteration over collection   remove override   move   line   Apply suggestions from code review   fix running stage   f401   fix cyclic import   Co-authored-by: github-actions[bot] 41898282+github-actions[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
Base classes for accelerator refactoring (#5715),0.75004107,Refactored Accelerators and Plugins (#5743),  add basic accelerator class. Co-Authored with @awaelchi   Add base plugin class. Co-authored with @awaelchi   add basic trainign type plugin. Co-Authored with @awaelchi   add basic precision plugin. Co-Authored with @awaelchi   Add missing inits. Co-authored with @awaelchi   pep8   Co-authored-by: @awaelchi   ignore  flake8   coverage omit   imports in init   lost   imports   flake8   .   .   chlog   Update pytorch_lightning/plugins/training_type/training_type_plugin.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/plugins/training_type/training_type_plugin.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/plugins/training_type/training_type_plugin.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/plugins/training_type/training_type_plugin.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/plugins/training_type/training_type_plugin.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/plugins/training_type/training_type_plugin.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/plugins/training_type/training_type_plugin.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
Refactor: drop LoggerStages (#5673),0.71505713,Removed LoggerStages (#5673),  drop LoggerStages   chlog ,1
formatting to PL utils (#5713),0.47577947,- Simplify the PL examples structure (shallower and more readable),  yapf pl base   over   dist   utils   Apply suggestions from code review   flake8   neew way ,0
yapf examples (#5709),0.36951634,Simplify the PL examples structure (shallower and more readable) (#1247),,0
add nvidia docker image (#5668),0.42461115,"adding compute environments (#3837, [#3842)",Co-authored-by: github-actions[bot] 41898282+github-actions[bot]@users.noreply.github.com,0
LR scheduler docs update (#5678),0.6629366,          lr_scheduler.step(),  doc updates   typo suggestions by rohit   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Apply suggestions from code review  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  update based on suggestions  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
drop bots (#5679),0.41333592,"decoupled DDP, DDP spawn (#3733, #3817, #3819, #3927)",,0
define Yapf config (#5591),0.41331768,Configuration Validator (#9779),  define YAPF   add check   add check   add temp ignore   apply yapf   ex ,0
[feat] 1/2 Add trainer.predict  (#5579),0.6250097,adding Trainer.tune() (#3293),  start adding predict   add predict   resolve test   add predict   remove limit_predict   update   add test for predict   typo   update on comments   remove predict_step   update ddp_shareded   check ddp_sharded   resolve on comments   resolve isort   update dp   add test dp 1 gpu   made default forward   resolve path   resolve bug   update on comments   resolve doc   resolve bug   update   resolve bug   update on comments   resolve pep8   update test doc   update on comments   solve special tests   resolve bug   resolve flake8   Update pytorch_lightning/callbacks/progress.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update pytorch_lightning/trainer/trainer.py  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   add predict to LightningModule   missing predict   typo   rename is_prediction to _predicting   add   update   update   update doc   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,0
[Metrics] AUC/AUROC class interface (#5479),0.6700893,Deprecated reorder parameter of the auc metric (#4237),  base files   auc done   init files   auc class interface   fixing auc   more fixes   working auroc   update auc   add docs   remove leftovers from merge   suggestions   fix f-string   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   add deprecated tests   make logic clearer   Update pytorch_lightning/metrics/classification/auroc.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   fix   fix   fix docs   fix isort   fix deprecated test   fix tests   fix tests   fix isort   Apply suggestions from code review   add enum   deprecate old impl   update from suggestions   chlog   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz,0
Fix isort failures in core (#5526),0.4959729,"Removed experimental fault-tolerance support (#16516, #16533)",Remove from skipped module in pyproject.toml and fix failures on: - pytorch_lightning/core/*.py,0
[Feat] Adding PruningCallback (#5618),0.4077184,Removed deprecated: (#2760),  wip   add pruning callback   add condition for duplicated weights   update on comments   update on comments   update on comments   add more tests   resolve flake8   resolve on comments   update changelog   update on comments   update on comments   change order   remove ddp_spawn skip   update   typo   Update pytorch_lightning/callbacks/pruning.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/callbacks/pruning.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   update on comments   forgot platform   update on comments   remove     @rank_zero_only   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,0
Refactor: legacy accelerators and plugins (#5645),0.9282224,Refactored Accelerators and Plugins (#5743),  tests: legacy   legacy: accel   legacy: plug   fix imports   mypy   flake8 ,1
Start version suffixes at 1 (#5008),0.5725011,Changed ModelCheckpoint version suffixes to start at 1 (#5008),  Rename original filepath to v0   Clean-up   Suggestions from code review   Revert renaming. Start version number at 1   Add ModelCheckpoint.STARTING_VERSION   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   Add note about class attributes   Update CHANGELOG   Fix doc   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
move to Pages dir (#4869),0.45787352,move lr_finder (#3434),  folders   common / advanced / extensions   paths   flake8   isort   Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,0
hotfix: skip unsupported metric compostions (#5664),0.6410581,"Removed experimental Metric API (#3868, #3943, #3949, #3946), listed changes before final removal:",  skip PT unsupported compositions   flake8 ,0
Compositional metrics (#5464),0.60618466,Sklearn metrics classes (#1327),  implement compositional metrics   implement composition functions for metrics   test compositions   docs   pytest   pep8   fix argument resolution   return all kwargs if filtering not possible   fix typo   implement hashing   Update pytorch_lightning/metrics/compositional.py   Co-authored-by: Nicki Skafte skaftenicki@gmail.com  Update docs/source/metrics.rst  Co-authored-by: Nicki Skafte skaftenicki@gmail.com   add representation   Apply suggestions from code review   Update docs/source/metrics.rst   Co-authored-by: Nicki Skafte skaftenicki@gmail.com   chlog   flake8   Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz,0
chlog,0.49998724,To log:,,0
fix logging test,0.6990878,Cleaning up stale logger tests (#3490),,0
fix imports / isort / flake8,0.41764647,There were two different ways of importing Lite in <= 1.9.0,,0
update CHANGELOG.md (#5482),0.5654886,Full Changelog,(cherry picked from commit 652df1886abc63820d956aef7a468ff4cf1bea7f),0
Fix merge _module_available,0.5349791,  * Removed the deprecated `pl_module` argument from the distributed module wrappers,,0
drop duplicated func _module_available,0.45110375,Renamed utils modules (#5199),,0
Fix imports & issues in lightning optimizer refactor merge,0.56156933,- Manual optimization is now required for working with multiple optimizers ([#16539](https://github.com/Lightning-AI/lightning/pull/16539)),,0
ci: update recurent events (#5480),0.3992635,Resolve TPU miss rendezvous (#6781),  ci: update recurent events   split events   .   .   Co-authored-by: github-actions[bot] 41898282+github-actions[bot]@users.noreply.github.com (cherry picked from commit c00d5709c4fb40259ad05fe827e24ad421ce2da2),0
update nightly & upgrade Twine (#5458),0.42013428,"Since 2.0 is a major release, we took the opportunity to take our APIs to the next level and make considerable changes to reduce the backwards incompatible changes in the future. To alleviate this, we will commit to continue supporting the 1.9.x line of releases by doing bug-fix releases with any important fixes that are necessary.",  update used Twine   .   .   install   install   .   .   .   .   .   .   Co-authored-by: github-actions[bot] 41898282+github-actions[bot]@users.noreply.github.com (cherry picked from commit 9611a7f8976657ae36b7c2af61178f5b80f5ce81),0
Add hydra experimental to correct location,0.5337497,better support for Hydra,,0
bugfix: Resolve interpolation bug with Hydra (#5406),0.9739748,Resolve interpolation bug with Hydra (#5406) ,  resolve bug   Apply suggestions from code review   resolve package import   resolve import   update on comments   update on comments   hacky fix   update   exit   update   to_container   typo   resolve import   update   resolve pep8   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Sean Naren sean.narenthiran@gmail.com (cherry picked from commit bb5031b3bf7a9b5afac0d2918b7476f3f887ee35),1
Fix merge issue,0.55877894,"merge backends (#3476, #3477, #3478, #3480, #3482)",,0
deprecate enable_pl_optimizer as it is not restored properly (#5244),0.8703303,Changed deprecated enable_pl_optimizer=True (#5244),"  update   clean test   still in progress   udpdate test   update   update   resolve flake   add test for zero_grad   update   works without accumulated_grad   update   update   resolve amp   revert back to True   update   clean tests   cleaned out   typo   update test   git repare bug   remove print   udpate   Fix formatting/optimizer imports   Refactor the test for cleanliness   Add vanilla model to the test, better var names   Fixed var names, let's clean up these mock tests   repare test   update test   resolve flake8   add manual_optimization   update tests   resolve flake8   add random accumulate_grad_batches   improve test   Update tests/trainer/optimization/test_parity_automatic_optimization.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update tests/trainer/optimization/test_parity_automatic_optimization.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   update   clean tests   correct bug   Apply suggestions from code review   format   adress comments   update on comments   wip   typo   depreceate enable_pl_optimizer   resolve latest bugs   update   resolve merge   add comment   Update pytorch_lightning/core/lightning.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update tests/deprecated_api/test_remove_1-3.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/trainer/connectors/optimizer_connector.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/trainer/trainer.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/trainer/trainer.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update tests/trainer/optimization/test_parity_automatic_optimization.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   update on comments   update restore   add a property   remove setstate as not needed anymore   update test   provide optimizer to on_before_zero_grad   update on comments   update on comments   Update pytorch_lightning/trainer/trainer.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update tests/trainer/optimization/test_parity_automatic_optimization.py  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update tests/trainer/optimization/test_parity_automatic_optimization.py  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update tests/trainer/optimization/test_parity_automatic_optimization.py  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   mofidy import   update changelog   resolve flake8   update   update   clean doc   Co-authored-by: SeanNaren sean@grid.ai Co-authored-by: Ubuntu ubuntu@ip-172-31-62-109.ec2.internal Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Sean Naren sean.narenthiran@gmail.com (cherry picked from commit f2e99d617f05ec65fded81ccc6d0d59807c47573)",1
[docs] Add ananthsub to core (#5476),0.42322144,This release has breaking API changes. See #124 for all details. ,  Update test_manual_optimization.py   Update governance.rst   Update test_manual_optimization.py   Update test_manual_optimization.py   (cherry picked from commit d30e316a35c0246296a4f62cf964f131759e851f),0
[BUG] Check environ before selecting a seed to prevent warning message (#4743),0.95458126,Check environ before selecting a seed to prevent warning message (#4743),"  Check environment var independently to selecting a seed to prevent unnecessary warning message   Add if statement to check if PL_GLOBAL_SEED has been set   Added seed test to ensure that the seed stays the same, in case   if   Delete global seed after test has finished   Fix code, add tests   Ensure seed does not exist before tests start   Refactor test based on review, add log call   Ensure we clear the os environ in patched dict   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: chaton thomas@grid.ai (cherry picked from commit 635df27880521c29d2d8d094e5f790a6658ad5a3)",1
populate some more legacy checkpoints (#5457),0.7213759,all the checkpoint issues should be gone now (including backward support for old checkpoints),  populate some more legacy checkpoints   .   pt freeze   .   skip   Co-authored-by: chaton thomas@grid.ai (cherry picked from commit f065ea65bf4a577e2fe050049bbdbcb0cad5effc),1
Add automatic optimization property setter to lightning module (#5169),0.99999976,Add automatic optimization property setter to lightning module (#5169),  add automatic optimization property setter to lightning module   Update test_manual_optimization.py   Co-authored-by: chaton thomas@grid.ai (cherry picked from commit 87482935a3d03cab3ffff336c88e0ae977a2beee),1
GH action - auto-update PRs (#5451),0.4067543,moved TPU xxx_step to backend (#3118),  GH action - auto-update PRs   .   (cherry picked from commit 92bbf2fdd6509a273795a9bda08d95cd952c7791),0
fix typos in validation_step and test_step docs (#5438),0.6314112,"Deprecated model steps training_end, validation_end and test_end (#1051, #1056)",  fixed docs in lightning.py   few more   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com (cherry picked from commit 499d5031e87e8a4ba54059ced69337f53b94e66f),0
GH action - label conflicts (#5450),0.36189634,Renames model steps (#1051),  GH action - label conflicts   .   trigger   trigger   .   (cherry picked from commit f1e28d1e436b852a92d6d2dc5ae7c080b006c748),0
[bugfix] Logging only on not should_accumulate() during training (#5417),0.8875028,Logging only on not should_accumulate() during training (#5417),  resolve bug   resolve tests   update   Update tests/loggers/test_tensorboard.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com (cherry picked from commit a053d758d03558d2aa5a328b2f6befbc133a0ebc),1
[bug-fix] Call transfer_batch_to_device in DDPlugin   (#5195),0.6487697,"With DPStrategy, the batch is not explicitly moved to the device (#11780)",  hacking out   update   remove useless on_before_forward   update   remove overriden   iremove os   use on_before_forward   resolve flake8   add test   update   add single_process_per_device   resolve flake8   update   resolve   update   update   update   add comment   resolve bug with sharded   update   remove property   update   resolve test   resolve bug   update on comments   update doc   Update pytorch_lightning/core/hooks.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   update on comments   Update pytorch_lightning/plugins/ddp_plugin.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update pytorch_lightning/plugins/ddp_plugin.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   resolve pep8   add device_ids to pipe   update on comments   update   resolve   update   update   update   Co-authored-by: Ubuntu ubuntu@ip-172-31-62-109.ec2.internal Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Sean Naren sean.narenthiran@gmail.com (cherry picked from commit d510707bc99d43dd2cfd877428d9cc16af8b4074),0
Add 1.1.4 section to CHANGELOG (#5378),0.5738324,Full Changelog,(cherry picked from commit 019e4ff8cddadfde4d3bd48b4e8b8d294950ca2a),0
tests for legacy checkpoints (#5223),0.6879545,all the checkpoint issues should be gone now (including backward support for old checkpoints),  wip   generate   clean   tests   copy   download   download   download   download   download   download   download   download   download   download   download   flake8   extend   aws   extension   pull   pull   pull   pull   pull   pull   pull   try   try   try   got it   Apply suggestions from code review   (cherry picked from commit 72525f0a8396ae6dce5cf78ddf71e75fbba2dbfc),0
Fix pre-commit trailing-whitespace and end-of-file-fixer hooks. (#5387),0.55407923,moved ___step_end hooks (#3130),(cherry picked from commit 4c6f36e6e14a5e3bace1fe32505ae0fe6f8bc682),0
docker: run ci only docker related files are changed (#5203),0.56946355,"Show a message when BuildConfig(dockerfile=""..."") is passed but a Dockerfile file is already present in the Work (#15799)",  only run ci on docker related files   docker related files changed!   install pytorch along with cudatoolkit   build docker only on SUN   conda exit status has been fixed   reverts back to old conda version   add more docker related files   conda env update --name   create env and install pytorch again   create env and install pytorch again   ${PYTORCH_CHANNEL}   dont update pytorch with conda env update   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   Update dockers/base-conda/Dockerfile   Apply suggestions from code review   remove checks in cron job   Apply suggestions from code review   readd #   readd #   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Roger Shieh sh.rog@protonmail.ch (cherry picked from commit cc624358c8e396e966f9c51b3010f6a986047fc6),0
"Update sharded install to latest fairscale release, add reasoning why fork required for sequential parallelism (#5380)",0.64897305,Removed support for FairScale's sharded training (strategy='ddp_sharded'|'ddp_sharded_spawn'). Use Fully-Sharded Data Parallel instead (strategy='fsdp') (#16329),(cherry picked from commit ee8373110aa89f1049d7ac53c5d491e7eba68cf1),0
[feat] Add PyTorch Profiler. (#5560),0.5914835,| Import pytorch_lightning.profiler                                                                          | 1.9             | pytorch_lightning.profilers                   |,  add profiler   add profiler   update   resolve flake8   update doc   update changelog   clean doc   delete prof file   merge pr codebase   update   update doc   update doc   update doc   update on comments   update docstring   update docstring   try   update test   Update pytorch_lightning/profiler/init.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/profiler/init.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   update on comments   remove old code   add support for ddp   resolve flake8   Update pytorch_lightning/profiler/init.py   Co-authored-by: Sean Naren sean.narenthiran@gmail.com   resolve tests   resolve flake8   Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Sean Naren sean.narenthiran@gmail.com,0
docs cleaning - testcode (#5595),0.50220937,"Cleaning (#5948, #5949, #5950)",  testcode - python   revert   simple   testcode @rst   pl   fix   pip   update   conf   conf   nn.   typo ,0
prune deprecated EvalResult (#5633),0.7773474,Removed deprecated EvalResult (#5633),  prune EvalResult   drop tests   drop usage   drop class   prune ,1
fix nightly releasing (#5596),0.53593826,This release fixes that core issue,,0
generalise setup tools (#5617),0.63695776,from setuptools import setup,  generalize setup tools   drop unused   imports   ci ,0
fix docs render (#5610),0.47996092,Docs improvements,,0
try to update failing dockers (#5611),0.46422055,Remove unnecessary intermediate layers in Dockerfiles (#5697),,0
flake8 & isort (#5647),0.39588407,"@Borda, @justusschock, @kandluis, @mauvilsa, @shuyingsunshine21, @tchaton",,0
Fix loading yaml (#5619),0.5285711,    hparams_file='/path/to/hparams_file.yaml',  fix yaml   chlog ,0
docs: Add BackboneLambdaFinetuningCallback (#5553),0.7298405,Changed computer_vision_fine_tunning example to use BackboneLambdaFinetuningCallback (#5377),  Add and fix the docs of BackboneLambdaFinetuningCallback   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Roger Shieh sh.rog@protonmail.ch Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,1
summarize total size of model params in bytes (#5590),0.62649125,Changed the model size calculation using ByteCounter (#10123),  simplified model size calc   fix spaces   fix newlines   minor refactor   Update pytorch_lightning/core/memory.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   make model size property   fix doctest   Update pytorch_lightning/core/memory.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   remove explicit doctest from file   better docs   model precalculate size 1.0 mbs   better comment   Update tests/core/test_memory.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update tests/core/test_memory.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   merge _model_size into model_size property itself   minor comment fix   add feature to changelog   added precision test   isort   minor def name typo   remove monkeypath set env as boringmodel wont need any torch hub cache   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,0
fix: freeze mypy (#5634),0.44599736,Renamed reset_on_epoch to reset_on_run (#9658),  update mypy for tests   freeze ,0
feat(wandb): add sync_step (#5351),0.71554685,Removed the deprecated sync_step argument from WandbLogger (#8763),  docs(wandb): add details to args   feat(wandb): no sync between trainer and W&B steps   style: pep8   tests(wandb): test sync_step   docs(wandb): add references   docs(wandb): fix typo   feat(wandb): more explicit warning   feat(wandb): order of args   style: Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  style: long line  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Sean Naren sean.narenthiran@gmail.com,1
Docs: upgrade packages (#5600),0.48459944,Parsed local package versions (#13933),  upgrade docs packages   cmd   -cmd ,0
add possibility for nested loaders (#5404),0.6199117,# pass loaders as list. This will create batches like this:,  add possibility for nested loaders   pep8: newline ,0
clarify Trainer running state atribs. (#5589),0.71828055,"Refactor RunningStage and TrainerState usage (#4945, #7173)",  update Trainer is_ attributes   tests   more   isort   split   rename   check   fix ,1
hotfix: drop pyyaml 5.4.* [feat-1.2] (#5609),0.57521176,- Fixed security vulnerabilities CVE-2020-1747 and CVE-2020-14343 caused by the `PyYAML` dependency ([#11099](https://github.com/PyTorchLightning/pytorch-lightning/pull/11099)),,0
Set progressbar refresh rate in Google Colab (#5516),0.84350157,Changed the default value for the progress_bar_refresh_rate Trainer argument in Google COLAB notebooks to 20 (#5516),  refresh   add tests   docs   chlog   chlog   Update pytorch_lightning/trainer/trainer.py   Co-authored-by: Nicki Skafte skaftenicki@gmail.com  update docstring  Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: chaton thomas@grid.ai,1
Raise TypeError instead of using asserting with condition of types. (#5536),0.515301,Changed type checker with explicit cast of ref_model object (#4457),Note that assert are being removed in optimized with compiling to optimised byte code (python -o producing *.pyo files).,0
Fix isort failures in trainer (#5529),0.61479557,Trainer(truncated_bptt_steps=2),Remove from skipped module in pyproject.toml and fix failures on: - pytorch_lightning/trainer/*.py,0
Fix isort failures in metrics (#5528),0.61902964,Removed deprecated metrics (#8586),Remove from skipped module in pyproject.toml and fix failures on: - pytorch_lightning/metrics/*.py,0
Classification metrics overhaul: precision & recall (4/n) (#4842),0.85858864,Classification metrics overhaul (#4837),  Add stuff   Change metrics documentation layout   Add stuff   Add stat scores   Change testing utils   Replace len(.shape) with .ndim   More descriptive error message for input formatting   Replace movedim with permute   PEP 8 compliance   WIP   Add reduce_scores function   Temporarily add back legacy class_reduce   Division with float   PEP 8 compliance   Remove precision recall   Replace movedim with permute   Add back tests   Add empty newlines   Add precision recall back   Add empty line   Fix permute   Fix some issues with old versions of PyTorch   Style changes in error messages   More error message style improvements   Fix typo in docs   Add more descriptive variable names in utils   Change internal var names   Revert unwanted changes   Revert unwanted changes pt 2   Update metrics interface   Add top_k parameter   Add back reduce function   Add stuff   PEP3   Add depreciation   PEP8   Deprecate param   PEP8   Fix and simplify testing for older PT versions   Update Changelog   Remove redundant import   Add tests to increase coverage   Remove zero_division   fix zero_division   Add zero_div + edge case tests   Reorder cls metric args   Add back quotes for is_multiclass   Add precision_recall and tests   PEP8   Fix docs   Fix docs   Update   Change precision_recall output   PEP8/isort   Add method _get_final_stats   Fix depr test   Add comment to deprecation tests   isort   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   Add typing to test   Add matc str to pytest.raises   Co-authored-by: chaton thomas@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
Fix pre-commit isort failure on pytorch_lightning/accelerators (#5503),0.69590425,- Removed `model_sharded_context` method from `Accelerator` ([#10886](https://github.com/PyTorchLightning/pytorch-lightning/pull/10886)),Remove from skipped module in pyproject.toml and fix failures on: - pytorch_lightning/accelerators/*.py,0
simple tests restructure (#5452),0.5427635,- fixed all the .test() calls,  simple tests restructure   logging_process   typo ,0
Fix isort a few failures (#5504),0.38915542,"Removed experimental fault-tolerance support (#16516, #16533)",Remove from skipped module in pyproject.toml and fix failures on: - pytorch_lightning/callbacks/.py - pytorch_lightning/cluster_environments/.py - pytorch_lightning/profiler/.py - pytorch_lightning/tuner/.py Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz,0
Fix isort failures in utilities (#5530),0.4357975,"Removed experimental fault-tolerance support (#16516, #16533)",Remove from skipped module in pyproject.toml and fix failures on: - pytorch_lightning/utilities/*.py,0
Fix isort failures in loggers (#5527),0.5835497,Changed automatic casting for LoggerConnector metrics (#5218),Remove from skipped module in pyproject.toml and fix failures on: - pytorch_lightning/loggers/*.py,0
Fix pre-commit isort failure on tests/base/*.py (#5429),0.49183595,Add missing python-multipart dependency (#17244),  Remove tests.base from skipped module in pyproject.toml   Fix pre-commit isort failure on tests/base/*.py ,0
Fix pre-commit isort failure on tests/backends/*.py (#5430),0.46482483,Add missing python-multipart dependency (#17244),  Fix pre-commit isort failure on tests/backends/*.py   Remove tests.backends from skipped module in pyproject.toml ,0
Implement log_graph for CometLogger. (#5295),0.63103247,Using .comet.config file for CometLogger (#1913),Co-authored-by: chaton thomas@grid.ai,0
Fix pre-commit isort failure on tests/loggers/*.py (#5425),0.5988939,Re-Enable Logger's ImportErrors (#1938),  Remove tests.loggers from skipped module in pyproject.toml   Fix pre-commit isort failure on tests/loggers/*.py ,0
Fix pre-commit isort failure on tests/callbacks/*.py (#5428),0.4996498,Removed legacy code to include step dictionary returns in callback_metrics. Use self.log_dict instead. (#6682),  Remove tests.callbacks from skipped module in pyproject.toml   Fix pre-commit isort failure on tests/callbacks/*.py ,0
anti-colision isort config (#5511),0.462924,Configuration Validator (#9779),,0
Fix pre-commit isort failure on tests/models/*.py (#5423),0.49252355,Users who relied ontrainer.test(ckpt_path=None)to load the latest model need to change their code totrainer.test(model)` and pass the model reference directly.,  Remove tests.models from skipped module in pyproject.toml   Fix pre-commit isort failure on tests/models/*.py ,0
Fix pre-commit isort failure on tests/trainer/*.py (#5421),0.5767649,Renamed and moved core/step_result.py to trainer/connectors/logger_connector/result.py (#7736),  Remove tests.trainer from skipped module in pyproject.toml   Fix pre-commit isort failure on tests/trainer/*.py ,0
Fix pre-commit isort failure on tests/metrics/*.py (#5424),0.5139501,Removed deprecated metrics (#8586),  Remove tests.metrics from skipped module in pyproject.toml   Fix pre-commit isort failure on tests/metrics/*.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Fix pre-commit isort failure on tests/plugins/*.py (#5422),0.48615777,Add missing python-multipart dependency (#17244),  Fix pre-commit isort failure on tests/plugins/*.py   Remove tests.plugins from skipped module in pyproject.toml   Update pyproject.toml   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Refactor LightningDistributedDataParallel (#5185),0.81898606,Deprecated LightningDistributedDataParallel in favor of new wrapper module LightningDistributedModule (#5185),"  add wrapper   add squeeze   replace LightningDistributedDP   update import   module access   inputs   refactor warning   update   resolve flake8   remove old class   set find unused params to False   update docstrings   update docs   update docs   add changelog   deprecation   rename wrapper -> module   rename pl_module   add unit tests   Revert ""add changelog""   This reverts commit 02ec0a6864f4ba2ace3bb6fc6ebc364e1a80ffd7.  Revert ""set find unused params to False""  This reverts commit 8e451515e6ba3227d00f4a5cb63f332cfedb7b30. Co-authored-by: Ubuntu thomas@grid.ai",1
set find_unused_parameters=False in DDP as in pytorch (#5435),0.8176787,"- When manual optimization is used with DDP, we no longer force `find_unused_parameters=True` ([#12425](https://github.com/PyTorchLightning/pytorch-lightning/pull/12425))",  set find unused params to False   add changelog   fix changelog   fix test   update docs   update changelog   Co-authored-by: chaton thomas@grid.ai,1
Add LambdaCallback (#5347),0.5076646,class MyCallback(Callback):,  Add LambdaCallback   docs   add pr link   Conflicts: CHANGELOG.md   convention   Fix Callback Typo   Update pytorch_lightning/callbacks/lambda_cb.py   Co-authored-by: Nicki Skafte skaftenicki@gmail.com  Update pytorch_lightning/callbacks/lambda_cb.py  Co-authored-by: Nicki Skafte skaftenicki@gmail.com  Update pytorch_lightning/callbacks/lambda_cb.py  Co-authored-by: Nicki Skafte skaftenicki@gmail.com   use Misconfigureation   update docs   sort export   use inspect   string fill   use fast dev run   isort   remove unused import   sort   hilightning   highlighting   highlighting   remove debug log   eq   res   results   add misconfig exception test   use pytest raises   fix   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update pytorch_lightning/callbacks/lambda_cb.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   hc   rm pt   fix   try fix   whitespace   new hook   add raise   fix   remove unused   rename   Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: chaton thomas@grid.ai,0
[Metrics] Disable default reset after compute (#5409),0.8018041,Metric compute() method will no longer automatically call reset() (#5409),  reset   self._cache -> cache (make cache local variable so it is not overwritten)   pep8   fix metric result integration   rm print statements   better comment   changelog   Update docs/source/metrics.rst   Co-authored-by: Roger Shieh sh.rog@protonmail.ch Co-authored-by: Teddy Koker teddy.koker@gmail.com Co-authored-by: Roger Shieh sh.rog@protonmail.ch,1
[tests/core] Updated with BoringModel and added BoringDataModule (#5432),0.5337426,Updated app testing (#16000),  update with BoringModel and introduce BoringDataModule   isort   fix   rm random_split   fix test   fix test   update   update test_results   val_step   update tests   rebase   rebase ,0
Add missing val/test hooks in LightningModule (#5467),0.66974634,1. Override LightningModule hook,  add missing val/test hooks   chlog   None   Co-authored-by: Roger Shieh sh.rog@protonmail.ch,0
simplify training phase as Enum (#5419),0.64528096,Simplified training phase as LightningEnum (#5419),  simplify training phase as Enum   tests   .   .   rename   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Apply suggestions from code review  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   rename   flake8   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
set minimal req. PT 1.4 (#5418),0.41921303,reduced all simplified forward (#3126),  set minimal req. PT 1.4   chlog ,0
unify LightningEnum (#5389),0.48292923,Updated metrics to use LightningEnum (#5689),  unify LightningEnum   hash   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update states.py  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Refactor: clean trainer device & distrib getters (#5300),0.86804277,Refactor: clean trainer device & distributed getters (#5300),  warnings   .   .   flake8   .   .   .   use_tpu   use_dp   .   use_ddp   .   use_horovod   .   .   . ,1
Fix pre-commit isort failure on tests/checkpointing/*.py (#5427),0.5690936,Updated model_checkpoint's to_yaml to use fsspec open (#3801),  Remove tests.checkpointing from skipped module in pyproject.toml   Fix pre-commit isort failure on tests/checkpointing/*.py ,0
Throw MisconfigurationError on unknown mode (#5255),0.52887607,Replace MisconfigurationException with warning in ModelCheckpoint Callback (#4560),  Throw MisconfigurationError on unknown mode   Add tests   Add match condition for deprecation message ,0
prune check on Trainer fit result (#5453),0.5991585,trainer.fit(model),  prune check on Trainer fit result   flake8   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  .  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
Fix pre-commit isort failure on tests/utilities/*.py (#5420),0.5036309,Add missing python-multipart dependency (#17244),  Remove tests.utilities from skipped module in pyproject.toml   Fix pre-commit isort failure on tests/utilities/*.py ,0
Add a performance section to TPU docs to address FAQ  (#5445),0.60230976,Enabled manual optimization for TPUs (#8458),  header   update docs   punctuation   adding another note   some more notes   Update docs/source/tpu.rst   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  punctuation  Co-authored-by: Lezwon Castelino lezwon@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: chaton thomas@grid.ai,0
Allow Callback instance as an argument of callbacks in Trainer (#5446),0.8966174,Changed callbacks argument in Trainer to allow Callback input (#5446),  fix   Update CHANGELOG   add test   fix   pep   docs   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
[tests/loggers] refactor with BoringModel (#5440),0.6283082,Cleaning up stale logger tests (#3490),  use BoringModel   use BoringModel   use BoringModel   trigger   limit_batches   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
add promxial policy optimization template to pl_examples  (#5394),0.4966218,Removed deprecated Trainer argument enable_pl_optimizer and automatic_optimization (#6163),"  add ppo rl lightning template   flake   import gym without try as in qnet example   fix import format   remove torch.optim import, not required   fix import format isort   add trainer argparse   change name of trajectory collection method   add repo in references   fix typo in comments   use isinstance to verify actionspace type   use fstring   deduplication of logic code   rename unused forloop variable   use pl.seed_everything instead   remove unused numpy import   format string printed on error   fix typo in comments   Co-authored-by: chaton thomas@grid.ai",0
Bugfix/all gather (#5221),0.57666415,Accelerator all_gather supports collection (#5221),"  resolve bug   add tests   add tests   resolve flake8   update   update   remove globals   typo   Update pytorch_lightning/utilities/distributed.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   update   update   add suport int, float   update   resolve pep8   Update pytorch_lightning/core/lightning.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update tests/utilities/test_all_gather_grad.py  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   update doc   add bool and np.ndarray   resolve conflicts   resolve conflicts   resolve pep8   add changelog   Update pytorch_lightning/core/lightning.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Ubuntu ubuntu@ip-172-31-62-109.ec2.internal Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com",0
Feat: Add BackboneLambdaFinetunningCallback (#5377),0.7128309,Changed computer_vision_fine_tunning example to use BackboneLambdaFinetuningCallback (#5377),  Feat: Add BackboneLambdaFinetunningCallback   update changelog   resolve pep8 and update changelog   add finetunning example   resolve example   iremove milestones from model   iupdate   update   Update pytorch_lightning/callbacks/init.py   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com  Update pytorch_lightning/callbacks/init.py  Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com   update   add comments   resolve test   Update pytorch_lightning/callbacks/finetuning.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update tests/trainer/logging/test_logger_connector.py  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   update on comments   resolve merge   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Enforce pre-commit to use a recent and fixed version of isort. (#5408),0.53965133,Avoid using the deprecated LooseVersion (#16162),"Previously pre-commit was using any version of isort found on developer machine. Now, used isort is from the official repository and version is set to 5.7.0 (which is nowadays the latest release). Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com",0
Reformat iou [func] and add IoU class (#4704),0.6643237,Changed iou [func] to allow float input (#4704),  added Iou   Create iou.py   Update iou.py   Update iou.py   Update CHANGELOG.md   Update metrics.rst   Update iou.py   Update iou.py   Update init.py   Update iou.py   Update iou.py   Update classification.py   Update classification.py   Update classification.py   Update init.py   Update init.py   Update iou.py   Update classification.py   Update metrics.rst   Update CHANGELOG.md   Update CHANGELOG.md   add iou   add test   add test   removed iou   add iou   add iou test   add float   reformat test_iou   removed test_iou   updated format   updated format   Update CHANGELOG.md   updated format   Update metrics.rst   Apply suggestions from code review   merge suggestions Co-authored-by: Nicki Skafte skaftenicki@gmail.com   added equations   reformat init   change format   change format   deprecate iou and test for this   fix changelog   delete iou test in test_classification   format change   format change   format   format   format   delete white space   delete white space   fix tests   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Apply suggestions from code review  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   better deprecation   fix docs   Apply suggestions from code review   fix todo   Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
[Metrics] MetricCollection (#4318),0.6791362,Sklearn metrics classes (#1327), docs + precision + recall + f_beta + refactor  Co-authored-by: Teddy Koker teddy.koker@gmail.com  rebase  Co-authored-by: Teddy Koker teddy.koker@gmail.com  fixes  Co-authored-by: Teddy Koker teddy.koker@gmail.com   added missing file   docs   docs   extra import   add metric collection   add docs + integration with log_dict   add test   update   update   more test   more test   pep8   fix doctest   pep8   add clone method   add clone method   merge-2   changelog   kwargs filtering and tests   pep8   fix test   update docs   Update docs/source/metrics.rst   Co-authored-by: Roger Shieh sh.rog@protonmail.ch   fix docs   fix tests   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   fix docs   fix doctest   fix doctest   fix doctest   fix doctest   Co-authored-by: ananyahjha93 ananya@pytorchlightning.ai Co-authored-by: Teddy Koker teddy.koker@gmail.com Co-authored-by: Nicki Skafte nugginea@gmail.com Co-authored-by: Roger Shieh sh.rog@protonmail.ch Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
added image-gradients (#4763)  [1/2] (#5056),0.5205289,    # 2. Inspect the (unscaled) gradients here,  added iamge-gradients (#4763)   fixed tests code format   made recommended fixes   removed explicit device flags   tried to fix doctest failure   pep8 and doctest fixes   added to docs/metrics   updated CHANGELOG   added the noqa flag   added suggested modification to changelog   Co-authored-by: Nicki Skafte skaftenicki@gmail.com  recommended update to docstring  Co-authored-by: Nicki Skafte skaftenicki@gmail.com  removed device from docstring  Co-authored-by: Nicki Skafte skaftenicki@gmail.com   made recommended fixes   Update CHANGELOG.md   Apply suggestions from code review   added 1-line docstrings   Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Roger Shieh sh.rog@protonmail.ch,0
[Feat] Cleanup ModelCheckpoint / EarlyStopping by moving logic to LoggerConnector (#5218),0.662958,Deprecated mode='auto' from ModelCheckpoint and EarlyStopping (#4695),  [bug-fix] Metric reduction with Logging (#5150)   add test   resolve bug   udpate test   wrongly copy / paste   update test   resolve a second bug   Co-authored-by: Ubuntu ubuntu@ip-172-31-62-109.ec2.internal   iupdate   resolve bugs   add test back   correct flake8   resolve flake8   update on comments   update tests   add a test   add test   update to Callable   Co-authored-by: Ubuntu ubuntu@ip-172-31-62-109.ec2.internal,0
fix some minor typos in docs (#5369),0.5707685,Syntax changes are: ,  fix docs typos   Apply suggestions from code review   Co-authored-by: Wansoo Kim rladhkstn8@gmail.com  flake8  Co-authored-by: Wansoo Kim rladhkstn8@gmail.com,0
test_cpu and test_gpu EvalModelTemplate deprecation (#4820),0.6413157,"Deprecated log_gpu_memory, gpu_metrics, and util funcs in favor of DeviceStatsMonitor callback (#9921)","  test_cpu refactoring - BoringModel and checkpoints; test_gpu refactoring - BoringModelboring_model refactoring - validation, testing; Fix - run_prediction as dispatcher for testing BoringModel   Removed EvalModelTemplate import from test_cpu and test_gpu   Reverting unintended changes   Issues with checkpointing   Fixed tests for logging and checkpointing   Fix for dispatcher   test_cpu refactoring - BoringModel and checkpoints; test_gpu refactoring - BoringModelboring_model refactoring - validation, testing; Fix - run_prediction as dispatcher for testing BoringModel   Removed EvalModelTemplate import from test_cpu and test_gpu   Reverting unintended changes   Issues with checkpointing   Fixed tests for logging and checkpointing   Fix for dispatcher   Fixed acc check for stocasticity of seeds   Fixed according to @borda suggestions   Hparams for boring_model   Deprecated RuntimeParamChagneModelAssing (functionality is tested in RuntimeParamChangeModelSaving)   Reduced boring_model parameters to just in and out features, test_cpu modelsinherit BoringModel to specify additional parameters (e.g., optimizer)   Fix PEP8   Update tests/base/develop_pipelines.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update tests/base/boring_model.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update tests/base/develop_pipelines.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update tests/models/test_cpu.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update tests/models/test_cpu.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   Merged test_early_stopping with all_features; added TODO for self.log   Fixed test_all_features trainer options   Ready for review!   Update tests/models/test_cpu.py   Thank you! :) Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update tests/models/test_cpu.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update tests/models/test_cpu.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update tests/models/test_cpu.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update tests/models/test_cpu.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   added optimizer_name, lr, and batch_size as hparams for save_hparameters()   Fixes for reducing PR size   Reverse test_hparams (removed DEPRECATED test for hparams direct assignment)   Changes for in_features   Fixed hparams   Fixed parameters for boring_model   Update tests/models/test_cpu.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update tests/models/test_cpu.py  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update tests/models/test_cpu.py  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   fix for pep8   Fixed run_predction and TODO   fix min acc for darwin/windows without pl_opt   eval as DEFAULT run_prediction strategy   Updated val_dataloader for running_test_no_val   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: chaton thomas@grid.ai Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com",0
fix num_workers for Windows example (#5375),0.69840217,"    num_workers=10,",  fix num_workers for Windows example   chlog   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  warn  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
drop duplicated metric helper (#5366),0.85023606,Drop duplicate metrics (#5014),  drop duplicated metric helper   .   fix tests   Co-authored-by: Nicki Skafte skaftenicki@gmail.com,1
fix formatting - flake8 + isort,0.30746657,Replaced _DataModuleWrapper with __new__ (#7289),,0
Prepare 1.1.3 release (#5365),0.51103383,[1.3.0] - 2021-05-06,  Prepare 1.1.3 release   Fix flake8 error   suppress   Remove 1.1.4 section   Add missing commits to CHANGELOG   Update PR template   Add missing commit   fix   Update CHANGELOG.md   Apply suggestions from code review   Apply suggestions from code review   Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz Co-authored-by: Jirka Borovec Borda@users.noreply.github.com (cherry picked from commit 4d9db866a11f3b4b9b923bca811911ac79dad914),0
FIX-5311: Cast to string _flatten_dict (#5354),0.47484744,Minor improvements to apply_to_collection and type signature of log_dict (#7851),  fix   params   add test   add another types   chlog   Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz Co-authored-by: Jirka Borovec Borda@users.noreply.github.com (cherry picked from commit 6536ea42fa88eda63f17dae60c51d9669b409b78),0
refactor imports of logger dependencies (#4860),0.72791266,Re-Enable Logger's ImportErrors (#1938),  refactor imports of logger dependencies   fix   fix   fix   name   fix   mocks   fix tests   fix mlflow   fix test tube   fix wandb import check   whitespace   name   name   hack   hack   rev   fix   update mlflow import check   try without installing conda dep   .   .   .   .   .   .   .   .   .   Co-authored-by: Adrian Wälchli adrian.waelchli@inf.unibe.ch Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com (cherry picked from commit ec0fb7a3ec709699243c76dae04ee1e4ce2406a0),1
Existence check for hparams now uses underlying filesystem (#5250),0.48974583,Don't raise a warning when nn.Module is not saved under hparams (#12669),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com (cherry picked from commit 410d67fbe866ee20069e88db1be729d27ae0af48),0
Updated metrics/classification/precision_recall.py (#5348),0.72565794,Renaming of precision recall metric (#3308),"There was a typo in Documentation of Code of the compute() function of Recall metric at line 210. It said ""Computes accuracy over state."" which should have been ""Computes recall over state."" (cherry picked from commit d568533b6ba0768ac7a6d028b3a25c5970a65e80)",1
Change the classifier input from 2048 to 1000. (#5232),0.5495274,# 3. Remove the `optimizer_idx` argument from `training_step`,  Change the classifier input from 2048 to 1000.   Update docs for Imagenet example   Thanks @rohitgr7  Apply suggestions from code review  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com (cherry picked from commit a40e3a325e71640786717094a67c41805ab30593),0
[bug-fix] Trainer.test points to latest best_model_path (#5161),0.7934722,trainer.test(ckpt_path=None) # load best model (NEW BEHAVIOR!),  resolve bug   update code   add set -e   Update pytorch_lightning/callbacks/model_checkpoint.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   update test   Update tests/checkpointing/test_trainer_checkpoint.py   Co-authored-by: Sean Naren sean.narenthiran@gmail.com  Update tests/checkpointing/test_trainer_checkpoint.py  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   update on comments   resolve test   convert to set   update   add error triggering   update   update on comments   update   resolve import   update   update   Update pytorch_lightning/plugins/rpc_plugin.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  update  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Ubuntu ubuntu@ip-172-31-62-109.ec2.internal Co-authored-by: Jirka Borovec Borda@users.noreply.github.com (cherry picked from commit d5b367871fa3924090ec74bf903bd172bd3e2343),1
Fix invalid value for weights_summary (#5296),0.6158922,Changed the default value of the Trainer argument weights_summary from full to top (#2029),  Fix weights_summary   use mode   fix   optional   what was I thinking   (cherry picked from commit 062800aa99cff6eb82838b355374305d9433507e),0
Allow log_momentum for adaptive optimizers (#5333),0.90554106,Allowed log_momentum for adaptive optimizers in LearningRateMonitor (#5333),  fix   fix   chlog   no momentum warning   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  ref  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com (cherry picked from commit 371daea594f1f9b6b1f3a7071688ba61fe0b335a),1
Add a check for optimizer attatched to lr_scheduler (#5338),0.80182344,Disabled lr_scheduler.step() in manual optimization  (#6825),  add a check for scheduler and optimizer   pep   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com (cherry picked from commit c7d0f4c3a29bd5524e0b66f9196f123b64d1587a),1
"Disable checkpointing, earlystopping and logging with fast_dev_run (#5277)",0.9807027,"Disabled checkpointing, earlystopping and logging with fast_dev_run (#5277)","  Disable checkpointing, earlystopping and logger with fast_dev_run   docs   chlog   disable callbacks and enable DummyLogger   add log   use dummy logger method   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: chaton thomas@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com (cherry picked from commit f7402455218e5087e61d8255a4a87a8db58a7194)",1
Add non-existing resume_from_checkpoint acceptance for auto-resubmit (#4402),0.7073108,Changed resume_from_checkpoint warning to an error when the checkpoint file does not exist (#7075),"  Add empty resume_from_checkpoint acceptance #4366   Fix general error catch with focused file check   Add fsspec HTTP extras   Add fsspec's HTTPFileSystem  support through http extras. pl has supported remote http file (e.g. #2925), so this commit do not add new functionality.   Fix potential too much logging in DDP   Add PR changelog   Add well-written argument explanation   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Fix DDP-compatible restore logging  Notify from where the states are restored. This feature temporally deleted as a result of PR review. With succeeding review, added with DDP compatibility.   Fix utility import pathes   Refactor load step commentaries   Refactor hpc ckpt suffix acquisition   Refactor restore/hpc_load match   Refactor hpc load trial   Refactor checkpoint dir check   Refactor unneeded function nest   Refactor nested If   Refactor duplicated cache clear   Refactor attempt flow with if/elif   Fix pip8   Refactor hook commentary   Co-authored-by: chaton thomas@grid.ai   Fix pep8   Refactor hpc load checkpoint path acquisition   Fix pip8   Fix typo   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Fix typo  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Fix doc  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   Refactor None Union type with Optional   Fix build-doc CI failure debuged in #5329   Fix fsspec import during build-doc #5329   Fix test epoch   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   Fix test with latest test models   .   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: chaton thomas@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: Roger Shieh sh.rog@protonmail.ch (cherry picked from commit b0051e8c036fa3312ad4d37aa7141bea64ac6148)",1
[Docs] update docs for resume_from_checkpoint (#5164),0.7653277,Changed resume_from_checkpoint warning to an error when the checkpoint file does not exist (#7075),  update docs and add pathlib support   fix   (cherry picked from commit dd442b6d335a5553961e7904cd9454b738cb72cd),1
docs: logits -> probs in Accuracy metric documentation (#5340),0.43501946,Doing this minor release because correct validation metrics logging is critical.,  fix: logits -> probs in accuracy metrics documentation   Update metrics.rst   Update metrics.rst   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com (cherry picked from commit 15a400b95faf45124f28fd8ca4ef7520d47df88a),0
"Reordered sections for intuitive browsing. (e.g. limit_train_batches was at the end of the page, far from limit_test/val_batches) (#5283)",0.5823606,"Moved attributes global_step, current_epoch, max/min_steps, max/min_epochs, batch_idx, and total_batch_idx to TrainLoop (#7437)",Co-authored-by: Jirka Borovec Borda@users.noreply.github.com (cherry picked from commit 0e593fb6a8b6abadb61f7cb6754b79b2117d7f0f),0
black formatting and migrated to self.log logging in finetuning example (#5229),0.5492836,switched from print to logging,  black formatting and migrated to self.log logging   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  migrated to accuracy in the metrics package  migrated to accuracy in the metrics package   removed trailing whitespace   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com (cherry picked from commit 17a0784c5e9789ec06e720f5f4a06a80756e73fa),0
uniques docs artefact name (#5336),0.39585266,Renamed xxx_AVAILABLE as protected (#5082),(cherry picked from commit 51af3957fc7f18a4ce101c6da2a8177a36217d74),0
update isort config (#5335),0.40321317,and run the command that was configured:,  update isort config   apply   (cherry picked from commit 724f1051f0cef154999dbeb7c1ce468ff13a8da5),0
supports --num-nodes on DDPSequentialPlugin() (#5327),0.7303836,Deprecated num_nodes and sync_batchnorm arguments in DDPPlugin and DDPSpawnPlugin (#7026),(cherry picked from commit d20fd8e5ab1a52747fee2cd53290a679d8b726d0),1
refactor python in GH actions (#5281),0.44747904,Refactoring,  refactor python in GH actions   .  .  (cherry picked from commit ab7512d7ba0b522114f97db58c8d0b9555d7b75a),0
[Docs] Mention that datamodules can also be used with .test() method (#5286),0.5270266,"Deprecated DataModule properties: has_prepared_data, has_setup_fit, has_setup_validate, has_setup_test, has_setup_predict, has_teardown_fit, has_teardown_validate, has_teardown_test, has_teardown_predict (#7657)",  docs   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  ref  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com (cherry picked from commit 64163c2662e0aae420388f84ca1525f25bd23e24),0
Fixed typo in docs for optimizer_idx (#5310),0.68032104,Removed the deprecated optimizer_idx from training_step as an accepted argument in manual optimization (#8576),"There were four instances where optimizer_idx was the argument for optimizer_step() under Step optimizers at arbitrary intervals. However, instead of using it what was being used inside (erroneously) was optimizer_i. (cherry picked from commit dd98a60e901ccd511136bd955e0964eaa5b4e8dd)",0
Fix metric state reset (#5273),0.71019894,Metric compute() method will no longer automatically call reset() (#5409),  Fix metric state reset   Fix test   Improve formatting   Co-authored-by: Ananya Harsh Jha ananya@pytorchlightning.ai (cherry picked from commit 4913cbb987a0516f8b33c016134b19c0588d107a),1
[Metrics] [Docs] Add section about device placement (#5280),0.5398393,device_stats = DeviceStatsMonitor(),  update docs   Update docs/source/metrics.rst   Co-authored-by: Shreeyak shreeyak.sajjan@gmail.com  Update docs/source/metrics.rst  Co-authored-by: Shreeyak shreeyak.sajjan@gmail.com  Update docs/source/metrics.rst  Co-authored-by: Shreeyak shreeyak.sajjan@gmail.com   Update docs/source/metrics.rst   Update docs/source/metrics.rst   Update docs/source/metrics.rst   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update docs/source/metrics.rst  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   Update docs/source/metrics.rst   Update docs/source/metrics.rst   try fix failing doc test   Co-authored-by: Roger Shieh sh.rog@protonmail.ch Co-authored-by: Shreeyak shreeyak.sajjan@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com (cherry picked from commit dabfeca92e0702e55f09ac53e9412672cd258cd3),0
Apply isort to pl_examples/ (#5291),0.44973224,Simplify the PL examples structure (shallower and more readable) (#1247),  Remove examples from isort ignore list   Apply isort   (cherry picked from commit 0c7c9e85404ce4be33cc65f95a029b6bc03d84e4),0
remove docs (#5287),0.5413939,Remove MetricsHolder (#7909),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com (cherry picked from commit eb1d61caaf49e6b87c0edd50472eb3474da17376),0
Trainer.test should return only test metrics (#5214),0.7590902,trainer.test(model),  resolve bug   merge tests   (cherry picked from commit 9ebbfece5e2c56bb5300cfffafb129e399492469),1
Fix typo in doc (#5270),0.58480066,Syntax changes are: ,(cherry picked from commit d1e97a4f114a285349e31e330c7bf8937bc1ee04),0
Minor doc fixes (#5139),0.60072505,Docs improvements,  minor doc fix   minor doc fix   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  suggestions  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com (cherry picked from commit 8d8098c04e716c8b9ccf5bc9208dd4f23b2b8e14),0
Update README.md (#5018),0.5449785,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com (cherry picked from commit 90c1c0f68b4983c685e9d009482890e578800439),0
Add TPU example (#5109),0.59199244,TPU training (#2708),  Add TPU example   add badge   add badge   add badge   bullets   name   trigger   add dataset name   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: chaton thomas@grid.ai (cherry picked from commit b930b5f2220f800c9f22eb74b19b3bcf8478c735),0
Fix typo in Trainer.test() (#5226),0.7874198,trainer.test(),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com (cherry picked from commit 1d533074b3b84729bd76c29456b750ba8f151d81),1
fix typo in Optimization (#5228),0.7192205,Refactored optimizer (#4658),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com (cherry picked from commit 5651c9cc1ab37efa48332a35b7d55c33da1bab7d),1
skip some description from pypi (#5234),0.44007963,py,  skip some description from pypi   flake8   (cherry picked from commit 9b3c6a3e843837996d890c9e02823889a10b1d77),0
update PR template (#5206),0.4134921,You can find a migration guide for this change in this PR's description.,Co-authored-by: Rohit Gupta rohitgr1998@gmail.com (cherry picked from commit b22b1c2df25156f6d93eb3d8b3c85cf7b072e57e),0
releasing feature as nightly (#5233),0.4396314,The point of this release is more bug fixes ahead of v 1.0.0. We now have CI tests on TPU thanks to @zcain117 from Google! :slightly_smiling_face:,Co-authored-by: Rohit Gupta rohitgr1998@gmail.com (cherry picked from commit c479351a938240fbda6774a404494ee399ff361a),0
add memory parity for PL vs Vanilla (#5170),0.4254046,  * Deprecated the internal `pl_multi_process` function,  refactor   memory   show   clean   clean   try   device   reset   fix   fix   mean   hook   format   add todo   Co-authored-by: chaton thomas@grid.ai Co-authored-by: chaton thomas@grid.ai (cherry picked from commit 6adc1b32bdeddbc34282abe0fd9f654c0cba570b),0
[bugfix] Group defaults to WORLD if None (#5125),0.79163337,Distributed group defaults to WORLD if None (#5125),  [bugfix] Group defaults to WORLD if None   fix no_grad   Update pytorch_lightning/utilities/distributed.py   Update pytorch_lightning/utilities/distributed.py   Co-authored-by: Gregor Koporec gregork@unicorn.gorenje.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Sean Naren sean.narenthiran@gmail.com (cherry picked from commit 176735097ab5be9ee21d3e7a3dedc174f3e0dd3f),1
update chlog for future 1.1.3rc (#5242),0.49809986,Doing this minor release because correct validation metrics logging is critical.,  update chlog for future 1.1.3rc   prune   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com (cherry picked from commit 27f3f973d6c55aeeba7895f1b6111ce743e16725),0
fix test - reduce metric,0.60362154,Metric reduction with Logging (#5150),,0
flake8 ++,0.36560464,"@airglow, @akshaykvnit, @AljoSt, @AntixK, @awaelchli, @baeseongsu, @bobkemp, @Borda, @calclavia, @Calysto, @djbyrne, @ethanwharris, @fdelrio89, @hadim, @hanbyul-kim, @jeremyjordan, @kuynzereb, @luiscape, @MattPainter01, @neggert, @onkyo14taro, @peteriz, @shoarora, @SkafteNicki, @smallzzy, @srush, @theevann, @tullie, @williamFalcon, @xeTaiz, @xssChauhan, @yukw777",,0
update for v1.1.2 (#5240),0.5353093,[1.1.6] - 2021-01-26,,0
Tighten up mypy config (#5237),0.46699858,    def configure_optimizers(self):,,0
add doctests for example 2/n segmentation (#5083),0.45443454,- Full tests that test specific functionality in trainer.,  draft   fix   drop folder   Co-authored-by: chaton thomas@grid.ai,0
add make cmd - clean (#5204),0.40098956,Updated fast_dev_run to accept integer representing num_batches (#4629),Co-authored-by: Roger Shieh sh.rog@protonmail.ch,0
Update warning if ckpt directory is not empty (#5209),0.55358994,"    'path/to/checkpoint.ckpt',",,0
skip multi-gpu test when running on single-gpu machine (#5186),0.52402043,Train on 2 GPUs in a Jupyter notebook,  skip test   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com,0
Remove Sourcerer (#5172),0.5870582,Remove MetricsHolder (#7909),  Remove Sourcerer   trigger   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
reduce verbosity level in drone ci (#5190),0.34645325,- Improved Trainer CLI arguments handling (generalization),  reduce verbosity level in drone   verbosity ,0
feat(wandb): offset logging step when resuming (#5050),0.6180543,WandbLogger does not force wandb reinit arg to True anymore and creates a run only when needed (#4648),  feat(wandb): offset logging step when resuming   feat(wandb): output warnings   fix(wandb): allow step to be None   test(wandb): update tests   feat(wandb): display warning only once   style: fix PEP issues   tests(wandb): fix tests   tests(wandb): improve test   style: fix whitespace   feat: improve warning   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  feat(wandb): use variable from class instance  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   tests(wandb): check warnings   feat(wandb): use WarningCache   tests(wandb): fix tests   style: fix formatting   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
[bugfix] Correct call to torch.no_grad (#5124),0.97372603,Corrected call to torch.no_grad (#5124),Co-authored-by: Gregor Koporec gregork@unicorn.gorenje.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Sean Naren sean.narenthiran@gmail.com,1
Github Actions deprecation (#5183),0.42662466,Removed deprecated model hooks (#3980),  Fix deprecation call   fix   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
update DALIClassificationLoader to not use deprecated arguments (#4925),0.9894333,Updated DALIClassificationLoader to not use deprecated arguments (#4925),  update DALIClassificationLoader to not use deprecated arguments   fix line length   dali version check added and changed args accordingly   versions   checking version using disutils.version.LooseVersion now   .   ver   import   Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
Fixed docs for WandbLogger (#5128),0.75966626,Removed the deprecated sync_step argument from WandbLogger (#8763),Fixed a small bug with the WandbLogger docs. Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Roger Shieh sh.rog@protonmail.ch,1
Prelease 1.1.2rc (#5171),0.5705617,[1.7.4] - 2022-08-31,"  update CHANGELOG.md, increment for RC   Add missing changelog update   Added a few more   Move to added   Address code review   Missing space   Remove unreleased   Remove lines   Update CHANGELOG.md   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com",0
Document speed comparison (#2072),0.5420467,Speed/memory optimizations.,  docs   script   dump   desc   import   import   if   norm   t   finished   isort   typing   Co-authored-by: Nicki Skafte skaftenicki@gmail.com   xlabel   pandas   time   Co-authored-by: Nicki Skafte skaftenicki@gmail.com,0
add doctests for example 1/n (#5079),0.44765365,Docs improvements,  define tests   fix basic   fix gans   unet   test   drop   format   fix   revert   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,0
prune ecosystem example (#5085),0.4583773,Pruned requirements duplicity (#13739),  draft   wip   CI   drop pl geometry   copy   logo ,0
temporarily suspend all mergify rules (#5112),0.4304424,Silenced some warnings. verified ddp refactors (#3483),,0
drop install FairScale for TPU (#5113),0.4679853,TPU core selection,  drop install FairScale for TPU   typo   Co-authored-by: Roger Shieh sh.rog@protonmail.ch,0
Disable pl optimizer temporarily to fix AMP issues (#5163),0.89221895,enable_pl_optimizer=False by default to temporarily fix AMP issues (#5163),  Disable pl optimizer temporarily to fix AMP issues   Add todo and enable pl optimizer in the test ,1
[bug-fix] Metric reduction with Logging (#5150),0.90613526,Metric reduction with Logging (#5150),  add test   resolve bug   udpate test   wrongly copy / paste   update test   resolve a second bug   Co-authored-by: Ubuntu ubuntu@ip-172-31-62-109.ec2.internal,1
[bugfix] remove nan loss in manual optimization (#5121),0.9137821,Remove nan loss in manual optimization (#5121),  remove nan loss whe missing   Update pytorch_lightning/core/lightning.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Apply suggestions from code review  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
Un-balanced logging properly supported (#5119),1.0000002,Un-balanced logging properly supported (#5119),  resolve bug   clean code   resolve comments   Update tests/trainer/optimization/test_multiple_optimizers.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   resolve another bug   add comments   use abs to find diff   update   resolve flake8   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
support number for logging with sync_dist=True (#5080),0.9999999,Support number for logging with sync_dist=True (#5080),  support number   add two tests   wip   add ddp in special test   remove a test   move device to bottom   simplify test   update test   Update pytorch_lightning/core/step_result.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  resolve sync_ddp  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Prune CHANGELOG.md (#5151),0.47778127,Silenced some warnings. verified ddp refactors (#3483),  Prune CHANGELOG.md   Update CHANGELOG.md   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update CHANGELOG.md  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update CHANGELOG.md  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update CHANGELOG.md  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update CHANGELOG.md  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
"Update changelog, increment version (#5148)",0.58540344,Set version as today (#13906),,0
Fix hang in DDP HPC accelerators (#5157),0.99087006,Fix hanging in DDP HPC accelerators (#5157), Fix hang in DDP HPC accelerators  init_device was never called  Update CHANGELOG.md,1
Fix reset TensorRunningAccum (#5106),1.0,Fix reset TensorRunningAccum (#5106),  Fix reset TensorRunningAccum   add test for TensorRunningAccum's reset method   fix CI failed due to PEP8   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
Update isort config (#5142),0.3939519,and run the command that was configured:,  Update isort config   Apply isort with new config   Fix typo in isort config   fix rebase   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
Fix saved filename in ModelCheckpoint if it already exists (#4861),1.0,Fix saved filename in ModelCheckpoint if it already exists (#4861),"  disable version if not required   disable version if not required   pep   chlog   improve test   improve test   parametrize test and update del_list   Update pytorch_lightning/callbacks/model_checkpoint.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   try appending version to already saved ckpt_file   Revert ""try appending version to already saved ckpt_file""   This reverts commit 710e05e01f738d982aabf1f36c09fa59293e5c0c.   add more assertions   use BoringModel   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: chaton thomas@grid.ai Co-authored-by: Roger Shieh sh.rog@protonmail.ch",1
add copyright to tests (#5143),0.3890054,Refactored setup_training and remove test_mode (#5388),,0
simplify changelog (#5135),0.5913582,Full Changelog,,0
Add Google Colab badges (#5111),0.42875832,Changed the default value for the progress_bar_refresh_rate Trainer argument in Google COLAB notebooks to 20 (#5516), Add colab badges to notebook  Add colab badges to notebook to notebooks 4 & 5  Add colab badges  Co-authored-by: chaton thomas@grid.ai,0
hotfix: dataloaders - add unimplemented methods (#5352),0.65411127,"To use this new feature, return any iterable (or collection of iterables) from the dataloader hooks. ",  add unimplemented methods   test   test   flake8 ,0
Add Support for multiple train loaders (#1959),0.5233162,"Working with multiple dataloaders (#16800, #16753)","  add support for wrong dtype in apply_func   apply loader resetting to possible collection of loaders   add combined loader iter class   integrate combined loader iter to training loop   fix imports   fix imports   finish supporters   add tests for supporters   add test for model with multiple loaders   fix trainer integration   fix instance check   Train loaders (#4032)   patch for issues discussed in #1959, encapsulating underlying datastructures returned from train_dataloader   update data_loading.py to it uses patch discussed in #1959   rename class   Separate CombinedLoaderIterator into two classes, and update related tests. (#4606)   Fix the bugs after rebasing.   Add custom get_len for apply_to_collection   Refactor MultiIterator to be as CombinedLoaderIterator   To get the right num_training_batches. Call the wrapper for multi trainloader in data_loading.py, instead of training_loop.py   Reload _loader_iters when calling iter   Don't transform DataLoader to CombinedLoaderIterator when it's along   Updates test_fit_multiple_train_loaders for testing num_training_batches   Seperate CombinedLoaderIterator into CombinedLoaderIterator and CombinedDataLoader. Add CombinedDataset for unified DataLoader format.   Initialize CombinedDataLoader before calculating num_training_batches. Also updating self._worker_check for multiple loaders   Update tests for supporters   Update tests for multiple trainloaders. Add tests about few_workers for multiple loaders.   Fix pep8 issues   Add tests for train_loader_patch.py   Add descriptions to multiple_trainloader_mode   Remove unused variables   Add docstrings and typing   Add more tests for better converage   Remove unused commented codes   Add sampler property   Remove extract_dataset   Update typing   pep8   Update train_loader_patch.py   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/trainer/supporters.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   reviewer comments   fix stupid import   add docs   add back line separator   fix line sep   pep8   Apply suggestions from code review   fix   fix   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Apply suggestions from code review  Co-authored-by: Nicki Skafte skaftenicki@gmail.com  flake8  Co-authored-by: Justus Schock justusschock@justuss-mbp.fritz.box Co-authored-by: Christofer Fransson christofer_fransson@yahoo.com Co-authored-by: YI-LIN SUNG r06942076@ntu.edu.tw Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz",0
Refactor: clean trainer device & distrib setters (#5297),0.82572794,Refactor: clean trainer device & distributed getters (#5300),  naive replace   simplify   clean   .   fix   .   fix   fix ,1
mark todo exceptions (#5320),0.48880655,Used raise .. from .. to explicitly chain exceptions (#3750),  mark todo exceptions   .   .   .   .   .   .   .   .   try   . ,0
drop deprecated TrainResult (#5323),0.921301,Removed deprecated TrainResult (#5323),  drop TrainResult   .   .   .   .   .   . ,1
Refactor/prune unused EvalModel methods (#5331),0.5915781,refactored inner eval loop (#3141),  model valid   model train   model test   model opt ,0
drop deprecated fbeta metrics (#5322),0.8631538,"Removed deprecated Fbeta, f1_score and fbeta_score metrics (#5322)",  drop deprecated fbeta metrics   flake8   imports   chlog ,1
drop deprecated checkpoint filepath (#5321),0.8698783,Removed deprecated checkpoint argument filepath (#5321),  drop deprecated checkpoint filepath   tests ,1
[Metrics] R2Score (#5241),0.6444287,Regression metrics (#2221),  add r2metric   change init   add test   add docs   add math   Apply suggestions from code review   Co-authored-by: Teddy Koker teddy.koker@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   changelog   adjusted parameter   add more test   pep8   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  add warnings for adjusted score  Co-authored-by: Teddy Koker teddy.koker@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
fix trainer distributed attributes (#5303),0.7588459,Changed Trainer connectors to be protected attributes:,  fix trainer distributed attributes   .   fix ,1
Classification metrics overhaul: stat scores (3/n) (#4839),0.89525616,Classification metrics overhaul (#4837),"  Add stuff   Change metrics documentation layout   Add stuff   Add stat scores   Change testing utils   Replace len(.shape) with .ndim   More descriptive error message for input formatting   Replace movedim with permute   PEP 8 compliance   WIP   Add reduce_scores function   Temporarily add back legacy class_reduce   Division with float   PEP 8 compliance   Remove precision recall   Replace movedim with permute   Add back tests   Add empty newlines   Add empty line   Fix permute   Fix some issues with old versions of PyTorch   Style changes in error messages   More error message style improvements   Fix typo in docs   Add more descriptive variable names in utils   Change internal var names   Break down error checking for inputs into separate functions   Remove the (N, ..., C) option in MD-MC   Simplify select_topk   Remove detach for inputs   Fix typos   Update pytorch_lightning/metrics/classification/utils.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update docs/source/metrics.rst  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   Minor error message changes   Update pytorch_lightning/metrics/utils.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   Reuse case from validation in formatting   Refactor code in _input_format_classification   Small improvements   PEP 8   Update pytorch_lightning/metrics/classification/utils.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update pytorch_lightning/metrics/classification/utils.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source/metrics.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update pytorch_lightning/metrics/classification/utils.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Apply suggestions from code review  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   Alphabetical reordering of regression metrics   Change default value of top_k and add error checking   Extract basic validation into separate function   Update to new top_k default   Update desciption of parameters in input formatting   Apply suggestions from code review   Co-authored-by: Nicki Skafte skaftenicki@gmail.com   Check that probabilities in preds sum to 1 (for MC)   Fix coverage   Split accuracy and hamming loss   Remove old redundant accuracy   Minor changes   Fix imports   Improve docstring descriptions   Fix imports   Fix edge case and simplify testing   Fix docs   PEP8   Reorder imports   Add top_k parameter   Update changelog   Update docstring   Update docstring   Reverse formatting changes for tests   Change parameter order   Remove formatting changes 2/2   Remove formatting 3/3   .   Improve description of top_k parameter   Apply suggestions from code review   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   Remove unneeded assert   Update pytorch_lightning/metrics/functional/accuracy.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   Remove unneeded assert   Explicit checking of parameter values   Apply suggestions from code review   Co-authored-by: Nicki Skafte skaftenicki@gmail.com   Apply suggestions from code review   Fix top_k checking   PEP8   Don't check dist_sync in test   add back check_dist_sync_on_step   Make sure half-precision inputs are transformed (#5013)   Fix typo   Rename hamming loss to hamming distance   Fix tests for half precision   Fix docs underline length   Fix doc undeline length   Replace mdmc_accuracy parameter with subset_accuracy   Update changelog   Fix unwanted accuracy change   Enable top_k for ML prob inputs   Test that default threshold is 0.5   Fix typo   Update top_k description in helpers   updates   Update styling and add back tests   Remove excess spaces   fix torch.where for old versions   fix linting   Update docstring   Fix docstring   Apply suggestions from code review (mostly docs)   Default threshold to None, accept only (0,1)   Change wrong threshold message   Improve documentation and add tests   Add back ddp tests   Change stat reduce method and default   Remove DDP tests and fix doctests   Fix doctest   Update changelog   Refactoring   Fix typo   Refactor   Increase coverage   Fix linting   Consistent use of backticks   Fix too long line in docs   Apply suggestions from code review   Fix deprecation test   Fix deprecation test   Default threshold back to 0.5   Minor documentation fixes   Add types to tests   Co-authored-by: Teddy Koker teddy.koker@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: chaton thomas@grid.ai Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com",1
Implement partial auroc metric (#3790),0.6266616,Deprecated reorder parameter of the auc metric (#4237),"  Implement partial auroc metric   Add pycodestyle changes   Added tests for max_fpr   changelog   version for tests   fix imports   fix tests   fix tests   Added more thresholds in (0,1] to test max_fpr   Removed deprecated 'reorder' param from auroc   changelog   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   remove old structure   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  fix test error  Co-authored-by: Nicki Skafte nugginea@gmail.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com",0
clean avail. imports & enums (#5256),0.68104124,"Separated utils: imports & enums (#5256, #5874)",  clean avail. imports   enums   fix missing ,0
add tests for Trainer attributes (#5261),0.7573443,trainer.test(),  add tests for Trainer attributes   drop empty ,1
CI: fix nightly release version (#5260),0.53975666,The point of this release is more bug fixes ahead of v 1.0.0. We now have CI tests on TPU thanks to @zcain117 from Google! :slightly_smiling_face:,  CI fix nigtly releases   format   fix unrelated flake8 ,0
Unify names in Utils (#5199),0.666489,Unified module names in Utils (#5199),  warnings   argparse   mutils   xla device   deprecated   tests   simple   flake8   fix   flake8   1.4 ,0
Classification metrics overhaul: accuracy metrics (2/n) (#4838),0.89413774,Classification metrics overhaul (#4837),"  Add stuff   Change metrics documentation layout   Add stuff   Change testing utils   Replace len(.shape) with .ndim   More descriptive error message for input formatting   Replace movedim with permute   PEP 8 compliance   Division with float   Style changes in error messages   More error message style improvements   Fix typo in docs   Add more descriptive variable names in utils   Change internal var names   Break down error checking for inputs into separate functions   Remove the (N, ..., C) option in MD-MC   Simplify select_topk   Remove detach for inputs   Fix typos   Update pytorch_lightning/metrics/classification/utils.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update docs/source/metrics.rst  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   Minor error message changes   Update pytorch_lightning/metrics/utils.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   Reuse case from validation in formatting   Refactor code in _input_format_classification   Small improvements   PEP 8   Update pytorch_lightning/metrics/classification/utils.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update pytorch_lightning/metrics/classification/utils.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source/metrics.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update pytorch_lightning/metrics/classification/utils.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Apply suggestions from code review  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   Alphabetical reordering of regression metrics   Change default value of top_k and add error checking   Extract basic validation into separate function   Update to new top_k default   Update desciption of parameters in input formatting   Apply suggestions from code review   Co-authored-by: Nicki Skafte skaftenicki@gmail.com   Check that probabilities in preds sum to 1 (for MC)   Fix coverage   Split accuracy and hamming loss   Remove old redundant accuracy   Minor changes   Fix imports   Improve docstring descriptions   Fix edge case and simplify testing   Fix docs   PEP8   Reorder imports   Update changelog   Update docstring   Update docstring   Reverse formatting changes for tests   Change parameter order   Remove formatting changes 2/2   Remove formatting 3/3   .   Improve description of top_k parameter   Apply suggestions from code review   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   Remove unneeded assert   Update pytorch_lightning/metrics/functional/accuracy.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   Remove unneeded assert   Explicit checking of parameter values   Apply suggestions from code review   Co-authored-by: Nicki Skafte skaftenicki@gmail.com   Apply suggestions from code review   Fix top_k checking   PEP8   Don't check dist_sync in test   add back check_dist_sync_on_step   Make sure half-precision inputs are transformed (#5013)   Fix typo   Rename hamming loss to hamming distance   Fix tests for half precision   Fix docs underline length   Fix doc undeline length   Replace mdmc_accuracy parameter with subset_accuracy   Update changelog   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   Suggestions from code review   Fix number in docs   Update pytorch_lightning/metrics/classification/accuracy.py   Replace topk by argsort in select_topk   Fix changelog   Add test for wrong params   Add Google Colab badges (#5111)   Add colab badges to notebook   Add colab badges to notebook to notebooks 4 & 5  Add colab badges  Co-authored-by: chaton thomas@grid.ai   Fix hanging metrics tests (#5134)   Use torch.topk again as ddp hanging tests fixed in #5134   Fix unwanted notebooks change   Fix too long line in hamming_distance   Apply suggestions from code review   Apply suggestions from code review   protect   Update CHANGELOG.md   Co-authored-by: Teddy Koker teddy.koker@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: chaton thomas@grid.ai Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Roger Shieh sh.rog@protonmail.ch Co-authored-by: Shachar Mirkin shacharmirkin@gmail.com",1
fix/enable - check F401 (#5201),0.5620462,Changed fsspec to tuner (#4458),  refactor - check F401   missed   fix ,0
refactor - check E501 (#5200),0.63387686,Refactoring,,0
refactor - check F841 (#5202),0.65647113,Refactoring,,0
annotat unused vars (#5017),0.43516958,            find_unused_parameters=True,  annotate all unused vars   rank_zero_warn   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  f1 fixed  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
[feat] Enable self.log in callbacks (#5094),0.8729377,Enabled self.log in callbacks (#5094),  enable to use self.log in callbacks   update   revert back to assert ,1
set 1.2.0dev (#5132),0.47266272,Changed default apex level to 'O2' (#2362),,0
set xxx_AVAILABLE as protected (#5082),0.6643844,Renamed xxx_AVAILABLE as protected (#5082),  sett xxx_AVAILABLE as protected   docs ,0
Do not warn when the name key is used in the lr_scheduler dict (#5057),1.0000002,Do not warn when the name key is used in the lr_scheduler dict (#5057),  Do not warn when the name key is used   Missing line   Consistency   Update pytorch_lightning/callbacks/lr_monitor.py   Update docs   Update pytorch_lightning/core/lightning.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update CHANGELOG  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
Refactor load in checkpoint connector (#4593),1.0,Refactor load in checkpoint connector (#4593),  Refactor load step commentaries   Refactor hpc ckpt suffix acquisition   Refactor restore/hpc_load match   Refactor hpc load trial   Refactor checkpoint dir check   Refactor unneeded function nest   Refactor nested If   Refactor duplicated cache clear   Refactor attempt flow with if/elif   Fix pip8   Refactor hook commentary   Co-authored-by: chaton thomas@grid.ai   Fix pep8   Refactor hpc load checkpoint path acquisition   Fix pip8   Fix doc   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Refactor None Union type with Optional  Co-authored-by: chaton thomas@grid.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Roger Shieh sh.rog@protonmail.ch,1
Improve some tests (#5049),0.54657817,Updated app testing (#16000),  Improve some tests   Add TrainerState asserts   Co-authored-by: Roger Shieh sh.rog@protonmail.ch,0
drop unused test with result api (#5058),0.60830355,Removed callback metrics from test results obj (#2994),Co-authored-by: chaton thomas@grid.ai Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
split tests for deprecated api (#5071),0.5187185,Deprecated the TestTubeLogger (#9065),  imports   imports   flake8   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
Allow any input in to_onnx and to_torchscript (#4378),1.0,Allow any input in to_onnx and to_torchscript (#4378),  branch merge   sample   update with valid input tensors   pep   pathlib   Updated with BoringModel and added more input types   try fix   pep   skip test with torch < 1.4   fix test   Apply suggestions from code review   update tests   Allow any input in to_onnx and to_torchscript   Update tests/models/test_torchscript.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   no_grad   try fix random failing test   rm example_input_array   rm example_input_array   Co-authored-by: chaton thomas@grid.ai Co-authored-by: Jeff Yang ydcjeff@outlook.com Co-authored-by: Roger Shieh sh.rog@protonmail.ch Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com,1
Remove beta arg from F1 class and functional (#5076),0.9999998,Remove beta arg from F1 class and functional (#5076),  remove beta from F1   remove from functional   Co-authored-by: Teddy Koker teddy.koker@gmail.com,1
Fix docs metrics formatting (#5077),0.6167588,"docs for all Metrics (#2184, #2209)",  fix functional f1 fbeta formatting   Update f_beta.py   remove line breaks   Update f_beta.py   add line breaks and pad  pad linea breaks with 2 spaces instead of tab,0
fix: MNIST minor bug (#5075),0.60896957,Changed the default behaviour to no longer include a NaN check with each training iteration. (#1475),Co-authored-by: Roger Shieh sh.rog@protonmail.ch,0
Update installation instructions for FairScale (#5099),0.47070268,"For reference, FairScale's implementation can be used with",Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
CI: upload report only on failer (#5086),0.4297898,Reset the dataloaders on OOM failure in batch size finder to use the last successful batch size (#14372),  CI: upload report only on failer   Apply suggestions from code review   Co-authored-by: chaton thomas@grid.ai  Apply suggestions from code review  Co-authored-by: chaton thomas@grid.ai Co-authored-by: Roger Shieh sh.rog@protonmail.ch Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
add back compatibility for deprecated metrics 2/n (#5068),0.8227215,Removed deprecated metrics (#8586),  add back compatibility for deprecated metrics   fix   imports   imports ,1
add back compatibility for deprecated metrics 1/n (#5067),0.8216026,Removed deprecated metrics (#8586),  add back compatibility for metrics   tests   Add deprecated metric utility functions back to functional (#5062)   add back deprecated metric utility functions to functional   pep   pep   suggestions   move   Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz   more   fix   import   docs   tests   fix   Co-authored-by: Teddy Koker teddy.koker@gmail.com,1
Pre release (#5098),0.51870894,This release includes:,  add rc release   update changelog   Update CHANGELOG.md   Update CHANGELOG.md   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
[hotfix] Extend Optimizer + update doc (#5095),0.6948525,Refactored optimizer (#4658),  resolve urgent bug   update pr   update doc   update   remove typo   add defaults   Update pytorch_lightning/init.py   Update setup.py   update doc   Update docs/source/optimizers.rst   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   update   resolve doc   debug test   update test   Update docs/source/optimizers.rst   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update docs/source/optimizers.rst  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update docs/source/optimizers.rst  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   remove useless import   Update docs/source/optimizers.rst   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
drop duplicate metrics (#5014),0.99999964,Drop duplicate metrics (#5014),  drop duplicate metrics   keep   fix ,1
Check if optimizer supports closure (#4981),1.0000001,Check if optimizer supports closure (#4981),  check if optimizer support closure   cleanup test   resolve tests   resolve flake   update test due to patch limit   update   update dep   Update tests/core/test_lightning_optimizer.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update tests/core/test_lightning_optimizer.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   resolve bug   update test   resolve tests   Update requirements/extra.txt   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   remove bolts dep   remove bolts   add missing bolts dep for tests   remove need for bolts   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
Added CHANGELOG section (#5065),0.68127525,Full Changelog,,0
Update DDP docs (#5046),0.70055354,DDP(2) backend (#2796),  Fix flake8 error to fix CI   Correct weights-loading to use correct callbacks   Fix dangling links   Co-authored-by: chaton thomas@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
Initialize trainer with None in DDPAccelerator (#4915),0.6477593,"trainer = Trainer(strategy=""ddp_spawn"", accelerator=""gpu"", devices=2)",  Initialize trainer with None   add typing to all accelerators   resolve imports   update   add typing   removed typo   update   Fix formatting and imports in accelerator   Co-authored-by: maxjeblick maxjeblick@users.noreply.github.com Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: SeanNaren sean@grid.ai Co-authored-by: Roger Shieh sh.rog@protonmail.ch,0
simplify accelerator steps (#5015),1.0000002,Simplify accelerator steps (#5015),  simplify accelerator steps   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
Add a notebook example to reach a quick baseline of ~94% accuracy on CIFAR (#4818),0.81516653,Add a notebook example to reach a quick baseline of ~94% accuracy on CIFAR10 using Resnet in Lightning (#4818),  Add a notebook example to reach a quick baseline of ~94% accuracy on CIFAR10 using Resnet in Lightning   Remove outputs   PR Feedback   some changes   some more changes   Co-authored-by: chaton thomas@grid.ai Co-authored-by: rohitgr7 rohitgr1998@gmail.com,1
update usage of deprecated automatic_optimization (#5011),0.6093038,"Executing the optimizer_closure is now required when overriding the optimizer_step hook (#9360). If you relied on the previous behavior, we recommend to switch to Manual Optimization alltogether.",  drop deprecated usage automatic_optimization   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Apply suggestions from code review  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
update usage of deprecated profiler (#5010),0.6057076,Moved profilers to their own file (#7822),  drop deprecated profiler   lut   Co-authored-by: Roger Shieh sh.rog@protonmail.ch,0
release 1.1.0 (#5048),0.65638703,[1.1.6] - 2021-01-26,  release 1.1.0   pep8 ,0
update usage of deprecated checkpoint_callback (#5006),0.730147,The signature and behavior of the on_load_checkpoint and on_save_checkpoint callback hooks have changed (#14835):,  drop usage of deprecated checkpoint_callback   fix   fix ,1
ref: clean config [1/n] add intermediate setters (#4990),0.542043,and run the command that was configured:,  add intermediate setters   show inputs   fix options   move   fix   less talk   fix   talk less   str   cases   rename   Co-authored-by: chaton thomas@grid.ai,0
Loss format from .3f to .3g in the tqdm (#4972),0.40807828,Changed smoothing in TQDM to decrease variability of time remaining between training/eval (#1194),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Simplify GPU and TPU accelerator (#5024),0.67276335,Simplify accelerator steps (#5015),,0
drop deprecated reorder from AUC (#5004),0.7115045,Deprecated reorder parameter of the auc metric (#4237),  drop deprecated reorder from AUC   chlog   fix   fix   simple   fix   fix   fix   Co-authored-by: Roger Shieh sh.rog@protonmail.ch,1
[feat] 3/n pp (#5036),0.3645068,[1.3.3] - 2021-05-26,"  add pp doc   udpate doc   update doc   update doc   Update docs   update doc   udpate   update doc   update doc   Formatting, update sharded zip link   Update docs/source/multi_gpu.rst   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   Apply suggestions from code review   Reference directly to section   Co-authored-by: SeanNaren sean@grid.ai Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",0
Add carmocca to core (#5038),0.51689804,@kaushikb11 @carmocca ,Co-authored-by: Roger Shieh sh.rog@protonmail.ch Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
fix GH release badges (#5040),0.42265475,Resolved a bug where the work statuses will grow quickly and be duplicated (#13970),  fix GH release badges   rtd ,0
[feat] pp 2/n (#5026),0.40732586,[1.2.2] - 2021-03-02,"  Added changes for RPC plugin   Add missing kwargs   Fix code format   Loading refactors by introducing is_distributed var, fix optimizer step flow   Add rpc guard   Added docstrings and typing   resolve comments   Add additional rpc hook, refactor name of exit process hook for clarity   remove annotation   Modify behaviour to allow optional return, add test for rpc plugin   resolve tests   rename is_ddp_based   update   update for windows   update   resolve test   code smell   Added sequential plugin   resolve bug   update   cleanup   add Exception   resolve docs   Remove ddp support   Revert distributed -> ddp   Update pl_examples/basic_examples/conv_sequential_example.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pl_examples/basic_examples/conv_sequential_example.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/plugins/ddp_sequential_plugin.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   Address code review points   Update pytorch_lightning/plugins/ddp_sequential_plugin.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/plugins/ddp_sequential_plugin.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   Add missing return   Fix formatting, add datamodule args   add small comment   resolve comments   resolve comments   update source for fairscale   update extras   remove staticmethod   resolve flake8   Skip tests that are failing due to bug upstream with multiple optimizers and shard   update   update on comments   clean test   latest comments   remove old comments   add todo   Update version   update   resolve bugs   resolve bugs   update test   remove hanging test   Update pytorch_lightning/plugins/ddp_sequential_plugin.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   resolve on comments   Update pytorch_lightning/plugins/ddp_sequential_plugin.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   resolve on comments   Update pytorch_lightning/plugins/ddp_sequential_plugin.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update pytorch_lightning/plugins/ddp_sequential_plugin.py  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update pytorch_lightning/plugins/ddp_sequential_plugin.py  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update pytorch_lightning/plugins/ddp_sequential_plugin.py  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  remove ImportError  Co-authored-by: SeanNaren sean@grid.ai Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com",0
adding missing changelogs (#5019),0.6862556,Full Changelog,  adding missing changelogs   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Apply suggestions from code review  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
fix ci: release (#5037),0.43759513,Resolved a bug where the install command was not installing the latest version of an app/component by default (#14181),,0
CI: update badges for release (#5002),0.4532849,Doing this minor release because correct validation metrics logging is critical.,  fix images   not sleep   a0   path   assets   assets   bitecode   rls   rls   badges   fix   org   drop   clean   codecov   fix   clean ,0
drop usage of deprecated distributed_backend (#5009),0.7654599,Remove deprecated distributed_backend from Trainer (#10017),Co-authored-by: chaton thomas@grid.ai Co-authored-by: Roger Shieh sh.rog@protonmail.ch,1
replace pyright by mypy (#5021),0.49180308,py,  drop pyright & add mypy   detail   name   fix   flake8   ver   Co-authored-by: Sean Naren sean.narenthiran@gmail.com,0
All gatherwith grads (#5012),0.34963256,@a-gardner1 @abhi-rf @abhinavarora @adamreeve @adamviola @AJSVB @akashkw @amin-nejad @AndresAlgaba @ant0nsc @armanal @bhadreshpsavani @CAIQT @catalys1 @chaddy1004 @chunyang-wen @circlecrystal @Code-Cornelius @Cyber-Machine @dennisbappert @DuYicong515 @edpizzi @franp9am @ftorres16 @ggare-cmu @guyang3532 @Honzys @idiomaticrefactoring @isvogor-foi @jerome-habana @jgibson2 @jlhbaseball15 @jona-0 @JoostvDoorn @josafatburmeister @konstantinjdobler @Kr4is @krishnakalyan3 @krshrimali @lemairecarl @lucmos @manangoel99 @mathemusician @mayeroa @mbortolon97 @NathanGodey @Nesqulck @nithinraok @ORippler @os1ma @peterdudfield @Piyush-97 @puhuk @qqueing @quancs @Raahul-Singh @Raalsky @Rajathbharadwaj @rasbt @rharish101 @rhjohnstone @rjkilpatrick @RobertLaurella @roschly @rsokl @rusty1s @SauravMaheshkar @sethvargo @shabie @shivammehta007 @srb-cv @ThomVett @wangraying @whokilleddb @zredeaux65,"  all_gather   ddp   horovod   grad tests   fixed ddp   ddp fixed, removed tpu, horovod for now   changelog   windows fix   windows fix   removed batch from ctx   all_gather   ddp   horovod   grad tests   fixed ddp   ddp fixed, removed tpu, horovod for now   changelog   windows fix   windows fix   removed batch from ctx   removed code duplication   merge   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",0
[feat] pp 1/n (#5016),0.38834235,[1.2.1] - 2021-02-23,"  Added changes for RPC plugin   Add missing kwargs   Fix code format   Loading refactors by introducing is_distributed var, fix optimizer step flow   Add rpc guard   Added docstrings and typing   resolve comments   Add additional rpc hook, refactor name of exit process hook for clarity   remove annotation   Modify behaviour to allow optional return, add test for rpc plugin   resolve tests   rename is_ddp_based   update   update for windows   update   resolve test   code smell   Revert back to init_ddp_connection for backwards compat   Swap to explicit name for property   Add missing speed parity increase for CI variability, fix call counts for child process   Co-authored-by: tchaton thomas@grid.ai",0
docs: minor spelling tweaks (#5022),0.52349013,Syntax changes are: ,,0
fast_dev_run can be int (#4629),0.75236547,Updated fast_dev_run to accept integer representing num_batches (#4629),  fast_dev_run can be int   pep   chlog   add check and update docs   logging with fdr   update docs   suggestions   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   fdr flush logs   update trainer.fast_dev_run   codefactor and pre-commit isort   tmp   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Roger Shieh sh.rog@protonmail.ch Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com,1
Initialize trainer with None (#4847),0.75336653,Trainer Argument Defaults,Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: chaton thomas@grid.ai Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com,1
Add experiment_key argument to online mode example (#4997),0.40576184,"def on_test_start(self, trainer, pl_module):",Co-authored-by: chaton thomas@grid.ai Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
remove irrelevant docs in optimizer_step (#4964),0.62336904,Removed the deprecated optimizer_idx from training_step as an accepted argument in manual optimization (#8576),Co-authored-by: chaton thomas@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
[hotfix] ddp + manual_optimisation (#4976),0.70579064,"Implemented {DDPShardedPlugin,DDPShardedSpawnPlugin}._setup_model_and_optimizers (#10028, #10064)","  Rely on ddp plugin for blocking sync behaviour, and skip if we're using manual optimization   debug   Revert ""debug""   This reverts commit ccca6b6b   Expose manual reduce for automatic optimization   Add input arguments   Enable parity test   clean imports   Expose hook after to ensure we reset   Fix naming   add   fix test   resolve on comments   typo   Update tests/trainer/optimization/test_manual_optimization.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update tests/trainer/optimization/test_manual_optimization.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   update on comments   resolve comments   Co-authored-by: SeanNaren sean@grid.ai Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",1
shard doc improvements (#4993),0.5133519,Native Fully Sharded Data Parallel Strategy,"  Rewording   Update fairscale install link to include bucket fix, add benchmark results   Added percentage gain   Address codereview   Update docs/source/multi_gpu.rst   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update multi_gpu.rst  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",0
Classification metrics overhaul: input formatting standardization (1/n) (#4837),0.7867452,Classification metrics overhaul (#4837),"  Add stuff   Change metrics documentation layout   Change testing utils   Replace len(.shape) with .ndim   More descriptive error message for input formatting   Replace movedim with permute   Style changes in error messages   More error message style improvements   Fix typo in docs   Add more descriptive variable names in utils   Change internal var names   Break down error checking for inputs into separate functions   Remove the (N, ..., C) option in MD-MC   Simplify select_topk   Remove detach for inputs   Fix typos   Update pytorch_lightning/metrics/classification/utils.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update docs/source/metrics.rst  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   Minor error message changes   Update pytorch_lightning/metrics/utils.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   Reuse case from validation in formatting   Refactor code in _input_format_classification   Small improvements   PEP 8   Update pytorch_lightning/metrics/classification/utils.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update pytorch_lightning/metrics/classification/utils.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source/metrics.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update pytorch_lightning/metrics/classification/utils.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Apply suggestions from code review  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   Alphabetical reordering of regression metrics   Change default value of top_k and add error checking   Extract basic validation into separate function   Update desciption of parameters in input formatting   Apply suggestions from code review   Co-authored-by: Nicki Skafte skaftenicki@gmail.com   Check that probabilities in preds sum to 1 (for MC)   Fix coverage   Minor changes   Fix edge case and simplify testing   Co-authored-by: Teddy Koker teddy.koker@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: chaton thomas@grid.ai Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com",1
Simplify optimization Logic (#4984),0.9999999,Simplify optimization Logic (#4984),"  Rely on ddp plugin for blocking sync behaviour, and skip if we're using manual optimization   debug   Revert ""debug""   This reverts commit ccca6b6b   Expose manual reduce for automatic optimization   Add input arguments   Enable parity test   clean imports   Expose hook after to ensure we reset   Fix naming   add   fix test   uniformize optimizer logic   resolve test   resovle flake8   resolve amp bug   update tests   remove bug   remove optimizer_step in accelerators   typo   update lightning optimizer   set doesn't work with ddp_spawn   resolve flake8   update threshold   ignore pyright   correct codeFactor   remove useless if   remove zer_grad function   simplify step   remove typo   resolve bug   Apply suggestions from code review   update on comments   resolve bugs   remove tests   Update pytorch_lightning/trainer/configuration_validator.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   simplify testing   add more tests   Co-authored-by: SeanNaren sean@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com",1
simplify CI horovod (#4951),0.5287129,reduced all simplified forward (#3126),  simplify CI horovod   reorder ,0
Added changeable extension variable for model checkpoints (#4977),0.6730747,Enhanced load_from_checkpoint to also forward params to the model (#1307),  Added changeable extension variable for model checkpoints   Removed whitespace   Removed the last bit of whitespace   Wrote tests for FILE_EXTENSION   Fixed formatting issues   More formatting issues   Simplify test by just using defaults   Formatting to PEP8   Added dummy class that inherits ModelCheckpoint; run only one batch instead of epoch for integration test   Fixed too much whitespace formatting   some changes   Co-authored-by: rohitgr7 rohitgr1998@gmail.com,0
Enableself.log in most functions. (#4969),0.86711913,Enabled self.log in most functions (#4969),  refactor   solve pyright   remove logging in batch_start functions   update docs   update doc   resolve bug   update   correct script   resolve on comments ,1
Create .drone.jsonnet (#4968),0.37222564,    model = Net()  # .to() not needed,Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Sean Naren sean.narenthiran@gmail.com,0
Fix exception message for invalid custom string plugin (#4979),0.5290983,Updated error message for interactive incompatible plugins (#9896),  Fix exception error from generator to list of valid names   Update pytorch_lightning/plugins/plugin_connector.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
[Bug Fix] Allow logger to support indexing (#4595),0.668388,"Finally, loggers are also now configurable with shorthand:", [Bug Fix] Allow logger to support indexing  This should fix #4540   Adding test for indexes for DummyLogger   Apply suggestions from code review   Co-authored-by: chaton thomas@grid.ai   pep8   added test for dummyexperiment   Co-authored-by: chaton thomas@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Sean Naren sean.narenthiran@gmail.com,0
Fixed PYTHONPATH for ddp test model (#4528),0.6070127,Users who relied ontrainer.test(ckpt_path=None)to load the latest model need to change their code totrainer.test(model)` and pass the model reference directly.,  Fixed PYTHONPATH for ddp test model   Removed debug calls   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: chaton thomas@grid.ai Co-authored-by: Sean Naren sean.narenthiran@gmail.com,0
Update lightning logo (#4490),0.5318031,Update the Lightning App docs (#13537),  docs logo   use png   cleaning   vectorial   icon   icon   use png   cleaning   aync   Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: chaton thomas@grid.ai,0
Auto convert to contiguous format for all_gather (#4907),0.73675925,Auto convert tensors to contiguous format when gather_all (#4907),  convert memory format   changelog   formatting   suggestions   retrigger tests   Co-authored-by: Nicki Skafte nugginea@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: chaton thomas@grid.ai,1
Improve epoch_result_store code quality (#4875),0.6036384,Refactored EpochResultStore (#5522),  Improve code quality   black -l 120 -S   Fix pyright error   Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: chaton thomas@grid.ai,0
fix examples running in DP (#4764),0.48350853,"| Argument Trainer(strategy=""ddp2"") and class pytorch_lightning.strategies.DDP2Strategy                    | 1.8             | No longer supported                             |","  add option to step result to do aggregation on a specific device   in dp: do aggregation on root gpu   Update CHANGELOG.md   pep8   trailing whitespace   uncomment DP   more cases   tmpdir   test   note   move to root   move result stupid result object revert to master undo import add ""to"" method to result generalize to try a test try a test Revert ""try a test"" This reverts commit 22e3c1001e6c5774ea18ad925830304c245bf145. Revert ""try a test"" This reverts commit 4d2d8fb2a52d552894809a0cbe51af126d78f070. new test max epochs super epoch end  log in test hanging test undo test initial test that fails on master step end pass step end step end epoch end print step check dev clean up test sanity check wtf is go ing on frustration debugging test test test test test test test test test unused import   dist backend -> accelerator   remove todo   Co-authored-by: Justus Schock justus.schock@rwth-aachen.de Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com",0
"[Metrics] PrecisionRecallCurve, ROC and AveragePrecision class interface (#4549)",0.6391264,The Recall and Precision metrics (and their functional counterparts recall and precision) can now be generalized to Recall@K and Precision@K with the use of top_k parameter (#4842),  initial changes   remove old   init files   add average precision   add precision_recall_curve   add roc   cleaning   docs   pep8   docs   pep8   changelog   examples prune duplicate roc   format   imports   fix   format   flake8   duplicate   fix   flake8   docs   docs   Co-authored-by: Teddy Koker teddy.koker@gmail.com Co-authored-by: Nicki Skafte nugginea@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz,0
Fix DP Logging Aggregation (#4138),0.631287,DDP + loggers should be fixed,"  add option to step result to do aggregation on a specific device   in dp: do aggregation on root gpu   Update CHANGELOG.md   pep8   trailing whitespace   move to root   move result stupid result object revert to master undo import add ""to"" method to result generalize to try a test try a test Revert ""try a test"" This reverts commit 22e3c1001e6c5774ea18ad925830304c245bf145. Revert ""try a test"" This reverts commit 4d2d8fb2a52d552894809a0cbe51af126d78f070. new test max epochs super epoch end  log in test hanging test undo test initial test that fails on master step end pass step end step end epoch end print step check dev clean up test sanity check wtf is go ing on frustration debugging test test test test test test test test test unused import   move chlog entry   clean   remove outdated changes   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz",0
[docs] Added description of saving using ddp (#4660),0.5254178,DDP(2) backend (#2796),  Added description of saving using ddp   Added code block example to explain DDP saving logic   Fixed underline   Added verbose explanation   Apply suggestions from code review   Added caveat when using custom saving functions   flake8   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz,0
[TEST] Min steps override early stopping (#4283),0.6503639,Checkpoint and early stopping now work without val. step (#1041),  test to make sure behaviour is enforced   test_min_steps_override_early_stopping_functionality   make sure Excepted Behaviour is reproduced   remove pollution from extra logging   update docstring   reduce test time   resolve pep8 ,0
Deprecate auto mode from ModelCheckpoint and EarlyStopping (#4695),0.945989,Deprecated mode='auto' from ModelCheckpoint and EarlyStopping (#4695),  remove auto mode from callbacks   chlog   remove auto mode from callbacks   mode   mode   move back   update docs   update docstrings   docstring warning   fix syntax   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   isort   default to 'auto'   syntax   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
Update to latest logging format and modify the accuracy method. (#4816),0.5869888,Changed automatic casting for LoggerConnector metrics (#5218),  Update to latest logging format and modify the accuracy method.   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: chaton thomas@grid.ai,0
Organize docstring (#4906),0.52778393,Docs improvements,  Organize docstring   Update pl_examples/domain_templates/reinforce_learn_Qnet.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: chaton thomas@grid.ai,0
:hammer: minor refactor in trainer. (#4801),0.67256576,Removed Warning from trainer loop (#1634),  :hammer: minor refactor in trainer.   :hammer: Use finally instead of else   :hammer: revert format   :hammer: check should skip inside try   Co-authored-by: chaton thomas@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
deprecate hprams setter method (#4813),0.556528,Deprecated the old way of assigning hyper-parameters through self.hparams = ... (#4813),  deprecate hprams setter method   update chlog   isort   update deprecation warning   Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: chaton thomas@grid.ai Co-authored-by: Ananya Harsh Jha ananya@pytorchlightning.ai Co-authored-by: Roger Shieh sh.rog@protonmail.ch Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Update cloud_io.py (#4936),0.5821465,- Deprecated all functions in `pytorch_lightning.utilities.cloud_io` in favor of `lightning_lite.utilities.cloud_io` ([#14515](https://github.com/Lightning-AI/lightning/pull/14515)), Update cloud_io.py  Solves AttributeError: 'PosixPath' object has no attribute 'startswith'  Update cloud_io.py  Co-authored-by: chaton thomas@grid.ai Co-authored-by: Teddy Koker teddy.koker@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
refactor imports of optional dependencies (#4859),0.61502236,Wrapped imports for traceability (#13924),  refactor imports of optional dependencies   fix   fix   fix   fix   fix   flake8   flake8   Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: chaton thomas@grid.ai,0
update codeowners (#4881),0.44958684,"Updated DeepSpeed ZeRO (#6546, #6752, #6142, #6321)",  update codeowners   install   Apply suggestions from code review   Apply suggestions from code review   Co-authored-by: chaton thomas@grid.ai   .   Update .github/CODEOWNERS   Co-authored-by: chaton thomas@grid.ai,0
Replace lightning logo asset (#4844),0.39557648,"Renamed the class LightningLite to Fabric (#15932, #15938)",  update logo   Add files via upload   Add files via upload   Delete lightning_logo-large.svg   Delete lightning_logo.svg   Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: Roger Shieh sh.rog@protonmail.ch Co-authored-by: chaton thomas@grid.ai,0
docs: default_root_path -> default_root_dir (#4942),0.5215411,Return the default_root_dir as the log_dir when the logger is a LoggerCollection (#8187),  docs: default_root_path -> default_root_dir   Apply suggestions from code review   fix   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  update notebook  Co-authored-by: Jethro Kuan jethro.kuan@bytedance.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
ci typo in cofig (#4954),0.4766543,"In addition, we fixed:",  ci typo   v++   Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com,0
increase release (#4949),0.47639176,Set version as today (#13906),,0
Update trainer.rst (#4952),0.74028283,adding Trainer.tune() (#3293),,1
CI: skip hanging (#4943),0.46710417,"As we continue to strengthen the codebase with more tests, we’re finally getting rid of annoying bugs that have been around for a bit now. Mostly around the inconsistent checkpoint and early stopping behaviour (amazing work @awaelchli  @jeremyjordan )",  CI: try increase time limit   try min 3.8   no ex   CI   dep   test   deps   deps   drop   drop   Co-authored-by: chaton thomas@grid.ai,0
drop sklearn dependency (#4912),0.746063,Removed dependency on scikit-learn (#801),  drop sklearn dependency   scipy   Co-authored-by: chaton thomas@grid.ai Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
Tpu save (#4309),0.5060936,TPU training (#2708),"  convert xla tensor to cpu before save   move_to_cpu   updated CHANGELOG.md   added on_save to accelerators   if accelerator is not None   refactors   change filename to run test   run test_tpu_backend   added xla_device_utils to tests   added xla_device_utils to test   removed tests   Revert ""added xla_device_utils to test""   This reverts commit 0c9316bb   fixed pep   increase timeout and print traceback   lazy check tpu exists   increased timeout removed barrier for tpu during test reduced epochs   fixed torch_xla imports   fix tests   define xla utils   fix test   aval   chlog   docs   aval   Apply suggestions from code review   Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: chaton thomas@grid.ai Co-authored-by: Sean Naren sean.narenthiran@gmail.com",0
Sharded DDP Docs (#4920),0.62376845,Ensure we check deepspeed/sharded in multinode DDP (#6297),"  Add doc fixes   Remove space   Add performance doc, fix flag   Fix up docs   Add install instructions   Update link   Add section for model parallelism, refactor into section   Address code review   fixed underline   Update docs/source/multi_gpu.rst   Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com   Address code review points   Added caveat, increase performance   Update docs/source/multi_gpu.rst   Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com  Update docs/source/multi_gpu.rst  Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com   Add cross reference   Swapped to just fairscale since new release contains all required code   Revert ""Swapped to just fairscale since new release contains all required code""   This reverts commit 21038e72  Update docs/source/multi_gpu.rst  Co-authored-by: chaton thomas@grid.ai  Fairscale install has been fixed  Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com Co-authored-by: chaton thomas@grid.ai",0
CI cleaning (#4941),0.6729259,"Cleaning (#5948, #5949, #5950)",  set   cut   env   oonce   env   env   env ,0
Added the function for downloading the badges locally and replace the url with downlaod path (#4250),0.42170787,the following works now:,"  Added the function for downloading the badge locally, replacing the url   Fixed the pep8 errors, pointed out during pull request   Update setup.py   refactor   format   test   Added Doctest on the functions   test   test   fix   format   fix   fix   prune   fiix   fiix   flake8   fix   imports   imports   imports   fixx   impoets   win   min   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz",0
Allow string plugins (#4888),0.5219236,Enabled plugins (#4041),"  Allow plugin to be chosen via string   Fix implementation, add tests   Fix codefactor issues   Added missing env patch   Skip test for windows   Reword reason   Add skip to invalid test   Create required_plugins function, move sharded amp requirement to plugin   Pass AMPType, fix setter for apex   Better doc strings   Add exception when using apex   Add trainer available_plugins function, warn user when plugins have been added automatically with option to override behaviour   Fixed pep8 indent   Fix codefactor issues   Add env variables   Update pytorch_lightning/cluster_environments/cluster_environment.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   Addressed code review   Update pytorch_lightning/plugins/plugin_connector.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/plugins/plugin_connector.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/plugins/plugin_connector.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   Addressed more code review feedback   Fixed docstrings   Swapped to verbose runtime error   Apply suggestions from code review   Apply suggestions from code review   Update pytorch_lightning/plugins/sharded_plugin.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   Change name   Pass trainer to plugins that may require it   Fix sharded plugin   Added test to ensure string sharded works   Removed trainer typing as this breaks pep8   Fixed doc issues   Fixed tests   Co-authored-by: chaton thomas@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",0
Create memory dynamically (#4938),0.45385075,Resolve memory leak for evaluation (#6326),  create window size dynamically.   pep8   Co-authored-by: chaton thomas@grid.ai,0
upgrade min deps (#4934),0.5023794,"Updated DeepSpeed ZeRO (#6546, #6752, #6142, #6321)",  upgrade min deps   unused   replace torchvision and torchtext   loggers   freeze pip   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz Co-authored-by: Sean Naren sean.narenthiran@gmail.com,0
update changelog (#4931),0.6377971,Full Changelog,,0
[HotFix] Logging - One epoch delay on training epoch metrics. (#4913),0.70439446,Changed the behaviour when logging evaluation step metrics to no longer append /epoch_* to the metric name (#7351),  add test   resolve logging bug   update   resolve pep8   resolve tests   Co-authored-by: Roger Shieh sh.rog@protonmail.ch,1
update logging docs and decorators (#4431),0.60327095,Refactored logging,  update logging docs   experiment   add decorators to base and csv logger methods   fix   doc fix   update docs   update docs   Update pytorch_lightning/loggers/base.py   Co-authored-by: chaton thomas@grid.ai,0
optimizer clean up (#4658),0.7455927,Refactored optimizer (#4658),  add LightningOptimizer   typo   add mock closure   typo   remove logic in optimizer_step   update   update   update   desactivate LightningOptimizer for hovorod   resolve flake   typo   check optimizer name   change name   added backward to LightningOptimizer   remove use_lightning_optimizer   move update   simplify init   resolve comments   resolve bug   update   update   resolve bugs   resolve flake8   set state   work manual_optimizer_step   add doc   add enable_pl_optimizer   make optimizer_step   add make_optimizer_step   add examples   resolve test   add test_optimizer_return_options_enable_pl_optimizer   add enable_pl_optimizer=True   update   update tests   resolve bugs   update   set Trainer to False   update   resolve bugs   update   remove from doc   resolve bug   typo   update   set to True   simplification   typo   resolve horovod   unwrap horovod   remove Optimizer   resolve horovod   move logic to amp_backend   doesn't seem to be pickable   update   add again   resolve some bugs   cleanup   resolve bug with AMP   change repr   round at -12   udpate   update   update   remove from horovod   typo   add convert_to_lightning_optimizers in each accelerators   typo   forgot   forgot a convert_to_lightning_optimizers   update   update   update   increase coverage   update   resolve flake8   update   remove useless code   resolve comments + add support for LightningOptimizer base class   resolve flake   check optimizer get wrapped back   resolve DDPSharded   reduce code   lightningoptimizer   Update pytorch_lightning/core/optimizer.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   Update pytorch_lightning/core/lightning.py   remove reference to step function   Apply suggestions from code review   update on comments   resolve   Update CHANGELOG.md   add back training_step in apex and native_amp   rename optimizer_step   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: Sean Naren sean.narenthiran@gmail.com,1
drop fairscale for PT <= 1.4 (#4910),0.5047453,Avoided requiring the FairScale package to use precision with the fsdp native strategy (#14092),"  drop fairscale for PT <= 1.4   fix   Add extra check to remove fairscale from minimal testing if using minimal torch version 1.3   Update ci_test-full.yml   Update gym to .3 to see if this fixes examples CI   Update omegaconf to minimum for hydra v1.0   Revert ""Update gym to .3 to see if this fixes examples CI""   This reverts commit 4221d4b9  Revert ""Update omegaconf to minimum for hydra v1.0""  This reverts commit 4f579217 Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: SeanNaren sean@grid.ai",0
freeze DALI (#4922),0.3888331,Moved TrainsLogger to Bolts (#2384),  freeze DALI   todos   only CI   Update .drone.yml   string   speed   Co-authored-by: Sean Naren sean.narenthiran@gmail.com,0
Fix codes in 'Lightning in 2 steps' docs (#4894),0.57165664,  * Removed `Loop.replace()` ([#16361](https://github.com/Lightning-AI/lightning/pull/16361)),  fix   fix1   Apply suggestions from code review   Update docs/source/new-project.rst   more fixes   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: chaton thomas@grid.ai,0
Fix typo in getting started docs (#4882),0.5018094,Syntax changes are: ,Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com,0
tweak imagenet docs to match current script (#4895),0.39487213,Updated semantic segmentation example with custom u-net and logging (#1371),Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com,0
Update reinforce_learn_Qnet.py (#4814),0.519919,- Renamed `training_type_plugin` file to `strategy` ([#11239](https://github.com/PyTorchLightning/pytorch-lightning/pull/11239)),Correct the text. Co-authored-by: chaton thomas@grid.ai Co-authored-by: Nicki Skafte skaftenicki@gmail.com,0
formatting (#4898),0.42817518,Model summary: add 1 decimal place (#4745),Co-authored-by: Sean Naren sean.narenthiran@gmail.com,0
Increase multiple optimizers parity for drone CI (#4884),0.60680354,Working with multiple optimizers (#16539),,0
build dockers XLA 1.7 (#4891),0.4481206,Remove unnecessary intermediate layers in Dockerfiles (#5697),  build XLA 1.7   night XLA 1.7   rename   use 1.7   tpu ver ,0
Merge pull request #4880 from PyTorchLightning/better_simple_profiler,0.6257746,Deprecated pytorch_lightning.profiler in favor of pytorch_lightning.profilers (#16059),Logging,0
Merge pull request #4856 from PyTorchLightning/feature/plug,0.5753476,Full commit list: https://github.com/PyTorchLightning/pytorch-lightning/compare/1.5.0...1.6.0,Backend unit tests,0
Skip a few tests to reduce drone CI wait times,0.44983107,test_percent_check in favour of limit_test_batches,,0
resolve test,0.43866485,- fixed all the .test() calls,,0
reduce to 0.22,0.4509892,Removed auto val reduce (#2462),,0
reduce parity test,0.3302524,- Code coverage (99%),,0
Merge branch 'master' into feature/plug,0.30308908,"merge backends (#3476, #3477, #3478, #3480, #3482)",,0
Merge branch 'better_simple_profiler' of https://github.com/PyTorchLightning/pytorch-lightning into better_simple_profiler,0.71755636,Deprecated pytorch_lightning.profiler in favor of pytorch_lightning.profilers (#16059),,1
remove capture on on_train_batch_end,0.67917687,| on_batch_end               | on_train_batch_end           |,,0
Merge branch 'master' into better_simple_profiler,0.47964317,Moved profilers to their own file (#7822),,0
add note,0.6721527,NOTE,,0
reduce parity to 0.22,0.40258855,Mixed precision overhaul (#16783),,0
optimize logging,0.72218704,Refactored logging,,1
ref: fix & simplify test callback (#4009),0.6584306,Removed callback metrics from test results obj (#2994),  simplify test callback   update   use mock   flake8 ,0
add more profiler,0.68701476,Split profilers module (#6261),,0
update,0.507003,Key updates,,0
better simple profiler,0.540059,Split profilers module (#6261),,0
"Reduce speed diff further, lack of GPU saturation is causing regressive times on drone CI",0.4284641,Reduction when batch_size < num_gpus (#1609),,0
Increase speed diff for drone,0.34548002,"many speed improvements (how we move data, adjusted some flags & PL now adds 300ms overhead per epoch only!)",,0
Tighten up regression testing,0.47767097,- Full tests that run multiple models in different configs,,0
Address code review,0.36597097,Extensively tested code.   ,,0
Update pytorch_lightning/trainer/connectors/precision_connector.py,0.772191,Removed pytorch_lightning.trainer.connectors.OptimizerConnector (#10120),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
Add none check for func,0.42846042,    return None,,0
Remove unneeded check,0.48361248,  validation_if_necessary(),,0
Fix imports,0.5817691,Wrapped imports for traceability (#13924),,0
[bug] Replace_sampler attach previous multiprocessing_context (#4742),0.59800637,Allow setting replace_sampler_ddp=True with a distributed sampler already added (#4273),  resolve bug   add test docstring   Update tests/trainer/test_dataloaders.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  update test  Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Combine utilities,0.40083814,"merge backends (#3476, #3477, #3478, #3480, #3482)",,0
Moved common functions into utilities,0.5839514,"Add deprecated metric utility functions back to functional (#5067, #5068)",,0
Fix var name,0.43881208,moved eval loop (#3412[#3408),,0
simplify imports Omegaconf (#4873),0.41658062,Deprecated self.log(sync_dist_op) in favor of self.log(reduce_fx). (#7891),  hydra   omegaconf ,0
simplify imports xla / TPU (#4872),0.48050964,Used XLA utility API to move data to CPU (Single TPU core) (#8078),  xla   tpu   fix   fix   flake8 ,0
Fix name,0.67413896,| Old name                     | New name                       |,,0
fix import and typo in AMP (#4871),0.54485065,  * Removed the `Trainer.amp_backend` property,  fix import and typo   docs   apex   fix   typo ,0
"Fix logic and add test for apex check, rename file, add DDP launcher tests",0.4325935,Fix saved filename in ModelCheckpoint if it already exists (#4861),,0
Ensure we add the condition to the case statement,0.44539362,Support strategy argument being case insensitive (#12528),,0
make device property always return a device with index (#4851),0.6358719,We cleaned up the properties related to device indices (#14829).,  make device property always return a device with index   pep8   Update test_dtype_device_mixin.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Add additional check to ensure apex is not used with sharded,0.47717214,Pass init args to ShardedDataParallel (#9483),,0
"Removed old eval logic, added eval tests",0.65868896,refactored inner eval loop (#3141),,0
Remove else check,0.48473644,        else:,,0
Fix formatting,0.4392829,"In addition, we fixed:",,0
Fix import order,0.41756493,Wrapped imports for traceability (#13924),,0
"Fixed imports, swap to relying on function for entire batch",0.48815757,# pass loaders as a dict. This will create batches like this:,,0
Addressed code review points,0.34336308,"If you want to customize ModelCheckpoint callback, without all the extra functionality this class provides, this release provides an empty class Checkpoint for easier inheritance. In all internal code, the check is made against the Checkpoint class in order to ensure everything works properly for custom classes.",,0
Update tests/plugins/test_sharded_plugin.py,0.6047377,def configure_sharded_model(self):,Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Ensure we do windows check first,0.43787226,Run ddp_spawn dataloader checks on Windows (#6930),,0
Add check to fairscale override,0.5526249,"    strategy=""fsdp_native"",  # or strategy=""fsdp"" for fairscale",,0
Fixes,0.7043789,Tons of bug fixes,,1
Add check for windows to plugin,0.32403243,- Added model configuration checking before it runs,,0
Remove amp check as guard now upstream,0.482302,enable_pl_optimizer=False by default to temporarily fix AMP issues (#5163),,0
Add explicit checkpoints for tests,0.6423124,A base Checkpoint class for extra customization,,0
Swap ordering of imports,0.46635765,"Separated utils: imports & enums (#5256, #5874)",,0
Removed lines,0.4733354,Removed attributes and methods:,,0
Fixes to import,0.47917813,Wrapped imports for traceability (#13924),,0
"Revert ""Add check to ensure 1.6""",0.47959793,Changed ModelCheckpoint version suffixes to start at 1 (#5008),This reverts commit ba312473,0
"Removed line, dont abs",0.43061924,"In addition, we fixed:",,0
Add additional else check,0.5155089,        else:,,0
Attempt try catch to prevent errors,0.5746675,Used raise .. from .. to explicitly chain exceptions (#3750),,0
[bugfix] Accumulated_gradient and TensoBoard (#4738),0.6125709,  * The `PrecisionPlugin.backward` signature changed: The `closure_loss` argument was renamed to `tensor`,  resolve bug   update   update   modify one test   remove paramters   update on comments   update changelog   update docstring   Co-authored-by: Nicki Skafte skaftenicki@gmail.com,0
Add check to ensure 1.6,0.47284168,Only check versions / env when not in the cloud (#15504),,0
Add fairscale requirement as zip before release,0.4564916,Avoided requiring the FairScale package to use precision with the fsdp native strategy (#14092),,0
"Move to percentage diff, increase diff",0.50615263,diff,,0
Add additional test cases,0.51853526,- Full tests that test specific functionality in trainer.,,0
"Removed comments, skip test",0.40599278,- fixed all the .test() calls,,0
update chlong after 1.0.8 (#4845),0.44960642,Release LAI docs as stable (#14250),,0
Add formulas and references to metrics docs (#4823),0.8189596,"docs for all Metrics (#2184, #2209)",  precision   precision   recall   f beta   confusion matrix   mse   mae   msle   expalained variance   psnr   ssim   text fp fn tp   accuracy   wiki -> sklearn for confusion metrix link   confusion matrix logging note   Co-authored-by: Roger Shieh sh.rog@protonmail.ch,1
Replace readme DQN link with bolts implementation (#4841),0.44427988,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",,0
Add additional checkpoint tests,0.63187,"Automatically reload the ""last"" checkpoint",,0
Ensure imports are not required explicitly for type casting,0.55952144,Wrapped imports for traceability (#13924),,0
Add catches around fairscale installation,0.5442635,Avoided requiring the FairScale package to use precision with the fsdp native strategy (#14092),,0
"Fixed configure_ddp, removed lr scheduler modification, added unit tests",0.60423076,Disabled lr_scheduler.step() in manual optimization  (#6825),,0
Update test for logging a metric object and state reset (#4825),0.567798,Metric compute() method will no longer automatically call reset() (#5409),  update test   docstring   Co-authored-by: Ananya Harsh Jha ananya@pytorchlightning.ai,0
Document behaviour when setting both on_step=True and on_epoch=True in self.log (#4327),0.7050665,Note: you will want to avoid logging with on_epoch=True in case of max_steps=-1.,"  update logging.rst   logger of choice   Co-authored-by: Nicki Skafte skaftenicki@gmail.com   add metrics reference   trigger ci   Revert ""trigger ci""   This reverts commit 97bf461cf9c00d182b0cc841c6b966a0ca9e85a4. Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: chaton thomas@grid.ai Co-authored-by: Roger Shieh sh.rog@protonmail.ch Co-authored-by: Ananya Harsh Jha ananya@pytorchlightning.ai",1
Update lr_monitor.py (#4826),0.50193393,move lr_finder (#3434),,0
Fixed a crash bug in MLFlow logger  (#4716),0.6757938,- Added support for running the `MLFlowLogger` with the `mlflow-skinny` package ([16513](https://github.com/Lightning-AI/lightning/pull/16513)),"  warnings.warn doesn't accept tuples, which causes ""TypeError: expected string or bytes-like object"" when the execution flow gets to this warning. Fixed that.   Try adding a mock test   Try adding a mock test   Co-authored-by: rohitgr7 rohitgr1998@gmail.com Co-authored-by: chaton thomas@grid.ai",0
Fix torchtext data to gpu (#4785),0.6335329,Corrected call to torch.no_grad (#5124),Co-authored-by: chaton thomas@grid.ai,0
[tests/checkpointing] refactor with BoringModel (#4661),0.5596745,Add function to remove checkpoint to allow override for extended classes (#16067),  [tests/checkpointing] refactor with BoringModel   [tests/checkpointing] refactor with BoringModel   [tests/checkpointing] refactor with BoringModel   LessBoringModel -> LogInTwoMethods   LessBoringModel -> LogInTwoMethods   LessBoringModel -> TrainingStepCalled   Co-authored-by: chaton thomas@grid.ai Co-authored-by: Ananya Harsh Jha ananya@pytorchlightning.ai,0
fix incomplete progress bar when refresh_rate > num batches (#4577),0.67930794,Changed progress_bar_refresh_rate trainer flag to disable progress bar when setting to 0. (#1108),  fix progress bar overshoot   fix updates for partially incomplete main  progress bar when val loop starts   add tests   chlog ,0
[docs] Add step to ensure sync_dist is adding to logging when multi-gpu enabled (#4817),0.63181645,- Removed the deprecated automatic logging of GPU stats by the logger connector ([#12657](https://github.com/Lightning-AI/lightning/pull/12657)),  Add additional check to ensure validation/test step are updated accordingly   Update docs/source/multi_gpu.rst   Co-authored-by: Nicki Skafte skaftenicki@gmail.com  Update docs/source/multi_gpu.rst  Co-authored-by: Nicki Skafte skaftenicki@gmail.com  Update docs/source/multi_gpu.rst  Co-authored-by: Nicki Skafte skaftenicki@gmail.com  Update docs/source/multi_gpu.rst  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Use high progress_bar_refresh_rate on Google Colab (#4654),0.81510514,Changed the default value for the progress_bar_refresh_rate Trainer argument in Google COLAB notebooks to 20 (#5516), Use high refresh rate on Google Colab (#3786)  Automatically override progress_bar_refresh_rate when on Google Colab. Also added a constant IS_COLAB in utilities to check whether it is being run in colab or not. (#3786)   Show a warning instead of overriding when rate is low on colab   Change warning to suggestion and move it   Moved warning to configure_progress_bar instead of on_trainer_init  Apply suggestions from code review  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  add a mock test  Co-authored-by: chaton thomas@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
Fixed name ref,0.4417779,| Old name                     | New name                       |,,0
Merge branch 'master' into feature/fairscale-817-6n,0.44452506,"merge backends (#3476, #3477, #3478, #3480, #3482)",Conflicts: pytorch_lightning/accelerators/accelerator.py pytorch_lightning/accelerators/ddp2_accelerator.py pytorch_lightning/accelerators/ddp_accelerator.py pytorch_lightning/accelerators/ddp_cpu_spawn_accelerator.py pytorch_lightning/accelerators/ddp_hpc_accelerator.py pytorch_lightning/accelerators/ddp_spawn_accelerator.py pytorch_lightning/accelerators/dp_accelerator.py pytorch_lightning/plugins/ddp_plugin.py pytorch_lightning/trainer/connectors/model_connector.py,0
feat(wandb): let wandb cli handle runs (#4648),0.6914536,WandbLogger does not force wandb reinit arg to True anymore and creates a run only when needed (#4648),  feat(wandb): reinit handled by CLI   fix: typo   docs(wandb): improve formatting   test(wandb): set wandb.run to None   test(wandb): fix tests   style: fix formatting   docs(wandb): fix documentation   Update code markup   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   docs(wandb): update CHANGELOG   test(wandb): init called only when needed   Update CHANGELOG.md   try fix the test   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: chaton thomas@grid.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: rohitgr7 rohitgr1998@gmail.com,0
5/n: Extract reference model call to plugins/accelerators (#4773),0.6033906,Refactored Accelerators and Plugins (#5743),"  Encapsulate extracting reference model within the plugin to allow custom wrapper logic to live within the plugin/accelerators   Add missing new lines   Fix call to accelerator   Removed double blank   Use accelerator backend   Handle case where wrapper has not been initialized within the plugin   Added basic get model tests, add better typing   Change model name   Split GPU/DDP test   Add stronger typing, skip ddp test on windows   Fix import   Fix import in dp   Fixed PEP8 definition   Add ddp launcher for ddp testing   Modify accelerator reference model to property, change name to reflect func   Revert property as this is incorrect.=   Revert across accelerators   Modified name to get_model_from_plugin   Code review changes, fix issue with dp   Add verb to function getter   Co-authored-by: chaton thomas@grid.ai",0
[Metrics] Unification of FBeta (#4656),0.64696956,Renamed class metric Fbeta >> FBeta (#4656),  implementation   init files   more stable reduction   add tests   docs   remove old implementation   pep8   changelog   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Nicki Skafte nugginea@gmail.com Co-authored-by: Teddy Koker teddy.koker@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Fix batch_arg_name bug (#4812),0.6531188,"Removed output argument from *_batch_end hooks (#3965, #3966)",Add batch_arg_name to all calls to _adjust_batch_size,0
Ensure we check if we should use sharded amp plugin,0.49344417,        :param use_amp: Whether amp was requested or not,,0
Fix conversion in on_before_forward,0.45085216,Hook on_after_backward is called only when optimizer_step is being called (#4439),,0
Add module wrapper code,0.45527077,| Function unwrap_lightning_module                                                             | 1.10             | Strategy.lightning_module          |,,0
Fixed reference,0.339756,Changed type checker with explicit cast of ref_model object (#4457),,0
Unified API upstream with suggestion to ben,0.44481918,Refactor cloud dispatch and update to new API (#16456),,0
Assert availability via imports,0.5012691,Wrapped imports for traceability (#13924),,0
Add base code,0.45427114,"Base classes (#1326, #1877)",,0
Revert across accelerators,0.5725899,reduced accelerator selection (#3211),,0
Merge branch 'master' into feature/817-fairscale-5n,0.4392136,"merge backends (#3476, #3477, #3478, #3480, #3482)",,0
Revert property as this is incorrect.=,0.50518113,Changed overwrite to True (#16009),,0
Fix #4375: Always use trainer.global_step for step (#4376),0.8440479,Deprecated the Trainer.train_loop property in favor of Trainer.fit_loop (#8025),"  Fix #4375: Always use trainer.global_step for step   Changelog   Remove superflous use ""epoch""   Update Changelog   Co-authored-by: Nicki Skafte skaftenicki@gmail.com",1
"Modify accelerator reference model to property, change name to reflect func",0.59426266,"To simplify this process, we've deprecated the per-accelerator properties to have accelerator agnostic properties. For example:",,0
don't override PYTHONWARNINGS (#4700),0.99747175,Do not override PYTHONWARNINGS (#4700),Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Clarify checkpoint deprecation message (#4640),0.6973704,all the checkpoint issues should be gone now (including backward support for old checkpoints),  Clarify checkpoint deprecation message   Update pytorch_lightning/trainer/connectors/callback_connector.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   Apply suggestions from code review   Apply suggestions from code review   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jeff Yang ydcjeff@outlook.com Co-authored-by: Roger Shieh sh.rog@protonmail.ch Co-authored-by: chaton thomas@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Implemented ModelSummary total params values (#4521),0.5783272,        :param model:," Implemented ModelSummary total params values  Signed-off-by: George Corrêa de Araújo george.gcac@gmail.com  Fixed documentation, handling modules that are containers for other modules when calculating total params  Signed-off-by: gca george.gcac@gmail.com  Reduced max line length, updated total number of params layout  Signed-off-by: gca george.gcac@gmail.com  Now using only top-level modules of main module to calculate total params  Signed-off-by: gca george.gcac@gmail.com  Added default value for named_modules param in summarize function  Signed-off-by: gca george.gcac@gmail.com  Removed summary function params, removed unused properties  Signed-off-by: gca george.gcac@gmail.com  Changed from np.prod(shape) to numel  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   changelog   Update pytorch_lightning/core/memory.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: chaton thomas@grid.ai Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com",0
Add prefix argument in loggers (#4557),0.67526615,"Finally, loggers are also now configurable with shorthand:","  Add prefix parameter in loggers   chlog   pep   patch test   remove args, access via self   try fix the test   try fix the test   try fix the test   prefix test   fix assert has calls   fix assert call Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com",0
Deprecate prefix argument in ModelCheckpoint (#4765),0.9698616,Deprecated prefix argument in ModelCheckpoint (#4765),  Deprecate prefix in ModelCheckpoint   chlog   Co-authored-by: chaton thomas@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
[docs] Remove the redundant indents in trainer.py (#4720),0.6142585,Deprecated the Trainer.train_loop property in favor of Trainer.fit_loop (#8025),  Remove the redundant indents in trainer.py   Update pytorch_lightning/trainer/trainer.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: chaton thomas@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Roger Shieh sh.rog@protonmail.ch,0
Model summary: add 1 decimal place (#4745),1.0000001,Model summary: add 1 decimal place (#4745),"Show 1999 parameters as 1.9 K and 1000 parameters as 1.0 K, rather than both as 1 K. Co-authored-by: chaton thomas@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Sean Naren sean.narenthiran@gmail.com",1
add version for CPU users (#4794),0.55527955,Set version as today (#13906),,0
increase Parity threshold (#4795),0.37967855,Mixed precision overhaul (#16783),  increase Parity threshold   typos   increase   increase ,0
Add ddp launcher for ddp testing,0.62963104,        Override to init DDP in a different way or use your own wrapper.,,0
Update examples - use DataModule (#4740),0.61237806,"Deprecated DataModule properties: has_prepared_data, has_setup_fit, has_setup_validate, has_setup_test, has_setup_predict, has_teardown_fit, has_teardown_validate, has_teardown_test, has_teardown_predict (#7657)",  rename   add mnist_datamodule.py   dm   fix   imports   clean   imports   transforms   skip ,0
Cast hparams to dict when not using omegaconf (#4770),0.56679845,Support **DictConfig for hparam serialization (#2519),  init fix   init test   more specific dict assert   update changelog   Update tests/checkpointing/test_model_checkpoint.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: chaton thomas@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
[FEAT] DDP: Create DDPLauncher (#4515),0.6669442,DDP(2) backend (#2796),  test   poc   add simpler test for ddp   typo   resolve pep8   try coverage testing   trying to add coverage inside ddp   resolve flake8   update   forgot coverage   move .coveragerc   update rcfile path   update   test   update   adding description   add DDPLauncher decorator   add undecorated   push update   update ddp testing   Update tests/backends/launcher.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update tests/backends/launcher.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   update on comments   update on comments   resolve comments   resolve isort   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Add lightning-geometric (#4771),0.49799243,LightningCLI additions:,  add link   typo   Co-authored-by: Nicki Skafte skaftenicki@gmail.com,0
update stale bot (#4769),0.44513452,Cleanup cluster waiting (#16054),Co-authored-by: chaton thomas@grid.ai,0
update docs (#4739),0.5671899,Docs improvements,,0
Fixed PEP8 definition,0.41121948,Fixing critical bugs in newly added hooks and hparams assignment.,,0
Fix import in dp,0.4204726,Merged the implementation of DDPSpawnStrategy into DDPStrategy and removed DDPSpawnStrategy (#14952),,0
Fix import,0.5026718,Module imports are now catching ModuleNotFoundError instead of ImportError (#9867),,0
"Add stronger typing, skip ddp test on windows",0.51909256,DDP Debugging Improvements,,0
Split GPU/DDP test,0.6788146,DDP is recommended for multi-GPU training,,0
Change model name,0.6172858,Renames model steps (#1051),,0
"Added basic get model tests, add better typing",0.51300186,Support shorthand notation to instantiate models (#9588),,0
Handle case where wrapper has not been initialized within the plugin,0.37256628,Raise an error when lightning replaces an existing sampler (#2020),,0
Use accelerator backend,0.8061935,Refactored accelerator backends:,,1
Removed double blank,0.40575954,"In addition, we fixed:",,0
Fix call to accelerator,0.70841146,Ensure accelerator is valid if running interactively (#5970),,1
Add missing new lines,0.3425004,"In addition, we fixed:",,0
Encapsulate extracting reference model within the plugin to allow custom wrapper logic to live within the plugin/accelerators,0.484378,Refactored Accelerators and Plugins (#5743),,0
Add masked language modeling to community examples (#4744),0.319181,Refactored setup for typing friendly (#6590),Add masked language modeling (based on Transformers) to community examples Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Proper casting for np scalars in hparams logging (#4647),0.5056275,Deprecated the old way of assigning hyper-parameters through self.hparams = ... (#4813),  first implementation   add test and changelog   Update tests/loggers/test_base.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   pep8   rounding   increase casting specificity to bool + number   bugfix   changelog formatting   single loop   Update CHANGELOG.md   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: chaton thomas@grid.ai,0
[metrics] Update SSIM (#4566),0.8606005,Updated SSIM metric (#4566)(#4656),  [metrics] Update SSIM   [metrics] Update SSIM   [metrics] Update SSIM   [metrics] Update SSIM   [metrics] update ssim   dist_sync_on_step True   [metrics] update ssim   Update tests/metrics/regression/test_ssim.py   Co-authored-by: chaton thomas@grid.ai  Update pytorch_lightning/metrics/functional/ssim.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   ddp=True   Update test_ssim.py   Co-authored-by: chaton thomas@grid.ai Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com,1
Change version in init.py 1.0.4 -> 1.1-dev (#4760),0.5254365,Use correct python version in lightning component template (#13790),  Change version in init.py 1.0.4 -> 1.0.7   fix ver   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Expose scaler in amp plugin (#4737),0.55994874,training AMP scaling refactor (#3135),,0
Move init_ddp_connection to DDP Plugin (#4407),0.7755319,        Override to init DDP in a different way or use your own wrapper.,  Move init_ddp_connection to DDP Plugin   cluster-env   trainer?   imports   Update ddp_plugin.py   Co-authored-by: Sean Naren sean.narenthiran@gmail.com,1
[Example] Add Pytorch Geometric Example (#4568),0.5182552,PyTorch,  add example for Pytorch Geometric   remove hydra   add docstring   remove description   rename folder   update script to not break test   remove .lock   add Pytorch Geometric to doc   add docstring at the begining   add comments   Update pl_examples/pytorch_ecosystem/pytorch_geometric/README.md   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pl_examples/pytorch_ecosystem/pytorch_geometric/README.md  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pl_examples/pytorch_ecosystem/pytorch_geometric/cora_dna.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  add toml  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai,0
Sharded Plugin 2/n: Allow ddp plugin to modify optimizer state saving (#4675),0.7261678,- Added support for saving sharded optimizer state dict outside of `DDPShardedStrategy` ([#14208](https://github.com/Lightning-AI/lightning/pull/14208)),"  Allow ddp plugin to modify optimizer state saving   Rely on the accelerator for optimizer states   Ensure we init the accelerator for the saving function   Better comment for optim state dump   Revert ""Ensure we init the accelerator for the saving function""   This reverts commit af65effa   Added accelerator check to initialize tuner before saving model checkpoint   Simplify comment   Revert ""Added accelerator check to initialize tuner before saving model checkpoint""   This reverts commit f9929c0c   Return single optimizer state to reduce duplication   Fixed docstring   Fixed typing   Fixed comment   Added CHANGELOG.md   Co-authored-by: chaton thomas@grid.ai",1
Sharded Plugin 3/n: Expose step input to DDP plugin (#4686),0.49360597,DDP custom implementation support (override these hooks):,"  Allow ddp plugin to move the input to a different device if needed   Swapped name to on_before_forward to align with hooks in the future   Update pytorch_lightning/plugins/ddp_plugin.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   Pass variable arg type to hook, add example   Remove blank space (pep check)   Added blank line   Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",0
Delay PyPI releasing (#4730),0.39202774,py,  Delay PyPI releasing   Delay PyPI releasing   Co-authored-by: chaton thomas@grid.ai,0
update chlog after 1.0.7 release (#4735),0.51889396,Here are two major changes that apply when using multiple loggers in 1.8:,,0
pytest default color (#4703),0.45926124,Changed default TQDM to use tqdm.auto for prettier outputs in IPython notebooks (#752),  pytest default color   time   Co-authored-by: chaton thomas@grid.ai,0
Add current_score to ModelCheckpoint.on_save_checkpoint (#4721),0.81122255,Removed the deprecated model argument from ModelCheckpoint.save_checkpoint (#8688),  Add current_score to ModelCheckpoint.on_save_checkpoint   Update CHANGELOG   [ci skip]  fix  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   fix2   Add test for NaN   Fix failing tests   Simplify line   Add test docstrings   Co-authored-by: chaton thomas@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
Apply import formatting to files in the 2nd top level (#4717),0.4420324,Wrapped imports for traceability (#13924),  Update pyproject.toml   Apply isort to files in second level   Co-authored-by: chaton thomas@grid.ai,0
test PL examples (#4551),0.6010745,Simplify the PL examples structure (shallower and more readable) (#1247),  test PL examples   minor formatting   skip failing   skip failing   args   mnist datamodule   refactor tests   refactor tests   skip   skip   drop DM   drop DM   Co-authored-by: Sean Naren sean.narenthiran@gmail.com,0
Update trainer.rst (#4722),0.742283,adding Trainer.tune() (#3293),small spelling fix,1
Minor typo in the description of Adam's beta 2 (#4715),0.439687,"In addition, we fixed:","Adam's beta 2 parameter was mistakenly referred to as the first order momentum of the gradient, whereas it should be the second order momentum. This has no effect on the correct working of the example.",0
Added experiment_id to NeptuneLogger (#3462),0.6747652,Changed to the NeptuneLogger (#16761):,"  1) Added experiment_id to NeptuneLogger initialization input arguments. 2) Now function _create_or_get_experiment() overrides ""experiment_name"", ""params"", ""properties"", ""tags"".   Added test case for existing experiment.   Revert ""Added test case for existing experiment.""   This reverts commit 9f3ba2e37b0917ccad18edc980e3763e9cb9d95a.   Added test case for existing experiment.   Fix merging issue.   Moved experiment_id assignment directly to the part with experiment initialization.   Update pytorch_lightning/loggers/neptune.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",0
quick fix (#4697),0.50492454,"Cleaning (#5948, #5949, #5950)",,0
[metrics] change default behaviour of state dict (#4685),0.8290838,Metric states are no longer as default added to state_dict (#4685),  fix state dict   Update docs/source/metrics.rst   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  changelog  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: chaton thomas@grid.ai,1
allow decorate model init with saving hparams (#4662),0.97248054,Allowing decorate model init with saving hparams inside (#4662),  addd tests   use boring model   parsing init   chlog   double decorate   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  bug  Co-authored-by: chaton thomas@grid.ai Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Roger Shieh sh.rog@protonmail.ch,1
Add  williamfalcon as owner for API changes (#4610),0.482687,Removed deprecated API (#2073),  Add   williamfalcon as owner for API changes   Update .github/CODEOWNERS   Co-authored-by: Teddy Koker teddy.koker@gmail.com  Update CODEOWNERS  Co-authored-by: Teddy Koker teddy.koker@gmail.com Co-authored-by: William Falcon waf2107@columbia.edu,0
[HOTFIX] Logging for evaluation (#4684),0.63305664,Doing this minor release because correct validation metrics logging is critical.,  resolve bugs   add should_flush_logs   remove should_flush   should work   update test   use something else   Update pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py   log mock_log_metrics.mock_calls   typo   don't use keys   convert to list   typo   check kwargs   resolve bug   resolve flake8   Co-authored-by: Sean Naren sean.narenthiran@gmail.com,0
Rename distributed_backend to accelerator in examples (#4657),0.6985793,Renamed all backends to Accelerator (#4066),  Rename distributed_backend to accelerator   Update submit_ddp2_job.sh   Update 05-trainer-flags-overview.ipynb   Co-authored-by: Jeff Yang ydcjeff@outlook.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com,0
Improve skipping step tests (#4109),0.5008573,"Refactored training_batch + tests to verify correctness (#2327, #2328)",Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: chaton thomas@grid.ai Co-authored-by: Sean Naren sean.narenthiran@gmail.com,0
[Fix]:  Improve documentation (#4670),0.6832631,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",Fix an error in training_step_end() documentation #4669 Co-authored-by: Sean Naren sean.narenthiran@gmail.com,0
isolate PL debugger in tests (#4643),0.48795316,DDP Debugging Improvements,  isolate PL debugger in tests   miss   Co-authored-by: Jeff Yang ydcjeff@outlook.com,0
Makes automatic optimization a model attribute (#4602),0.8021087,Changed automatic_optimization to be a model attribute (#4602),  Makes automatic optimization a model attribute   Update trainer.py   remove setting property in model   Update pytorch_lightning/core/lightning.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update pytorch_lightning/trainer/trainer.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update trainer.py  Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: Roger Shieh sh.rog@protonmail.ch Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Jeff Yang ydcjeff@outlook.com,1
Increase parity to match logging refactor (#4651),0.6514689,Un-balanced logging properly supported (#5119),Co-authored-by: Jeff Yang ydcjeff@outlook.com Co-authored-by: chaton thomas@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Fix docs typo: train_batch => val_batch (#4659),0.73518795,"Moved attributes global_step, current_epoch, max/min_steps, max/min_epochs, batch_idx, and total_batch_idx to TrainLoop (#7437)",Co-authored-by: Jeff Yang ydcjeff@outlook.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
Fix setup callback hook to pass LightningModule through (#4608),0.6694046,1. Override LightningModule hook,  Fix setup callback hook   Update CHANGELOG.md   Update test_trainer.py   Update test_trainer.py   Update test_trainer.py   fix chlog   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
CI: Added isort import check for the code on pull-request (#4242),0.40199327,The point of this release is more bug fixes ahead of v 1.0.0. We now have CI tests on TPU thanks to @zcain117 from Google! :slightly_smiling_face:,"  added isort CI job and updated isort config   changed CI check output from files to full diff   added isort pre-commit hook   Added missing first party and restricted files affected by isort   Applied isort to root-level, docs and benchmarks   Apply suggestions from code review   Co-authored-by: Nathan Painchaud nathanpainchaud@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: chaton thomas@grid.ai",0
logger docs and api docs (#3950),0.66682696,for logger in loggers:,"  logger and api docs   remove gpu_usage_logger, lr_logger   update docstring   fix wandb example   remove step result   charts   add some charts info   Co-authored-by: Teddy Koker teddy.koker@gmail.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com",0
CI: TPU drop install horovod (#4622),0.49185735,horovod deprecation (#16141),Co-authored-by: chaton thomas@grid.ai,0
[FEAT] Add lambda closure to manual_optimizer_step (#4618),0.73267365,"Executing the optimizer_closure is now required when overriding the optimizer_step hook (#9360). If you relied on the previous behavior, we recommend to switch to Manual Optimization alltogether.",  added lambda_closure   move to types   add 2 new tests   make example more complex   add complex example to doc   added more tests   resolve doc   typo   update   update tpu optimizer_step   Apply suggestions from code review   Update pytorch_lightning/core/lightning.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  update  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
Sharded Accelerator 1/n: Expose clip gradients to plugins via abstract class (#4639),0.6129161,Update Gradient Clipping for the TPU Accelerator (#6576),"  Added abstract precision plugin to expose clip_gradients function, use within accelerator to clip gradients   Exclude model from override, keep optimizer (needed for sharded clip gradients), add override for O2 support apex   Fix doc   Applied codereview changes   Refactored clip function to encapsulate tpu changes with tpu accelerator. Default to standard clip function for vanilla torch   Pass correct grad clip val   Moved var to property   Apply code review suggestions ",0
[FIX] Average Pbar Metrics (#4534),0.5226045,Classification metrics overhaul (#4837),  wip   update   normalize loss   update test   resolve bug   update test and add TODO   make sure it can be sync   add TODO   update sol ,0
Conda: PT 1.8 (#3833),0.38459486,  * Deprecated the `pytorch_lightning.utilities.device_parser.is_cuda_available` in favor of `lightning_lite.accelerators.cuda.is_cuda_available`,  PT 1.8   unfreeze PT   drop nightly from full   add PT 1.8 to workflow   readme table   cuda   skip cuda   test 1.8   unfreeze torch vision   Co-authored-by: ydcjeff ydcjeff@outlook.com Co-authored-by: chaton thomas@grid.ai Co-authored-by: Nicki Skafte skaftenicki@gmail.com,0
[FEAT] Add pytest section to Contribution how to ?  (#4633),0.41936404,Remove pytest as a requirement to run app by @manskx in https://github.com/Lightning-AI/lightning/pull/15449,  update contributing   formatting ,0
[make] Create Makefile (#4620),0.3690536,Direct support for compiled models (#15922),  [make] Create Makefile   exclude makefile   contributing info   rm .run_local_test.sh ,0
update changelog after 1.0.6 (#4624),0.6080291,Full Changelog,  update changelog after 1.0.6   fix formatting ,0
Small typo correction on CONTRIBUTING.md (#4625),0.50694263,"In addition, we fixed:", Update CONTRIBUTING.md  Small typo correction.  Update .github/CONTRIBUTING.md  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Roger Shieh sh.rog@protonmail.ch,0
Prevent crash if sync_dist=True on CPU (#4626),1.0000001,Prevent crash if sync_dist=True on CPU (#4626),"  Added test/fix for sync_dist raising NotImplementedError   Fixed comments/formatting   Revert base class change, enforce sync tensors across accelerators, added GPU test ",1
[FEAT] Refactor logging 3/3 [v1] (#4552),0.67517745,Refactored logging,  wip   wip check how many tests break   wip   resolve some bugs   resolve more bugs   resolve 2 bugs   resolve   temp fix   update   remove useless code   remove result   try to resolve bug   update changelog   formatting   remove pl   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Sean Naren sean.narenthiran@gmail.com,0
[Fix] Move log value to cpu. (#4592),0.5284045,moved eval loop logging to loggers (#3408),  move value to cpu to save memory   update   move to cpu   try something   update   update   add back out_dict.update({k: v})   add move_metrics_to_cpu   update   Update pytorch_lightning/utilities/memory.py   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com   resolve comments   Update pytorch_lightning/core/step_result.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Sean Naren sean.narenthiran@gmail.com,0
[bug-fix] DDP and automatic_optimization=False (#4485),0.7182009,"- When manual optimization is used with DDP, we no longer force `find_unused_parameters=True` ([#12425](https://github.com/PyTorchLightning/pytorch-lightning/pull/12425))","  resolve bug   add self._running_manual_optim   update   update tests   update lightning module   resolve bug   update tests   update   resolve pep8   update   replace by ddp_spawn   temporary fix   update   update   move update to training_loop   make both ddp_spawn   introduce manual_optimizer_step   update changelog   added changelog wrong place   add force_optimizer_step   update docstring for tests   update optimizer_step   update zero_grad   resolve flake8   move update into manual_optimizer_step   add zero_grad   remove zero_grad tests   remove manual_backward in AMP, it doesn't help   update   loosen tests   update   update doc   add TODO   Removed unnecessary get model from native amp   Remove try except with pytest raise   Add seed, clean up imports, remove try catch to reproduce error   update code   update test   revert back   formatting   Update pytorch_lightning/core/lightning.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: SeanNaren sean@grid.ai Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",1
fix mock pkgs in docs (#4591),0.511965,Pkg: fix parsing versions by @Borda in https://github.com/Lightning-AI/lightning/pull/15401,  fix mock pkgs in docs   sphinx   CI   Co-authored-by: chaton thomas@grid.ai,0
"Find parameters which are specified in the LightningDataModule, only (#4347)",0.58559996,Changed logging of LightningModule and LightningDataModule hyperparameters to raise an exception only if there are colliding keys with different values (#9496),  search for attribute in datamodule if not found elsewhere   add test for datamodule   add lightning_getattr test for datamodule   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   Update CHANGELOG.md   Update CHANGELOG.md   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
fix logged keys in mlflow logger (#4412),0.6163508,- Allow logging to an existing run ID in MLflow with `MLFlowLogger` ([#12290](https://github.com/PyTorchLightning/pytorch-lightning/pull/12290)),"  [#4411] fix gpu_log_memory with mlflow logger   sanitize parenthesis instead of removing for all loggers   apply regex for mlflow key sanitization   replace ',' with '.' typo   add single warning and test   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: chaton thomas@grid.ai",0
[req] Set min version for skimage for tests (#4598),0.49789876,Set version as today (#13906),Co-authored-by: Nicki Skafte skaftenicki@gmail.com,0
Replace a MisconfigurationException with warning in ModelCheckpoint callback (#4560),0.99569285,Replace MisconfigurationException with warning in ModelCheckpoint Callback (#4560),  replace MisconfigurationException with warning   update test   check raising UserWarning   Co-authored-by: chaton thomas@grid.ai Co-authored-by: Nicki Skafte skaftenicki@gmail.com,1
Metric ddp bugfix (#4482),0.68958473,brand new Metrics package with built-in DDP support (by @justusschock and  @SkafteNicki),  changes   fix spelling   small note   trying to fix ddp test   fix ddp   fix for test   suggestion   CHANGELOG   Update pytorch_lightning/metrics/metric.py   Co-authored-by: chaton thomas@grid.ai Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: Sean Naren sean@grid.ai,0
Skip tuner algorithms on fast dev (#3903),0.81276935,Tuner algorithms will be skipped if fast_dev_run=True (#3903),  skip on fast dev   fix error   changelog   fix recursive issue   combine tests   pep8   move logic to base funcs   fix mistake   Update pytorch_lightning/tuner/lr_finder.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  pep  Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: Nicki Skafte nugginea@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: chaton thomas@grid.ai,1
Fix load disparity between normal and hpc (#4526),0.5004142,Fix hanging in DDP HPC accelerators (#5157),  Add missing load functionality in hpc   Add general file load for hpc   Add mark in CHANGELOG   Fix Typo Lihgtning   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Refactor line separation  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   Fix entangled fixation commit   Fix naming of restore_model_states   Fix amp restore place   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: chaton thomas@grid.ai,0
[dockers] install nvidia-dali-cudaXXX (#4532),0.4808532,  * Deprecated the `pytorch_lightning.utilities.device_parser.num_cuda_devices` in favor of `lightning_lite.accelerators.cuda.num_cuda_devices`,  [dockers] install nvidia-dali-cuda100   Apply suggestions from code review   build DALI   build DALI   build DALI   dali from source   dali from source   use binaries   qq   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai Co-authored-by: Sean Naren sean.narenthiran@gmail.com,0
Missing TorchScript trace's update (#4586),0.6174205,- Fixed an issue with comparing torch versions when using a version of torch built from source ([#17030](https://github.com/Lightning-AI/lightning/pull/17030)),Co-authored-by: stef-ubuntu stef@webempath.com Co-authored-by: Jeff Yang ydcjeff@outlook.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com,0
Fix docstring (#4585),0.43956244,Docs,Co-authored-by: Jeff Yang ydcjeff@outlook.com,0
Adding pytorch-forecasting to community examples (#4575),0.5461508,Highlights of this release are adding support for TorchElastic enables distributed PyTorch training jobs to be executed in a fault-tolerant and elastic manner; auto-scaling of batch size; new transfer learning example; an option to provide seed to random generators to ensure reproducibility.,PyTorch Forecasting is a new library that is designed for time series forecasting practitioners and researchers alike. It is based on the awesome work on PyTorch Lightning. Thanks a lot for creating such an asset! Have a look at the documentation for more information. Co-authored-by: chaton thomas@grid.ai Co-authored-by: Jeff Yang ydcjeff@outlook.com,0
[Docs] Note on running metric in dp (#4494),0.7055764,brand new Metrics package with built-in DDP support (by @justusschock and  @SkafteNicki),  note   Update docs/source/metrics.rst   Co-authored-by: chaton thomas@grid.ai Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: Jeff Yang ydcjeff@outlook.com,1
Accelerator docs (#4583),0.7751118,reduced accelerator selection (#3211),  accelerator docs   accelerator docs ,1
"ref: unify slurm and TE under backendPlugin 5/n"" (#4582)",0.7300856,"Unify sLURM/TorchElastic under backend plugin (#4578, #4580, #4581, #4582, #4583)",  ref: unify slurm and TE under backendPlugin 4/n   ref: unify slurm and TE under backendPlugin 5/n ,1
ref: unify slurm and TE under backendPlugin 3/n (#4581),0.72879696,"Unify sLURM/TorchElastic under backend plugin (#4578, #4580, #4581, #4582, #4583)",,1
ref: unify slurm and TE under backendPlugin 2/n (#4580),0.7242823,"Unify sLURM/TorchElastic under backend plugin (#4578, #4580, #4581, #4582, #4583)",,1
ref: unify slurm and TE under backendPlugin 1/n (#4578),0.722102,"Unify sLURM/TorchElastic under backend plugin (#4578, #4580, #4581, #4582, #4583)",  ref: unify slurm and TE under backendPlugin   ref: unify slurm and TE under backendPlugin ,1
Adds shortcut for path to log (#4573),0.58301127,"Finally, loggers are also now configurable with shorthand:",  added log_dir shortcut to trainer properties for writing logs   added log_dir shortcut   added log_dir shortcut   added log_dir shortcut   added log_dir shortcut   added log_dir shortcut   added log_dir shortcut   added log_dir shortcut   added log_dir shortcut ,0
updated trainer docs (#4571),0.7273648,API changes to the trainer,,1
updated trainer docs (#4570),0.72490144,API changes to the trainer,,1
added trainer api docs (#4569),0.75292003,API changes to the trainer,,1
add congratulations at the end of our notebooks (#4555),0.40068465,"@ashwinb, @awaelchli, @Borda, @cmpute, @festeh, @jbschiratti, @justusschock, @kepler, @kumuji, @nanddalal, @nathanbreitsch, @olineumann, @pitercl, @rohitgr7, @S-aiueo32, @SkafteNicki, @tgaddair, @tullie, @tw991, @williamFalcon, @ybrovman, @yukw777",  add congratulations at the end of our notebooks   udpate image ,0
Add Dali MNIST example (#3721),0.39323428,"Epoch 8:  53%|█████    | 17/32 [5.13/s, v_num=2, loss=0.5643]","  add MNIST DALI example, update README.md   Fix PEP8 warnings   reformatted using black   add mnist_dali to test_examples.py   Add documentation as docstrings   add nvidia-pyindex and nvidia-dali-cuda100   replace nvidia-pyindex with --extra-index-url   mark mnist_dali test as Linux and GPU only   adjust CUDA docker and examples.txt, fix import error in test_examples.py   adjust the GPU check   Exit when DALI is not available   remove requirements-examples.txt and DALI pip install   Refactored example, moved to new logging api, added runtime check for test and dali script   Patch to reflect the mnist example module   add req.   Apply suggestions from code review   Removed requirement as it breaks CPU install, added note in README to install DALI   add DALI to Drone   test examples   Apply suggestions from code review   imports   ABC   cuda   cuda   pip DALI   Move build into init function   Co-authored-by: SeanNaren sean@grid.ai Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Sean Naren sean.narenthiran@gmail.com",0
[ci] tag v1.4.1 for pypa/gh-action-pypi-publish (#4548),0.40225327,@akihironitta @awaelchli @Borda @carmocca @kaushikb11 @krshrimali @mauvilsa @otaj @pre-commit-ci @rohitgr7 @semaphore-egg @tkonopka @wayi1,,0
show progressbar only on progress_rank 0 on ddp_slurm (#4437),0.64038044,progress bar,Co-authored-by: chaton thomas@grid.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
[feat] Logging refactor 2/n - train (#4495),0.6313906,Refactored logging,"  update logging   solve more bugs   replace Mapping by Dict   update on comments   resolve pep8   Apply suggestions from code review   Co-authored-by: ananthsub ananth.subramaniam@gmail.com  Update pytorch_lightning/trainer/connectors/logger_connector/epoch_result_store.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   update on comments   typo   update for coverage   update test   update   Update tests/models/test_hooks.py   Co-authored-by: Sean Naren sean.narenthiran@gmail.com  Update tests/models/test_hooks.py  Co-authored-by: Sean Naren sean.narenthiran@gmail.com   update on comments   remove deepcopy   remove useless look for   another small optim   extra optim   remove lastest optim, can be source of bug   resolve bug   add docstring   optimize coverage   Update pytorch_lightning/trainer/connectors/logger_connector/epoch_result_store.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/trainer/connectors/logger_connector/epoch_result_store.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update tests/trainer/logging_tests/test_distributed_logging.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/trainer/evaluation_loop.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update tests/trainer/logging/test_logger_connector.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update tests/trainer/logging_tests/test_train_loop_logging_1_0.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   update on comments   update   update on comments   update parity speed   get it down to 0.65   update   0.8 max_dif   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: William Falcon waf2107@columbia.edu",0
update PR template (#4523),0.40352255,Syntax changes are: ,  update PR template   Update .github/PULL_REQUEST_TEMPLATE.md   Co-authored-by: Roger Shieh sh.rog@protonmail.ch  Apply suggestions from code review  Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com Co-authored-by: chaton thomas@grid.ai Co-authored-by: Roger Shieh sh.rog@protonmail.ch Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com,0
Horovod: fixed early stopping and added metrics aggregation (#3775),0.56431466,"As we continue to strengthen the codebase with more tests, we’re finally getting rid of annoying bugs that have been around for a bit now. Mostly around the inconsistent checkpoint and early stopping behaviour (amazing work @awaelchli  @jeremyjordan )",  Fixed early stopping for Horovod   Refactored to sync_dist_if_available   Bump min Horovod version to support hvd.is_initialized   Changelog   Added back change for Horovod   Removed redundant checks for initialization   Implement metrics gathering for Horovod   Added test for EvalResult   Renamed ddp_sync_on_step -> dist_sync_on_step   Added metric test for Horovod   Added option pass callable allgather function to metric base class   Added dist_sync_fn   Fixed calls to private _sync_dist   Fixed Horovod test   Added sync_tensor to the distributed backend   Skip Windows   Insert test path   Removed redundant import   Updated drone   Unset HOROVOD_GPU_ALLREDUCE   Unset   No cache dir   No uninstall   Unset variables   Uninstall Horovod during initialization   Replaced more references to ddp_sync_on_step   Fixed imports   Fixed attribute   Added back default   Lint   Added back docstring   Made gather_all_tensors default   Added whitespace   Update tests/models/test_horovod.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/metrics/metric.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update CHANGELOG.md  Co-authored-by: Teddy Koker teddy.koker@gmail.com Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
[dockers] use inline cache (#4511),0.4888599,- Added support for running the works without cloud compute in the default container ([#14819](https://github.com/Lightning-AI/lightning/pull/14819)),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Sean Naren sean.narenthiran@gmail.com,0
update changelog after 1.0.5 (#4505),0.63745815,Full Changelog,  update changelog   update   Co-authored-by: chaton thomas@grid.ai Co-authored-by: Sean Naren sean.narenthiran@gmail.com,0
Bugfix/4449 dict attribute error (#4480),0.54670954,Increased DeepDiff's verbose level to properly handle dict changes (#13960),  resolve a bug   resolve a bug   remove todo   resolve more bugs   update tests   Update pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py   Co-authored-by: Sean Naren sean.narenthiran@gmail.com  Update pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py  Co-authored-by: Sean Naren sean.narenthiran@gmail.com  resolve pyright  Co-authored-by: Teddy Koker teddy.koker@gmail.com Co-authored-by: Sean Naren sean.narenthiran@gmail.com,0
is_picklable: catch AttributeError (addresses #3771) (#4508),0.70322007,Make the is_picklable function more robust (#17270),  is_picklable: catch AttributeError (addresses #3771)   edit   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: chaton thomas@grid.ai,1
release v1 (#4516),0.6573349,This release includes:,,0
update contrib notes (#4514),0.56787306,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",Co-authored-by: chaton thomas@grid.ai,0
Update default filename (#4500),0.51285666,Fix saved filename in ModelCheckpoint if it already exists (#4861),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jeff Yang ydcjeff@outlook.com,0
extend release testing (#4506),0.60704994,Updated app testing (#16000),  extend release testing   Drone   also PR to release   actions versions ,0
Avoid torchscript export for Metric forward (#4428),0.66810006,Avoid the deprecated onnx.export(example_outputs=...) in torch 1.10 (#11116),  Update metric.py   add test   Update CHANGELOG.md   Update test_metric_lightning.py   Update test_metric_lightning.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Switch to PyTorch 1.6 in Drone CI (#4393),0.7327355,Enable PyTorch 1.7 compatibility (#3541),  switch to 1.6   readme   1.7   back to normal [ci skip]   horovodrun --verbose   try with apex   add apex test   change base   description   test with 1.7   back to 1.6   no gradient_clip_val   re-add gradient_clip_val   no amp   temp skip torch.cuda.amp + horovod test   Apply suggestion from code review   Co-authored-by: Sean Naren sean.narenthiran@gmail.com   Fix formatting   ddp   Moved extended model outside of function to prevent pickling issue for drone   typo   resolve bug   extract automatic_automization   Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: SeanNaren sean@grid.ai Co-authored-by: chaton thomas@grid.ai,1
feature/ Add note about Argparse. (#4321),0.8038099,argparse_utils >> argparse,  add a note about argparse   update   Co-authored-by: Jeff Yang ydcjeff@outlook.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
Add CheckpointConnector internal commentaries (#4421),0.6124539,Refactor load in checkpoint connector (#4593),  Add CheckpointConnector commentaries   Fix comment format   Fix save/load schema as function comments   Co-authored-by: chaton thomas@grid.ai,0
"Update old ""module_arguments"" and ""hparams"" references in docs (#4417)",0.60483867,Deprecated the old way of assigning hyper-parameters through self.hparams = ... (#4813),  replace module_arguments refernces   update hparams docs   add missing save_hyperparameters in example   deprecate instead of remove   Update docs/source/hyperparameters.rst   Co-authored-by: chaton thomas@grid.ai  Update docs/source/hyperparameters.rst  Co-authored-by: Teddy Koker teddy.koker@gmail.com Co-authored-by: chaton thomas@grid.ai Co-authored-by: Sean Naren sean.narenthiran@gmail.com,0
Add fsspec to tuner  (#4458),0.87729466,Changed fsspec to tuner (#4458),  Add fsspec to tuner   suggestions   pathlib   pep   missed pep ,1
Disable training when limit_train_batches=0 (#4371),0.90439785,Disabled training when limit_train_batches=0 (#4371),  Disable training when limit_train_batches=0   chlog   pep   limit_train_batches   BoringModel   Co-authored-by: Roger Shieh sh.rog@protonmail.ch,1
Disable saving checkpoints if not trained (#4372),0.99999994,Disable saving checkpoints if not trained (#4372),  Disable saving checkpoints if not trained   chlog   update test   fix   Co-authored-by: chaton thomas@grid.ai,1
[test] Accumulated gradient optimization tests (#4477),0.5334004,Refactored the logic around manual and automatic optimization inside the optimizer loop (#7526),  adding tests   wip   update   Update tests/trainer/test_trainer.py   Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: Sean Naren sean.narenthiran@gmail.com,0
[bug] [docs] Clearer optimizer_step override instructions (#4455),0.7634667,"Executing the optimizer_closure is now required when overriding the optimizer_step hook (#9360). If you relied on the previous behavior, we recommend to switch to Manual Optimization alltogether.",  fix   flags   remove defaults ,1
[FEAT]  logging refactors 1/n (#4439),0.6385007,Refactored logging,  introducing new logging object   typo   typo   Update pytorch_lightning/trainer/logging.py   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com  Update pytorch_lightning/trainer/logging.py  Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com   update on comments   update on comments   add more doctstring   Update pytorch_lightning/core/lightning.py   Co-authored-by: Sean Naren sean.narenthiran@gmail.com   resolve on comments   solve pyright   Update pytorch_lightning/trainer/connectors/logger_connector/epoch_result_store.py   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com   update on comments   Update pytorch_lightning/trainer/connectors/logger_connector/epoch_result_store.py   Co-authored-by: Sean Naren sean.narenthiran@gmail.com  update on comments  Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Sean Naren sean.narenthiran@gmail.com,0
[Metrics] Detach bugfix (#4313),0.65714777,"Removed experimental Metric API (#3868, #3943, #3949, #3946), listed changes before final removal:",  detach on buffer   doc update   remove file   changelog   suggestions   Update docs/source/metrics.rst   Co-authored-by: Teddy Koker teddy.koker@gmail.com   fix for 4266   Update docs/source/metrics.rst   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update CHANGELOG.md  Co-authored-by: Teddy Koker teddy.koker@gmail.com Co-authored-by: chaton thomas@grid.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Ananya Harsh Jha ananya@pytorchlightning.ai Co-authored-by: Roger Shieh sh.rog@protonmail.ch Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
Rename conflicting test directories (#4451),0.4899112,Rename failed -> error in tables (#15608),  logging -> logging_tests   warnings -> warnings_tests   Co-authored-by: Jeff Yang ydcjeff@outlook.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
[BUGFIX] AMP + Precision unscale grad (#4441),0.63610935,Apex mixed precision gets replaced with AMP (#16149),  move unscale within Native plugin   remove gradient tracking from lightning backward   forgot trainer.fit   typo   update   cleanup   set to 1.6   typo   skip if below 1.6 strict   update changelog   remove useless code   Update tests/plugins/test_amp_plugin.py   Co-authored-by: Sean Naren sean.narenthiran@gmail.com  Update tests/plugins/test_amp_plugin.py  Co-authored-by: Sean Naren sean.narenthiran@gmail.com   update changelog   Update CHANGELOG.md   Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: Jeff Yang ydcjeff@outlook.com,0
[DOC] Clarify tpu_cores training. (#4475),0.72024834,  * Removed the `Trainer(tpu_cores=...)` argument,  better explanation around tpu_cores   more details on tpu training   Apply suggestions from code review   Co-authored-by: Jeff Yang ydcjeff@outlook.com,1
Add step index in checkpoint name (#3807),0.6168597,"Removed epoch and step arguments from ModelCheckpoint.format_checkpoint_name(), these are now included in the metrics argument (#7344)","  true final value of global step   ch check   tests   save each validation interval   wip   add test   add test   wip   fix tests, revert old edits, fix merge conflicts, update doctests   test + bugfix   sort files   format test   suggestion by ananth   added changelog   naming   docs   example   suggestion   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   fix test   pep   pep   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com",0
Add manual logging to training_step manual optimization (#4476),0.7049087,Consistently use step=trainer.global_step in LearningRateMonitor independently of logging_interval (#4376),,1
[BUG-FIX] WandbLogger _sanitize_callable (#4422),0.7271137,Removed the deprecated sync_step argument from WandbLogger (#8763),  fix   resolve CodeFormatter   Update pytorch_lightning/loggers/base.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Fix a typo in README (#4474),0.52801436,Pkg: fix parsing versions by @Borda in https://github.com/Lightning-AI/lightning/pull/15401,,0
update docs on checkpoint_callback Trainer argument (#4461),0.8440108,Deprecated checkpoint_callback from the Trainer constructor in favor of enable_checkpointing (#9754),  docs update   update callbacks docs   docs   notebook examples   warning   line lenght   update deprecation   Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: Roger Shieh 55400948+s-rog@users.noreply.github.com,1
lock pytorch nightly version (#4469),0.7440994,Drop PyTorch 1.9 support (#15347),,1
timeout for tpu check (#4340),0.79745394,Increased TPU check timeout from 20s to 100s (#5598),  timeout for tpu check   added tests   updated CHANGELOG.md   fixed windows tests   Update pytorch_lightning/utilities/xla_device_utils.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  requested changes  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Fix type checker issue with explicit cast of ref_model object (#4457),0.97502685,Changed type checker with explicit cast of ref_model object (#4457),Co-authored-by: Jeff Yang ydcjeff@outlook.com,1
[docker] Lock cuda version (#4453),0.55479795,  * Deprecated the `pytorch_lightning.utilities.device_parser.num_cuda_devices` in favor of `lightning_lite.accelerators.cuda.num_cuda_devices`,  lock cuda version   back to normal ,0
ReduceLROnPlateau doc fixed (#4459),0.6400292,We have also added support for the ReduceLROnPlateau scheduler with shorthand notation:,,0
[Metrics] Add multiclass auroc (#4236),0.6649772,Sklearn metrics classes (#1327),  Add functional multiclass AUROC metric   Add multiclass_auroc tests   fixup! Add functional multiclass AUROC metric   fixup! fixup! Add functional multiclass AUROC metric   Add multiclass_auroc doc reference   Update CHANGELOG   formatting   Shorter error message regex match in tests   Set num classes as pytest parameter   formatting   Update CHANGELOG   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com,0
PyTorch 1.7 Stable support (#3821),0.8438404,Enable PyTorch 1.7 compatibility (#3541),"  prepare for 1.7 support [ci skip]   tpu [ci skip]   test run 1.7   all 1.7, needs to fix tests   couple with torchvision   windows try   remove windows   1.7 is here   on purpose fail [ci skip]   return [ci skip]   1.7 docker   back to normal [ci skip]   change to some_val [ci skip]   add seed [ci skip]   4 places [ci skip]   fail on purpose [ci skip]   verbose=True [ci skip]   use filename to track   use filename to track   monitor epoch + changelog   Update tests/checkpointing/test_model_checkpoint.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com",1
[Changelog] 1.0.4 (#4440),0.71893185,[1.4.0] - 2021-07-27,  changelog 1.0.4   changelog 1.0.4 ,1
[Metrics] Confusion matrix class interface (#4348),0.6594728,Sklearn metrics classes (#1327), docs + precision + recall + f_beta + refactor  Co-authored-by: Teddy Koker teddy.koker@gmail.com  rebase  Co-authored-by: Teddy Koker teddy.koker@gmail.com  fixes  Co-authored-by: Teddy Koker teddy.koker@gmail.com   added missing file   docs   docs   extra import   add confusion matrix   add to docs   add test   pep8 + isort   update tests   move util function   unify functional and class   add to init   remove old implementation   update tests   pep8   add duplicate   fix doctest   Update pytorch_lightning/metrics/classification/confusion_matrix.py   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com   changelog   bullet point args   bullet docs   bullet docs   Co-authored-by: ananyahjha93 ananya@pytorchlightning.ai Co-authored-by: Teddy Koker teddy.koker@gmail.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: chaton thomas@grid.ai Co-authored-by: Roger Shieh 55400948+s-rog@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
fix ModelCheckpoint docs (#4426),0.81624943,"Removed deprecated ModelCheckpoint arguments prefix, mode=""auto"" (#6162)",,1
deprecate passing ModelCheckpoint instance to Trainer(checkpoint_callback=...) (#4336),0.964118,Deprecated passing ModelCheckpoint instance to checkpoint_callback Trainer argument (#4336),  first attempt   update tests   support multiple   test bugfix   changelog   pep   pep   import order   import   improve test for resuming   test   update test   add references test   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  docstring suggestion deprecation  Co-authored-by: Jeff Yang ydcjeff@outlook.com  paramref  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jeff Yang ydcjeff@outlook.com,1
[docs] distributed_backend -> accelerator (#4429),0.7304307,Refactored accelerator backends:,  distributed_backend -> accelerator   distributed_backend -> accelerator   use_amp -> precision   format   Co-authored-by: rohitgr7 rohitgr1998@gmail.com,1
Skips DDP parameter sync (#4301),0.603981,Deprecated num_nodes and sync_batchnorm arguments in DDPPlugin and DDPSpawnPlugin (#7026),  ddp no-sync   Update pytorch_lightning/trainer/training_loop.py   Co-authored-by: ananthsub ananth.subramaniam@gmail.com   Update training_loop.py   factor enter and exit out to separate context manager   delete _updated_model_last_step   Co-authored-by: justusschock justusschock@pc125.lfb.rwth-aachen.de Co-authored-by: Teddy Koker teddy.koker@gmail.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: chaton thomas@grid.ai Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
"fix: nb is set total number of devices, when nb is -1. (#4209)",0.59845644,    devices=1," fix: nb is set total number of devices, when nb is -1.  Refs: #4207  feat: add test code test combination auto_select_gpus, gpus options using Trainer test pick_multiple_gpus function directly    Refs: #4207  docs: modify contents in Select GPU devices  Refs: #4207  refactore: reflect the reuslt of review  Refs: #4207  refactore: reflect the reuslt of review  Refs: #4207  Update CHANGELOG.md  Co-authored-by: chaton thomas@grid.ai Co-authored-by: Roger Shieh 55400948+s-rog@users.noreply.github.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com",0
Fix CSV logger warning (#4419),0.5773606,Re-Enable Logger's ImportErrors (#1938),Co-authored-by: Jeff Yang ydcjeff@outlook.com,0
move example inputs to correct device when tracing module (#4360),0.4565599,"-         input, target = input.to(device), target.to(device)","  use move_data_to_device instead of to; docstring also allow tuple of Tensor; not supported log error when example_inputs is a dict; commented docstring trace example   Use isinstance to check if example_inputs is a Mapping, instead of type   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com   import Mapping for isinstance check   multi-line docstring code to test TorchScript trace()   Fix PEP8 f-string is missing placeholders   minor code style improvements   Use (possibly user overwritten) transfer_batch_to_device instead of move_data_to_device   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   fixed weird comment about trace() log error   Remove unused import   Co-authored-by: Jeff Yang ydcjeff@outlook.com  Remove logger warning about dict not example_inputs not supported by trace  Co-authored-by: stef-ubuntu stef@webempath.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Jeff Yang ydcjeff@outlook.com",0
feat(wandb): log in sync with Trainer step (#4405),0.7861309,W&B log in sync with Trainer step (#4405),  feat(wandb): log in sync with Trainer step   docs: update CHANGELOG   style(test_wandb): fix formatting   parentheses   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
Add individuals to Metrics in CODEOWNERS (#4413),0.5212454,Removed deprecated metrics (#8586),  ananyahjha93 and teddykoker to codeowners for metrics   add Justus   Co-authored-by: Sean Naren sean.narenthiran@gmail.com,0
add option to log momentum (#4384),0.52663463,"Finally, loggers are also now configurable with shorthand:",  add option to log momentum   add docstring   refactor for cleanliness   Co-authored-by: Nicki Skafte skaftenicki@gmail.com,0
[Docs] Warning on metric states (#4388),0.68955123,Metric states are no longer as default added to state_dict (#4685),  warning on states   suggestion   Update metrics.rst   Co-authored-by: Sean Naren sean.narenthiran@gmail.com,0
Add optimizer hooks in callbacks (#4379),0.78196245,Optimizer Hooks,  Add optimizer hooks in callbacks   optimizer param   update test   Co-authored-by: Nicki Skafte skaftenicki@gmail.com,1
"Add ""monitor"" to saved ModelCheckpoints (#4383)",0.80189824,Allow ModelCheckpoint monitor to be None (#3633),  Add key   Remove unused variables   Update CHANGELOG [skip ci]   best_model_monitor -> monitor   Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
Update init.py,0.67009526,"    def init(self, args, kwargs):",,0
Fix  COMET_EXPERIMENT_KEY environment variable usage in comet logger (#4230),0.6651336,Using .comet.config file for CometLogger (#1913),  Fix  COMET_EXPERIMENT_KEY environment variable usage   Remove unused arg   Update comet.py   Add test by Lothiraldan   remove blank   Co-authored-by: chaton thomas@grid.ai Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Sean Naren sean.narenthiran@gmail.com,0
Enable DDP Plugin to pass through args to LightningDistributedDataParallel (#4382),0.6792798,Deprecated LightningDistributedDataParallel in favor of new wrapper module LightningDistributedModule (#5185),"  Update ddp_plugin.py   Update ddp_plugin.py   Update ddp_plugin.py   Update test_ddp_plugin.py   Update pytorch_lightning/plugins/ddp_plugin.py   Update pytorch_lightning/plugins/ddp_plugin.py   Fixed imports, make ddp_kwargs protected   Co-authored-by: SeanNaren sean.narenthiran@gmail.com",0
feature: Allow str arguments in Trainer.profiler (#3656),0.78651285,Removed support for passing a bool value to profiler argument of Trainer (#6164),"  allow trainer's profiler param to have a str value   add tests   update docs   update exception message   Update CHANGELOG   fix pep8 issues   cleanup test code   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   Add deprecation warning if using bool for profiler   Add deprecation tests and move deprecated tests   Remove bool option to profiler from docs   Deprecate bool args to profiler in CHANGELOG   fixup! Add deprecation warning if using bool for profiler   fixup! Add deprecation tests and move deprecated tests   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   Implement suggestions, remove whitespace   fixup! Implement suggestions, remove whitespace   Allow bool, str (case insensitive), BaseProfiler   Add info about bool deprecation to trainer   fixup! Add info about bool deprecation to trainer   Move deprecate todo to test_deprecated   Test wrong profiler type, improve error message   fixup! Test wrong profiler type, improve error message   Update pytorch_lightning/trainer/connectors/profiler_connector.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   Apply suggestions from code review   Readd bool to profiler types, test cli profiler arg   Remove extra whitespace in doc   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Apply suggestions from code review  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update deprecation versions  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com",1
update (#4343),0.4719762,"Updated DeepSpeed ZeRO (#6546, #6752, #6142, #6321)",Co-authored-by: chaton thomas@grid.ai,0
get help from docstring (#4344),0.457857,Docs,  Add geting help message from docstring   Fix pep8 issue   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Apply suggestions from code review  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Sean Naren sean.narenthiran@gmail.com,0
[Doc] Fix on_train_batch_end description (#4330),0.75569415,"Changed the seq of on_train_batch_end, on_batch_end & on_train_epoch_end, on_epoch_end hooks (#5688)",  fix   more doc fixes   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: chaton thomas@grid.ai,1
Update ddp_plugin.py (#4363),0.6145297,Merged the implementation of DDPSpawnStrategy into DDPStrategy and removed DDPSpawnStrategy (#14952),Co-authored-by: Sean Naren sean.narenthiran@gmail.com,0
BUG - Wandb: Sanitize callable. (#4320),0.6449158,WandbLogger does not force wandb reinit arg to True anymore and creates a run only when needed (#4648),  add _sanitize_callable_params   add call on _val if callable   clean code formatter   resolve pep8   default return function name   resolve pep8   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update CHANGELOG.md  Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Implement finalize for WandbLogger (#4341),0.7302056,Removed wandb logger's finalize method (#1193),  wandb finish   experiment   upload at end of run   changelog   comment   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Sean Naren sean.narenthiran@gmail.com,1
Drone: use nightly build cuda docker images (#3658),0.4909468,  * Deprecated the `pytorch_lightning.utilities.device_parser.num_cuda_devices` in favor of `lightning_lite.accelerators.cuda.num_cuda_devices`,  upgrade PT version   update docker   docker   try 1.5   badge   fix typo: dor -> for (#3918)   prune   prune   env   echo   try   notes   env   env   env   notes   docker   prune   maintainer   CI   update   just 1.5   CI   CI   CI   CI   CI   CI   CI   CI   CI   CI   CI   docker   CI   CI   CI   CI   CI   CI   CI   CI   CI   push   try   prune   CI   CI   CI   CI   Co-authored-by: Klyukin Valeriy mr.clyukin@gmail.com Co-authored-by: Jeff Yang ydcjeff@outlook.com,0
Enable custom apex and amp plugins (#4355),0.531983,Enabled plugins (#4041),"  enable custom apex, amp plugin   enable custom apex, amp plugin   enable custom apex, amp plugin   enable custom apex, amp plugin ",0
Enable profilers to write to remote files with fsspec (#4162),0.5219755,Used fsspec instead of gfile for all IO (#3320), Update profilers.py  Enable profilers to use write to remote files with fsspec   Update profilers.py   Update CHANGELOG.md   Update pytorch_lightning/profiler/profilers.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  formatting  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
update old references to filepath in ModelCheckpoint docs (#4339),0.8738107,Deprecated filepath in ModelCheckpoint (#4213),Co-authored-by: Nicki Skafte skaftenicki@gmail.com,1
Cache docker builds (#3659),0.53534365,Remove unnecessary intermediate layers in Dockerfiles (#5697), parent faa357648f49a75341b501ec9c9bbd4985357c10 author ydcjeff ydcjeff@outlook.com 1601049378 +0630 committer ydcjeff ydcjeff@outlook.com 1601469495 +0630  cache docker builds lock horovod at 0.19.5 done [ci skip] [CI SKIP] use --cache-from [ci skip] typo and horovod [ci skip] exclude pt 1.3 py3.8 [ci skip] conda no cache [ci skip] fix   revert   align with master [ci skip]   retry   remove empty continuation lines   add comment   fix build-args ,0
Update to NeMo Documentation (#4328),0.44172084,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ", updated header and small fixes  Signed-off-by: ericharper complex451@gmail.com  updated header and small fixes  Signed-off-by: ericharper complex451@gmail.com  updated header and small fixes  Signed-off-by: ericharper complex451@gmail.com  updated header and small fixes  Signed-off-by: ericharper complex451@gmail.com,0
remove redundant whitespace in new-project.rst (#4346),0.30959025,Show a message when BuildConfig(requirements=[...]) is passed but a requirements.txt file is already present in the Work (#15799),resolve https://github.com/PyTorchLightning/pytorch-lightning/issues/4345 Co-authored-by: Nicki Skafte skaftenicki@gmail.com,0
[Metrics] Fix/4237 auc unstable reorder (#4281),0.8189604,Removed reorder parameter of the auc metric (#5004),  =Add deprecation warning for auc reorder   =Add test for deprecation warning for auc reorder   Update CHANGELOG   Add reorder deprecation warning to auc docstring   Fix pep8 f-string error   remove duplicate import   Co-authored-by: Nicki Skafte skaftenicki@gmail.com,1
Set correct device ids in DDP [wip] (#4297),0.7106819,"    def configure_ddp(self, model, device_ids):"," repro  debug c d dd d d d ads d d d f rank f v d d d d d d d d d d d set  drop PL_DDP_PID clean up keep set gpus revert Revert ""drop PL_DDP_PID"" This reverts commit 7d88cae469541ef19128f9c20919fd3a6f863039. d pid gpus clean up clean up  misconfig? misconfig clean clean   fix pep   changelog   remove script   Co-authored-by: chaton thomas@grid.ai Co-authored-by: William Falcon waf2107@columbia.edu",1
Bug/4319 ddp checkpoint (#4323),0.6244087,Silenced some warnings. verified ddp refactors (#3483),"  Broadcast best model path to ensure we sync with main process + wait for main process to save   Add barrier call to ensure all processes are in sync   Added changelog commit   Move sync of best model path/score to model checkpoint, keep barrier to ensure all processes complete   Ensure we broadcast as tuple   Add init check   Update pytorch_lightning/callbacks/model_checkpoint.py   Co-authored-by: ananthsub ananth.subramaniam@gmail.com  Update pytorch_lightning/callbacks/model_checkpoint.py  Co-authored-by: ananthsub ananth.subramaniam@gmail.com   Removed model checkpoint code, added barrier to trainer to enforce we syncronize and wait for all processes to finish before completing training   Add barrier within teardown call, removed horovod teardown to inherit from base accelerator   Co-authored-by: ananthsub ananth.subramaniam@gmail.com",0
Add favicon path (#4331),0.3280174,Pinning starsessions to 1.x (#14333),,0
"tweak trainer call, minor spelling tweaks in notebooks (#4315)",0.54713196,Refactored setup for typing friendly (#6590),,0
Skip replacing dataloader sampler if it's already a distributed sampler (#4273),0.7640562,Allow setting replace_sampler_ddp=True with a distributed sampler already added (#4273),  Update data_loading.py   Update data_loading.py   add test + update flag description   add to changelog   Update test_dataloaders.py   fix-pickle   Update test_dataloaders.py   Added missing reference calls   Update tests/trainer/test_dataloaders.py   Apply suggestions from code review   Update data_loading.py   Update test_dataloaders.py   Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
Load checkpoint from Bytes (#4314),0.7191537,"def load_checkpoint(self, path):",  load directly from fs   if not str or path   pep8   type annotation   Co-authored-by: Sean Naren sean.narenthiran@gmail.com,1
[HOTFIX] ModelCheckpoint - Don't increase current_epoch and global_step if not trained (#4291),0.82573533,ModelCheckpoint now runs at the end of the training epoch by default (#8389),  add two tests w/wo tempdir   resolve flake8   this test is failing   update bug report   resolve bug and add test   remove bug_report   resolve flake8   resolve bug   resolve pep8   resolve pep8   Co-authored-by: Teddy Koker teddy.koker@gmail.com,1
Update init.py (#4308),0.5910172,"    def init(self, args, kwargs):",  Update init.py   Update pytorch_lightning/init.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Add dirpath and filename parameter in ModelCheckpoint (#4213),0.8154409,Deprecated filepath in ModelCheckpoint (#4213),  Add dirpath and filename parameter in ModelCheckpoint   remove old function   chlog   codefactor   update tests   docs   fix doctest and added tests   pathlib dirpath   dep version and docs   try fix doctest   pep   suggestions Co-authored-by: carmocca carlossmocholi@gmail.com   suggestions   fix test   pep   trigger tests   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   suggestions   try fix windows test   add and update some tests   trigger tests   Apply suggestions from code review   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: William Falcon waf2107@columbia.edu,1
add bug_report_model to bug_report (#4307),0.49828652,Changed ModelCheckpoint version suffixes to start at 1 (#5008),  add bug_report_model to bug_report   add notebook should be made public   update ,0
Protect functions not to be accessed by user (#4305),0.7323581,Restricted public access to several internal functions (#8024),,1
Fix bug comparing max_steps to global step which inits at 0 (#4278),0.67360693,"To resolve fault-tolerance issues, we changed where the global step value gets increased.",  Fix bug comparing max_steps to global step which inits at 0   Added test to ensure accumulate grad batch works with max steps   check fix with TODO test   correct call counts   Add check to ensure we've finished accumulation of this global step before exiting loop in conjuction with max steps   Remove + 1 check in test as this was incorrect   Update incorrect expected outputs in lr finder test   Added brackets for clarity   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
limit monitor callback with log_every_n_steps (#3881),0.5463689,bug fix with logging val epoch end + monitor (#3812),  limit monitor callback with row_log_interval   try fix gpu test   log_every_n_steps   Apply suggestions from code review   Apply suggestions from code review   rebase and staticmethod   suggestions   Co-authored-by: Jeff Yang ydcjeff@outlook.com,0
enable ddp as a plugin (#4285),0.7575512,        Override to init DDP in a different way or use your own wrapper.,  enable custom ddp plugin   enable custom ddp plugin   enable custom ddp plugin   enable custom ddp plugin   enable custom ddp plugin   enable custom ddp plugin   enable custom ddp plugin   enable custom ddp plugin   enable custom ddp plugin   enable custom ddp plugin   enable custom ddp plugin   Co-authored-by: chaton thomas@grid.ai,1
minor doc fix (#4296),0.5876148,"In addition, we fixed:",  minor doc fix   minor doc fix   fix problem with apex closure (#4292)   Co-authored-by: William Falcon waf2107@columbia.edu   minor doc fix   minor doc fix   Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
fix problem with apex closure (#4292),0.5087389,Check if optimizer supports closure (#4981),Co-authored-by: William Falcon waf2107@columbia.edu,0
Create CODEOWNERS,0.34507024,Contributors,,0
[Metrics] Unification of regression (#4166),0.7717761,Regression metrics (#2221),"  moved to utility   add files   unify   add desc   update   end of line   Apply suggestions from code review   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com  Apply suggestions from code review  Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com   add back functional test in new interface   pep8   doctest fix   test name fix   unify psnr + add class psnr, TODO: psnr test refactor ala mean squared error   unify psnr   rm unused code   pep8   docs   unify ssim   lower tolerance for ssim   fix import   pep8   docs   flake8   test smaller images   trying to fix test   no ddp test for ssim   pep8   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Teddy Koker teddy.koker@gmail.com",1
Allow changing the logged step value in validation_step (#4130),1.0000001,Allow changing the logged step value in validation_step (#4130),  Fix to bug identified in https://github.com/PyTorchLightning/pytorch-lightning/issues/4102   update tests   chlog   Co-authored-by: rohitgr7 rohitgr1998@gmail.com,1
Clean up optimizer code (#3587),0.82621646,Refactored optimizer (#4658),  Update optimizer code   Update CHANGELOG   Fix tuple of one list case   Update docs   Fix pep issue   Minor typo [skip-ci]   Use minimal match   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Apply suggestions from code review  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
Optimizer closure (#4190),0.8189411,Check if optimizer supports closure (#4981),  closure for all optimizers   rename hook and take care of alternating backwards   add comment   training_loop_fix   closure whenever possible   training_loop   simple tests that count backward calls   fix test to work with closure   remove debugging statement   better place   check grads after backward   start fixing manual optimization   skip step when result returned by closure was None   fix gradient clipping test to work with closure   attribute dict result only for automatic optimization   adjust backward calls in accelerator   adjust where to call gradient clipping   adjust backward calls in tests   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  pass kwargs to xla optimizer  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Fix info message when EarlyStopping 'mode' not provided [ci skip] (#4282),0.6634776,Removed mode='auto' from EarlyStopping (#6167),  Fix info message when EarlyStopping 'mode' not provided   fixup! Fix info message when EarlyStopping 'mode' not provided   Apply suggestions from code review   Co-authored-by: Jeff Yang ydcjeff@outlook.com Co-authored-by: Jeff Yang ydcjeff@outlook.com,0
add svg logo [SKIP CI] (#3988),0.30410033,Pinning starsessions to 1.x (#14333),  docs logo   use png   cleaning   vectorial   Apply suggestions from code review   Co-authored-by: Nicki Skafte skaftenicki@gmail.com  Apply suggestions from code review  Co-authored-by: Nicki Skafte skaftenicki@gmail.com,0
update examples (#4233),0.4787432,Syntax changes are: ," Removed image generation inside the training step.  It was overwriting the image grid generated in on_epoch_end. I also made adversarial_loss a static method.  Incorporated Hyperparameter best practices  Using ArgumentParser and hparams as defined in the Hyperparameters section of the documentation. This way we can set trainer flags (such as precision, and gpus) from the command line.  Incorporated Hyperparameter best practices  Using ArgumentParser and hparams as defined in the Hyperparameters section of the documentation. This way we can set trainer flags (such as precision, and gpus) from the command line.   Split the data part into a LightningDataModule   Update pl_examples/domain_templates/generative_adversarial_net.py   Co-authored-by: Jeff Yang ydcjeff@outlook.com",0
make save fx part of model checkpoint cb (#4284),0.58830774,Removed the deprecated model argument from ModelCheckpoint.save_checkpoint (#8688),,0
Feature/4244 iou input expectations (#4261),0.58711106,Changed iou [func] to allow float input (#4704),  =Add iou input checks   =Add test for iou input checks   =Update docstring for iou pred and target ,0
docs: Add empty lines in docstring [ci skip] (#4232),0.36159492,Docs improvements,  Add empty lines in docstring for proper docs   Remove Returns:   Remove unnecessary Returns:   Update pytorch_lightning/accelerators/ddp2_accelerator.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  fix returns  Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
Add strict option to lr_scheduler dict (#3586),0.7860036,Do not warn when the name key is used in the lr_scheduler dict (#5057),"  Add strict option to lr_scheduler dict   Update docs   Unnecessary ""else"" after ""raise""   Update CHANGELOG   Fix rebase ",1
New section for changelog [ci skip] (#4279),0.60271496,Full Changelog,  changelog   Apply suggestions from code review   Co-authored-by: Nicki Skafte nugginea@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Fix unintended zero-only target in iou example (#4262),0.6722035,Changed IoU score behavior for classes absent in target and pred (#3098),,0
Corrected f_beta computation (#4183),0.5100623,Remove beta arg from F1 class and functional (#5076), Update f_beta.py  Added METRIC_EPS in the denominator to avoid nan values in f_beta score.  Update f_beta.py  Made changes flake8 compliant  Update f_beta.py  Makes use of class_reduce for macro f_beta computation to avoid nans  Update f_beta.py  Made flake8 compliant   Corrected F beta computation   Removed offset to make the computation precise ,0
encourage draft PR submission (#4274),0.32537842,Deprecation warning (#3844),,0
update stale bot (#4205),0.4485821,Cleanup cluster waiting (#16054),  update stale bot   Apply suggestions from code review   Co-authored-by: chaton thomas@grid.ai  Apply suggestions from code review  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Apply suggestions from code review  Co-authored-by: chaton thomas@grid.ai Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
Docs/changelog for 1.0.3 (#4267),0.6715066,This section outlines notable changes that are not backward compatible with previous versions. The full list of changes and removals can be found in the Full Changelog below.,  formatting   miss   missing & ver++   path ,0
Added fix to ensure that custom logged metrics within test_epoch_end are appended to the result object even without step reduced metrics (#4251),0.8251023,Changed the behaviour when logging evaluation step metrics to no longer append /epoch_* to the metric name (#7351),,1
feat: finishing #4258 in parts (#4263),0.40403163,"Cleaning (#5948, #5949, #5950)",,0
update documentation for callbacks (#4253),0.78025645,Support for user-defined callbacks (#889 and #950),,1
activated color in all pytest runs (#4254),0.4463795,Remove pytest as a requirement to run app by @manskx in https://github.com/Lightning-AI/lightning/pull/15449,  activated color in all pytest runs   Update .drone.yml   Co-authored-by: Jeff Yang ydcjeff@outlook.com Co-authored-by: Jeff Yang ydcjeff@outlook.com,0
add test for model hooks (#4010),0.6326302,- Added model configuration checking before it runs,,0
prune ignore (#4240),0.5491182,Sanitize None params during pruning (#6836),  prune ignore   try drop loggers ,0
CI: add flake8 (#4239),0.42759347,"If we forgot somebody or you have a suggestion, find us on Discord :zap:",,0
allow codecov upload to fail (#4221),0.4247597,Early stopping checks on_validation_end (#1458),,0
move base req. to root (#4219),0.93242615,Moved base req. to root (#4219),  move base req. to root   check-manifest   check-manifest   manifest   req ,1
Use checkpoint_connector.hpc_save in SLURM (#4217),0.9388503,Used checkpoint_connector.hpc_save in SLURM (#4217),,1
remove unused folder (#4211),0.47739303,Remove .item which causes sync issues (#1254),,0
update PR template (#4210),0.40007725,Syntax changes are: ,,0
Annotate return type of TrainerProperties.from_argparse_args(...) (#4192),0.83887535,Trainer.add_argparse_args classmethod fixed. Now it adds a type for the arguments (#1147),  Annotate return type of TrainerProperties.from_argparse_args(...)   Added second empty line between class and typevar   Renamed all uses of the typevar to _T ,1
Update multi_gpu.rst (#4201),0.629639,"Deprecated the use of Trainer(gpus=""i"") (string) for selecting the i-th GPU; from v1.5 this will set the number of GPUs instead of the index (#6388)",torch.cuda.device_count() returns the number of available GPUs,0
fixed typos in style guide (#4181),0.58179307,"In addition, we fixed:",,0
Add load_from_checkpoint function to Loading docs (#4196),0.75985044,"-def on_load_checkpoint(self, checkpoint):",  add autodoc load_from_checkpoint to docs   autofunction -> automethod   Co-authored-by: Jeff Yang ydcjeff@outlook.com Co-authored-by: Jeff Yang ydcjeff@outlook.com,1
Add persistent flag to Metric.add_state (#4195),0.74091583,Change Metrics persistent default mode to False (#4685),  add persistant flag to add_state in metrics   wrap register_buffer with try catch   pep8   use loose version   test   pep8 ,1
fix hparams assign in init (#4189),0.62225294,    hparams_file='/path/to/hparams_file.yaml',,0
Fix broken trainer flags nb (#4159),0.62308973,Removed deprecated property Trainer.running_sanity_check in favor of Trainer.sanity_checking (#9209),  :pencil: add newline   Created using Colaboratory   Edited using Colaboratory   Apply suggestions from code review   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com   typo   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Use Optional for arguments set to None by default (#4164),0.61462235,            find_unused_parameters=True,"  Use Optional for variables set to None by default   Use Optional instead of Union[None, ...] for consistency ",0
Fix 1.0.0 changelog (#4180),0.59788,"    version=""0.0.1"",",,0
Fix docs (#4174),0.5577713,Docs improvements,,0
update chlog (#4177),0.5545579,Removed LoggerStages (#5673),,0
Bugfix/4156 filter hparams for yaml - fsspec (#4158),0.583814,    hparams_file='/path/to/hparams_file.yaml',  add test   fix   sleepy boy   chlog   Apply suggestions from code review   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,0
remove duplicate metric vs step log for train loop (#4173),0.97310776,Removed duplicate metric vs step log for train loop (#4173),  remove duplicate metric vs step log   remove duplicate metric vs step log   remove duplicate metric vs step log   fix ddp index issue ,1
fix jit for PT1.7 (#4172),0.47460723,Changed ModelCheckpoint version suffixes to start at 1 (#5008),,0
Fixes #4141 (#4169),0.57566637,"In addition, we fixed:",  fix val epoch agg   fix val agg metrics   fix val agg metrics   fix val agg metrics ,0
save initial arguments (#4163),0.5103439,"    def init(self, a=123, args, kwargs):",  save initial arguments   typing   chlog   . ,0
Tests: clean metrics (#4152),0.65842974,Removed deprecated metrics (#8586),  namme inputs   sk rename   imports ,0
Call on_load_checkpoint before loading state_dict (#4057),0.90389144,Called on_load_checkpoint before loading state_dict (#4057),,1
docs: fix fomatting,0.5431923,Docs improvements,,0
Speedup of metric tests (#4122),0.5810337,Metric reduction with Logging (#5150),  speedup   something working   update the rest   more desc   recurse tests/metrics again   pep8   Co-authored-by: Teddy Koker teddy.koker@gmail.com,0
[Doc] Lightning Module (#4123),0.6334878,Update the Lightning App docs (#13537),"  make current_epoch and global_step to be same as trainer, after model restore.   remove assignment here   test   minor modification   merge with parent's master   doc fix / improve   doc fix!   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com",0
Add trainer flag step [ci skip] (#4147),0.6516701,Changed default value of the max_steps Trainer argument from None to -1 (#9460), Add trainer flag step  Add step to disable automatic optimization in the trainer @justusschock  Apply suggestions from code review  Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com,0
new log section [skip ci] (#4121),0.54425025,moved eval loop logging to loggers (#3408),  new logs   formatting   1.0.1 ,0
"reverted ""temporary drop metrics tests while speeding them up"" and SKIP (#4115)",0.64195514,Removed callback metrics from test results obj (#2994)," Revert ""temporary drop metrics tests while speeding them up (#4071)""  This reverts commit 86c70622fbae611dd45ccb104830e7e28639fe44.   skip metrics tests   skipping ",0
Add trace functionality to the function to_torchscript (#4142),0.5571527,Changed PyTorchProfiler to use torch.autograd.profiler.record_function to record functions (#6349),  Add trace functionality to the function to_torchscript   used wrong parameter name in test   fix indentation to confirm to code style ,0
Added getstate/setstate method for torch.save serialization (#4127),0.5868086,class MyDataLoader(torch.utils.data.DataLoader):,"  Added getstate/setstate method for torch.save serialization, added additional Optional Typing to results object   Added tests to ensure torch.save does not fail   Added flags to ensure compatible ddp cpu environment   Removed torch version check due to minimum already being 1.3, reduced epochs for speed   Moved tests to separate file   Update to accelerator, move to ddp_spawn to prevent hanging ddp ",0
created minor doc fixes [ci skip] (#3958),0.51680994,"In addition, we fixed:",  created minor fixes   adjusted the underline   Update docs/source/amp.rst   suggestion from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
notices (#4118),0.48813015,Renamed several Trainer atributes:  (#567),,0
chlogs for 1.0 [skip ci] (#3978),0.47587988,Doing this minor release because correct validation metrics logging is critical.,  chlogs   logs   space   date   logs   logs   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   logs   logs   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Fixes broken LM links (#4117),0.48492905,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",,0
Fixes #3276 (#4116),0.57445556,"At last, lots of bug fixes (see below).",,0
Mention skipping steps in docs (#4108),0.42696995,Removed no return warning from val/test step (#6139),  Mention skipping in docs   Use :class: ,0
Update callbacks.rst,0.6203301,  callbacks:,,0
docs,1.0,Docs,,1
docs (#4107),0.70814866,Docs,,1
Minor fix to example in docstring of lr_finder (#4104),0.631913,move lr_finder (#3434),Changed trainer.lr_find(...) to trainer.tuner.lr_find(...) Co-authored-by: pbmstrk pbmstrk@users.noreply.github.com,0
docs (#4106),0.7197355,Docs,  docs   docs   docs ,1
docs (#4105),0.71445405,Docs,,1
docs (#4101),0.6988553,Docs,,0
docs (#4096),0.7416017,Docs,,1
clean up docs (#4095),0.58167106,Docs improvements,  docs   docs   docs   docs ,0
docs (#4093),0.7329675,Docs,  enabled manual returns   style   docs ,1
enabled manual returns (#4089),1.0000001,Enabled manual returns (#4089),,1
Update bug_report.md,0.49177086,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",,0
depre (#4088),0.6462218,Removed deprecated: (#2760),,0
fix optimizer docs (#4084),0.7031414,Refactored optimizer (#4658),,1
LM docs (#4085),0.46548843,Release LAI docs as stable (#14250),  docs   docs   docs   docs   docs ,0
Sg3 (#4082),0.39472878,(#16002),  docs   docs   docs ,0
docs (#4081),0.7325687,Docs,,1
"clean docs, enable grad clip in manual mode (#4078)",0.4888047,Trainer.fit now raises an error when using manual optimization with unsupported features such as gradient_clip_val or accumulate_grad_batches (#7788),  docs   docs ,0
docs fixes (#4080),0.648322,Docs improvements,  docs   docs   docs   docs   docs   docs   docs   docs   docs ,0
Covv1 (#4072),0.39750463,"If we forgot somebody or you have a suggestion, find us on Discord :zap:",  temporary drop metrics tests while speeding them up   cov   cov   docs ,0
added templates (#4077),0.46651947,Included app templates to the lightning and app packages (#13731),  docs   docs ,0
temporary drop metrics tests while speeding them up (#4071),0.5309444,Removed callback metrics from test results obj (#2994),,0
ref: accelerator names (#4066),0.6748514,reduced accelerator selection (#3211),  ref: accelerator names   docs ,0
docs clean up (#4068),0.5639156,Docs improvements,  docs   docs ,0
ref: decouple apex second attemp part n/n (#4065),0.50869715,decoupled DDP2 (#3816),  ref: decouple apex second attemp part n/n   ref: decouple apex second attemp part n/n ,0
ref: decouple apex second attemp part 10/n (#4064),0.52442366,decoupled DDP2 (#3816),  ref: decouple apex second attemp part 9/n   ref: decouple apex second attemp part 9/n   ref: decouple apex second attemp part 9/n ,0
ref: decouple apex second attemp part 9/n (#4063),0.5159963,decoupled DDP2 (#3816),  ref: decouple apex second attemp part 9/n   ref: decouple apex second attemp part 9/n ,0
ref: decouple apex second attemp part n/n (#4062),0.5105572,decoupled DDP2 (#3816),  ref: decouple apex second attemp part 8/n   ref: decouple apex second attemp part 8/n   ref: decouple apex second attemp part 8/n   ref: decouple apex second attemp part 8/n ,0
use automethod for LightningModule method (#4025),0.66549116,1. Override LightningModule hook,  autofunction -> automethod   Update pytorch_lightning/core/lightning.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
ref: decouple apex second attemp part 7/n (#4061),0.5117029,decoupled DDP2 (#3816),  ref: decouple apex second attemp part 7/n   ref: decouple apex second attemp part 7/n   ref: decouple apex second attemp part 7/n ,0
ref: decouple apex second attemp part 6/n (#4060),0.5268657,decoupled DDP2 (#3816),  ref: decouple apex second attemp part 6/n   ref: decouple apex second attemp part 6/n ,0
ref: decouple apex second attemp part 5/n (#4058),0.50770366,decoupled DDP2 (#3816),,0
classification metrics (#4043),0.89195687,Classification metrics overhaul (#4837), docs + precision + recall + f_beta + refactor  Co-authored-by: Teddy Koker teddy.koker@gmail.com  rebase  Co-authored-by: Teddy Koker teddy.koker@gmail.com  fixes  Co-authored-by: Teddy Koker teddy.koker@gmail.com   added missing file   docs   docs   extra import   Co-authored-by: Teddy Koker teddy.koker@gmail.com,1
Fix to print scaler value in progress bar (#4053),0.67464066,Changed the default progress bar to print to stdout instead of stderr (#531),  Fix to print scaler value in progress bar   chlog   Fix to print scaler value in progress bar   Fix to print scaler value in progress bar ,0
ref: decouple apex second attemp part 4/n (#4056),0.50698406,decoupled DDP2 (#3816),  ref: decouple apex second attemp part 4/n   ref: decouple apex second attemp part 4/n   Update lightning.py   ref: decouple apex second attemp part 4/n ,0
ref: decouple apex second attemp part 3/n (#4055),0.5107145,decoupled DDP2 (#3816),,0
ref: decouple apex second attemp part 2/n (#4054),0.51270455,decoupled DDP2 (#3816),  ref: decouple apex second attemp part 2/n   ref: decouple apex second attemp part 2/n ,0
ref: decouple apex second attemp part 1/n (#4052),0.50430876,decoupled DDP2 (#3816),,0
enable passing in custom accelerators (#4050),0.9821962,Enabled passing in custom accelerators (#4050),"  enable custom accelerators   ref: finish decoupling apex, LM and backward   ref: finish decoupling apex, LM and backward   ref: finish decoupling apex, LM and backward ",1
ref: enable custom clusters (1/n) (#4048),0.8452503,Enabled custom clusters (#4048),  enable cluster plugins   enable cluster plugins + test backend choices   enable cluster plugins + test backend choices   enable cluster plugins + test backend choices   enable cluster plugins + test backend choices   enable cluster plugins + test backend choices   enable cluster plugins + test backend choices ,1
add trainer flags nb (#4018),0.64182985,Barebones Trainer mode (#16854),  :sparkles: add trainer flags nb   fix typos   fix typos   Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com,0
enables plugins (#4041),0.9469591,Enabled plugins (#4041),  plugin hardware   plugin hardware   plugin hardware ,1
"Revert ""Remove limitation of batch scaler (#4006)"" (#4040)",0.6999177,Reset epoch progress with batch size scaler (#13846),This reverts commit 7e756ca11f2d3938ef73010fe63aa45d699bf0bc.,0
hotfix Drone install Horovod (#4038),0.46448922,"refactored Horovod backend (#3121, #3122)",  hotfix Drone install Horovod   notes ,0
add parsing OS env vars (#4022),0.65098584,  * `env_parse`,  add parsing OS env vars   fix env   Apply suggestions from code review   overwrite init   Apply suggestions from code review ,0
Update explained variance metric (#4024),0.9470651,Updated explained variance metric (#4024),  update metrics   pep8   Update pytorch_lightning/metrics/regression/explained_variance.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   add typing for testing utils   change from assert to raise exception   add test for raised shape error   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
swap example (#4029),0.40126312,Disabled val and test shuffling (#1600),,0
Removed broken link (#4031),0.51693654,Removed LoggerStages (#5673),module index link in documentation is broken. Co-authored-by: pbmstrk pbmstrk@users.noreply.github.com,0
[Docs] checkpoints (#4034),0.73458487,all the checkpoint issues should be gone now (including backward support for old checkpoints),  Update init.py   Update weights_loading.rst   docs for checkpoints ,1
Update README.md (#4036),0.55526215,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",,0
[rfc] Make Result.unpack_batch_size a static method (#4019),0.5220892,- Added dataclass support to `extract_batch_size` ([#12573](https://github.com/Lightning-AI/lightning/pull/12573)),This could be a useful utility elsewhere in lightning for calculating the batch size,0
"add on_train_batch_start/end, chpt hooks in hooks docs (#4023)",0.67202365,| on_batch_start             | on_train_batch_start         |,,0
Remove limitation of batch scaler (#4006),0.7159422,Reset epoch progress with batch size scaler (#13846),  working code   add tests   fix scaling   move patch dataloader to utils   renaming   fix tests   add changelog   update docs   pep8 ,1
Minor edit in README (#3959),0.53095174,Syntax changes are: ,Align the number of steps in README to the documentation Update README Fix grammar Update README.md typos Co-authored-by: pbmstrk pbmstrk@users.noreply.github.com,0
add s-rog to core (#4017),0.40815303,This release fixes that core issue,,0
Fixes #3993 (#3996),0.5938175,"In addition, we fixed:",,0
Implement Explained Variance Metric + metric fix (#4013),0.86192155,Updated explained variance metric (#4024),"  metric fix, explained variance   one more test   pep8   remove comment   fix add_state condition   Co-authored-by: ananyahjha93 ananya@pytorchlightning.ai",1
Add Metric <-> Lightning Module integration tests (#4008),0.63389164,Updated metrics to use LightningEnum (#5689),  lightning module metric tests   whitespace   pep8 ,0
Multi opts tests and clarification  (#4016),0.48324066,Disabled optimizers setup during testing (#3059),  ref: clean up opts docs   ref: clean up opts docs ,0
[skip ci] metric docs (#3992),0.6979036,"docs for all Metrics (#2184, #2209)",  update metric docs   doc fix   add assert desc   update based on suggestion ,0
try update horovod (#4004),0.5679989,horovod deprecation (#16141),,0
docs (#4003),0.7183574,Docs,,1
add myself (#3999),0.35776973,"If we forgot somebody or you have a suggestion, find us on Discord :zap:",,0
Bugfix/update trainer properties (#3975),0.7812339,Removed TrainerProperties mixin and moved property definitions directly into Trainer (#9495),"  make current_epoch and global_step to be same as trainer, after model restore.   remove assignment here   test   minor modification   merge with parent's master   [bug-fix]: update trainer properties   minor comment fix   minor comment fix   reset train loader in on_train_epoch_start hook   makes sure the changes work   minor chane   update changelog   adding unit test for reload_dataloaders_every_epoch arg   modified changelog, to add PR number   revert imports   changes to unit test   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com",1
Change reference to depricated result object to self (#3989),0.6393911,Completely overhaul the Result object in favor of ResultMetric (#7882),,0
Update README.md (#3986),0.5269853,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",,0
Fix valid accuracy computed with train accuracy in docs (#3983),0.5799116,- Updated `val_check_interval`(int) to consider total train batches processed instead of `_batches_that_stepped` for validation check during training ([#12832](https://github.com/Lightning-AI/lightning/pull/12832),,0
remove deprecated callbacks (#3979),0.95576155,Removed deprecated callbacks (#3979),,1
remove deprecated model hooks (#3980),0.9521446,Removed deprecated model hooks (#3980),,1
remove deprecated early_stop_callback (#3982),0.961642,Removed deprecated early_stop_callback (#3982),,1
Add video tutorials to docs (#3977),0.4805203,Docs improvements,  videos in trainer api   videos in docs   videos in docs   videos in trainer api   videos in docs   videos in docs   videos in docs   videos in docs   Update new-project.rst   docs   Update new-project.rst ,0
removed deprecated trainer flags (#3969),0.7814573,"Removed deprecated trainer attributes - on_cpu, on_tpu, use_tpu, on_gpu, use_dp, use_ddp, use_ddp2, use_horovod, use_single_gpu (#7501)",  removed deprecated flags   removed es callback flag ,1
integrate metrics API with self.log (#3961),0.969496,Integrated metrics API with self.log (#3961), metrics integration into self.log  Co-authored-by: Teddy Koker teddy.koker@gmail.com  ddp and regualr test for self.log + metrics  Co-authored-by: Teddy Koker teddy.koker@gmail.com   pep8   fix log tests   Co-authored-by: Teddy Koker teddy.koker@gmail.com  docs  Co-authored-by: Teddy Koker teddy.koker@gmail.com Co-authored-by: Teddy Koker teddy.koker@gmail.com,1
removed support for EvalResult and TrainResult (#3968),1.0000001,Removed support for EvalResult and TrainResult (#3968),,1
added tests for the training epoch end (#3967),0.64195716,Made training_epoch_end behave like validation_epoch_end (#1357),,0
outputs in __batch_end hooks (#3966),0.8756559,"Removed output argument from *_batch_end hooks (#3965, #3966)",  train_batch_end outputs   added tests for the output hooks ,1
new chlog template (#3963),0.56798375,Refactored logging,,0
0.10.0 (#3965),0.81789863,0.4.0,,1
Use viewcode extension instead of linkcode (#3503),0.3828461,- Removed the deprecated code in:,  use viewcode instead of linkcode   use viewcode instead of linkcode   Apply suggestions from code review   Co-authored-by: William Falcon waf2107@columbia.edu  Apply suggestions from code review  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: William Falcon waf2107@columbia.edu,0
0.10.0 (#3954),0.81187916,0.4.0,,1
[CI SKIP] Sequential data & TPU support docs fix (#3956),0.5152868,The point of this release is more bug fixes ahead of v 1.0.0. We now have CI tests on TPU thanks to @zcain117 from Google! :slightly_smiling_face:,  sequential data docs fix   TPU support docs fix   Co-authored-by: Iryna Koroliuk irynakoroliuk@Irynas-MacBook-Pro.local,0
Documentation Fixes [skip ci] (#3955),0.6020215,The point of this release is more bug fixes ahead of v 1.0.0. We now have CI tests on TPU thanks to @zcain117 from Google! :slightly_smiling_face:," Documentation Fixes  Just did some scanning for errors. Fixed indentation spelling, and grammar changes.  Apply suggestions from code review  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com",0
[ci skip] NeMo Documentation for PyTorch Lightning (#3707),0.6059104,  * ([#10403](https://github.com/PyTorchLightning/pytorch-lightning/pull/10403)), initial commit  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  doc clean up  Co-authored-by: William Falcon waf2107@columbia.edu,0
Add functional metric docs [skip ci] (#3946),0.702635,"docs for all Metrics (#2184, #2209)",  add functional docs   drop   format   Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai,1
missing logs [to be merged before release][skip ci] (#3131),0.5656101,Doing this minor release because correct validation metrics logging is critical.,  missing logs   chlog   add chlo to docs   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   chlog   docs   log   metrics   logs   log   format   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
latest restore func metrics (#3949),0.60681874,Removed deprecated metrics (#8586),  latest restore   latest restore ,0
Minor formatting & grammar fixes in docs [CI SKIP] (#3952),0.61012816,Docs improvements,  Minor formatting & grammar fixes in docs   Few more tentative doc fixes ,0
LightingDataModule doc fix (#3948),0.6411041,- Removed the deprecated `dim` and `size` arguments from the `LightningDataModule` constructor([#12780](https://github.com/Lightning-AI/lightning/pull/12780)),Co-authored-by: Iryna Koroliuk irynakoroliuk@Irynas-MacBook-Pro.local,0
Fixes #3945 (#3947),0.5836267,"In addition, we fixed:",,0
[CI SKIP] Fix early stop docs (#3940),0.5601788,"As we continue to strengthen the codebase with more tests, we’re finally getting rid of annoying bugs that have been around for a bit now. Mostly around the inconsistent checkpoint and early stopping behaviour (amazing work @awaelchli  @jeremyjordan )",  Update early_stopping.rst   Update init.py   Update new-project.rst   Update early_stopping.rst   Update init.py   Update early_stopping.rst   Update init.py   Co-authored-by: William Falcon waf2107@columbia.edu,0
restore functional metrics (#3943),0.7070966,Removed deprecated metrics (#8586),  restore functional metrics   clean   fix ,1
skip files in coverage (#3944),0.46353912,- Code coverage (99%),,0
removing this troubling test that has random behavior (#3941),0.51798546,Disabled val and test shuffling (#1600),  threshold   threshold ,0
codecov can't parse GPU/TPU coverage - temporary skip until that is fixed on their end (#2743),0.48578912,Tuner algorithms will be skipped if fast_dev_run=True (#3903),  skip file   todo   skip   skip   note   move ,0
clean and organize fit (#3938),0.56031156,Trainer.fit hook clean up (#3198),  clean and organize fit   clean and organize fit   clean and organize fit   clean and organize fit   clean and organize fit ,0
remove logging from callback (#3939),0.62891406,Removed deprecated callbacks (#3979),,0
tests for multiple optimizers and dataloader combinations (#3937),0.62432104,"Redesigned multi-dataloader support (#16743, #16784, #16939)",  added tests for multiple optimizers and dataloaders   added tests for multiple optimizers and dataloaders   added tests for multiple optimizers and dataloaders ,0
Update ci_dockers.yml (#3935),0.5132146,Remove unnecessary intermediate layers in Dockerfiles (#5697),,0
Test to ensure ckpt filepath contains correct val score (#3933),0.48951554,"trainer.test(model, ckpt_path=""/path/to/checkpoint.ckpt"")",  Added test to ensure ckpt filepath contains the correct val score reported from the trainer   Modified to check all saved ckpt files ,0
Update lightning.py (#3929),0.6394009,Use correct python version in lightning component template (#13790),,0
Update to_disk to use fsspec for remote file support (#3930),0.5506935,Add support to upload files to the Drive through an asynchronous upload_file endpoint (#14703),  Update supporters.py   Update CHANGELOG.md   Update supporters.py   Update supporters.py   Update supporters.py   Update supporters.py   Update supporters.py   Update supporters.py   Update CHANGELOG.md   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
moves configure ddp to each backend (#3924),1.0000002,moves configure ddp to each backend (#3924),  moves configure ddp to each backend   moves configure ddp to each backend   moves configure ddp to each backend   added torch manual seed in test_mean_error   test for complicated batch structure   test for complicated batch structure   test for complicated batch structure   Co-authored-by: ananyahjha93 ananya@pytorchlightning.ai,1
Mocking Loggers Part 5/5 (final) (#3926),0.8856526,"Mocking loggers (#3596, #3617, #3851, #3859, #3884, #3853, #3910, #3889, #3926)","  base   add xfail   new test   import   missing import   xfail if not installed   include mkpatch fix test  mock comet  comet mocks fix test remove dep undo merge duplication   line   line   convert doctest   doctest   docs   prune Results usage in notebooks (#3911)   notebooks   notebooks   revamp entire metrics (#3868)   removed metric   Co-authored-by: Teddy Koker teddy.koker@gmail.com  added new metrics  Co-authored-by: Teddy Koker teddy.koker@gmail.com  pep8  Co-authored-by: Teddy Koker teddy.koker@gmail.com  pep8  Co-authored-by: Teddy Koker teddy.koker@gmail.com  docs  Co-authored-by: Teddy Koker teddy.koker@gmail.com  docs  Co-authored-by: Teddy Koker teddy.koker@gmail.com  win ddp tests skip  Co-authored-by: Teddy Koker teddy.koker@gmail.com  win ddp tests skip  Co-authored-by: Teddy Koker teddy.koker@gmail.com  win ddp tests skip  Co-authored-by: Teddy Koker teddy.koker@gmail.com  win ddp tests skip  Co-authored-by: Teddy Koker teddy.koker@gmail.com  reset in compute, cache compute  Co-authored-by: Teddy Koker teddy.koker@gmail.com  reduce_ops handling  Co-authored-by: Teddy Koker teddy.koker@gmail.com  sync -> sync_dist, type annotations  Co-authored-by: Teddy Koker teddy.koker@gmail.com  wip docs  Co-authored-by: Teddy Koker teddy.koker@gmail.com   mean squared error   docstring   added mean ___ error metrics   added mean ___ error metrics   seperated files   accuracy doctest   gpu fix   remove unnecessary mixin   metric and accuracy docstring   Co-authored-by: Teddy Koker teddy.koker@gmail.com  metric docs  Co-authored-by: Teddy Koker teddy.koker@gmail.com  pep8, changelog  Co-authored-by: Teddy Koker teddy.koker@gmail.com   refactor dist utils, pep8   refactor dist utils, pep8   Co-authored-by: Teddy Koker teddy.koker@gmail.com   Callback docs with autosummary (#3908)   callback docs with autosummary   do not show private methods   callback base docstring   skip some docker builds (temporally pass) (#3913)   skip some docker builds   todos   skip   use badges only with push (#3914)   testtube   mock test tube   mock mlflow   remove mlflow   clean up   test   test   test   test   test   test   code blocks   remove import   codeblock   logger   wandb causes stall   Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Ananya Harsh Jha ananya@pytorchlightning.ai Co-authored-by: Teddy Koker teddy.koker@gmail.com Co-authored-by: Jeff Yang ydcjeff@outlook.com",1
test for complicated batch structure (#3928),0.6249571,"Refactored training_batch + tests to verify correctness (#2327, #2328)",  test for complicated batch structure   test for complicated batch structure ,0
fixes #3871 (#3919),0.5818016,"In addition, we fixed:",  fixes #3871   :white_check_mark: tests   :white_check_mark: tests   :white_check_mark: tests   :white_check_mark: tests   :white_check_mark: tests   :white_check_mark: tests   :white_check_mark: tests   moves sync bn to each backend   moves sync bn to each backend   Co-authored-by: nateraw nxr9266@g.rit.edu,0
moves sync bn to each backend (#3925),0.9999999,moves sync bn to each backend (#3925),,1
fixed ddp flag crash (#3927),0.6192239,Silenced some warnings. verified ddp refactors (#3483),,0
"Mocking Loggers (part 4c, mlflow) (#3889)",0.7575586,"Mocking loggers (#3596, #3617, #3851, #3859, #3884, #3853, #3910, #3889, #3926)",  base   add xfail   new test   import   missing import   Co-authored-by: William Falcon waf2107@columbia.edu,1
moves init apex from LM to apex connector (#3923),1.0,moves init apex from LM to apex connector (#3923),,1
"Mocking Loggers (part 4b, mlflow) (#3885)",0.7548864,"Mocking loggers (#3596, #3617, #3851, #3859, #3884, #3853, #3910, #3889, #3926)",  base   mock test   Co-authored-by: William Falcon waf2107@columbia.edu,1
Update tensorboard.py (#3920),0.7681581,Improved the error message for installing tensorboard or tensorboardx (#17053),,1
fixes metrics pickle issue (#3921),0.7322693,Removed deprecated metrics (#6161),Co-authored-by: Teddy Koker teddy.koker@gmail.com Co-authored-by: Teddy Koker teddy.koker@gmail.com,1
Complete mocking Comet and remove dep (#3910),0.50376976,"Mocking loggers (#3596, #3617, #3851, #3859, #3884, #3853, #3910, #3889, #3926)", xfail if not installed  include mkpatch fix test  mock comet  comet mocks fix test remove dep undo merge duplication   line   line   convert doctest   doctest   docs ,0
update docs on logging (#3916),0.6424844,Refactored logging,  Update loggers.rst   Update loggers.rst   Update index.rst   Create logging.rst   Delete experiment_reporting.rst   Delete experiment_logging.rst   Update init.py ,0
use badges only with push (#3914),0.34013778,Pinning starsessions to 1.x (#14333),,0
skip some docker builds (temporally pass) (#3913),0.6184801,Remove unnecessary intermediate layers in Dockerfiles (#5697),  skip some docker builds   todos   skip ,0
Callback docs with autosummary (#3908),0.5995314,  callbacks:,  callback docs with autosummary   do not show private methods   callback base docstring ,0
revamp entire metrics (#3868),0.67101175,Removed deprecated metrics (#6161)," removed metric  Co-authored-by: Teddy Koker teddy.koker@gmail.com  added new metrics  Co-authored-by: Teddy Koker teddy.koker@gmail.com  pep8  Co-authored-by: Teddy Koker teddy.koker@gmail.com  pep8  Co-authored-by: Teddy Koker teddy.koker@gmail.com  docs  Co-authored-by: Teddy Koker teddy.koker@gmail.com  docs  Co-authored-by: Teddy Koker teddy.koker@gmail.com  win ddp tests skip  Co-authored-by: Teddy Koker teddy.koker@gmail.com  win ddp tests skip  Co-authored-by: Teddy Koker teddy.koker@gmail.com  win ddp tests skip  Co-authored-by: Teddy Koker teddy.koker@gmail.com  win ddp tests skip  Co-authored-by: Teddy Koker teddy.koker@gmail.com  reset in compute, cache compute  Co-authored-by: Teddy Koker teddy.koker@gmail.com  reduce_ops handling  Co-authored-by: Teddy Koker teddy.koker@gmail.com  sync -> sync_dist, type annotations  Co-authored-by: Teddy Koker teddy.koker@gmail.com  wip docs  Co-authored-by: Teddy Koker teddy.koker@gmail.com   mean squared error   docstring   added mean ___ error metrics   added mean ___ error metrics   seperated files   accuracy doctest   gpu fix   remove unnecessary mixin   metric and accuracy docstring   Co-authored-by: Teddy Koker teddy.koker@gmail.com  metric docs  Co-authored-by: Teddy Koker teddy.koker@gmail.com  pep8, changelog  Co-authored-by: Teddy Koker teddy.koker@gmail.com   refactor dist utils, pep8   refactor dist utils, pep8   Co-authored-by: Teddy Koker teddy.koker@gmail.com",0
prune Results usage in notebooks (#3911),0.44521508,Sanitize None params during pruning (#6836),  notebooks   notebooks ,0
Additional test for logging during validation loop (#3907),0.57941955,"Simplified ""should run validation"" logic (#7682)",  Added test for logging in validation step when using dict dataset with string value   fix recursive issue   fix recursive issue   Co-authored-by: Nathan Painchaud nathanpainchaud@gmail.com Co-authored-by: William Falcon waf2107@columbia.edu,0
nb steps in early stop (#3909),0.5382174,Checkpoint and early stopping now work without val. step (#1041),  nb steps   if   skip   rev   seed   seed ,0
add current_epoch to dumped_params (#3261),0.6218318,Removed output argument from *_epoch_end hooks (#3967),  add current epoch to __dumped_params   log   reset   add to test   Update CHANGELOG.md   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Added check to verify xla device is TPU (#3274),0.6063679,xla_device_utils >> xla_device,"  tpu device check   replaced with xmp spawn   Revert ""replaced with xmp spawn""   This reverts commit 6835380f   replaced all instances of XLA_AVAILABLE   moved inner_f to global scope   made refactors   added changelog   added TPU_AVAILABLE variable   fix codefactor issues   removed form trainer and early stopping   add TORCHXLA_AVAILABLE check   added tests   refactoring   Update pytorch_lightning/utilities/xla_device_utils.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   updated function names   fixed bug   updated CHANGELOG.md   added todo   added type hints   isort and black   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: William Falcon waf2107@columbia.edu",0
Adds tests to make sure logging doesn't happen multiple times (#3899),0.69506395,Cleaning up stale logger tests (#3490),  Makes sure logging doesn't ever happen from non-root zero   Makes sure logging doesn't ever happen from non-root zero   Makes sure logging doesn't ever happen from non-root zero   added bug report model   fix local model   fix local model   fix local model   fix local model ,0
Ensure global seed exists before passing into env subprocess.Popen call (#3904),0.4665358,seed_everything now fails when an invalid seed value is passed instead of selecting a random seed (#8787),,0
fic CI parsing Horovod version (#3804),0.52088064,horovod deprecation (#16141),,0
"Rename log_save_interval, row_log_interval (#3748)",0.6925758,add_log_row_interval -> row_log_interval,  Rename row_log_interval -> log_every_n_steps log_save_interval -> flush_logs_every_n_steps   Changelog   fixed title underline length   typo   Update pytorch_lightning/trainer/trainer.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/trainer/trainer.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   pep8 + deprecation test   'todo: remove in 1.1 comment'   1.1 -> 0.11   log   docs   depr API   add depr tests   note   miss   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai,0
xfail if not installed (#3860),0.38734397,Improved support for running apps when dependencies aren't installed (#15711),include mkpatch fix test,0
update bug template (#3902),0.5491124,Fixing critical bugs in newly added hooks and hparams assignment.,,0
doc update (#3894),0.6026025,Docs improvements,,0
Fix lr finder for optimizers with states (#3897),0.567033,Refactored optimizer (#4658),  fix lr finder   changelog   add test ,0
added bug report model (#3901),0.5905807,This release fixes that core issue,,0
Fix apt repo issue for docker (#3823),0.44761544,Pkg: fix parsing versions by @Borda in https://github.com/Lightning-AI/lightning/pull/15401,  fix docker repo issue   docker   docker   docker   no cudnn   no cudnn   try 16.04   Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai,0
Fixes #2936 (no fix needed) (#3892),0.62574023,"At last, lots of bug fixes (see below).",,0
"Mocking Loggers (part 3b, comet) (#3853)",0.81675565,"Mocking loggers (#3596, #3617, #3851, #3859, #3884, #3853, #3910, #3889, #3926)","  ref   Mocking Loggers (part 3c, comet) (#3859)   mock comet   new line ",1
Fix docs for auto_lr_find (#3883),0.5827734,move lr_finder (#3434),  Fix docs for auto_lr_find   change testcode to codeblock   we are not showing a complete example here,0
"Mocking Loggers (part 4a, mlflow) (#3884)",0.7442398,"Mocking loggers (#3596, #3617, #3851, #3859, #3884, #3853, #3910, #3889, #3926)",  extensive mlflow test   revert accidental commits ,1
"Fixes #3668, #3887 as a bonus (#3888)",0.6223068,"In addition, we fixed:","  Fixes #3668, #3887 as a bonus   Fixes #3668, #3887 as a bonus ",0
Write predictions in LightningModule instead of EvalResult (#3882),0.9910475,Write predictions in LightningModule instead of EvalResult [#3882,  :sparkles: add self.write_prediction   :sparkles: add self.write_prediction_dict to lightning module ,1
Fix for load_from_checkpoint (#2776),0.71751595,Changed resume_from_checkpoint warning to an error when the checkpoint file does not exist (#7075),  Fix.   Fix #2550: allow to load model from checkpoint if self.save_hyperparameters() was not called.   Fix? Cleaner way of not calling self.save_hyperparameters in EvalModelTemplate.   Fix? _load_model_state cleanup   Fix?   Fix #2550: allow to load model from checkpoint if self.save_hyperparameters() was not called.   Fix.   Fix? Cleaner way of not calling self.save_hyperparameters in EvalModelTemplate.   Fix? _load_model_state cleanup   Fixed side effect in test_load_model_from_checkpoint_extra_args.   Apply suggestions from code review   fix   try   fixed missing arg in evalmodel   fixed missing arg in evalmodel   fix   update   fix loading   add test   prune   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai Co-authored-by: William Falcon waf2107@columbia.edu,1
[Bug-Fix]:properties current_epoch and global_step between model and trainer same always  (#3785),0.80286807,The current_epoch and global_step attributes now get restored irrespective of the Trainer task (#9413),"  make current_epoch and global_step to be same as trainer, after model restore.   remove assignment here   test   minor modification   Update pytorch_lightning/core/lightning.py   type check, better clarity Co-authored-by: ananthsub ananth.subramaniam@gmail.com  Update pytorch_lightning/core/lightning.py  type check, better clarity Co-authored-by: ananthsub ananth.subramaniam@gmail.com   comments for current_epoch and global_step properties   Update tests/models/test_restore.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   update comments according to the changes made   Update tests/models/test_restore.py   add current_epoch, global_step to jit ignore list   Add comments to CHANGELOG   Update CHANGELOG.md   Update tests/models/test_restore.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",1
fix init nan for checkpointing (#3863),0.64793,Changed resume_from_checkpoint warning to an error when the checkpoint file does not exist (#7075),  add test for checkpoint nan   fix   pep ,0
Fixes #2678 - enables training_step to return None (#3862),0.6990399,Changed the default behaviour to no longer include a NaN check with each training iteration. (#1475),  Fixes #2678 - enables training_step to return None   Fixes #2678 - enables training_step to return None ,0
Update lightning_module.rst (#3854),0.49713695,  * Removed the deprecated `pytorch_lightning.overrides.fairscale.unwrap_lightning_module_sharded` function,*_epoch_out methods expects a return of None.,0
Fixes #2792 (#3857),0.58820236,"In addition, we fixed:",,0
refactor (#3851),0.5898351,Refactoring,,0
Fixes #2551 (#3858),0.5963391,"In addition, we fixed:",,0
Fixed #2143 and many more :) (#3855),0.51826715,"In addition, we fixed:",,0
fixed model checkpoint frequency (#3852),0.60423845,Enable None model checkpoint default (#3669),  fixed model checkpoint frequency   fixed model checkpoint frequency   fixed model checkpoint frequency   fixed model checkpoint frequency   merged ,0
Fixes #2479 (#3856),0.6175399,"In addition, we fixed:",,0
"Mocking loggers (part 2, neptune)  (#3617)",0.8219066,"Mocking loggers (#3596, #3617, #3851, #3859, #3884, #3853, #3910, #3889, #3926)",  mock neptune base tests   neptune doctest   remove extra   mock loggers   typo   mock import   neptune not compatible with multigpu   add back experiment ,1
added tbptt test for logging (#3850),0.5597922,"Removed support for self.log(tbptt_reduce_fx) and self.log(tbptt_pad_token). Please, open a discussion explaining your use-case if you relied on these. (#7644)",  added tbptt test for logging   added tbptt test for logging ,0
fixes #3798 (#3849),0.58692974,"In addition, we fixed:",  fix #3798   added tbptt test for logging ,0
Deprecate early_stop_callback Trainer argument (part 2) (#3845),0.99087167,Deprecate early_stop_callback Trainer argument (#3845),  update tests with EarlyStopping default   imports   revert legacy tests   fix test   revert   revert ,1
docs/fix_typo (#3847),0.5514178,"In addition, we fixed:",,0
Fix tbptt_reduce_fx when non-floating tensors are logged (#3796),0.6553973,"Removed support for self.log(tbptt_reduce_fx) and self.log(tbptt_pad_token). Please, open a discussion explaining your use-case if you relied on these. (#7644)",  Add failing test   force all tbptt vals to be floats for reduce   Co-authored-by: William Falcon waf2107@columbia.edu,0
Add back sanity checks (#3846),0.448181,Removed deprecated: (#2760),  Add back sanity checks   pep ,0
test selecting the correct backend. temp backends while slurm and TE are decoupled (#3848),0.9233872,test selecting the correct backend. temp backends while slurm and TorchElastic are decoupled (#3848),  test selecting the correct backend. tem backends while slurm and TE are decoupled   test selecting the correct backend. tem backends while slurm and TE are decoupled ,1
Eval epoch can now log independently (#3843),0.85460305,epoch can now log independently (#3843),  ref: routed epoch outputs to logger   ref: routed epoch outputs to logger   ref: routed epoch outputs to logger   ref: routed epoch outputs to logger ,1
use docker for conda CI (#3841),0.41467875,- Added support for running the works without cloud compute in the default container ([#14819](https://github.com/Lightning-AI/lightning/pull/14819)),  use docker in conda CI   update env if needed   update with pip   remove setting pytorch ,0
deprecation warning (#3844),0.9999999,Deprecation warning (#3844),,1
ref: adding compute environments (2/n) (#3842),0.796781,"adding compute environments (#3837, [#3842)",  ref: adding compute environments (2/n)   ref: adding compute environments (2/n)   ref: adding compute environments (2/n)   ref: adding compute environments (2/n) ,1
Fix val_progress_bar total with num_sanity_val_steps (#3751),0.64293367,- Fixed main progress bar counter when `val_check_interval=int` and `check_val_every_n_epoch=None` ([#12832](https://github.com/Lightning-AI/lightning/pull/12832),  Fix val_progress_bar total with num_sanity_val_steps   chlog   Fix val_progress_bar total with num_sanity_val_steps   move test   replaced with sanity flag and suggestions ,0
added broadcast option to tpu (#3814),0.6275774,Moveed HPU broadcast override to the HPU strategy file (#17011),  added broadcast option to tpu   add device   moved tpu broadcast to tpu_backend   removed Lightning dist   decode bytes   pep8 fix   fix bug   test for broadcast   updated changelog ,0
ref: adding compute environments (1/n) (#3837),0.7832021,"adding compute environments (#3837, [#3842)",  ref: adding compute environments (1/n)   ref: adding compute environments (1/n)   ref: adding compute environments (1/n) ,1
Explicitly point out where should we set the random seed (#3839),0.65684056,seed_everything now fails when an invalid seed value is passed instead of selecting a random seed (#8787),  Explicitly point out where should we set the random seed   Update docs/source/multi_gpu.rst   Co-authored-by: Jeff Yang ydcjeff@outlook.com Co-authored-by: Qinru Li q4li@eng.ucsd.edu Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: Jeff Yang ydcjeff@outlook.com,0
ref: callback system and init ddp (1/n) (#3836),0.86520696,callback system and init DDP (#3836),  refactored callback system and init ddp   refactored callback system and init ddp   refactored callback system and init ddp   refactored callback system and init ddp ,1
Update trainer.py (#3834),0.78147167,adding Trainer.tune() (#3293),,1
verified epoch logging (#3830),0.7291304,epoch can now log independently (#3843),  ref: fix epoch logging   verified epoch logging   verified epoch logging   verified epoch logging   verified epoch logging   verified epoch logging   verified epoch logging   verified epoch logging   verified epoch logging ,1
"[WIP] ref: decoupled ddp, ddp spawn (finish 3733) (#3819)",0.84412265,"decoupled DDP, DDP spawn (#3733, #3817, #3819, #3927)",  ref: finish #3733   remove deprecated test   remove deprecated test   remove deprecated test   remove deprecated test   remove deprecated test   remove deprecated test   remove deprecated test   remove deprecated test   remove deprecated test   remove deprecated test   remove deprecated test   remove deprecated test   remove deprecated test   Update pytorch_lightning/accelerators/ddp_backend.py   Co-authored-by: ananthsub ananth.subramaniam@gmail.com   remove deprecated test   remove deprecated test   remove deprecated test   Co-authored-by: ananthsub ananth.subramaniam@gmail.com,1
ref: training flag tests (val_check_interval) (#3825),0.59365785,Trainer(val_check_interval=100),  added test_val_check_interval tests   added test_val_check_interval tests   added test_val_check_interval tests ,0
Use fsspec in load to resolve more paths/URLs from storage backends (#3692),0.43003726,Swaped torch.load for fsspec load in DDP spawn backend (#3787),  special case http for torch hub load   Update CHANGELOG.md   Update test.txt ,0
remove deprecated test (#3820),0.6194427,Deprecated the TestTubeLogger (#9065),,0
ref: bug fix with logging val epoch end + monitor (#3812),0.96661186,bug fix with logging val epoch end + monitor (#3812),  ref: fix metric err   ref: fix metric err   ref: fix metric err   ref: merge   ref: merge   ref: merge   ref: merge   ref: decoupled ddp2   ref: decoupled ddp2   ref: decoupled ddp2   ref: decoupled ddp2   ref: decoupled ddp2   ref: clean up ddp before final fix   ref: clean up ddp before final fix   ref: clean up ddp before final fix   ref: clean up ddp before final fix   ref: clean up ddp before final fix   ref: clean up ddp before final fix   ref: clean up ddp before final fix   ref: clean up ddp before final fix   ref: clean up ddp before final fix   ref: clean up ddp before final fix   ref: clean up ddp before final fix ,1
ref: clean up ddp before final fix (#3817),0.6926198,Silenced some warnings. verified ddp refactors (#3483),  ref: clean up ddp before final fix   ref: clean up ddp before final fix   ref: clean up ddp before final fix   ref: clean up ddp before final fix   ref: clean up ddp before final fix ,0
ref: decoupled ddp2 (#3816),0.8939035,decoupled DDP2 (#3816),,1
Some docs update (#3794),0.6696895,Docs improvements,  docs update   docs update   suggestions   Update docs/source/introduction_guide.rst   Co-authored-by: William Falcon waf2107@columbia.edu,0
ref: separate slurm from ddp (#3809),0.9152191,separate SLURM from DDP (#3809),  ref: separate slurm from ddp   ref: separate te from ddp   ref: merge   ref: merge   ref: merge ,1
handle fsspec inconsistency in PyArrowHDFS (#3805),0.50896394,Updated model_checkpoint's to_yaml to use fsspec open (#3801),,0
ref: separate te from ddp (#3810),0.66830385,decoupled DDP2 (#3816),  ref: separate te from ddp   ref: separate te from ddp   ref: separate te from ddp ,0
ref: remove weight loading hack for ddp_cpu (#3808),0.957145,remove weight loading hack for ddp_cpu (#3808),,1
ref: part 8 of #3733 (#3806),0.4566239,model reference provided:,,0
Fix on_train_batch_start hook to end epoch early (#3700),0.77101773,"Changed the seq of on_train_batch_end, on_batch_end & on_train_epoch_end, on_epoch_end hooks (#5688)",  init   add test   changelog and docs   fix test   Apply suggestion from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
Swap torch.load for fsspec load in ddp spawn backend (#3787),0.98131645,Swaped torch.load for fsspec load in DDP spawn backend (#3787),  Update ddp_spawn_backend.py   Update ddp_cpu_spawn_backend.py   log   Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai,1
Update model_checkpoint.py (#3801),0.7669412,Removed the deprecated model argument from ModelCheckpoint.save_checkpoint (#8688),,1
ref: part 7 of #3733 (#3802),0.44598427,model reference provided:,  ref: part 7 of #3733   ref: part 7 of #3733 ,0
fix warning (#3800),0.6608746,Deprecation warning (#3844),,0
[TPU CI] Use timestamp+pythonVersion to form the docker image tag. (#3779),0.4611573,"Show a message when BuildConfig(dockerfile=""..."") is passed but a Dockerfile file is already present in the Work (#15799)",  Use timestamp+pythonVersion to form the docker image tag.   Remove temporary step to check new env var. ,0
Use fsspec with OmegaConf saving in saving.py (#3782),0.47800434,Updated model_checkpoint's to_yaml to use fsspec open (#3801),,0
remove torch<1.3.0 warning from tb logger (#3784),0.6503775,    * Some configuration errors that were previously raised as `MisconfigurationException`s will now be raised as `ProcessRaisedException` (torch>=1.8) or as `Exception` (torch<1.8),,0
revert backend types (#3788),0.6059614,move backends back to individual files (#3712),  revert backend types   todo   todo ,0
Add link to PL forum in GH questions template (#3708),0.34040827,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",  Update how-to-question.md   Update how-to-question.md   Apply suggestions from code review   typo   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
fix path in CI for release & python version in all dockers & duplicated badges (#3765),0.452609,"Show a message when BuildConfig(dockerfile=""..."") is passed but a Dockerfile file is already present in the Work (#15799)",  typo   path   check   trigger   fix conda   pip ver   fix cuda   fix XLA   fix xla   ci   docker   BIULD   unBIULD   update   py 3.8   apex   apex ,0
Use raise .. from .. to explicitly chain exceptions (#3750),0.9522759,Used raise .. from .. to explicitly chain exceptions (#3750),  Fix exception chaining   names   Change exception names for consistency   Co-authored-by: Nicki Skafte skaftenicki@gmail.com  Change exception names for consistency  Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai Co-authored-by: Nicki Skafte skaftenicki@gmail.com,1
part 5 of #3733 (#3774),0.35560137,"Cleaning (#5948, #5949, #5950)",  ref: part 4 of #3733   ref: part 4 of #3733   ref: part 4 of #3733 ,0
ref: part 4 of #3733 (#3773),0.41667023,model reference provided:,  ref: part 4 of #3733   ref: part 4 of #3733   ref: part 4 of #3733   ref: part 4 of #3733 ,0
Fix for PyTorch 1.7 CI (#3768),0.816376,Enable PyTorch 1.7 compatibility (#3541), changed to jit_unsed_properties,1
Metric aggregation testing (#3517),0.58685577,brand new Metrics package with built-in DDP support (by @justusschock and  @SkafteNicki),  aggregation testing   add more tests   mse   more tests   fix tests   fix doctest   fix codefactor   fix import error   fix doctest   revert docfix   test for model integration   fix integration test   added test cases   fix rmsle   aggregation testing   add more tests   mse   more tests   fix tests   fix doctest   fix codefactor   fix import error   fix doctest   revert docfix   test for model integration   fix integration test   fix psnr   add warning/valueerror to embedding similarity   fixed f scores   disable some test   fix tests   fixing codefactor   fix pep8   changelog   fix doctest   cleaning test   fix pickle error   pickle fix   fix pickle error   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   code cleanup + changes based on suggestions   update based on suggestion   update based on suggestions   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Nicki Skafte nugginea@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
ref: continue #3733 (#3767),0.3769758,[1.6.4] - 2022-06-01,  ref: #3733 part 2   ref: #3733 part 2 ,0
ref: part a of #3733 (#3766),0.46941823,model reference provided:,  ref: part a of #3733   ref: part a of #3733 ,0
[metrics] Accuracy num_classes error fix (#3764),0.65439904,Sklearn metrics classes (#1327),  change accuracy error to warning   changelog ,0
skip best_model_path if checkpoint_callback is None (#2962),0.9218321,Skipped best_model_path if checkpoint_callback is None (#2962),  skip best_model_path if checkpoint_callback is None   removed test ,1
Add datamodule parameter to lr_find() (#3425),0.5967801,tuner.lr_find(...),  Add datamodule parameter to lr_find()   Fixed missing import   Move datamodule parameter to end   Add datamodule parameter test with auto_lr_find   Change test for datamodule parameter   Apply suggestions from code review   Co-authored-by: Nicki Skafte skaftenicki@gmail.com  Fix lr_find documentation  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   formatting   Add description to datamodule param in lr_find   pep8: remove trailing whitespace on line 105   added changelog   Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Nicki Skafte nugginea@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
ref: fixes logging for eval steps (#3763),0.7076401,moved eval loop logging to loggers (#3408), fixes logging for eval steps,1
Added gradient clip test for native AMP (#3754),0.61431247,"    gradient_clip_val,",  added gradient clip test for fp16   pep8 ,0
add dist lib to enable syncing anything across devices (#3762),0.4758688,- Removed deprecated signature for `transfer_batch_to_device` hook. The new argument `dataloader_idx` is now required ([#10480](https://github.com/PyTorchLightning/pytorch-lightning/pull/10480)), add dist lib to enable syncing anything across devices,0
Finish Allow on_save_checkpoint... (#3688),0.71699315,Removed support for the deprecated on_save_checkpoint signature. The hook now takes a checkpoint positional parameter (#8697),  Finish #3562   Apply suggestions from code review   Apply suggestions from code review   fix tests   Finish #3562   Apply suggestions from code review   Apply suggestions from code review   fix tests   fix structure   fix structure   make save_last test pass   unnecessary global rank check   fix test   update test   update test   test   test   run save on all   remove assert   tracking saves   check if fails   test   clean up   adjust horovod test   clean up   remove unnecessary makdirs   change   undo   debug   debug   debug   debug   mock   undo debug code   add extra assertions   test   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli adrian.waelchli@inf.unibe.ch Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Add trainer attribute to datamodule (#3749),0.70310676,Removed deprecated property LightningModule.datamodule in favor of Trainer.datamodule (#9233),  Split out changes from #3563 to make that PR easier to review. This formats the file according to the Black formatter   Store a reference to the trainer on the datamodule   Fixes #3682   Update data_connector.py   Update data_connector.py   Update test_datamodules.py   Add attribute to datamodule for trainer ,1
ref: decoupled ddp spawn (#3746),0.84926987,"decoupled DDP, DDP spawn (#3733, #3817, #3819, #3927)",,1
Do not set PYTHONHASHSEED #2156 (#3745),0.47029588,Do not override PYTHONWARNINGS (#4700),,0
nightly release to tests (#3718),0.6121423,The point of this release is more bug fixes ahead of v 1.0.0. We now have CI tests on TPU thanks to @zcain117 from Google! :slightly_smiling_face:,,0
PR for Issue#3114 (#3741),0.47121578,[1.7.1] - 2022-08-09,Co-authored-by: Sasikanth sasikanth@Sasikanths-MacBook-Pro.local,0
run TPU tests with multiple versions (#3024),0.5004868,The point of this release is more bug fixes ahead of v 1.0.0. We now have CI tests on TPU thanks to @zcain117 from Google! :slightly_smiling_face:,  rename   multi build   multi build   copy   copy   copy   copy   copy   copy   clean   note   docker   formatting   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: William Falcon waf2107@columbia.edu,0
return simple docs to methods (#3645),0.49433178,"Separately, return whatever you want from methods:",  return simple docs to methods   sorting   imports   miss ,0
Make ModelCheckpoint(save_top_k=-1) track the best models (#3735),0.7940352,"Removed the save_best_only argument from ModelCheckpoint, use save_top_k=1 instead (#128)",  fix topk=-1 tracking best   update test   clean up   add changelog   enable loading best topk in trainer.test()   make trivial   return right away   make windows test path happy ,1
fix PT version in CUDA docker images (#3739),0.5385525,  * Deprecated the `pytorch_lightning.utilities.device_parser.num_cuda_devices` in favor of `lightning_lite.accelerators.cuda.num_cuda_devices`,  upgrade PT version   update docker   docker   try 1.5   fix docker versions   old   badge ,0
define distributed as a type (#3740),0.42537594,- Deprecated `DistributedType` in favor of `_StrategyType` ([#10505](https://github.com/PyTorchLightning/pytorch-lightning/pull/10505)),  define type   miss   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   miss   warn   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
Drop all result docs. Make the separation between flow and logging clear (#3744),0.51459265,moved eval loop logging to loggers (#3408), remove results docs. separate flow from log,0
log/save_interval based on global step (#3667),0.6121731,Consistently use step=trainer.global_step in LearningRateMonitor independently of logging_interval (#4376),  log interval based on global step   test   test   test   test   pep   pep   added changelog   pep   merge   remove unused arg ,0
Update new-project.rst (#3734),0.40894848,Set version as today (#13906),,0
tests for val step flow and logging (#3731),0.78228575,tests for val loop flow (#2605),  ref: test val epoch end   ref: test val epoch end   ref: test val epoch end   ref: test log dict   ref: test log dict   ref: test log dict   ref: test log dict ,1
Support checkpoint hooks on data module (#3563),0.78997767,DataModule hooks for loading and saving checkpoints,  Split out changes from #3563 to make that PR easier to review. This formats the file according to the Black formatter   Store a reference to the trainer on the datamodule   Fixes #3682   Update data_connector.py   Update data_connector.py   Update test_datamodules.py   Split out changes from #3563 to make that PR easier to review. This formats the file according to the Black formatter   support checkpoint hooks for datamodule   refactor on_{save/load}_checkpoint to a separate hook class that both the lightning module and data module inherit add spots in callback connector to call new datamodule hooks if available   hooks formatting   Update hooks.py   Update checkpoint_connector.py   Update lightning.py   update based on upstream/master   checkout upstream/master   Update checkpoint_connector.py   add tests   undo format revert   Updated CHANGELOG.md   add checkpoint hooks   add Dict type   import CheckpointHooks ,1
ref: test val flow steps (#3723),0.798564,tests for val loop flow (#2605),  ref: test val epoch end   ref: test val epoch end   ref: test val epoch end ,1
Add a more direct test of multi-gpu training working (#2084),0.7339571,GPU training (#2704),  Add a more direct test of multi-gpu training working   Update tests/base/develop_pipelines.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Fix ModelCheckpoint period (#3630),0.80777884,Removed deprecated property ModelCheckpoint.period in favor of ModelCheckpoint.every_n_epochs (#9213),"  Fix ModelCheckpoint period   Remove comma   Minor changes   skip check   Revert ""skip check""   Already pushed to master This reverts commit 00d9e77b81e7dc9a6cb90c676f744bce23514cb7. Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai",1
Update lr_finder.rst (#3714),0.6976059,move lr_finder (#3434), Update lr_finder.rst  Misspelling correction  Update docs/source/lr_finder.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
formatting @Will (#3719),0.49738145,"@justusschock, @quinor, @williamFalcon",,0
ref: enable self.log for eval loop metrics (#3715),0.6961864,moved eval loop logging to loggers (#3408),  ref: test val epoch end   ref: test val epoch end   ref: test val epoch end   ref: test val epoch end   ref: test val epoch end   ref: test val epoch end ,0
ref: move backends back to individual files (1/5) (ddp_cpu) (#3712),0.78678095,move backends back to individual files (#3712),  ref: make each backend independent for easier debugging and independent debugging   ref: make each backend independent for easier debugging and independent debugging   ref: make each backend independent for easier debugging and independent debugging   ref: make each backend independent for easier debugging and independent debugging   ref: make each backend independent for easier debugging and independent debugging   ref: make each backend independent for easier debugging and independent debugging   ref: test val epoch end   ref: test val epoch end ,1
disable optimizers setup during testing (#3059),0.89506644,Disabled optimizers setup during testing (#3059),  disable configure_optimizers during testing   minor changes   hvd and ddp   fix precision during testing   fix ddp   fix amp   fix cpu   update dp   simplify optimizers   add test   codefactor   ref optimizer setup   chlog   suggestions   isort   rebased with master ,1
ref: separate flow vs log tests (#3704),0.51225215,Cleaning up stale logger tests (#3490),,0
ref: enable self.log from val step (#3701),0.6217648,"Deprecated passing flush_logs_every_n_steps as a Trainer argument, instead pass it to the logger init if supported (#9366)",  .log in eval   ref   ref: enable self.log in val step ,0
ref: (2/n) fix no log in epoch end (#3699),0.7058114,epoch can now log independently (#3843),,1
Make Trainer.__test_using_best_weights use cloud_io's load to support more storage backends (#3694),0.63281006,"Standardized the dataloaders arguments of trainer.{fit,valdiate,test,tune} (#7431)",  Split out changes from #3563 to make that PR easier to review. This formats the file according to the Black formatter   Store a reference to the trainer on the datamodule   Fixes #3682   Update data_connector.py   Update data_connector.py   Update test_datamodules.py   Support more storage backends in trainer.test using best weights   Similar to #3692   Update trainer.py   Update trainer.py   use cloud_io load directly,0
"[WIP] ref: deprecated results obj, added support for simpler comms (1/n) (#3681)",0.8614821,"deprecated results obj, added support for simpler comms (#3681)","  ref: deprecated results obj, added support for simpler comms. Decouples logging from loops   ref: deprecated results obj, added support for simpler comms. Decouples logging from loops   ref: deprecated results obj, added support for simpler comms. Decouples logging from loops   ref: deprecated results obj, added support for simpler comms. Decouples logging from loops   ref: deprecated results obj, added support for simpler comms. Decouples logging from loops   ref: deprecated results obj, added support for simpler comms. Decouples logging from loops   fix global step err   fix global step err   fix global step err   fix global step err   fix global step err   fix typing err   fix str   fix typing err ",1
ref: (results 1/n) enable tracking original metric when step and epoch are both true (#3685),0.9033984,enable tracking original metric when step and epoch are both true (#3685), enable tracking original metric when step and epoch are both true,1
remove flake 8 (#3687),0.3871143,Remove MetricsHolder (#7909),,0
ref: add .log to lightning module (1/n) (#3686),0.89685935,"add .log to lightning module (#3686, #3699, #3701, #3704, #3715)",,1
Fix global step increment on training_epoch_end (#3673),0.70559454,"def training_epoch_end(self, outputs):",  fix   fix global step err   fix global step err   fix global step err   fix global step err   fix global step err   fix global step err   Co-authored-by: William Falcon waf2107@columbia.edu,1
"change default save_top_k, save_last to None (#3680)",0.7841215,Changed defaults of save_top_k and save_last to None in ModelCheckpoint (#3680),  topk default   fix test that doesn't have best available   remove print   3680 changes   fix backward   temp revert   te   add warning by carmocca   format docstring for test   specify monitor in ES test with top k   improve docstring for save_last   remove commented lines   revert passing model to test   undo regex mistake   changelog   fix test covering case monitor=None and savetopk=-1   docstring   fix test for saving all checkpoints   don't save checkpoints for save_top_k=0   add test for savetopk=0   Co-authored-by @carmocca Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
Add a reference to the Trainer on the LightningDataModule (#3684),0.8549608,reference to the Trainer on the LightningDataModule (#3684),  Split out changes from #3563 to make that PR easier to review. This formats the file according to the Black formatter   Store a reference to the trainer on the datamodule   Fixes #3682   Update data_connector.py   Update data_connector.py   Update test_datamodules.py ,1
Add ModelCheckpoint.to_yaml method (#3048),0.70449364,"Removed deprecated ModelCheckpoint arguments prefix, mode=""auto"" (#6162)",  Add ModelCheckpoint.to_json()   Add ModelCheckpoint.to_json() test   Fix W292: Add new line at end of file   Apply suggestions from code review   Apply suggestions from code review   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com   Fixed tests   Update pytorch_lightning/callbacks/model_checkpoint.py   Apply suggestions from code review   fix test   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
enable None model checkpoint default (#3669),1.0000001,Enable None model checkpoint default (#3669),  enable None model checkpoint default   enable None model checkpoint default   enable None model checkpoint default   enable None model checkpoint default   enable None model checkpoint default   enable None model checkpoint default   enable None model checkpoint default   enable None model checkpoint default   enable None model checkpoint default   enable None model checkpoint default   enable None model checkpoint default   enable None model checkpoint default   enable None model checkpoint default   enable None model checkpoint default   enable None model checkpoint default   enable None model checkpoint default   enable None model checkpoint default   enable None model checkpoint default   enable None model checkpoint default   enable None model checkpoint default   enable None model checkpoint default   enable None model checkpoint default ,1
spec Horovod version (#3661),0.59304,horovod deprecation (#16141),"  spec Horovod version   MAKEFLAGS=""-j2""   tests   CI   docker   CI   docker ",0
Enable PyTorch 1.7 in conda CI (#3541),0.81494224,Enable PyTorch 1.7 compatibility (#3541),"  enable pt 1.7   readme   nightly diff version testing, will delete later   nightly diff version testing, will delete later   back to normal [ci skip]   use ignored_properties   define ignored_properties in respective modules   change log   formatting   Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",1
update stale config (#3509),0.55142975,Configuration Validator (#9779),  update stale conf   labels ,0
"Mocking loggers (part 1, wandb) (#3596)",0.85866046,"Mocking loggers (#3596, #3617, #3851, #3859, #3884, #3853, #3910, #3889, #3926)",  mocking for wandb   remove wandb import in amp test   mock loggers in sphinx   check tests   Update extra.txt   setup   dev   min   revert   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai,1
dockers nightly (#3615),0.4627154,Remove unnecessary intermediate layers in Dockerfiles (#5697),  dockers nightly   typo   Apply suggestions from code review   Co-authored-by: Jeff Yang ydcjeff@outlook.com Co-authored-by: Jeff Yang ydcjeff@outlook.com,0
Allow ModelCheckpoint monitor to be None (#3633),1.0000001,Allow ModelCheckpoint monitor to be None (#3633),  Fix ModelCheckpoint period   Test for less epochs ,1
"Fix incorrect ""Saving latest checkpoint"" warning (#3588)",0.7754452,Changed resume_from_checkpoint warning to an error when the checkpoint file does not exist (#7075),"  Fix incorrect ""Saving latest checkpoint"" warning   Replace warning with info. Run PyCharm's optimize imports   Remove unused class variable. Refactor logic. Improve test   Fix De Morgan's ",1
fix building nightly (#3642),0.46304157,  * Removed `optimizer_idx` argument from `LightningModule.lr_scheduler_step`,,0
Wrap prepare_data and setup only once inside DataModule (#3654),0.68396056,"DataModules now avoid duplicate {setup,teardown,prepare_data} calls for the same stage (#7238)",Fix #3652,0
Split GPUStatsMonitor function (#3644),0.6253681,Refactor GPUStatsMonitor to improve training speed (#3257),  Split function   Add docstrings   Add typing annotations   Minor refactor   Make static to add a test ,0
Lightning docker image based on base-cuda (#3637),0.5506587,  * Deprecated the `pytorch_lightning.utilities.device_parser.num_cuda_devices` in favor of `lightning_lite.accelerators.cuda.num_cuda_devices`,  use lightning CI docker   exclude py3.8 and torch1.3   torch 1.7   mergify   Apply suggestions from code review   Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Update docstring for early_stop_callback default Trainer argument (#3641),0.8830088,Deprecate early_stop_callback Trainer argument (#3845),,1
test examples (#3643),0.5263053,- Full tests that test specific functionality in trainer.,  test examples   testing   testing   typo   req   exception   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
fix examples (#3631),0.49417606,"In addition, we fixed:",  fix examples   fix examples ,0
Add stronger typing to gradient accumulation scheduler callback (#3558),0.60115826,Separate the Gradient Accumulation Scheduler from Trainer (#16729), Update gradient_accumulation_scheduler.py  add types for gradient accumulation scheduler callback  Update gradient_accumulation_scheduler.py,0
use tmpdir in tests when writing predictions to disk (#3561),0.48433176,"Improved flexibility for naming of TensorBoard logs, can now set version to a str to just save to that directory, and use name='' to prevent experiment-name directory (#804)",  save to tmpdir   path ,0
fix examples (#3623),0.47680396,Simplify the PL examples structure (shallower and more readable) (#1247),  fix examples   fix examples ,0
Adding clarifying documentation on the usage of second_order_closure (#3551),0.4705756,Check if optimizer supports closure (#4981),  Adding clarifying documentation on the usage of second_order_closure   oops typo   making functions more sane   fixing spacing issues - I think   Apply suggestions from code review   suggestions   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
fix on_fit_start (#3616),0.5467766,"def on_fit_start(self, *args, **kwargs):",  init   fix call_hook args ,0
fix dp issues + update examples and test examples (#3618),0.592399,DDP Debugging Improvements,  fix dp   fix dp   fix dp   fix dp   fix examples   fix examples   fix examples   fix examples   fix examples   fix examples   fix examples   fix examples   fix examples   fix examples   fix examples   fix examples   fix examples   fix examples   fix examples   fix examples   fix examples   fix examples ,0
build more docker configs (#3533),0.62305015,Remove unnecessary intermediate layers in Dockerfiles (#5697),  update build cases   list   matrix   matrix   builds   docker   -j1   -q   -q   sep   docker   docker   mergify   -j1   -j1   horovod   copy ,0
enable any logged metric to be accessible in callbacks (#3598),0.64333594,Integrated metrics API with self.log (#3961),  enable any logged or written metric to be accessible in callbacks   enable any logged or written metric to be accessible in callbacks   enable any logged or written metric to be accessible in callbacks   enable any logged or written metric to be accessible in callbacks   enable any logged or written metric to be accessible in callbacks   enable any logged or written metric to be accessible in callbacks   enable any logged or written metric to be accessible in callbacks   enable any logged or written metric to be accessible in callbacks   enable any logged or written metric to be accessible in callbacks   enable any logged or written metric to be accessible in callbacks   enable any logged or written metric to be accessible in callbacks   enable any logged or written metric to be accessible in callbacks   enable any logged or written metric to be accessible in callbacks   enable any logged or written metric to be accessible in callbacks   enable any logged or written metric to be accessible in callbacks   enable any logged or written metric to be accessible in callbacks   enable any logged or written metric to be accessible in callbacks   enable any logged or written metric to be accessible in callbacks   enable any logged or written metric to be accessible in callbacks   enable any logged or written metric to be accessible in callbacks   enable any logged or written metric to be accessible in callbacks   clarify forward   clarify forward   clarify forward   clarify forward ,0
update ambiguous info (#3613),0.4235799,Details changes,,0
update readme with new notebooks (#3608),0.438501,No API changes - We commit to backward compatibility in the 2.0 series,,0
clarify forward (#3611),0.5831214,reduced all simplified forward (#3126),  clarify forward   clarify forward   clarify forward   clarify forward ,0
clarify forward (#3609),0.5506337,reduced all simplified forward (#3126),  clarify forward   clarify forward ,0
Edited using Colaboratory (#3601),0.3678917,"In addition, we fixed:",,0
fix warning (#3538),0.65782315,Deprecation warning (#3844),,0
Small documentation fix (#3589),0.5529993,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",  fixes resnet   Fixes formatting and resnet ,0
faster tests (#3604),0.5102062,Extensively tested code.   ,,0
Docs- update lightning 2 steps guide (#3602),0.61985,Update the Lightning App docs (#13537),  new-project   Update new-project.rst   enable any logged or written metric to be accessible in callbacks   enable any logged or written metric to be accessible in callbacks   enable any logged or written metric to be accessible in callbacks   enable any logged or written metric to be accessible in callbacks   enable any logged or written metric to be accessible in callbacks   enable any logged or written metric to be accessible in callbacks   enable any logged or written metric to be accessible in callbacks   enable any logged or written metric to be accessible in callbacks   enable any logged or written metric to be accessible in callbacks   Co-authored-by: William Falcon waf2107@columbia.edu,0
Add torch log API usage for lightning module instantiation (#3603),0.6808057,"add .log to lightning module (#3686, #3699, #3701, #3704, #3715)",  Split out changes from #3563 to make that PR easier to review. This formats the file according to the Black formatter   support checkpoint hooks for datamodule   refactor on_{save/load}_checkpoint to a separate hook class that both the lightning module and data module inherit add spots in callback connector to call new datamodule hooks if available   hooks formatting   Update hooks.py   Update hooks.py   Update checkpoint_connector.py   Update lightning.py   update based on upstream/master   checkout upstream/master   Update lightning.py   Update lightning.py   Co-authored-by: William Falcon waf2107@columbia.edu,0
Add missing line. Add a test (#3594),0.50862765,- fixed all the .test() calls,,0
Update gitignore (#3595),0.45593733,Pkg: fix parsing versions by @Borda in https://github.com/Lightning-AI/lightning/pull/15401,  Update gitignore   Update .gitignore   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
fix notebooks/README.md colab links (#3599),0.47354263,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",,0
add lightning colab tutorial notebooks (#3591),0.51148885,Updated LightningTemplateModel to look more like Colab example (#1577),  :zap: add lightning colab tutorial notebooks   Update notebooks/01-mnist-hello-world.ipynb   Update notebooks/03-basic-gan.ipynb   Update notebooks/04-transformers-text-classification.ipynb   Update notebooks/02-datamodules.ipynb   add jupytext   :bug: comment ipynb related things in conf.py   :fire: remove .py files   Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai,0
Docs clean up of results and forward vs training_step confusion (#3584),0.6214663,Refactored result handling in training loop (#7506),  docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs ,0
[Metrics] AUROC error on multilabel + improved testing (#3350),0.6064186,Removed reorder parameter of the auc metric (#5004),  error on multilabel   fix tests   fix pep8   changelog   update doc test   fix doctest   fix doctest   update from suggestion   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   Update test_classification.py   Update test_classification.py   retrigger test   'pep8   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Black format pytorch_lightning/core/datamodule.py (#3574),0.671915,Removed the deprecated pytorch_lightning.core.lightning module in favor of pytorch_lightning.core.module (#16318),Split out changes from #3563 to make that PR easier to review,0
Black format pytorch_lightning/core/hooks.py (#3575),0.6797495,- Fixed to avoid common hook warning if no hook is overridden ([#12131](https://github.com/PyTorchLightning/pytorch-lightning/pull/12131)),Split out changes from #3563 to make that PR easier to review. This formats the file according to the Black formatter,0
Black format pytorch_lightning/core/lightning.py (#3576),0.7212505,Removed the deprecated pytorch_lightning.core.lightning module in favor of pytorch_lightning.core.module (#16318), Black format pytorch_lightning/core/hooks.py  Split out changes from #3563 to make that PR easier to review. This formats the file according to the Black formatter  Split out changes from #3563 to make that PR easier to review. This formats the file according to the Black formatter,1
ref: result 1/n (make monitor default to checkpoint_on to simplify re… (#3571),0.9176588,Result - make monitor default to checkpoint_on to simplify (#3571),  ref: result 1/n (make monitor default to checkpoint_on to simplify result syntax)   ref: result 1/n (make monitor default to checkpoint_on to simplify result syntax)   ref: result 1/n (make monitor default to checkpoint_on to simplify result syntax)   ref: result 1/n (make monitor default to checkpoint_on to simplify result syntax)   ref: result 1/n (make monitor default to checkpoint_on to simplify result syntax)   ref: result 1/n (make monitor default to checkpoint_on to simplify result syntax)   ref: result 1/n (make monitor default to checkpoint_on to simplify result syntax)   ref: result 1/n (make monitor default to checkpoint_on to simplify result syntax)   ref: result 1/n (make monitor default to checkpoint_on to simplify result syntax)   ref: result 1/n (make monitor default to checkpoint_on to simplify result syntax)   ref: result 1/n (make monitor default to checkpoint_on to simplify result syntax)   ref: result 1/n (make monitor default to checkpoint_on to simplify result syntax)   ref: result 1/n (make monitor default to checkpoint_on to simplify result syntax)   ref: result 1/n (make monitor default to checkpoint_on to simplify result syntax)   ref: result 1/n (make monitor default to checkpoint_on to simplify result syntax)   ref: result 1/n (make monitor default to checkpoint_on to simplify result syntax)   ref: result 1/n (make monitor default to checkpoint_on to simplify result syntax)   ref: result 1/n (make monitor default to checkpoint_on to simplify result syntax)   ref: result 1/n (make monitor default to checkpoint_on to simplify result syntax)   Update pytorch_lightning/callbacks/model_checkpoint.py   Co-authored-by: ananthsub ananth.subramaniam@gmail.com   ref: result 1/n (make monitor default to checkpoint_on to simplify result syntax)   force crash when max_epochs < epochs in a checkpoint   Co-authored-by: ananthsub ananth.subramaniam@gmail.com,1
force crash when max_epochs < epochs in a checkpoint (#3580),0.5819464,"Removed the on_epoch guard from the ""should stop"" validation check (#7701)",  force crash when max_epochs < epochs in a checkpoint   force crash when max_epochs < epochs in a checkpoint ,0
Fix deterministic behavior in ddp_spawn (#3573),0.7497328,"decoupled DDP, DDP spawn (#3733, #3766, #3767, #3774, #3802, #3806)",  docs   set env variable   fix   changelog ,1
fixes 3549 (#3564),0.5558269,"In addition, we fixed:",,0
"Fix overfit_batches > 0 on distributed_backend = ""ddp"" (#3534)",0.6365714,Removed support for FairScale's sharded training (strategy='ddp_sharded'|'ddp_sharded_spawn'). Use Fully-Sharded Data Parallel instead (strategy='fsdp') (#16329),  example   ex   example   sampler   fix   fix   remove example   changelog ,0
fix warnings on windows (#3555),0.5889137,warning_utils >> warnings,,0
nightly releases (#3552),0.57093596,This release includes:,  nightly   nightly   ls ,0
Black format the model checkpoint callback (#3559),0.61283743,Enable None model checkpoint default (#3669)," Update gradient_accumulation_scheduler.py  add types for gradient accumulation scheduler callback  Apply black formatting to model checkpoint callback  auto-format, no other changes  Update gradient_accumulation_scheduler.py  drop other changes Co-authored-by: William Falcon waf2107@columbia.edu",0
Add type hints to model checkpoint callback (#3560),0.7123491,"If you want to customize ModelCheckpoint callback, without all the extra functionality this class provides, this release provides an empty class Checkpoint for easier inheritance. In all internal code, the check is made against the Checkpoint class in order to ensure everything works properly for custom classes."," Update gradient_accumulation_scheduler.py  add types for gradient accumulation scheduler callback  Apply black formatting to model checkpoint callback  auto-format, no other changes  Update gradient_accumulation_scheduler.py  drop other changes   Add type hints to model checkpoint callback   Update model_checkpoint.py   remove trainer/lightning modules types to avoid circular import",1
Fix typo in loggers.rst (#3557),0.60334015,- pickling errors with loggers (txs @awaelchli),,0
Fixed failed import warning for torch.distributed.group (#3553),0.7072618,- Deprecated specifying the process group backend through the environment variable `PL_TORCH_DISTRIBUTED_BACKEND` ([#11745](https://github.com/PyTorchLightning/pytorch-lightning/pull/11745)),,1
Allow kwargs in Wandb & Neptune + kwargs docstring (#3475),0.47223413,WandbLogger does not force wandb reinit arg to True anymore and creates a run only when needed (#4648),  Allow kwargs in WandbLogger   isort   kwargs docstring   typo   kwargs for other loggers   pep and isort   formatting   fix failing test   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
drop v0.10 deprecated (#3454),0.55129445,Deprecation warning (#3844),  drop v0.10 deprecated   import   missed ,0
Make dims a property in datamodule (#3547),0.6097995,Support shorthand notation to instantiate datamodules (#10011),  :bug: make dims a property   :bug: fix ,0
Early stopping doc (#3193),0.4830279,Deprecate early_stop_callback Trainer argument (#3845), Updated explanation to enabling early stopping via boolean flag.  Now also includes case of returning Result objects.   Improved API documentation for checkpoint_on and early_stp_on in results.   Apply suggestions from code review   Fix terminology.   Fix wrong documentation. Strict checking is disabled when using structured results.   element typo   update remaining edits from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Improve Comet Logger pickled behavior (#2553),0.65694714,- pickling errors with loggers (txs @awaelchli),  Improve Comet Logger pickled behavior   Delay the creation of the actual experiment object for as long as we can.  Save the experiment id in case an Experiment object is created so we can   continue the same experiment in the sub-processes.  Run pre-commit on the comet file.   Handle review comment   Make most Comet Logger attribute protected as they might not reflect the final Experiment attributes. Also fix the typo in the test name.   Ensure that CometLogger.name and CometLogger.version always returns str   Add new test for CometLogger.version behavior   Add new tests for CometLogger.name and CometLogger.version   Apply review suggestions   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   Remove extraneous comments in Comet logger tests   Fix lint issues   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Fix ModelCheckpoints name formatting (#3163),0.73666435,Changed ModelCheckpoint version suffixes to start at 1 (#5008),  Fix ModelCheckpoint's name formatting   Fix failing tests   Add dot to CHECKPOINT_SUFFIX   Set variables to their default values at the end of tests   Fix logic for filepath='' and filename=None. Add test   Fix Windows tests   Fix typo. Remove leading line break and zeroes   Remove CHECKPOINT_SUFFIX   Fix typos. Use appropriate f-string format   Apply suggestions from code review   Fix broken tests after #3320   Finish changes suggested by Borda   Use explicit test var names   Apply suggestions   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Apply suggestions  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   Update CHANGELOG   Apply suggestions from code review   for   prepend whitespace in warn msg   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Fix early stopping with training step's return dict (#3347),0.6248156,Removed training loop explicitly calling EarlyStopping.on_validation_end if no validation is run (#7069)," Fixes the test for early stopping without val step.  The expression which checked, if early stopping was triggered, had an off-by-one error and hence was true even if early stopping was not triggered. Furthermore set patience to 0 and max epochs to 10, to ensure loss has enough time to flatten.  Fixes early stopping without val step.  The issue has been, that only early_stop_on key was checked and not an arbitrary monitor key.  Fixes branch, which checks whether early stopping is done during validation.  Before only val_early_stop_on was checked. Since arbitrary keys can be used, the set of possible validation keys cannot be exhaustive. Hence this disables ""early stopping on_train_epoch_end"" via an instance attribute if early stopping was executed in on_validation_epoch_end. Furthermore adds a test, which ensures arbitrary keys work.  Improve check whether eval results are used.  Only disable early checking with train results if eval results are actually used. Before they were always disabled in on_validation_epoch_end. Rename and document instance variable, to make it more clear.   Remove wrong documentation on behaviour of early stopping with train result' dict.   Apply suggestions from code review   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",0
Fix misuse of transforms in docs (#3546),0.45192742,"Deprecated DataModule properties: train_transforms, val_transforms, test_transforms, size, dims (#8851)",  :pencil: docs   :pencil: docs   :pencil: docs   :pencil: docs   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Fix log debug call (#3528),0.6040256,Removed logger_connector legacy code (#6733),,0
"stable, dev PyTorch in Dockerfile and conda gh actions (#3074)",0.59648657,- Include the `pytorch_lightning` version as a header in the CLI config files ([#12532](https://github.com/Lightning-AI/lightning/pull/12532)),  dockerfile and actions file   dockerfile and actions file   added pytorch conda cpu nightly   added pytorch conda cpu nightly   recopy base reqs   gh action include torch nightly   add pytorch nightly & conda gh badge   rebase   fix horovod   proposal refactor   Update .github/workflows/ci_pt-conda.yml   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update .github/workflows/ci_pt-conda.yml  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   update   update   fix cmd   filled &&   fix   add -y   torchvision >0.7 allowed   explicitly install torchvision   use HOROVOD_GPU_OPERATIONS env variable   CI   skip 1.7   table   Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
fix lib paths after Wandb 0.10 (#3520),0.5990588,- Fixed wandb `save_dir` is overridden by `None` `dir` when using CLI ([#14878](https://github.com/Lightning-AI/lightning/pull/14878)),  try   try   drop 0.20   drop 0.19.5   -U   Fixed Horovod in CI due to wandb==0.10.0 sys.path modifications (#3525)   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   format   wb freeze   types   Co-authored-by: Travis Addair taddair@uber.com,0
Fix IoU score for classes not present in target or pred (#3098),0.90050226,Changed IoU score behavior for classes absent in target and pred (#3098)," Fix IoU score for classes not present in target or pred  Fixes #3097  Allow configurable not_present_score for IoU for classes   not present in target or pred. Defaults to 1.0.  Also allow passing num_classes parameter through from iou   metric class down to its underlying functional iou   call.   Changelog: move IoU not-present score fix to [unreleased]   IoU: avoid recomputing class presence in target and pred   Use already-computed support, true positives, and false positives to determine if a class is not present in either target or pred.  Test IoU against sklearn jaccard_score  Also add TODO to test our IoU's not_present_score against sklearn's jaccard_score's zero_division when it beecomes available.  IoU: remove_bg -> ignore_index  Fixes #2736  Rename IoU metric argument from remove_bg -> ignore_index. Accept an optional int class index to ignore, instead of a bool and   instead of always assuming the background class has index 0.  If given, ignore the class index when computing the IoU output,   regardless of reduction method.   Improve documentation for IoU not_present_score   Update default IoU not_present_score to 0.0   Add note about IoU division by zero   Rename IoU not_present_score -> absent_score   Update IoU absent score changelog wording   Condense IoU absent_score argument docstring   Remove unnecessary IoU ignore_index comment   docstrings   isort   flake8   Fix test of IoU against sklearn jaccard   Use macro instead of micro averaging in sklearn's jaccard score, to match multi-class IoU, which conventionally takes per-class scores before averaging. Co-authored-by: rohitgr7 rohitgr1998@gmail.com",1
fix tensorboard version (#3132),0.8003254,Improved the error message for installing tensorboard or tensorboardx (#17053),  tensorboard version   WIP test tb hparams logs (#3040)   optional   req   tensorboard>=2.2.0   data   data   TB   Co-authored-by: Rosario Scalise rosario@cs.washington.edu,1
fix gradient norm tracking for row_log_interval > 1 (#3489),0.71705085,Gradient norm tracking (#16745),  fix + test   changelog   Apply suggestions from code review   Co-authored-by: Tim Chard timchard@hotmail.com  improve test  Co-authored-by: Tim Chard timchard@hotmail.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com,1
ref: precision plugins 1/n (#3504),0.8794981,precision plugins (#3504),  ref: precision plugins 1/n   ref: precision plugins 1/n ,1
[Metrics] Class reduction similar to sklearn (#3322),0.8091813,Changed class_reduction similar to sklearn for classification metrics (#3322),  new class reduce interface   update docs   pep8   update_class_metrics   fix doctest   changelog   fix docs   fix codefactor   fix codefactor   formatting   fix typo   fix typo   typo pr -> per   update from suggestion   fix error   Apply suggestions from code review   Update CHANGELOG.md   formatting   timeouts   docstring formatting for reg metrics   pep   flake8   revert workflow changes   suggestions   Co-authored-by: Nicki Skafte nugginea@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai Co-authored-by: rohitgr7 rohitgr1998@gmail.com,1
Pass epoch argument to Comet Logger (#3438),0.65047026,Using .comet.config file for CometLogger (#1913),  Pass epoch argument   Copy epoch instead of inplace pop   Remove whitespace   Add test for epoch logging   add docstring   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
ref: apex plugin (#3502),0.7892156,apex plugin (#3502),  ref: apex plugin   ref: apex plugin   ref: apex plugin ,1
build docs on master (#3492),0.43503618,Docs improvements,  build docs on master   fomatting ,0
Disable train dataloader shuffle when overfit_batches is active. (#3501),0.668696,"Overriding the on_train_batch_{start,end} hooks in conjunction with taking a dataloader_iter in the training_step no longer errors out and instead shows a warning (#16062)",  Disable train dataloader shuffle when overfit_batches is active.   pep8   Co-authored-by: William Falcon waf2107@columbia.edu,0
Metric aggregation (#3321),0.6425166,Sklearn metrics classes (#1327),  metric aggregation   metric aggregation   add at_least_1d   fix output formatting   add metric tests   add missing test case   remove reduce_op frm metric classes   fix reduce_op stuff   start test fixing   fix tests due to aggregation   fix faulty import   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   remove reduce_op docstrings   add compute   remove import   remove collection metric   update base class   update tests   Update metric.py   Update metric.py   Apply suggestions from code review   change default aggregate   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com,0
reverted achange from testcode:: to code-block:: python (#3453),0.49416625,Removed deprecated auto_move_data decorator (#9231),Co-authored-by: David Waterworth david.waterworth@cim.io,0
"docs: configure_sync_batchnorm, amp, readme python/conda badge (#3328)",0.5899042,- The strategies that support `sync_batchnorm` now only apply it when fitting ([#11919](https://github.com/PyTorchLightning/pytorch-lightning/pull/11919)),  fix(docs): change to configure_sync_batchnorm for sync bn hook   Update docs/source/lightning-module.rst   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   distinguish pypi and conda download badges   python version badge and original pypi download badge   Update docs/source/lightning-module.rst   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
fix normalize mode at confusion matrix (replace nans with zeros) (#3465),0.58908606,Remove nan loss in manual optimization (#5121),  replace nans to 0 at conf. matrix & update tests   cm.isnan() -> torch.isnan(cm)   fix row-wise division while normalize   update tests   pep8 fix   Update tests/metrics/test_classification.py   add comment to test Co-authored-by: Nicki Skafte skaftenicki@gmail.com  Update tests/metrics/functional/test_classification.py  Co-authored-by: Nicki Skafte skaftenicki@gmail.com  Update pytorch_lightning/metrics/functional/classification.py  Co-authored-by: Nicki Skafte skaftenicki@gmail.com  final update  Co-authored-by: Nicki Skafte skaftenicki@gmail.com,0
docfix (#3463),0.56334245,Docs,Co-authored-by: Nicki Skafte nugginea@gmail.com,0
update docs on log_save_interval (#3345),0.5268151,add_log_row_interval -> row_log_interval,  update docs for log_save_interval   formatting   empty   link reporting ,0
cleaning up stale logger tests + flake8 (#3490),0.8627036,Cleaning up stale logger tests (#3490),  cleaning up stale logger tests   cleaning up stale logger tests   cleaning up stale logger tests   cleaning up stale logger tests   cleaning up stale logger tests   cleaning up stale logger tests ,1
Silenced some warnings. verified ddp refactors (#3483),1.0000002,Silenced some warnings. verified ddp refactors (#3483),  ref: ddp verify   ref: ddp verify   ref: ddp verify   ref: ddp verify   ref: ddp verify   ref: ddp verify   ref: ddp verify   ref: ddp verify   ref: ddp verify   ref: ddp verify   Update ddp_base_backend.py ,1
"docs: use ref for anchor links, fix a few typo (#3486)",0.5207738,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",,0
ref: merge backends x/n (#3482),0.78045404,"merge backends (#3476, #3477, #3478, #3480, #3482)",,1
ref: merge backends x/n (#3480),0.77845305,"merge backends (#3476, #3477, #3478, #3480, #3482)",,1
ref: merge backends x/n (#3478),0.77866304,"merge backends (#3476, #3477, #3478, #3480, #3482)",  ref: merge backends x/n   ref: merge backends x/n   ref: merge backends x/n   ref: merge backends x/n ,1
ref: merge backends x/n (#3477),0.7793041,"merge backends (#3476, #3477, #3478, #3480, #3482)",,1
ref: slurm connector 1/n (#3476),0.6026694,group connectors (#3472),  ref: slurm connector 1/n   ref: slurm connector 1/n   ref: slurm connector 1/n   ref: slurm connector 1/n ,0
ref: checkpoint connector methods 4/n (#3474),0.72420794,Refactor load in checkpoint connector (#4593),  ref: checkpoint connector methods 4/n   ref: checkpoint connector methods 4/n   ref: checkpoint connector methods 4/n   ref: checkpoint connector methods 4/n   ref: checkpoint connector methods 4/n   ref: checkpoint connector methods 4/n   ref: checkpoint connector methods 4/n   ref: checkpoint connector methods 4/n   ref: checkpoint connector methods 4/n ,1
ref: checkpoint connector methods 3/n,0.7077235,Refactor load in checkpoint connector (#4593),,1
ref: group connectors (#3472),0.8737247,group connectors (#3472),  ref: accelerator connector methods 3/n   ref: accelerator connector methods 3/n ,1
ref: accelerator connector methods x/n (#3470),0.8932026,"accelerator connector methods x/n (#3469, #3470, #3474)",,1
ref: accelerator connector methods x/n (#3469),0.8756629,"accelerator connector methods x/n (#3469, #3470, #3474)",  ref: accelerator connector methods x/n   ref: accelerator connector methods x/n ,1
forward key metrics (#3467),0.55104434,Updated SSIM metric (#4566)(#4656),,0
Fix trivial comparison in model checkpoint test (#3464),0.6619227,Skipped best_model_path if checkpoint_callback is None (#2962),We were comparing keys across the same checkpoint dict instead of ckpt_last vs ckpt_last_epoch All other changes here are formatting,0
ref: move specific accelerator code x/n (#3457),0.90068895,move specific accelerator code (#3457),  ref: organize args x/n   ref: move specific accelerator code x/n   ref: move specific accelerator code x/n   ref: move specific accelerator code x/n ,1
implement fix and test (#3459),0.62946814,- fixed all the .test() calls,,0
[Metrics] class based embedding similarity + tests (#3358),0.6296698,Classification metrics overhaul (#4837),  embedding similarity class + test   fix tests   fix pep8   add docs   noindex   Update docs/source/metrics.rst   Update pytorch_lightning/metrics/self_supervised.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update pytorch_lightning/metrics/self_supervised.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   suggestions   changes to init   move all   fix imports   Apply suggestions from code review   assert typo   change import   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Nicki Skafte nugginea@gmail.com,0
ref: organize args 4/n (#3456),0.71326184,"organize args (##3435, #3442, #3447, #3448, #3449, #3456)",,1
drop nb sphinx (#3452),0.41854176,"If we forgot somebody or you have a suggestion, find us on Discord :zap:",,0
add num_classes argument to confusion matrix (#3450),0.5278094,"Removed multiclass_roc and multiclass_precision_recall_curve, use roc and precision_recall_curve instead (#4549)",  add num_classes arg to confusion matrix   update ConfusionMatrix test   final update) ,0
add CI for building dockers (#3383),0.46836782,"adding compute environments (#3837, [#3842)",  rename   fix badges   add docker build   mergify   update   env   ci   times   CI   name   comment ,0
Fix batch_outputs with optimizer frequencies (#3229),0.605178,Updated fast_dev_run to accept integer representing num_batches (#4629),  Fix batch_outputs with optimizers frequencies   optimizers   fix batch_outputs with optimizer frequencies   clean test   suggestion   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   chlog   failing doctest   failing doctest   update doctest   chlog   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
ref: organize args 3/n (#3449),0.72626513,"organize args (##3435, #3442, #3447, #3448, #3449, #3456)",  ref: organize args 3/n   ref: organize args 3/n   ref: organize args 3/n   ref: organize args 3/n   ref: organize args 3/n   ref: organize args 3/n ,1
ref: organize args 2/n (#3448),0.71963066,"organize args (##3435, #3442, #3447, #3448, #3449, #3456)",  ref: organize args 2/n   ref: organize args 2/n   ref: organize args 2/n ,1
"Updated ""Models defined by data"" (#3433)",0.6185092,Support shorthand notation to instantiate models (#9588),"  Updated ""Models defined by data""   Fixed WARNING: Inline substitution_reference start-string without end-string.   Co-authored-by: David Waterworth david.waterworth@cim.io",0
Fix Train result in val and test steps doc (#3440),0.6673473,Removed no return warning from val/test step (#6139),,0
ref: organize args 3/n (#3447),0.7262691,"organize args (##3435, #3442, #3447, #3448, #3449, #3456)",  ref: organize args 2/n   ref: organize args 2/n   ref: organize args 2/n   ref: organize args 2/n ,1
ref: organize args 2/n (#3442),0.71511304,"organize args (##3435, #3442, #3447, #3448, #3449, #3456)",  ref: organize args 2/n   ref: organize args 2/n ,1
ref: organize args 1/n (#3435),0.70901877,"organize args (##3435, #3442, #3447, #3448, #3449, #3456)",  ref: organize args 1/n   ref: organize args 1/n ,1
📝Update Readme.md (Code Error Minimal Example) (#3416),0.48429066,"Deprecated DataModule properties: has_prepared_data, has_setup_fit, has_setup_validate, has_setup_test, has_setup_predict, has_teardown_fit, has_teardown_validate, has_teardown_test, has_teardown_predict (#7657)"," 📝Update Readme.md (Code Error Minimal Example)  to use gpus we need to use trainer = pl.Trainer(max_epochs=1, gpus=8) but In readme only Trainer() is used   📝Update Readme.md(added import in GPU/TPU stmts)   📝Update Readme.md ( Added Import to all imports)   as per the suggestion from @awaelchli Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com",0
ref: move lr_finder (#3434),0.9185175,move lr_finder (#3434),  ref: move lr_finder   ref: move lr_finder   ref: move lr_finder   ref: move lr_finder   ref: move lr_finder   ref: move lr_finder   ref: move lr_finder ,1
ref: separate properties (#3432),0.5972274,New properties,  ref: separate properties   ref: separate properties   ref: separate properties   ref: separate properties ,0
ref: separate argparse (#3428),0.74914885,argparse_utils >> argparse,,1
make PyTorch Lightning PEP 561 Compliant  (#3187),0.7537662,Removed deprecated code in pytorch_lightning.utilities.meta (#16038),  Added py.typed.   Move py.typed inclusion to MANIFEST.in   Added path separator for inclusion ,1
ref: trainer argparse 1/n (#3421),0.6451442,"trainer/separate argparse (#3421, #3428, #3432)",  ref: trainer argparse 1/n   ref: trainer argparse 1/n   ref: trainer argparse 1/n   ref: trainer argparse 1/n   ref: trainer argparse 1/n   ref: trainer argparse 1/n   ref: trainer argparse 1/n   ref: trainer argparse 1/n ,0
license (#3423),0.37398145,Configuration Validator (#9779),,0
Fix #3417 (#3419),0.5657788,"In addition, we fixed:",,0
ref: separate trainer docstrings poc (#3418),0.6656323,trainer = Trainer(...),  ref: trainer 1/n   ref: separate trainer docstrings poc   ref: separate trainer docstrings poc   ref: separate trainer docstrings poc   ref: separate trainer docstrings poc ,0
ref: trainer 1/n (#3412),0.6733887,  trainer:,  ref: moved eval loop 2/n   ref: moved eval loop 2/n   ref: trainer 1/n   ref: trainer 1/n   ref: trainer 1/n ,0
ignore types in files (#3409),0.36447293,- all the file path errors with loggers (txs @awaelchli),  ignore types in files   CI timeout ,0
Get experiment_id from MLFlow only once instead of each training loop (#3394),0.6102713,- Allow logging to an existing run ID in MLflow with `MLFlowLogger` ([#12290](https://github.com/PyTorchLightning/pytorch-lightning/pull/12290)),  Get experiment_id from MLFlow only once instead of each training loop.   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   add test that asserts mlflow client is called to retrieve experiment id only once   make pep8 happy   logs   Co-authored-by: Patrick Orlando patrick.orlando@rea-group.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai,0
limit auto scaling batch size to the size of the training dataset (#3271),0.6908071,tuner.scale_batch_size(...),  fix   fix and test   fix merge error   test for max dataset size   changelog   update docs   fix merge   unused imports   imports ,0
ref: moved eval loop logging to loggers 1/n (#3408),0.92531943,moved eval loop logging to loggers (#3408),,1
ref: added model connector (#3407),0.659735,group connectors (#3472),  ref: added model connector   ref: added model connector   ref: added model connector ,0
ref: device to gpus (#3405),0.61998093,GPU training (#2704),  ref: device to gpus   ref: device to gpus   ref: device to gpus   ref: device to gpus   ref: device to gpus ,0
Added check for apex AMP and unit tests for Horovod + AMP (#3404),0.49529088,Trainer now raises an exception when requesting amp_level with native amp_backend (#9755),  Added check for apex AMP and unit tests for Horovod + AMP   Changelog   Fixed order of Horovod and Apex optimizer wrapping ,0
ref: device parser (#3400),0.9105891,"device parser (#3400, #3405)",  ref: train loop refactors part 2: 1/n   ref: device parser   ref: device parser   ref: device parser   ref: device parser   ref: device parser   ref: device parser   ref: device parser   ref: device parser ,1
ref: remove inner train loop 1/n (#3397),0.71520853,"inner train loop (#3359, #3361, #3362, #3363, #3365, #3366, #3367, #3368, #3369, #3370, #3371, #3372, #3373, #3374, #3375, #3376, #3385, #3388, #3397)",  ref: remove inner train loop 1/n   ref: remove inner train loop 1/n ,1
ref: all logging related calls in a connector 1/n (#3395),0.90424544,all logging related calls in a connector (#3395),  ref: all results+dict actions in a connector 1/n   ref: all results+dict actions in a connector 2/n   ref: all results+dict actions in a connector 2/n   ref: all results+dict actions in a connector 2/n ,1
ensure calling test multiple times does not change results (#3391),0.54111755,"Refactored training_batch + tests to verify correctness (#2327, #2328)",,0
Fix sample code of LightningModule DataLoaders (#3360),0.68206644,- Fixed issue where the wrapped dataloader `iter()` would be called twice ([#16841](https://github.com/Lightning-AI/lightning/pull/16841)), Fix sample code of LightningModule DataLoaders  In the LightningModule DataLoaders sample code the train DataLoader is again passed to DataLoader constructor which is incorrect.   Pass transform to MNIST dataset in documenattion.   Apply suggestions from code review   minor changes   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
Refa22 (#3388),0.37082282,[1.2.2] - 2021-03-02,  ref: inner train loop (intermediate step) 20/n   ref: inner train loop (intermediate step) 21/n   ref: inner train loop (intermediate step) 21/n   ref: inner train loop (intermediate step) 21/n   ref: inner train loop (intermediate step) 21/n   ref: inner train loop (intermediate step) 21/n ,0
ref: inner train loop (intermediate step) 19/n (#3385),0.66537523,"inner train loop (#3359, #3361, #3362, #3363, #3365, #3366, #3367, #3368, #3369, #3370, #3371, #3372, #3373, #3374, #3375, #3376, #3385, #3388, #3397)",  ref: inner train loop (intermediate step) 19/n   Update debugging.py   ref: inner train loop (intermediate step) 19/n ,0
ref: inner train loop (intermediate step) 17/n (#3376),0.67356145,"inner train loop (#3359, #3361, #3362, #3363, #3365, #3366, #3367, #3368, #3369, #3370, #3371, #3372, #3373, #3374, #3375, #3376, #3385, #3388, #3397)",  ref: inner train loop (intermediate step) 17/n   ref: inner train loop (intermediate step) 17/n   ref: inner train loop (intermediate step) 17/n ,0
Use full yaml loader (load is deprecated) (#3357),0.54843074,Deprecated @data_loader decorator  (#926),,0
ref: inner train loop (intermediate step) 16/n (#3375),0.6748962,"inner train loop (#3359, #3361, #3362, #3363, #3365, #3366, #3367, #3368, #3369, #3370, #3371, #3372, #3373, #3374, #3375, #3376, #3385, #3388, #3397)",  ref: inner train loop (intermediate step) 16/n   ref: inner train loop (intermediate step) 16/n   ref: inner train loop (intermediate step) 16/n   ref: inner train loop (intermediate step) 16/n   ref: inner train loop (intermediate step) 16/n   ref: inner train loop (intermediate step) 16/n ,0
ref: inner train loop (intermediate step) 16/n,0.6478104,"inner train loop (#3359, #3361, #3362, #3363, #3365, #3366, #3367, #3368, #3369, #3370, #3371, #3372, #3373, #3374, #3375, #3376, #3385, #3388, #3397)",,0
ref: inner train loop (intermediate step) 15/n (#3374),0.67046654,"inner train loop (#3359, #3361, #3362, #3363, #3365, #3366, #3367, #3368, #3369, #3370, #3371, #3372, #3373, #3374, #3375, #3376, #3385, #3388, #3397)",  ref: inner train loop (intermediate step) 15/n   ref: inner train loop (intermediate step) 15/n ,0
ref: inner train loop (intermediate step) 14/n (#3373),0.66999614,"inner train loop (#3359, #3361, #3362, #3363, #3365, #3366, #3367, #3368, #3369, #3370, #3371, #3372, #3373, #3374, #3375, #3376, #3385, #3388, #3397)",  ref: inner train loop (intermediate step) 14/n   ref: inner train loop (intermediate step) 14/n ,0
ref: inner train loop (intermediate step) 12/n (#3372),0.6685129,"inner train loop (#3359, #3361, #3362, #3363, #3365, #3366, #3367, #3368, #3369, #3370, #3371, #3372, #3373, #3374, #3375, #3376, #3385, #3388, #3397)",  ref: inner train loop (intermediate step) 12/n   ref: inner train loop (intermediate step) 12/n   ref: inner train loop (intermediate step) 12/n   ref: inner train loop (intermediate step) 12/n   ref: inner train loop (intermediate step) 12/n   ref: inner train loop (intermediate step) 12/n ,0
ref: inner train loop (intermediate step) 12/n (#3371),0.67046314,"inner train loop (#3359, #3361, #3362, #3363, #3365, #3366, #3367, #3368, #3369, #3370, #3371, #3372, #3373, #3374, #3375, #3376, #3385, #3388, #3397)",  ref: inner train loop (intermediate step) 12/n   ref: inner train loop (intermediate step) 12/n   ref: inner train loop (intermediate step) 12/n ,0
ref: inner train loop (intermediate step) 11/n (#3370),0.6659688,"inner train loop (#3359, #3361, #3362, #3363, #3365, #3366, #3367, #3368, #3369, #3370, #3371, #3372, #3373, #3374, #3375, #3376, #3385, #3388, #3397)",  ref: inner train loop (intermediate step) 11/n   ref: inner train loop (intermediate step) 11/n ,0
ref: inner train loop (intermediate step) 10/n (#3369),0.67109543,"inner train loop (#3359, #3361, #3362, #3363, #3365, #3366, #3367, #3368, #3369, #3370, #3371, #3372, #3373, #3374, #3375, #3376, #3385, #3388, #3397)",,0
ref: inner train loop (intermediate step) 9/n (#3368),0.6930256,"inner train loop (#3359, #3361, #3362, #3363, #3365, #3366, #3367, #3368, #3369, #3370, #3371, #3372, #3373, #3374, #3375, #3376, #3385, #3388, #3397)",  ref: inner train loop (intermediate step) 9/n   ref: inner train loop (intermediate step) 9/n   ref: inner train loop (intermediate step) 9/n   ref: inner train loop (intermediate step) 9/n ,0
"ref: inner train loop (intermediate step) 8/n"" (#3367)",0.695422,"inner train loop (#3359, #3361, #3362, #3363, #3365, #3366, #3367, #3368, #3369, #3370, #3371, #3372, #3373, #3374, #3375, #3376, #3385, #3388, #3397)",  ref: inner train loop (intermediate step) 7/n   ref: inner train loop (intermediate step) 8/n ,0
ref: inner train loop (intermediate step) 6/n (#3366),0.6716436,"inner train loop (#3359, #3361, #3362, #3363, #3365, #3366, #3367, #3368, #3369, #3370, #3371, #3372, #3373, #3374, #3375, #3376, #3385, #3388, #3397)",  ref: inner train loop (intermediate step) 6/n   ref: inner train loop (intermediate step) 6/n   ref: inner train loop (intermediate step) 6/n ,0
ref: inner train loop (intermediate step) 5/n (#3365),0.6783032,"inner train loop (#3359, #3361, #3362, #3363, #3365, #3366, #3367, #3368, #3369, #3370, #3371, #3372, #3373, #3374, #3375, #3376, #3385, #3388, #3397)",,0
ref: inner train loop (intermediate step) 3/n (#3363),0.65425235,"inner train loop (#3359, #3361, #3362, #3363, #3365, #3366, #3367, #3368, #3369, #3370, #3371, #3372, #3373, #3374, #3375, #3376, #3385, #3388, #3397)",,0
ref: inner train loop (intermediate step) 3/n (#3362),0.65851533,"inner train loop (#3359, #3361, #3362, #3363, #3365, #3366, #3367, #3368, #3369, #3370, #3371, #3372, #3373, #3374, #3375, #3376, #3385, #3388, #3397)",  ref: inner train loop (intermediate step) 3/n   ref: inner train loop (intermediate step) 3/n   ref: inner train loop (intermediate step) 3/n   ref: inner train loop (intermediate step) 3/n   ref: inner train loop (intermediate step) 3/n   ref: inner train loop (intermediate step) 3/n ,0
ref: inner train loop (intermediate step) 1/n (#3361),0.6405092,"inner train loop (#3359, #3361, #3362, #3363, #3365, #3366, #3367, #3368, #3369, #3370, #3371, #3372, #3373, #3374, #3375, #3376, #3385, #3388, #3397)",,0
ref: inner train loop (intermediate step) 1/n (#3359),0.64050627,"inner train loop (#3359, #3361, #3362, #3363, #3365, #3366, #3367, #3368, #3369, #3370, #3371, #3372, #3373, #3374, #3375, #3376, #3385, #3388, #3397)",,0
Correct documentation examples of optimizer_step (#3326),0.65060526,          optimizer.step(), Correct documentation examples of optimizer_step  Without the default arguments set in optimizer_step the examples fail due to the arguments not being provided  Apply suggestions from code review  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
added emb similarity (#3349),0.3728044,adding Trainer.tune() (#3293),,0
Ref: Pull duplicate data interface definition up into DataHooks class (#3344),0.9307204,duplicate data interface definition up into DataHooks class (#3344),  pull data hooks up into a common interface   fix multiple inheritance ordering   docs reference datahooks ,1
Refactor GPUStatsMonitor to improve training speed (#3257),1.0000004,Refactor GPUStatsMonitor to improve training speed (#3257),  Refactor GPUMonitor to improve training speed   added gpu ids to monitor   update tests   added deprecation warning   pep   fix test   fix docs   fix log_gpu_memory   move deprecation check   chlog   Update CHANGELOG.md   suggestions and fix   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
Fix non autoplaying videos on Safari iOS (#3343),0.4055708,- Removed deprecated `LightningModule.on_post_move_to_device` ([#13548](https://github.com/Lightning-AI/lightning/pull/13548)),,0
update batch size in DataModule when auto scaling batch size (#3266),0.6643236,Reset epoch progress with batch size scaler (#13846),  fix datamodule hasattr   fix patch check   fix setattr   update docs   revert patch fix   changelog   fix datamodule passed in as fit arg   docs   set datamodule batch size in lightning_setattr   fix merge   check with has_attr   access datamodule via trainer   pass fit args down to tuner   docs   fix typos in docs   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
to_torchscript method for LightningModule (#3258),0.78618205,- Fixed torchscript error with containers of LightningModules ([#14904](https://github.com/Lightning-AI/lightning/pull/14904)),"  script   docs   simple test   move test   fix doctest   no grad context   extend tests   test test   datamodule test   clean up test   docs   name   fix import   update changelog   fix import   skip pytorch 1.3 in test   update codeblock   skip bugged 1.4   typehints   doctest not working on all pytorch versions   rename TestGAN to prevent pytest interference   add note about pytorch version   fix torchscript version inconsistency in tests   reset training state + tests   update docstring   Apply suggestions from code review   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com   update docstring, dict return   add docs to index   add link   doc eval mode   forward   optional save to file path   optional   test torchscript device   test save load with file path   pep   str   Commit typing suggestion   Co-authored-by: ananthsub ananth.subramaniam@gmail.com  skip test if cuda not available  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com",1
Changed LearningRateLogger to LearningRateMonitor (#3251),0.99999976,Changed LearningRateLogger to LearningRateMonitor (#3251),  Change LearningRateLogger to LearningRateMonitor   file rename   docs   add LearningRateLogger with deprecation warning   deprecated LearningRateLogger   move deprecation check   chlog   Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai,1
use fsspec instead of gfile for all IO (#3320),0.9100961,Used fsspec instead of gfile for all IO (#3320), use fsspec instead of gfile for all IO  This better supports remote (and local) file operations with a dedicated package  Apply suggestions from code review  Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com  chlog  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai,1
Fix: gather_all_tensors cross GPUs  in DDP (#3319),0.70402443,"We made a few changes to Callbacks to test ops on detached GPU tensors to avoid CPU transfer. However, it made callbacks unpicklable which will crash DDP. ",  Fix: gather_all_tensors cross GPUs in metrics   add a test case for gather_all_tensors_ddp in #3253 ,1
Docs/improved multigpu doc clarity (#3194),0.6088968,"For even more memory savings and model sharding advice, check out stage 2 & 3 as well in our multi-GPU docs.",   clarifying doc    improved documentation clarity   docs minor correction   remove device from type_as   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
"Add conda version/downloads, discourse badges to readme (#3247)",0.4253914,A header with the version that generated the config is now included.,  add conda badge to readme   Update README.md   Update README.md   Apply suggestions from code review   make same color with pypi package   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Fix (weights only) checkpoints loading without pl (#3287),0.6954766,"if the folder the checkpoint callback uses has weights, it loads the last weights automatically.",  cast pl AttributeDict to dict   fix for omegaconf ,0
ref: moving train loop to own object 2/n (intermediate steps) (#3314),0.7390507,"train loop refactor - moving train loop to own object (#3310, #3312, #3313, #3314)",  ref: moving train loop to own object 2/n (intermediate steps)   ref: moving train loop to own object 2/n (intermediate steps) ,1
ref: moving train loop to own object 2/n (intermediate steps) (#3313),0.73757744,"train loop refactor - moving train loop to own object (#3310, #3312, #3313, #3314)",  ref: moving train loop to own object 2/n (intermediate steps)   ref: moving train loop to own object 2/n (intermediate steps) ,1
ref: moving train loop to own object (intermediate steps) (#3312),0.7888795,"train loop refactor - moving train loop to own object (#3310, #3312, #3313, #3314)",  ref: moving train loop to own object (intermediate steps)   ref: moving train loop to own object (intermediate steps)   ref: moving train loop to own object (intermediate steps)   ref: moving train loop to own object (intermediate steps) ,1
ref: train loop refactor (mid-step) (#3310),0.8347199,Refactored training loop (#2336),,1
ref: moved accelerator router (#3309),0.8960556,moved accelerator router (#3309),  ref: moved accelerator   ref: moved accelerator   ref: moved accelerator   ref: moved accelerator ,1
[metrics] Renaming of precision recall metric (#3308),0.97908276,Renaming of precision recall metric (#3308),  rename metrics   update docs ,1
ref: move prepare_data to data connector (#3307),0.9384783,move prepare_data to data connector (#3307),  ref: moved argparse code to central class   ref: moved argparse code to central class   ref: moved argparse code to central class ,1
bugfix/3185 transpose (#3252),0.43929082,"In addition, we fixed:","  change t() to transpose() as xla devices do not support .t() on 1-dim tensor   detach tensor before copying   Revert ""detach tensor before copying""   This reverts commit 37cc7bbe   changed dims   added test_result_obj_on_tpu   detach before copying   detach before copying   detach before copying   replace torch.cat with sum ",0
Added missing term 'Data' in 'LigthningModuleAPI' (#3284),0.57963884,Replaced _DataModuleWrapper with __new__ (#7289), Added missing term 'Data' in 'LigthningModuleAPI'  This could be a possible typo!  Update datamodules.rst  Co-authored-by: William Falcon waf2107@columbia.edu,0
distinguished between overfit_batches examples (#3298),0.7318797,overfit_pct in favour of overfit_batches,Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,1
Fix typo in results.rst (#3277),0.5216155,Completely overhaul the Result object in favor of ResultMetric (#7882),,0
ref: move train outside of setup training (#3297),0.91964936,move train outside of setup training (#3297),  ref: move train outside of setup training   ref: move train outside of setup training   ref: move train outside of setup training   ref: move train outside of setup training ,1
ref: run_pretrain_routine -> setup_training (#3294),0.8057599,move run_pretrain_routine -> setup_training (#3294),  ref: .tune()   ref: run_pretrain_routine -> setup_training ,1
ref: .tune() (temporary) (#3293),0.6313654,trainer.tune() now returns the tuning result (#7258),  ref: .tune()   ref: .tune()   ref: .tune()   ref: .tune()   ref: .tune()   ref: .tune() ,0
ref: modular is_overridden (#3290),0.93047494,modular is_overridden (#3290),  ref: modular is_overridden   ref: modular is_overridden   ref: modular is_overridden   ref: modular is_overridden ,1
ref: added data connector (#3285),0.5862638,group connectors (#3472),  ref: added data connector   ref: added data connector   ref: added data connector   ref: added data connector   ref: added data connector   ref: added data connector   ref: added data connector   ref: added data connector   ref: added data connector   ref: added data connector ,0
Update README.md (#3289),0.54286265,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",,0
Update docs/source/new-project.rst (#3272),0.45685703,Pinned sphinx-autodoc-typehints with <v1.15 (#11400),,0
clean docs new guide (#3270),0.6053417,Docs improvements,  updated docs   updated docs   updated docs   updated docs   updated docs ,0
updated docs (#3268),0.6353204,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",  updated docs   updated docs ,0
simplify docs (#3267),0.5568898,reduced all simplified forward (#3126),  updated docs   updated docs   updated docs   updated docs   updated docs   updated docs ,0
Simpledocs (#3265),0.48990482,Docs,  updated docs   updated docs   updated docs ,0
"Parse Union[bool, str] arguments (#3235)",0.5211751,The brittle argument parsing utilities (#16708),"  Parse Union[bool, str] arguments   Address review   Co-authored-by: William Falcon waf2107@columbia.edu",0
callback method for on_save_checkpoint (#2501),0.8071791,"-def on_save_checkpoint(self, checkpoint):",  initial draft   fix test   Update pytorch_lightning/trainer/callback_hook.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   fix tests   remove old code   untested upgrade script   document limitations   clean up and add tests   Update pytorch_lightning/trainer/training_io.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   reflect PR comments   fix formatting   Update docs/source/callbacks.rst   clarify docs   revert change for loading checkpoints   small edits   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Bug Fix: Remote Logging with Tensorboard (#3236),0.7750558,Changed the default logger to TensorBoardLogger (#609),  Changed standard open to cloud_open   Changed how version numbers are extracted to remove terminal / from paths   formatting   Co-authored-by: James Bockman james@aiml.team Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
warn user when dropping unpicklable hparams (#2874),0.5879677,Don't raise a warning when nn.Module is not saved under hparams (#12669),  refactored clean_namespace   Update try except to handle pickling error   Consolidated clean_namespace. Added is_picklable   PEP8   Change warning to use rank_zero_warn. Added Test to ensure proper hparam filtering   Updated imports   Corrected Test Case ,0
Follow up of #2892 (#3202),0.37699667,This conversation was marked as resolved by carmocca,  Follow up of #2892   typo   iterabledataset ,0
add desc for minimize (#3216),0.4573685,Refactored optimizer (#4658),,0
Fix GpuUsageLogger to work on different platforms (#3008),0.64481145,"Deprecated log_gpu_memory, gpu_metrics, and util funcs in favor of DeviceStatsMonitor callback (#9921)",  Fix GpuUsageLogger   docstrings   misconfigexception   add basic tests   skip doctest   fix parameter and docstring   rm cl   skip doctest   cleanup   chlog   add suggestions from review   add test from suggestions   fix import   fix test   fix test   fix test   fix test   rename GpuUsageLogger to GPUStatsMonitor   doc fix   Apply suggestions from code review   update docs format   update docs   miss   merge   fix title formatting   unindent   punctuation   simplify if statements   fix test   suggestions   pep   Update CHANGELOG.md   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   fix on_train_batch_*   use AttributeDict   usage   rank zero   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   import   minor changes   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Adrian Wälchli adrian.waelchli@inf.unibe.ch Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
update core contributors (#3224),0.516491,"If we forgot someone due to not matching the commit email with the GitHub account, let us know :]",,0
tests to ensure correct dataloader calls (#3221),0.6279365,"Accessing dataloaders (#16726, #16800)",  tests to ensure correct dataloading interval and sequence   tests to ensure correct dataloading interval and sequence   tests to ensure correct dataloading interval and sequence   tests to ensure correct dataloading interval and sequence   tests to ensure correct dataloading interval and sequence ,0
DP device fix (#3196),0.5750407,"device parser (#3400, #3405)",,0
Fix potential typo in early stopping monitor keys  (#3213),0.6016564,Deprecated default value of monitor argument in EarlyStopping callback to enforce monitor as a required argument (#7907),  Fix typo   ref: group prepare data hook (6) (#3212)   group prepare data hook   group prepare data hook   group prepare data hook   group prepare data hook   group prepare data hook   group prepare data hook   group prepare data hook   Fix typo   Co-authored-by: William Falcon waf2107@columbia.edu,0
ref: group prepare data hook (6) (#3212),0.9004561,group prepare data hook (#3212),  group prepare data hook   group prepare data hook   group prepare data hook   group prepare data hook   group prepare data hook   group prepare data hook   group prepare data hook ,1
group fit data links,0.39928246,group connectors (#3472),,0
reduced accelerator selection (#3211),1.0000001,reduced accelerator selection (#3211),,1
ddp backend refactor (#3210),0.885478,"refactor DDP backend (#3204, #3207, #3208, #3209, #3210)",,1
ddp backend refactor (#3209),0.8827545,"refactor DDP backend (#3204, #3207, #3208, #3209, #3210)",,1
ref: ddp backend refactor (3) (#3208),0.7960349,"refactor DDP backend (#3204, #3207, #3208, #3209, #3210)",  ddp backend refactor   ddp backend refactor ,1
ddp backend refactor (#3207),0.88473296,"refactor DDP backend (#3204, #3207, #3208, #3209, #3210)",,1
ddp backend refactor (#3204),0.88716507,"refactor DDP backend (#3204, #3207, #3208, #3209, #3210)",,1
ref: ddps train hooks (#3203),0.87995183,DDPs train hooks (#3203),  ddps train   ddps train ,1
Update index.rst (#3201),0.3891738,Changed epoch indexing from 1 instead of 0 (#2206),,0
acceleartor fit 1 (#3200),0.47097906,move specific accelerator code (#3457),,0
ref: .fit hook clean up (#3198),0.72730136,Trainer.fit hook clean up (#3198),  eval loop clean up   eval loop clean up   eval loop clean up   eval loop clean up ,1
ref: remove _evaluate fx (#3197),0.94085234,remove _evaluate fx (#3197),  remove _evaluate   remove _evaluate   remove _evaluate   remove _evaluate   remove _evaluate   remove _evaluate   remove _evaluate   remove _evaluate ,1
fix ONNX model save on GPU (#3145),0.4839171,refactored GPU backend __step (#3120),  added to(device)   added test   fix test on gpu   Update pytorch_lightning/core/lightning.py   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com  Update pytorch_lightning/core/lightning.py  Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com  remove multi gpu check  Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com   updated message   Update pytorch_lightning/core/lightning.py   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com   updated test   onxx to onnx   Update pytorch_lightning/core/lightning.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update tests/models/test_onnx.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  add no grad  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   add isinstance back   chlog   error is input_sample is not Tensor   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai,0
ref: moved hooks around in eval loop (#3195),0.95626295,moved hooks around in eval loop (#3195),  moved hooks around in eval loop   moved hooks around in eval loop   moved hooks around in eval loop   moved hooks around in eval loop ,1
Fix RMSLE metric (#3188),0.60377634,Updated SSIM metric (#4566)(#4656),  fix rmsle   Updated test to match rmsle fix   Updated RMSLE example result to match functional   chlog   add randomized test   fix pep8   Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai Co-authored-by: Nicki Skafte skaftenicki@gmail.com,0
Fixed example implementation of AutoEncoder. (#3190),0.4762286,"Changed the order of backward, step, zero_grad to zero_grad, backward, step (#6147)","The previous implementation trained a auto encoder and evaluated classificator. I try to fix this by replacing the evaluation metric with an auto encoder metric. Hence, no classification is done. I'm not 100% sure what the original authors intent was, since he extends a classification model (LitMNIST) but does not use it. The following model is an AutoEncoder and does not do any classification.  Small textual changes. forward() now implements encoding and not decoding (as it was described  in the text.) _shared_eval uses MSE loss instead of class loss, since no  classification weights are learned. initialized MSE in init, since calling MSE directly is not  supported.",0
New modular metric interface (#2528),0.52527404,"Add deprecated metric utility functions back to functional (#5067, #5068)",  new base structure   missing packages   updated interface   revert some changes   fixes   add changelog   fix bug   added description   test for pickable   fixing test   fixing test   fix pickle issue   reduceop typehints back   remove redundant module arg   add save/load test   add aggregate method   text clarification   fix doctest   Apply suggestions from code review   change test to results obj   fix docs   formatting   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   formatting   pep   Update CHANGELOG.md   suggestions   fix tests   fix pep8   fix tests   Co-authored-by: Nicki Skafte nugginea@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
Update training_tricks.py (#3151),0.68170387,Deprecated the Trainer.train_loop property in favor of Trainer.fit_loop (#8025),  Update training_tricks.py   pep   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,0
Fix typo (#3174),0.5056387,"In addition, we fixed:",,0
ref: restore on_eval_start hook (#3183),0.5532111,Renamed reset_on_epoch to reset_on_run (#9658), restore eval loop hook,0
remove on_perf check hooks (#3178),0.5889921,moved ___step_end hooks (#3130),,0
ref: remove on_eval_start hook (#3176),0.6104373,clean up hooks in run_evaluation (#3156),  remove on_eval_start hook   remove on_eval_start hook ,0
Set pep8speaks' max-line-length to 120 (same as black) (#3173),0.39761072,    self.truncated_bptt_steps = 10,,0
expand eval loop out (#3165),1.0,expand eval loop out (#3165),,1
ref: clean up data reset (#3161),0.8743407,clean up data reset (#3161),  clean up data reset   clean up data reset ,1
GANs in pl-examples updated for lightning-0.9 (#3152),0.53517705,  * `pl.utilities.seed` ([#16422](https://github.com/Lightning-AI/lightning/pull/16422)),  gan updated for lightning-0.9   bugs fixed ,0
Added missing docs on[ TrainResult and EvalResult source docs. (#3157),0.7820319,Removed support for EvalResult and TrainResult (#3968),  Added missing parameter 'minimize' docs in TrainResult   Added missing docs for parameters in TrainResult and EvalResult ,1
ref: clean up hooks in run_evaluation (#3156),0.9426512,clean up hooks in run_evaluation (#3156),  clean up hooks in run_evaluation   clean up hooks in run_evaluation   clean up hooks in run_evaluation   clean up hooks in run_evaluation   clean up hooks in run_evaluation   clean up hooks in run_evaluation   clean up hooks in run_evaluation ,1
ref: final inner eval loop hooks (#3154),0.89453423,final inner eval loop hooks (#3154),  final inner eval loop hooks   final inner eval loop hooks ,1
README.md typo correction (#3147),0.5128881,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",,0
ref: refactored inner eval loop (#3141),0.9312258,refactored inner eval loop (#3141),  refactored dataloader process hook   refactored dataloader process hook   refactored dataloader process hook ,1
refactored dataloader process hook (#3139),0.99999976,refactored dataloader process hook (#3139),,1
ref: add eval loop object to streamline eval loop (#3138),0.95638067,add eval loop object to streamline eval loop (#3138),  added eval loop   added eval loop   added eval loop   added eval loop   added eval loop   added eval loop ,1
eval step scaling factor (#3136),1.0000004,eval step scaling factor (#3136),,1
training amp scaling refactor (#3135),1.0000001,training AMP scaling refactor (#3135),,1
training forward refactor (#3134),0.9999998,training forward refactor (#3134),,1
ref: moved ___step_end hooks (#3130),0.94012487,moved ___step_end hooks (#3130),  moved eval hooks   moved eval hooks   moved eval hooks   moved eval hooks   moved eval hooks   moved eval hooks   moved eval hooks ,1
ref: refactor eval loop to use hooks. use test_mode for if so we can split later (#3129),0.9781344,refactor eval loop to use hooks - use test_mode for if so we can split later (#3129),  moved eval hooks   moved eval hooks   moved eval hooks   moved eval hooks ,1
ref: added hook base method (#3127),0.65060097,We removed some Callback hooks that were ambiguous to use Removed deprecated callback hooks (#14834):,  added hook base method   added hook base method ,0
ref: reduced all simplified_forward (#3126),0.86658293,reduced all simplified forward (#3126),  simplified training_forward   simplified training_forward   simplified training_forward ,1
ref: remove obscure forward call in eval + CPU backend ___step (#3123),0.9701443,remove obscure forward call in eval + CPU backend ___step (#3123),  remove obscure forward call in eval   remove obscure forward call in eval   remove obscure forward call in eval   remove obscure forward call in eval   remove obscure forward call in eval   remove obscure forward call in eval ,1
refactored horovod backend (#3122),0.9837986,"refactored Horovod backend (#3121, #3122)",,1
Make trainer.state a read-only property (#3109),0.6309313,The Trainer now raises an error if it is given multiple stateful callbacks of the same time with colliding state keys (#15634),  Make trainer.state a read-only property   Update states.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
ref: refactored horovod backend (#3121),0.93953323,"refactored Horovod backend (#3121, #3122)",  refactored horovod backend   refactored horovod backend ,1
ref: refactored gpu backend __step (#3120),0.9368494,refactored GPU backend __step (#3120),  refactored gpu backend __step   refactored gpu backend __step   refactored gpu backend __step   refactored gpu backend __step ,1
refactored ddp backend forward (#3119),0.99999976,refactored DDP backend forward (#3119),,1
Refactor 1: moved tpu xxx_step to backend (#3118),0.8705618,moved TPU xxx_step to backend (#3118),  moved tpu training_step   refactored eval step   refactored eval step   refactored eval step ,1
drop packaging (#3105),0.33989406,decoupled DDP2 (#3816),,0
Fix tpu cleanup (#3056),0.63707215,Resolve TPU miss rendezvous (#6781),  Only try to delete jobs if there are any to delete.   Reorder jobs.   Remove cleanup from the jobs that run on every commit. ,0
Update README.md (#3106),0.5310103,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",Automating,0
Fix an import deprecation warning (#3110),0.74295294,Deprecation warning (#3844),,1
fix tb hparams logging (#2974),0.58202124,Allow logging of metrics together with hparams (#1630), log_hyperparams add default metric  also adds scalar support   fix typos and style   another typo   keep original logging implementation   remove missed line   fix capitalization   add step to leg_metrics for tests   disable hp metric none (-1) logging   to pass tests   initial arg implementation   add step to log_metrics   add hp_metric case to log test   add docs    and minor formatting   fix broken else   pep8 style   edit tests   Update pytorch_lightning/loggers/tensorboard.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update pytorch_lightning/loggers/tensorboard.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
Fix log_graph in TensorBoardLogger (#3092),0.83302677,Changed the default logger to TensorBoardLogger (#609),,1
Fix warning in ModelCheckpoint (#3094),0.7708591,"Removed deprecated ModelCheckpoint arguments prefix, mode=""auto"" (#6162)",,1
fix(docs): test_dataloader use transformed dataset (#3090),0.6951979,Removed test_dataloaders parameter from Trainer.fit() (#1434),  fix(docs): test_dataloader use transformed dataset   suggestion   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
Fix num_sanity_val_steps is clipped to limit_val_batches (#2917),0.6366211, - Sanity checking: `Trainer(num_sanity_val_steps>0)`,  Fix num_sanity_val_steps according to limit_val_steps   fix test   add num_sanity_batches   pep   update docstring in test   add more test   chlog   update comments and docstring in test   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Adrian Wälchli adrian.waelchli@inf.unibe.ch Co-authored-by: Ananya Harsh Jha ananya@pytorchlightning.ai,0
changelogs clean (#3082),0.62724227,Full Changelog,  clean   ver ,0
0.9.0 (#3081),0.8046401,0.4.0,,1
fix for multiple types in trainer add_argparse_args (#3077),0.8862914,Trainer.add_argparse_args classmethod fixed. Now it adds a type for the arguments (#1147),  fix   fix   fix   fix   temp   fix   0.9.0 readme   0.9.0 readme   0.9.0 readme   Co-authored-by: William Falcon waf2107@columbia.edu,1
Readme changes (#3078),0.6222288,This section outlines notable changes that are not backward compatible with previous versions. The full list of changes and removals can be found in the Full Changelog below.,  Readme changes   Update README.md   Update README.md   0.9.0 readme   0.9.0 readme   Co-authored-by: William Falcon waf2107@columbia.edu,0
0.9.0 readme (#3075),0.59039724,[1.7.1] - 2022-08-09,  0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme ,0
changelog for release 0.9 (#2998),0.6971991,This section outlines notable changes that are not backward compatible with previous versions. The full list of changes and removals can be found in the Full Changelog below.,  miss   fix wrong changelog entry   miss   update changelog   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
remove last of bad result obj warning (#3073),0.5925011,fix result obj DP auto reduce (#3013),  fixed bad warn for result obj   fixed bad warn for result obj ,0
fixed bad warn for result obj (#3072),0.5733459,fix result obj DP auto reduce (#3013),,0
Change structure of walkthrough (#2949),0.46375108,Renames model steps (#1051),  deconflict   fix links   deconflict   deconflict   deconflict   deconflict   deconflict   deconflict   deconflict   deconflict   deconflict   deconflict   deconflict   deconflict   Co-authored-by: William Falcon waf2107@columbia.edu,0
Add transfer_batch_to_device hook to DataModule (#3038),0.66067743,"With DPStrategy, the batch is not explicitly moved to the device (#11780)",  :sparkles: add dm to_device logic in trainer   :fire: remove unnecessary comment   :sparkles: add to_device logic to datamodule   :white_check_mark: add test   updated docs   Co-authored-by: William Falcon waf2107@columbia.edu,0
flake8 fixes (#3064),0.46680152,"At last, lots of bug fixes (see below).",  flake8 fixes   fix pep8   fix pep8   Co-authored-by: William Falcon waf2107@columbia.edu,0
Make sure all kwargs come after args in _load_model_state() (#3063),0.5959156,Users who relied ontrainer.test(ckpt_path=None)to load the latest model need to change their code totrainer.test(model)` and pass the model reference directly.,,0
More robust way of collecting init argument names for LightningModules (#3066),0.5764447,Allow passing hparams as a keyword argument to LightningModule when loading from checkpoint (#1639),"When a LightningModule inherits from a class that implements __new__() such as typing.Generic, inspect.signature(cls) short-circuits and returns the signature of __new__() instead of __init__(). So, we need to be more specific and call inspection directly on the init function.",0
make progress bar match internal epoch counter (#3061),0.83422405,Changed progress bar epoch counting to start from 0 (#3061),"  fix 3018, 3032   changed progress bar for 3032 ",1
Add checkpoint saving on graceful shutdown (ctr+c) (#3067),0.6491457,    # put all logic related to saving a checkpoint here,,0
added copyright notices (#3062),0.36076567,Made type hints public (#17100),,0
Create 3 steps to Lightning guide to replace quick-start (#3055),0.43803185,We've also made it easier to replace Lightning's loops with your own. For example:,  Update new-project.rst   Update new-project.rst   Create 3_steps.rst   revert   remove the callbacks vid   fix blank line   change ref   spelling   spelling   Update docs/source/new-project.rst   Co-authored-by: Nathan Raw nxr9266@g.rit.edu   spelling   spelling   spelling   spelling   spelling   spelling   spelling   Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: Nathan Raw nxr9266@g.rit.edu,0
re-enabled naming metrics in ckpt name (#3060),1.0,Re-enabled naming metrics in ckpt name (#3060),  re-enabled naming metrics in ckpt name   re-enabled naming metrics in ckpt name   re-enabled naming metrics in ckpt name   re-enabled naming metrics in ckpt name   re-enabled naming metrics in ckpt name   re-enabled naming metrics in ckpt name ,1
Feature/log computational graph (#3003),0.62745124,Auto log the computational graph for loggers that support this (#3003),  add methods   log in trainer   add tests   changelog   fix tests   fix tests   fix tests   fix tests   fix tests   fix tests   fix tests   text   added argument   update tests   fix styling   improve testing ,0
fix setting batch_size attribute in batch_size finder (finishing PR #2523) (#3043),0.7460808,"    batch_size=32,",  lightning attr fix   revert refactor   create test   separate test   changelog update   tests   revert   Update pytorch_lightning/trainer/training_tricks.py   Co-authored-by: William Falcon waf2107@columbia.edu,1
[docs] Add copy and toggle buttons to sphinx (#3054),0.48172793,Pinned sphinx-autodoc-typehints with <v1.15 (#11400),  Support sphinx toggle and copy buttons   add buttons to conf ,0
Retrieve last logged val from result by key (#3049),1.0,Retrieve last logged val from result by key (#3049),  return last logged value   Update test_results.py   Update step_result.py   Update step_result.py   pep8   pep8 ,1
fix auto scale batch size not working with precision=16 (#3045),0.643012,Reset epoch progress with batch size scaler (#13846),  add test   test   test   add fix   changelog   check batch size changed ,0
Fix result gathering with varying tensor shapes (#3020),0.7009226,Auto convert tensors to contiguous format when gather_all (#4907),  test for gethering results   fix gather   document tests   changelog   assert dtype   default to concat   additional test ,1
set device to root gpu (#3042),0.54115725,"Deprecated the use of Trainer(gpus=""i"") (string) for selecting the i-th GPU; from v1.5 this will set the number of GPUs instead of the index (#6388)",,0
Fix typo in Quick Start/Step-by-step walk-through (#3007),0.5294954,"In addition, we fixed:","  Fix typo in Quick Start/Step-by-step walk-through   Fix typo in Quick Start/Step-by-step walk-through   Fix snippets in lightning module   Remove testblock    doctest does not have torch with CUDA, so x.cuda() will fail  Remove test code  ""..."" is not python, so doctests fail   Fix #3005   Fix indentation, stage in docs   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Teddy Koker teddy.koker@gmail.com Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com",0
Freeze Tensorboard to 2.2.0 (#3039),0.69341904,Switch from tensorboard to tensorboardx in TensorBoardLogger (#15728),  freeze tb to 2.2.0   Update environment.yml ,0
Update datamodules.rst (#3026),0.5555765,Replaced _DataModuleWrapper with __new__ (#7289),Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,0
quick start docs changes (#3028),0.5612251,Docs improvements,  updated code example   added warning when changing monitor and using results obj   added warning when changing monitor and using results obj   added warning when changing monitor and using results obj   added warning when changing monitor and using results obj   added warning when changing monitor and using results obj   added warning when changing monitor and using results obj   added warning when changing monitor and using results obj   added warning when changing monitor and using results obj   added warning when changing monitor and using results obj   added warning when changing monitor and using results obj   added warning when changing monitor and using results obj   added warning when changing monitor and using results obj ,0
fix result obj dp auto reduce (#3013),1.0000001,fix result obj DP auto reduce (#3013),  fix result for dp   fix result for dp   fix result for dp   fix result for dp   fix result for dp   fix result for dp   fix result for dp   fix result for dp   fix result for dp   fix result for dp   fix result for dp   fix result for dp   added warning when changing monitor and using results obj ,1
added warning when changing monitor and using results obj (#3014),0.6556676,Removed a redundant warning with ModelCheckpoint(monitor=None) callback (#9875),  added warning when changing monitor and using results obj   added warning when changing monitor and using results obj   added warning when changing monitor and using results obj ,0
Use tensorboard 2.3.0 (#3011),0.77637637,Switch from tensorboard to tensorboardx in TensorBoardLogger (#15728),,1
added lr scheduler test using dev debugger (#3004),0.6712836,lr_scheduler now activated after epoch    ,  added lr scheduler test using dev debugger   added lr scheduler test using dev debugger   added lr scheduler test using dev debugger ,0
ddp fix for trainer.test() + add basic ddp tests (#2997),0.6917826,trainer.test(),  add ddp script variations   add ddp test   rename   shell   test   test   try call   try without subprocess   test   display the error   list all variations   try string   try copy env   debug   pythonpath   path   update test   change   simple ddp test   replace   remove random port   random port   str   clean up   check run spawn   clean up   docs   docs   update test   docs   changelog   changelog ,0
updated docs (#2999),0.6136158,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",  updated docs   updated docs ,0
task docs harness (#2996),0.44960684,Reset metrics before each task starts (#9410),  updated docs   updated docs ,0
Merge branch 'master' of https://github.com/PyTorchLightning/pytorch-lightning,0.6895777,Full commit list: https://github.com/PyTorchLightning/pytorch-lightning/compare/1.5.0...1.6.0,,0
tasks docs,0.47781852,Docs improvements,,0
updated docs,0.675487,Docs improvements,,0
updated docs (#2995),0.6302233,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",,0
missing (#2990),0.43490976,Renamed several Trainer atributes:  (#567),,0
removed callback metrics from test results obj (#2994),1.0,Removed callback metrics from test results obj (#2994),  removed callback metrics from test results obj   removed callback metrics from test results obj ,1
re-trigger build (#2988),0.45818812,Show a message when BuildConfig(requirements=[...]) is passed but a requirements.txt file is already present in the Work (#15799),  fixed build   fixed build ,0
build XLA with py3.6 (#2863),0.50927395,"Following Python's end-of-life, support for Python 3.6 has been removed.",  build py3.6   info   conda   update   version   version   builds   builds   builds   builds   builds ,0
Fix accumulate_grad_batches for last batch (#2853),0.85680205,The default value of accumulate_grad_batches has changed from 1 to None (#9652).,"  first attempt   update changelog   fix pep8 and tests   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   added new tests   fixed tests   Apply suggestions from code review   used num_training_batches   fixed pep8   fixed with is_last_batch suggested by @awaelchli   fixed with num_training_batches   fixed with num_training_batches   cleanup   fix test and update docs   fixed for alignment, update docs   minor changes   update doc   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com",1
fix docs (#2987),0.5582143,Docs improvements,,0
Fixes #2972 #2946 (#2986),0.57189655,"In addition, we fixed:",  add val step arg to metrics   add val step arg to metrics   add val step arg to metrics   add val step arg to metrics   add val step arg to metrics   add val step arg to metrics   add val step arg to metrics   add val step arg to metrics   add val step arg to metrics   add step metrics   add step metrics ,0
fix tb version (#2985),0.4830686,Renamed TPUHalfPrecisionPlugin to TPUBf16PrecisionPlugin (#10026),,0
Save test predictions on multiple GPUs (#2926),0.63385594,"where you can use the outputs of training_step (performed on each GPU with a portion of the batch),", Save test predictions on multiple GPUs,0
add linked badge (#2983),0.33812398,try:,,0
Fixes #2407 (#2981),0.5772361,"In addition, we fixed:", fix gpus index error,0
add docker badge (#2980),0.39250618,"Show a message when BuildConfig(dockerfile=""..."") is passed but a Dockerfile file is already present in the Work (#15799)",,0
nb. devices (#2973),0.66688097,    devices=1,,0
Fixes #2942 (#2969),0.59064925,"In addition, we fixed:",  Fixes #2942   doc fix ,0
Fixes #2943 (#2970),0.58179504,"In addition, we fixed:",,0
autoplay (#2968),0.38253716,try:,,0
Add docs for GpuUsageLogger (#2945),0.67671293,"Deprecated log_gpu_memory, gpu_metrics, and util funcs in favor of DeviceStatsMonitor callback (#9921)",  add docs   fix spelling ,0
Bugfix/2956 tpu distrib backend fix (#2959),0.6259094,moved TPU xxx_step to backend (#3118),  override dist backend when using tpus   added test   updated doc string   drop redundant info...   more redundant info   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: rohitgr7 rohitgr1998@gmail.com,0
Add labels to sphinx docs (#2964),0.4940318,Pinned sphinx-autodoc-typehints with <v1.15 (#11400),  Add label   add ref   add ref   add ref   add label   add label   add label   add label   Update fast_training.rst   label   label   label   label   label   label   label   label   label   label   label   Update performance.rst   Update production_inference.rst   Update profiler.rst   Update results.rst   Update sequences.rst   Update single_gpu.rst   Update slurm.rst   Update test_set.rst   Update tpu.rst   Update trainer.rst   Update training_tricks.rst   Update transfer_learning.rst   Update weights_loading.rst   Update governance.rst   Update hooks.rst   Update bolts.rst   Update child_modules.rst   Update hyperparameters.rst   Update transfer_learning.rst ,0
pep 8 (#2967),0.48055056,Removed deprecated: (#2760),,0
Replace docs gifs with videos snippets so user can play at own speed (#2966),0.29779097,Docs improvements, update docs,0
update PR template (#2965),0.42063022,Syntax changes are: ,  template   typo   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
fix(docs): docstring for amp_backend (#2960),0.66003025,Deprecated the Trainer(amp_backend=...) argument,  fix(docs): docstring for amp_backend   fix(docs): early_stop_checkpoint -> early_stop_callback   docs   Co-authored-by: ananyahjha93 ananya@pytorchlightning.ai,0
Fix enforce_datamodule_dataloader_override() for iterable datasets (#2957),0.6032049,Replaced _DataModuleWrapper with __new__ (#7289),"This function has the if statement if (train_dataloader or val_dataloaders) and datamodule:. The issue is similar to that in https://github.com/PyTorchLightning/pytorch-lightning/pull/1560. The problem is that the if(dl) translates to if(bool(dl)), but there's no dataloader.bool so bool() uses dataloader.len > 0. But... dataloader.len uses IterableDataset.len for IterableDatasets for which len is undefined. The fix is also the same, the if dl should be replaced by if dl is not None. Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com",0
Added strict=False for load_from_checkpoint (#2819),0.74517715,"def load_checkpoint(self, path):","  Added strict=False and hparams_file accepcts dict   Apply suggestions from code review   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com   Type check fix   Added tests   Linting & test fix   Removed redundant code & test   Added strict=False and hparams_file accepcts dict   Apply suggestions from code review   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com   Type check fix   Added tests   Linting & test fix   Removed redundant code & test   Apply suggestions from code review   tests   tests   chlog   Update tests/models/test_restore.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   update test comments   Added docstring for the strict attribute   Added supplementary tests   Update saving.py   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  pep8, removed extra func  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: ananyahjha93 ananya@pytorchlightning.ai",1
track batch size (#2954),0.81883013,Batch Size Finder (#11089),,1
track batch size (#2950),0.8208139,Batch Size Finder (#11089),,1
add apex test (#2921),0.527321,- Code coverage (99%),  add apex test   rename   level   events   wrap   evt   miss   apex   apex   apex   apex   apex   apex   Update tests/models/test_amp.py   Co-authored-by: William Falcon waf2107@columbia.edu   notes   notes   Co-authored-by: William Falcon waf2107@columbia.edu,0
Resultd (#2947),0.46346992,@a-gardner1 @awaelchli @carmocca @justusschock @Raahul-Singh @rohitgr7 @SeanNaren @tchaton , updated docs,0
Clean save (#2933),0.5487551,Moved save_function to accelerator (#6689),  thr deterministic=True   clean   clean   Apply suggestions from code review   Co-authored-by: Vadym Stupakov vadim.stupakov@gmail.com  Apply suggestions from code review  Co-authored-by: Vadym Stupakov vadim.stupakov@gmail.com,0
deterministic=True (#2944),0.66721886,Determinism,,0
Add magicleap/atlas to community examples (#2937),0.39742765,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",,0
GPU Usage Logger (#2932),0.7222592,Log just the GPU stats,  GPU utilisation Callback   GPU utilisation Callback   Fixing style   Fixing style   Fixing CodeFactor: partial executable path   Fix a misspelling in the Class name ,1
update changelogs,0.7945262,Full Changelog,,1
Fix hparams loading for model that accepts *args (#2911),0.6184634,Freezed models hparams as Namespace property (#1029),  fix hparams loading for model that accepts *args   add test case   changelog   pep   fix test   Co-authored-by: William Falcon waf2107@columbia.edu,0
Support **DictConfig hparam serialization (#2519),0.9810804,Support **DictConfig for hparam serialization (#2519), change to OmegaConf API  Co-authored-by: Omry Yadan omry@fb.com   Swapped Container for OmegaConf sentinel; Limited ds copying   Add Namespace check.   Container removed. Pass local tests.   Co-authored-by: Omry Yadan omry@fb.com,1
add weighted average to results obj (#2930),0.94394904,weighted average in results obj (#2930), track batch size in result obj,1
Update CONTRIBUTING.md (#2927),0.4474119,"If we forgot someone due to not matching the commit email with the GitHub account, let us know :]",  Update CONTRIBUTING.md   Update CONTRIBUTING.md   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Move optimizer creation after device placement for ddp backends. (#2904),0.8089253,Moved optimizer creation after device placement for DDP backends (#2904](https://github.com/PyTorchLightning/pytorch-lighting/pull/2904)),,1
fix checkpointing to remote file paths (#2925),0.761461,Removed deprecated checkpoint argument filepath (#5321),,1
document lightiningmodule better (#2920),0.5064213,class MyLightningModule(pl.LightningModule):, updated docs,0
Use kubectl to get logs from TPU CI instead of gcloud logging. (#2918),0.57102674,Resolved a bug where the wrong client was passed to collect cloud logs (#14684),  Use kubectl to get logs from TPU CI instead of gcloud logging.   Update Github Action to read logs from kubectl rather than gcloud logging. ,0
Do not pass non_blocking=True if it does not support this argument (#2910),0.48802185,"Removed the on_epoch guard from the ""should stop"" validation check (#7701)",  add docs   non blocking only on tensor   changelog   add test case   add test comment   update changelog   changelog chlog,0
Fix typo (#2907),0.517366,"In addition, we fixed:",Variable defined as mnist_dm but used as mnist. Change to use mnist_dm.,0
Add missing arg to docker build. (#2905),0.6339754,"Show a message when BuildConfig(dockerfile=""..."") is passed but a Dockerfile file is already present in the Work (#15799)",,0
Mapkeys (#2900),0.40058154,@alanhdu @carmocca @justusschock @tkng,  added a map dict   added a map dict ,0
fix missing return statement. Do not normalize remote paths (#2894),0.45630404,- all the file path errors with loggers (txs @awaelchli),  fix missing return statement. Do not normalize remote paths   Update pytorch_lightning/utilities/cloud_io.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   Add some documentation that we now support s3 and hdfs paths   suggestion from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,0
filter drafts (#2897),0.33789507,@dmitsf @hhsecond @tchaton @nohalon @krshrimali @pritamsoni-hsr @nmiculinic @ethanwharris @yurijmikhalevich @Felonious-Spellfire @otaj @Borda ,,0
constant root seed in reset_seed (tests) (#2895),0.6454568,seed_everything now fails when an invalid seed value is passed instead of selecting a random seed (#8787),  fix root_seed in reset_seed   seed value ,0
Minor doc fixes (#2893),0.5973819,"In addition, we fixed:",  Minor language fixes   Typo fix ,0
Update lr_logger.py (#2847),0.6383817,Re-Enable Logger's ImportErrors (#1938)," Update lr_logger.py  when logging learning_rate, we should provide different choices to log including 'step' and 'epoch'  Update lr_logger.py  add some type annotations and docstrings  Update lr_logger.py  fixed a bug where on_train_batch_start() can't be triggered, instead, we should use on_batch_start(); add interval args so that we can record learning_rates with respect to global_step or current_epoch.  Update lr_logger.py  restore _extract_lr()   suggestion   Update lr_logger.py   modify _extract_lr(), it no more need to pass interval parameter.  Update test_lr_logger.py  SkafteNicki 's suggetion   log_interval now supports None, step, epoch   change log_interval to logging_interval   Update test_lr_logger.py   Update lr_logger.py   put types check into on_train_start()   cleanup   docstring typos   minor changes from suggestions   Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai Co-authored-by: rohitgr7 rohitgr1998@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com",0
missing chlogs (#2806),0.57254094,- all the file path errors with loggers (txs @awaelchli),  missing   miss ,0
Add tracking of basic states in Trainer [wip - to-be-merged after v0.9] (#2541),0.6401414,W&B log in sync with Trainer step (#4405),"  Add initial tracking of states in Trainer.   Add INTERRUPTED state, improve tests, move state switching from callback to a trainer.   Move part of a trainer state switching to a decorator.   Add documentation.   Fix docs, rename state enum, restore state to previous on exit if None, add tests for decorator only.   Fix callback typing.   Co-authored-by: William Falcon waf2107@columbia.edu",0
Squashed commit of the following: (#2164),0.56704295,@akihironitta @awaelchli @Borda @carmocca @kaushikb11 @krshrimali @mauvilsa @otaj @pre-commit-ci @rohitgr7 @semaphore-egg @tkonopka @wayi1,commit 29fb0506cd38a15c359e369cc8bc4435916b0c78 Author: Brendan Fahy bmfahy@gmail.com Date:   Sat Aug 8 19:35:30 2020 +0000 fix checking for version for docs to build  commit 467fd640db02275972c7111af031c86bb59333e9 Author: Brendan Fahy bmfahy@gmail.com Date:   Sat Aug 8 18:56:05 2020 +0000 remove no local test  commit a7cc9f88de00feec1a5406874d05313c42bd004c Author: Brendan Fahy bmfahy@gmail.com Date:   Sat Aug 8 18:46:44 2020 +0000 fix  commit 3fdbb729da79ae9348c83410a138666bad467951 Author: Brendan Fahy bmfahy@gmail.com Date:   Sat Aug 8 18:23:30 2020 +0000 revert requirements  commit 9b8686bd83e2bc243cf329e26f1c667c6949cf67 Author: Brendan Fahy bmfahy@gmail.com Date:   Sat Aug 8 18:16:42 2020 +0000 make it a fixture  commit eec74953d24c8b25268d3b6dde3cc4affdd5cb8f Author: Brendan Fahy bmfahy@gmail.com Date:   Sat Aug 8 18:01:32 2020 +0000 fix up the testing  commit 896d94a0e60083d52c81db2a036b7f1e015cad11 Author: Brendan Fahy bmfahy@gmail.com Date:   Sat Aug 8 17:47:28 2020 +0000 fix some tests  commit 6d22bde19767bf2b71dfd44839b01efdf6888f83 Merge: 6175d4e2 6ebe0d72 Author: Brendan Fahy bmfahy@gmail.com Date:   Sat Aug 8 10:20:47 2020 +0000 Merge remote-tracking branch 'origin/master' into tb_use_gfile  commit 6175d4e26b15a43c412c26d501762cd0b570616a Author: Brendan Fahy bmfahy@gmail.com Date:   Fri Aug 7 10:16:36 2020 +0000 Use tensorboard.compat.gfile to support remote writing,0
fix reduction docstring and clean tests (#2885),0.5080389,"Refactored training_batch + tests to verify correctness (#2327, #2328)",  fix reduction docstring   Update docstring and some cleanup   miss   suggestion from code review   Co-authored-by: Ananya Harsh Jha ananya@pytorchlightning.ai Co-authored-by: Ananya Harsh Jha ananya@pytorchlightning.ai,0
Finish PR #2432: Imagenet example updates + basic testing (#2889),0.5464356,Add a notebook example to reach a quick baseline of ~94% accuracy on CIFAR10 using Resnet in Lightning (#4818),"  fix imagenet example: lr_scheduler, loader workers, batch size when ddp   Fix evaluation for imagenet example   add imagenet example test   cleanup   gpu   add imagenet example evluation test   fix test output   test is fixed in master, remove unecessary hack   CHANGE   Apply suggestions from code review   image net example   update imagenet example   update example   pep   imports   type hint   docs   obsolete arg   [wip] fix imagenet example: lr_scheduler, loader workers, batch size when ddp (#2432)   fix imagenet example: lr_scheduler, loader workers, batch size when ddp   Fix evaluation for imagenet example   add imagenet example test   cleanup   gpu   add imagenet example evluation test   fix test output   test is fixed in master, remove unecessary hack   CHANGE   Apply suggestions from code review   Co-authored-by: Jirka jirka@pytorchlightning.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   update chlog   add missing chlog   pep   pep   Co-authored-by: Ruotian Luo rluo@ttic.edu Co-authored-by: Jirka jirka@pytorchlightning.ai",0
tracks all outputs including TBPTT and multiple optimizers (#2890),1.0,Tracks all outputs including TBPTT and multiple optimizers (#2890),  pl 0.9 update   pl 0.9 update   pl 0.9 update   pl 0.9 update   pl 0.9 update   pl 0.9 update   pl 0.9 update   pl 0.9 update   pl 0.9 update   pl 0.9 update   pl 0.9 update   pl 0.9 update   pl 0.9 update   pl 0.9 update   pl 0.9 update   pl 0.9 update   pl 0.9 update   pl 0.9 update   pl 0.9 update   pl 0.9 update   pl 0.9 update ,1
deepcopy model state_dict in tests (#2887),0.60459584,def state_dict(self):,Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,0
Clarify what gpus=0 means in docs (#2876),0.5878166,Parsing of GPU Argument,  docs clarify what gpus=0 means   add example suggested by @ydcjeff ,0
Fix docstring (#2884),0.4200334,Docs," Fix docstring  ""mean absolute loss"" rather than ""root mean absolute loss""  minor docstring fix  Co-authored-by: rohitgr7 rohitgr1998@gmail.com",0
save last model after saving top_k when save_last=True (#2881),0.74042016,Changed defaults of save_top_k and save_last to None in ModelCheckpoint (#3680),"  save_last should be last   changelog   seed, docs   retrigger ci   compare filenames   move constants   fix test   epoch, global step   improve test ",1
hotfix on classification metrics (#2878),0.83344984,Classification metrics overhaul (#4837),  Faster classfication stats   Faster accuracy metric   minor change on cls metric   Add out-of-bound class clamping   Add more tests and minor fixes   Resolve code style warning   Update for #2781   hotfix   Update pytorch_lightning/metrics/functional/classification.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   Update about conversation   Add docstring on stat_scores_multiple_classes   Fixing #2862   Co-authored-by: Younghun Roh yhunroh@mindslab.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
allow using apex with any PT version (#2865),0.45376998,Changed default apex level to 'O2' (#2362),  wip   setup   type   name   wip   docs   imports   fix if   fix if   use_amp   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Apply suggestions from code review  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   fix tests   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   fix tests   todos   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Fix Trainer arg name in docs (#2879),0.714862,- Removed the legacy and unused `Trainer.get_deprecated_arg_names()` ([#14415](https://github.com/Lightning-AI/lightning/pull/14415)),  Fix Trainer arg name in docs   Fix a PR comment ,1
Fix docs typo (#2880),0.5093143,"In addition, we fixed:",,0
Add PL mastercalss to readme (#2873),0.4406977,Changed deprecated enable_pl_optimizer=True (#5244),  Update index.rst   add thumbanil   update readme   Update README.md   Update index.rst   image   Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,0
Fix install setup - push pypi (#2872),0.49256045,"Fixing missing packaging package in dependencies, which was affecting the only installation to a very blank system.",  fix setup install   fix setup install   :pencil: edit docs install command   Co-authored-by: nateraw nxr9266@g.rit.edu Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,0
clean imports (#2867),0.5846229,Wrapped imports for traceability (#13924),  clean imports   miss ,0
simplify tests & cleaning (#2588),0.5863558,"Cleaning (#5948, #5949, #5950)",  simplify   tmpdir   revert   clean   accel   types   test   edit test acc   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update test acc  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Update index.rst (#2870),0.38792413,Changed epoch indexing from 1 instead of 0 (#2206),,0
fix set_epoch on TPUs (#2740),0.6176354,Changed epoch indexing from 0 instead of 1 (#2289),  fix https://github.com/PyTorchLightning/pytorch-lightning/issues/2622   Update training_loop.py ,0
updated hooks (#2850),0.73993576,New Hooks,  modified hooks   modified hooks   modified hooks   modified hooks   modified hooks   modified hooks   modified hooks   modified hooks   modified hooks ,1
Add support to Tensorboard logger for OmegaConf hparams (#2846),0.70801955,We have added own custom Tensorboard logger as default logger. ," Add support to Tensorboard logger for OmegaConf hparams  Address https://github.com/PyTorchLightning/pytorch-lightning/issues/2844 We check if we can import omegaconf, and if the hparams are omegaconf instances. if so, we use OmegaConf.merge to preserve the typing, such that saving hparams to yaml actually triggers the OmegaConf branch   avalaible   chlog   test   Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai",1
do not fails all dockers (#2861),0.4905352,Remove unnecessary intermediate layers in Dockerfiles (#5697),,0
do not fails all dockers (#2860),0.48809075,Remove unnecessary intermediate layers in Dockerfiles (#5697),,0
Support limit_mode_batches (int) for infinite dataloader (#2840),0.6328212,"Refactor dataloading, supports infinite dataloader (#955)",  Support limit_mode_batches(int) for infinite dataloader   flake8   revert and update   add and update tests   pep8   chlog   Update CHANGELOG.md   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   Add suggestions by @awaelchli   docs   Apply suggestions from code review   Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk   Apply suggestions from code review   fix   max   check   add and update tests   max   check   check   check   chlog   tests   update exception message   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai,0
Update CONTRIBUTING.md (#2855),0.44954985,"If we forgot someone due to not matching the commit email with the GitHub account, let us know :]", Update CONTRIBUTING.md  Added docker option to testing section.  Apply suggestions from code review  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Use .comet.config file for CometLogger (#1913),0.9730166,Using .comet.config file for CometLogger (#1913),  Use .comet.config file or env var for API key.   Make CometLogger API key changes backwards compatible.   Fix line too long.   Add documentation about loading from ~/.comet_config.   Update required comet_ml version.   Comet logger: allow offline experiments with config file.   This adds a new argument to the logger to control the online / offline mode explicitly so that if you give an API key and a save_dir (e.g. to control where checkpoints go while having ~/.comet.config) you can specify which mode you want.   Make CometLogger API key changes backwards compatible.   Comet logger: change online argument to be offline.   For consistency with other loggers.  chlog  Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai,1
fix PR link (#2858),0.47523442,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",,0
Support returning python scalars in DP (#1935),0.50596046,Moved ignore_scalar_return_in_dp warning suppression to the DataParallelPlugin class (#7421),  Override the default gather method to support scalars   add computing average of a list   bug: change if to elif   add some tests   change style   change documentation   use apply_to_collection in DP gather   use apply_to_collection in DP gather   fix warning msg   override gather method in DP   add tests for python scalars   add python scalars to docstring   Update message   override gather method in DP   formatting   chlog   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai,0
Tests/install pkg (#2835),0.4086932,Moved accelerators and plugins to its legacy pkg (#5645),  add install matrix   nb tests   win   cfg   torch   link   Update .github/workflows/install-pkg.yml   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com   try   try   try   try   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,0
Bugfix: Lr finder and hparams compatibility (#2821),0.4936036,Fixing critical bugs in newly added hooks and hparams assignment.,  fix hparams lr finder bug   add tests for new functions   better tests   fix codefactor   fix styling   fix tests   fix codefactor   Apply suggestions from code review   modified hook   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,0
Update contributing guide (#2830),0.55201817,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",  update contributing.md   Update CONTRIBUTING.md   Update CONTRIBUTING.md   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   Update .github/CONTRIBUTING.md   suggestion from code review   minor changes   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
add wandb autoclass in logger.rst (#2854),0.6942159,Changed WandbLogger(log_model={True/'all'}) to log models as artifacts (#6231),,0
clean tests imports (#2834),0.55666876,Module imports are now catching ModuleNotFoundError instead of ImportError (#9867),,0
Setup extras (#2831),0.5744936,setup(,  :art: use package extras   :art: get extras from reqs   :art: .   :pencil: docs   :art: . ,0
add test for none checkpoint in ddp_spawn (#2845),0.66038644,Changed the default of find_unused_parameters back to True in DDP and DDP Spawn (#6438),  add test for none checkpoint in ddp_spawn   fix code style   make sure checkpoint_callback is none   Fix tests   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,0
Added basic file logger (#2721),0.5519048,Change default logger to a dedicated one (#1064),  Added basic file logger #1803   fixup! Added basic file logger #1803   fixup! Added basic file logger #1803   fixup! Added basic file logger #1803   fixup! Added basic file logger #1803   fixup! Added basic file logger #1803   csv   Apply suggestions from code review   tests   tests   tests   miss   docs   Co-authored-by: xmotli02 xmotli02@users.noreply.github.com Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Faster Accuracy metric (#2775),0.63324887,Classification metrics overhaul (#4837),  Faster classfication stats   Faster accuracy metric   minor change on cls metric   Add out-of-bound class clamping   Add more tests and minor fixes   Resolve code style warning   Update for #2781   hotfix   Update pytorch_lightning/metrics/functional/classification.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   Update about conversation   Add docstring on stat_scores_multiple_classes   Co-authored-by: Younghun Roh yhunroh@mindslab.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
add ddp sync for logging in result step (#2822),0.5553478,DDP custom implementation support (override these hooks):,  add ddp sync for logging in result step   pep8   pep8   make ddp tests run also on cpu (except windowws)   create class instance in ddp test   revert automated formatting   pep8 ,0
clarify batch hooks (#2842),0.6246675,"Removed output argument from *_batch_end hooks (#3965, #3966)",  modified hook   modified hook   modified hook   modified hook   modified hook   modified hook   modified hook   modified hook   modified hook   modified hook   modified hook   modified hook   modified hook ,0
updated sync bn (#2838),0.6636847,moves sync bn to each backend (#3925),  updated sync bn   updated sync bn   updated sync bn   updated sync bn   updated sync bn   updated sync bn   updated sync bn   updated sync bn   added ddp_spawn test   updated test   clean   clean   Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai,0
"Revert ""Support limit_mode_batches (int) for infinite dataloader"" (#2839)",0.64783394,Reset val_dataloader in tuner/batch_size_scaling (#9857)," Revert ""Support limit_mode_batches (int) for infinite dataloader (#2787)""  This reverts commit de9c9f0864418a83f295e4c87be50e12645bd83a.  Update training_tricks.py",0
another try to filter master from CircleCI jobs (#2734),0.44300482,Cleanup cluster waiting (#16054),  circleci config   Apply suggestions from code review   miss ,0
[DOCS] title clarification in Results page (#2827),0.3967012,Completely overhaul the Result object in favor of ResultMetric (#7882),  title tweak   remove changes in new-project   Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,0
save apex scaler states (#2828),0.5098659,Saves apex states automatically and restores it for a checkpoint.,,0
fix apex gradient clipping (#2829),0.7022571,def configure_gradient_clipping(,,1
Improve SSIM (#2833),0.72696173,Updated SSIM metric (#4566)(#4656),  make ssim fast   remove padding   pep8   add comments for readability   plus -> coef ,1
add support for sync_bn (#2801),0.6554168,moves sync bn to each backend (#3925),  initial commit for sync_bn   updated changelog   tests   tests   ddp tests hanging with script tests   updated trainer   updated params   test   passingtests   passing tests   passing tests   passing tests   tests   removed apex   doc   doc   doc   doc   docs   tests   tests   tests ,0
Support limit_mode_batches (int) for infinite dataloader (#2787),0.63927925,"Refactor dataloading, supports infinite dataloader (#955)",  Support limit_mode_batches(int) for infinite dataloader   flake8   revert and update   add and update tests   pep8   chlog   Update CHANGELOG.md   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   Add suggestions by @awaelchli   docs   Apply suggestions from code review   Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk   Apply suggestions from code review   fix   max   check   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai,0
Docs for auto_select_gpu (#2836),0.80917084,Support auto_select_gpus with the accelerator and devices API (#12608),  added docs   Update docs/source/multi_gpu.rst   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  testcode change to example  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,1
check if checkpoint_callback exists (#2832),0.6989888,"-def on_load_checkpoint(self, checkpoint):",check checkpoint_callback before setting best_model_path,0
Add remaning sklearn metrics (#2562),0.7611258,Sklearn metrics classes (#1327),"  added balanced accuracy   added dcg score   added mean absolute error   added mean squared error   fix   added mean squared log error   add median absolute error and r2 score   switch arguments   added mean poisson deviance   add mean gamma deviance and mean tweedie deviance   fix styling   added explained variance score   added cohen kappa score   added hamming, hinge, jaccard   fix styling   update sklearn requirement to newer version   update requirement   fix doctest   fix tests   added balanced accuracy   added dcg score   added mean absolute error   added mean squared error   fix   added mean squared log error   add median absolute error and r2 score   switch arguments   added mean poisson deviance   add mean gamma deviance and mean tweedie deviance   fix styling   added explained variance score   added cohen kappa score   added hamming, hinge, jaccard   fix styling   update sklearn requirement to newer version   fix doctest   fix tests   fix doctest   fix failing docs   fix test   trying to fix errors   Apply suggestions from code review   format   Co-authored-by: Nicki Skafte nugginea@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai",1
Support Mean in DDP Sync (#2568),0.51856637,Deprecated num_nodes and sync_batchnorm arguments in DDPPlugin and DDPSpawnPlugin (#7026),  Update converters.py   Update test_converters.py   pep8   pep8 tests   Update test_datamodules.py   Update test_converters.py   Update converters.py   Update test_datamodules.py   Update test_converters.py   Update test_converters.py   fix tests   fix ddp tests on windows   chlog   Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai,0
Fix docs typo (#2778),0.5086652,"In addition, we fixed:",,0
:bug: fix dm prepare_data call (#2811),0.5632748,"Deprecated DataModule properties: has_prepared_data, has_setup_fit, has_setup_validate, has_setup_test, has_setup_predict, has_teardown_fit, has_teardown_validate, has_teardown_test, has_teardown_predict (#7657)",,0
docs update and follow up of #2789 (#2797),0.52533084,Docs improvements,  docs update and follow up of #2789   pep8   Update trainer.py   Update trainer.py   Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,0
More clear docstring for val_check_interval (#2802),0.485689,Trainer(val_check_interval=100),  More clear docstring for val_check_interval   Update trainer.py ,0
Fix docs typo (#2803),0.50892955,"In addition, we fixed:",  Fix docs typo   Fix docs typo ,0
Fix import deprecation warning (#2800),0.73980665,Deprecation warning (#3844),,1
update GPU to PT 1.5 (#2779),0.54459786,"Deprecated the use of Trainer(gpus=""i"") (string) for selecting the i-th GPU; from v1.5 this will set the number of GPUs instead of the index (#6388)",  update gpu PT 1.6   fix docker   use PT 1.5   Update tests/install_AMP.sh   Co-authored-by: Nathan Raw nxr9266@g.rit.edu Co-authored-by: Nathan Raw nxr9266@g.rit.edu,0
Gpu idx (#2796),0.5436411,GPU training (#2704), ddp refactor,0
missing chlogs (#2672),0.5786918,- all the file path errors with loggers (txs @awaelchli),  missing   miss   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   miss   note   notes   update CI testing with pip upgrade (#2380)   try pt1.5   cpu   upgrade   tpu   user   [blocked by #2380] freeze GPU PT 1.4 (#2780)   freeze   user   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
fix shell injection vulnerability in subprocess call (#2786),0.47340938,Removed environment variable PL_EXP_VERSION from DDP subprocesses (#7403),,0
Fix false num_classes warning in metrics (#2781),0.6883404,Sklearn metrics classes (#1327), Fix num_classes warning  Put to_categorical before get_num_classes in metrics/functional/classification.py  Update classification.py  Remove whitespaces in blank line.,0
Fix shuffle for distributed sampler (#2789),0.67546535,Allow setting replace_sampler_ddp=True with a distributed sampler already added (#4273),  Fix shuffle for distributed sampler   add test   test   chlog   update test   update test   update test   assertions via callback   define callback outside for pickling   skip ddp test on windows   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
fix selecting GPUs using CUDA_VISIBLE_DEVICES (#2739),0.6868106,Support auto_select_gpus with the accelerator and devices API (#12608),  fix https://github.com/PyTorchLightning/pytorch-lightning/issues/2407   Update pytorch_lightning/trainer/distrib_data_parallel.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
callback docs (#2794),0.84241337,  callbacks:,  added logging docs   added logging docs   added logging docs   added logging docs ,1
Resultdocs (#2793),0.49742603,@akihironitta @alvitawa @awaelchli @Borda @carmocca @code-review-doctor @ethanfurman @HenryLau0220 @krshrimali @otaj ,  added logging docs   added logging docs   added logging docs   added logging docs   added logging docs   added logging docs   added logging docs   added logging docs   added logging docs   added logging docs ,0
Call DataModule hooks implicitly in trainer (#2755),0.65451574,"Related to this, we cleaned up which hooks need the dataloader_idx as an input argument. Now it's only required if you use multiple dataloaders. Don't worry, the Trainer will automatically check if it's required for you and tell you about it.",  :sparkles: call dm hooks in trainer implicitly   :white_check_mark: update tests   :pencil: remove unused stage arg from dm docs   :white_check_mark: update tests   :white_check_mark: update tests   :construction: include stage in datamodule.setup   :pencil: docs   :pencil: docs   added more dm tests   added more dm tests   :bug: call dm.setup everywhere   :fire: pickle tests now implied by accelerator tests   :art: set dm as attr of trainer   :bug: .   :construction: wip   add can prepare test   add can prepare test   verified setup in fit   fixed setup call   fixed setup call   fixed setup call   Co-authored-by: William Falcon waf2107@columbia.edu,0
update mergify (#2784),0.4856958,"Updated DeepSpeed ZeRO (#6546, #6752, #6142, #6321)",,0
update CI testing with pip upgrade (#2380),0.4790117,The point of this release is more bug fixes ahead of v 1.0.0. We now have CI tests on TPU thanks to @zcain117 from Google! :slightly_smiling_face:,  try pt1.5   cpu   upgrade   tpu   user   [blocked by #2380] freeze GPU PT 1.4 (#2780)   freeze   user ,0
test dockers & add AMP in pt-1.6 (#1584),0.42075115,"adding compute environments (#3837, [#3842)",  exist images   names   images   args   pt 1.6 dev   circleci   update   refactor   build   fix   MKL ,0
Bugfix/torchtext include lengths (#2689),0.6828569,- Removed deprecated support for old torchtext versions ([#14375](https://github.com/Lightning-AI/lightning/pull/14375)),  Test using torchtext.data.Field with include_lengths=True/False   Fix issue that Tensors in a Batch generated by torchtext with torchtext.data.Field configured as include_lengths=True   Add description for fix of issue #2688   changes to accomodate CodeFactor issues   Another attemt to make last CodeFactor issue pass (it's a false alarm)   temporarly disable test of test_grad_tracking to check if testing will pass   reenable test in test_grad_norm   Update CHANGELOG.md   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   Renamed get_torchtext_data_iterator to _get_torchtext_data_iterator as suggested by @borda   Update pytorch_lightning/utilities/apply_func.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   adding tests more specific to batch_move_data_to_device with tochtext Batch   added check that Tensors were moved to target device   removed tests using RNN models to be moved into a separate PR   fixing FLAKE8 errors that showed up after merge from master branch     modified:   tests/base/datamodules.py     modified:   tests/callbacks/test_model_checkpoint.py   parameterized test to reduce code duplication   Added check only if length tensor exist. Removed left over comments.   rearranged device parameterization and added pytest.param   Try to figure out why only one device is tested on Linux machines   Testing on CPU and GPU devices (GPU test is skip if no cuda device is available.   added test for TPU device (experimental)   Adding test parameterization for TPU test (experimental)   change import statement to limit what is imported for a TPU environment   made test work with TPU   Change to trigger CI   Change to trigger CI   uncommented TPU test to check CI   reenabling TPU test   small change to trigger CI build   small change to trigger CI build   small change to trigger CI build   adding tests/utilities/test_apply_func_torchtext.py to CI TPU test   try to make test not skipped on CI with TPU   remove testing on TPU   undo an accidental change to test_tpu.py (file should not have been touched)   small change to trigger CI build   small change to trigger CI build   Update tests/utilities/test_apply_func_torchtext.py   Revert to previous version   Apply suggestions from code review   Change to trigger CI   Co-authored-by: Thomas Schaaf tschaaf@mmm.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Thomas Schaaf tschaaf@cs.cmu.edu,0
re-enable skipped tests (#2762),0.54189044,Disabled optimizers setup during testing (#3059),  re-enable skipped   timeout ,0
Fix doc typo (#2773),0.49467748,"In addition, we fixed:",,0
conda speedup (#2546),0.5285873,"many speed improvements (how we move data, adjusted some flags & PL now adds 300ms overhead per epoch only!)",  conda speedup   cache   add pip cache   suggestion   cache   cache   req ,0
Add onnx export (#2596),0.54876006,Avoid the deprecated onnx.export(example_outputs=...) in torch 1.10 (#11116),  export model to onnx   prepare data before exporting   support for dataloaders and tensors   added tests   use example_input_array add to changelog   updated docstring   added onnx inference tests   temp commit   removed schema valid test   add onnxruntime to environment.yml   moved onnxruntime to environment.yml pip   add example in doc   add lines between code block   added PR to changelog   is file check   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  remove *  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   infer example outputs   added doctest for onnx   fix windows tests   moved eval within condition block   self.forward to self   added docs   fixed docs error   added to toctree   Update CHANGELOG.md   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
pytorch 1.6 (#2745),0.7862053,PyTorch,  pt 1.6   don't use the new zipfile serialization for now   quick flake8 fixes   remove unnecessary f   coalesce strings   remove comma   remove extra commas   Apply suggestions from code review   Co-authored-by: Peter Yu 2057325+yukw777@users.noreply.github.com   set _use_new_zipfile_serialization to False only for pytorch 1.6.0   remove unnecessary comments   flake8 fixes   use pkg_resources instead of packaging   readme   format   version   chlog   Co-authored-by: Peter Yu peter@asapp.com Co-authored-by: Peter Yu 2057325+yukw777@users.noreply.github.com,1
Horovod & py3.8 (#2764),0.641769,horovod deprecation (#16141),,0
remove deprecated in v0.9 (#2760),0.75002015,Removed deprecated: (#2760),  remove deprecated in v0.9   data_loader   import   hook   args ,1
Fix typo on tpu.rst (#2759),0.4985742,Resolve TPU miss rendezvous (#6781), Fix typo on tpu.rst  There're 3 ways :)  Update docs/source/tpu.rst  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,0
Speed up gradient clipping and allow parameters on multiple devices. (#2767),0.65181077,Gradient Clipping Customization,"The speed up is achieved by: - Moving the ""where"" out of the loop (and replacing with min for simplicity). - Replacing manual sum and pow with torch.norm. Even though this results   in unnessecary computation (computing pow(root)) this is still a lot   faster. - Preallocating the output gives a slight speed up. Note that calling .to for all parameters results in a small speed penalty (~4 ms in my case) but allows parameters on different devices. Overall this reduces the time used for gradient clipping from 206ms to 74 ms for my model (Resnet50 + few additional vars, all vars on GPU).",0
Misleading exception raised during batch scaling (#2223),0.64605963,Reset epoch progress with batch size scaler (#13846), Misleading exception raised during batch scaling  Use batch_size from model.hparams.batch_size instead of model.batch_size   Improvements considering #1896   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
limit (#2756),0.288772,- Raise an error if there are insufficient training batches when using a float value of `limit_train_batches` ([#12885](https://github.com/Lightning-AI/lightning/pull/12885)),,0
Check CI_PULL_REQUEST and set GITHUB_REF accordingly. (#2741),0.5058677,"If we forgot someone due to not matching the commit email with the GitHub account, let us know :]",,0
Add missing methods to logger collection (#2723),0.63062817,Change default logger to a dedicated one (#1064),  Add missing methods to logger collection   Update CHANGELOG.md   Fix errors after merge   Fix codefactor issues   Update CHANGELOG.md   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Fix a deprecation warning (#2746),0.91074324,Deprecation warning (#3844),,1
Fix docs typo (#2747),0.5032891,"In addition, we fixed:",,0
fix: corrected attribute in *_dataloader in datamodule (#2748),0.7281656,Replaced _DataModuleWrapper with __new__ (#7289),,1
Correct CWD for ddp subprocesses when using Hydra (#2719),0.60925776,Re-introduced fix for Hydra directory sync with multiple process (#5993),"  when hydra is enabled, set the cwd of subprocesses to the original cwd for ddp   move imports up   clean up imports ",0
truncate long version number in progress bar (#2594),0.8825468,Truncated long version numbers in progress bar (#2594),  truncate version number   add docs and example   extend docs   docs   docs   changelog   show last   Update pytorch_lightning/core/lightning.py   Update pytorch_lightning/core/lightning.py   Co-authored-by: William Falcon waf2107@columbia.edu,1
fix https://github.com/PyTorchLightning/pytorch-lightning/issues/2635 (#2738),0.8327471,- Fixed bug that forced overriding `configure_optimizers` with the CLI ([#11672](https://github.com/PyTorchLightning/pytorch-lightning/pull/11672)),,1
make the error message readable (#2729),0.47060788,Improved the error message when the LightningWork is missing the run method (#14759)," make the error message readable  make the error message readable by adding spaces, fixing a type ""his -> this"",   cleanup   Update pytorch_lightning/trainer/auto_mix_precision.py   Apply suggestions from code review   Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk",0
freeze PT 1.5 for Horovod issue (#2744),0.53240556,horovod deprecation (#16141),  freeze pt 1.5   torchtext   Apply suggestions from code review   Co-authored-by: Peter Yu 2057325+yukw777@users.noreply.github.com  timeout  Co-authored-by: Peter Yu 2057325+yukw777@users.noreply.github.com,0
Add files via upload,0.5214293,Add support to upload files to the Drive through an asynchronous upload_file endpoint (#14703),,0
tests: add default_root_dir=tmpdir (#2392),0.4954303,Return the default_root_dir as the log_dir when the logger is a LoggerCollection (#8187),  tests: add default_root_dir=tmpdir   remove duplicate tmpdir args   add missing fixture   test requires multi gpu   typo   resize   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
skip CircleCI config on master (#2732),0.3699574,Enable None model checkpoint default (#3669),  circleci config   circleci config   circleci config   circleci config ,0
quick start docs (#2731),0.5656352,Docs,  added tests   added tests   added tests   added tests   added tests   added tests   added tests   added tests   added tests   added tests   added tests   added tests   added tests   added tests   added tests   added tests   added tests   added tests   added tests   added tests   added tests   added tests   added tests   added tests   added tests   added tests   added tests   added tests ,0
fixing TPU tests (#2632),0.5998794,This means we fixed many TPU bugs we hadn’t caught before because we had no tests.,  init   rename   tpu_core_idx   idx 8   idxs   @pl_multi_process_test   assert   assert   deamon   no close   imort   msg   use_single_gpu   dataset   idx   fix idx   dataset   format   add pickable   typo   apex   typo   wip   wip   wip   wip   wip   wip   wip   wip   docs   typo   tests   tests   tests   tests   tests   tests   tests   tests   tests   tests   tests   tests   tests   tests   tests   tests   tests   docs   docs   Apply suggestions from code review   Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Apply suggestions from code review  Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk   docs   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
Fix max_batches with fast_dev_run. (#2581),0.8410558,Updated fast_dev_run to accept integer representing num_batches (#4629),"  Fix fast_dev_run to run for all val_dataloaders   fast_dev_run check   changelog   explicit   limit_batches with fast_dev_run in init   add test   whitespace and comment fix   comment and assertion   added tests   Fix fast_dev_run to run for all val_dataloaders   fast_dev_run check   changelog   explicit   limit_batches with fast_dev_run in init   add test   whitespace and comment fix   comment and assertion   added tests   added tests   added tests   added tests   update rtol   Revert ""update rtol""   This reverts commit 4320329540798c112cf45dcbd6f677993e4c6ad6.  added tests  Co-authored-by: William Falcon waf2107@columbia.edu",1
update conda packages (#2593),0.418852,  * Deprecated the `pytorch_lightning.utilities.device_parser.is_cuda_available` in favor of `lightning_lite.accelerators.cuda.is_cuda_available`,,0
Fix weights_save_path when logger is used + simplify path handling + better docs (#2681),0.63670987,- Changed checkpoints save path in the case of one logger and user-provided weights_save_path from `weights_save_path/name/version/checkpoints` to `weights_save_path/checkpoints` ([#12372](https://github.com/Lightning-AI/lightning/pull/12372)),  fix weights_save path and drop ckpt_path   add tests   unused import   update docs   changelog   pep8   fix horovod test   make backward compatible   perform same test for all loggers   fix for when logger=False and weights_save_path is set   update changelog   update docs   update tests   do not set save dir dynamically   remove duplicate test   remove duplicated tests   update tests   update tests   remove remaining ckpt_path references   move defaults to init as suggested by @Borda   test deprecation ,0
Add a GKE cleanup workflow to run once per hour. (#2682),0.5128164,Cleanup cluster waiting (#16054),  Add a GKE cleanup workflow to run once per hour.   Add fixes. Temp use workflow as triggered by commit so we can see that command works.   Add back in schedule. ,0
DM docs (#2713),0.52450264,Docs, updated docs,0
cpu backend (#2712),0.53486335,CPU stats monitoring,  cpu backend   cpu backend   cpu backend ,0
refactor 4 (#2711),0.6563083,Refactoring, refactor ddp_spawn,0
copyright (#2710),0.34361556,Renamed xxx_AVAILABLE as protected (#5082),,0
refactor 3/n (#2709),0.58502495,Refactoring,  reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator ,0
Refactor 2/n (#2708),0.6152393,Refactoring,  reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator ,0
Add Rohit (rohitgr7) to core (#2706),0.5608195,@awaelchli @Borda @carmocca @dependabot @kaushikb11 @otaj @rohitgr7,,0
refactor 1/n for v1.0.0 (#2704),0.49361303,training forward refactor (#3134),  reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator ,0
Enable val/test loop disabling + datamodule tests (#2692),0.8610015,Enabling val/test loop disabling (#2692),  :art: warn instead of error out on loaders   :bug: test misconfiguration should still fail   :construction: .   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   Co-authored-by: William Falcon waf2107@columbia.edu,1
Add Peter Yu (yukw777) to core maintainers (#2690),0.5213641,"If we forgot someone due to not matching the commit email with the GitHub account, let us know :]",,0
fix setup call while testing (#2624),0.62477475,Refactored setup_training and remove test_mode (#5388),  fix setup call while testing   changelog   drop if condition   add test to check setup call   flake8   update test to check model stage   Co-authored-by: William Falcon waf2107@columbia.edu,0
Fix logging interval (#2694),0.5708403,bug fix with logging val epoch end + monitor (#3812),,0
fix nb tests for auto-merge (#2686),0.4556743,"Refactored training_batch + tests to verify correctness (#2327, #2328)",,0
Datamodule (#2668),0.68247485,Replaced _DataModuleWrapper with __new__ (#7289),  :sparkles: Add copy of pl_bolts datamodule to lightning   :sparkles: add datamodule to necessary init files   :construction: add datamodule property to LightningModule   :construction: .   :art: Let DataModule do its own thing   :construction: add back setup and run both hooks implicitly   :construction: .   :bug: fix add_argparse_args   :lipstick: apply black formatting and isort   :pencil: docstrings   :pencil: .   :pencil: .   :bug: overwrite cls prepare_data instead of instance   :pencil: .   :white_check_mark: add some tests   Update datamodule.py   Update datamodule.py   Update datamodule.py   Co-authored-by: William Falcon waf2107@columbia.edu,0
remove duplicate tests (#2685),0.5985131,Drop duplicate metrics (#5014),  remove duplicate test   remove duplicated tests ,0
quick-fix for --gpus flag bug (#2674),0.63261634,"Deprecated log_gpu_memory, gpu_metrics, and util funcs in favor of DeviceStatsMonitor callback (#9921)",  quick-fix for --gpus flag bug   warning added   warning added   set on_gpu using data_parallel_device_ids   self.on_gpu repositioned   Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,0
Horovod: adjust base LR used by schedulers to scale with the number of workers (#2626),0.58385575,Disabled lr_scheduler.step() in manual optimization  (#6825),  Horovod: Adjust base LR used by schedulers to match that of the optimizer after scaling by number of workers   Added unit test   Removed debug statements   Updated changelog   Apply suggestions from code review   Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
metrics: add SSIM (#2671),0.812311,Updated SSIM metric (#4566)(#4656),  metrics: add SSIM   Update CHANGELOG.md   fix codefactor issue fix doctest fix doctest fix test  added test for raise Error,1
integrate with CircleCI (#2486),0.27153066,Mixed precision overhaul (#16783),  add circleCI   wip   CircleCI setup that worked on my private repo. Use a working pytorch-lightning commit   Fix the orb imports   Update circleci header comment   Try to pull the GITHUB_REF from the CI_PULL_REQUEST   Use null instead of space for 'sed'   Add TODO for codecov   Remove echo of GKE_CLUSTER since it will be redacted by CircleCI.   Try running codecov upload.   Try using codecov orb   Use pip install codecov   Use codecov orb again since it should be approved   dockers/tpu-tests/Dockerfile   action   suggestions   drop suggestion   suggestion   Co-authored-by: Jirka jirka@pytorchlightning.ai,0
support num_sanity_val_steps=-1 (#2246),0.6787594, - Sanity checking: `Trainer(num_sanity_val_steps>0)`,  support sanity_val_step=-1   fix list size   simplification   simplify   add test for num_sanity_val_steps=-1   update test   update docs   extend tests to multiple dataloaders   changelog   Update tests/trainer/test_trainer.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   improve test   refactor the sanity check decision   fix merge   Update trainer.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: William Falcon waf2107@columbia.edu,0
EvalResult support for val loop (PR 3/5) (#2651),0.74340105,"EvalResult support for train and val. loop (#2615, #2651)", add EvalResult to support to val/test loops,1
unambiguous (#2664),0.34963363,"| ""64-true""                         | ""64"", 64   |",,0
metrics: add BLEU (#2535),0.5151362,Removed deprecated metrics (#8586),"  metrics: added bleu score and test bleu   metrics: fixed type hints in bleu   bleu score moved to metrics/functional/nlp.py   refactor with torch.Tensor   Update test_sequence.py   refactor as Borda requests and nltk==3.2   locked nltk==3.3   nltk>=3.3, parametrized smooth argument for test   fix bleu_score example   added class BLEUScore metrics and test   added class BLEUScore metrics and test   update CHANGELOG   refactor with torchtext   torchtext changed to optional import   fix E501 line too long   add else: in optional import   remove pragma: no-cover   constants changed to CAPITALS   remove class in tests   List -> Sequence, conda -> pip, cast with tensor   add torchtext in test.txt   remove torchtext from test.txt   bump torchtext to 0.5.0   bump torchtext to 0.5.0   Apply suggestions from code review   ignore bleu score in doctest, renamed to nlp.py   back to implementation with torch   remove --ignore in CI test, proper reference format   apply justus comment   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",0
Fix missing docs (#2659),0.53475505,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",  dataloader_idx typo   typo   update test_step docs   missing optimizer_idx ,0
Update lightning_template model to save hparams. (#2665),0.5911501,"- Removed the deprecated `LightningModule.{on_hpc_load,on_hpc_save}` hooks in favor of the general purpose hooks `LightningModule.{on_load_checkpoint,on_save_checkpoint}` ([#14315](https://github.com/Lightning-AI/lightning/pull/14315))",,0
fix dtype/device property not getting updated in submodules (#2657),0.5864792,Moved DeviceDtypeModuleMixin and HyperparametersMixin mixin to core (#8396),  recursive dtype device apply   simplify   simple test   submodule test   rename   explicit   type hints   test for dp backend   fix test skip   rename   add ddp_spawn test   fix None index in test   try fix ddp_spawn test   changelog   move _dtype and _device to mixin   additional doctest ,0
Structured results (train loop only. val loop separate PR) (PR 2/5) (#2615),0.621698,Refactored result handling in training loop (#7506),  r   r   r   patched optimizer closure with sr   patched optimizer closure with sr   patched optimizer closure with sr   added train step structured result   added train step structured result   added train step structured result   added train step structured result   added train step structured result   added train step structured result   added train step structured result   added train step structured result   added train step structured result   added train step structured result   added train step structured result   added train step structured result   added train step structured result   added train step structured result   added train step structured result   added train step structured result   added train step structured result   added train step structured result   added train step structured result   added train step structured result   added autoreduce for train step   added auto reduce on train   added auto reduce on train   added auto reduce on train   added auto reduce on train   added auto reduce on train   added auto reduce on train   added hooks   added hooks   added hooks   added hooks   added hooks   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   cache   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   Update pytorch_lightning/callbacks/early_stopping.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/callbacks/early_stopping.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/callbacks/early_stopping.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   Update pytorch_lightning/callbacks/model_checkpoint.py   Update pytorch_lightning/core/step_result.py   finished tests for structured results on train epoch   finished tests for structured results on train epoch   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com  Apply suggestions from code review  Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com  Apply suggestions from code review  Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com   simple   finished tests for structured results on train epoch   simple   simple   revert   finished tests for structured results on train epoch   finished tests for structured results on train epoch   Update tests/base/deterministic_model.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   finished tests for structured results on train epoch   docstring typos   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   Update pytorch_lightning/core/step_result.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update pytorch_lightning/overrides/data_parallel.py  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka jirka@pytorchlightning.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,0
[docs] add ray tune tutorial link to sidebar and readme (#2648),0.47865283,You can find more documentation about the tuner here.,Co-authored-by: Kai Fricke kai@anyscale.com,0
Fix typo (#2646),0.537071,"In addition, we fixed:",,0
[docs] Add link to Ray Tune in hyperparameters.rst (#2647),0.47787482,"In this release, we focused on Tuner improvements and introduced two new callbacks that can help you customize the batch size finder and learning rate finder as per your use case.",Co-authored-by: Kai Fricke kai@anyscale.com,0
Fix local rank zero casting (#2640),0.43524492,Remove hardcoding of local rank in accelerator connector (#6878)," Fix local rank zero casting  The environment variable 'LOCAL_RANK' can be a string, causing the if rank_zero_only.rank == 0 check to fail  Update distributed.py  address comment",0
"Clarify limit_{train,val,test}_batches behaviour (#2630)",0.68992674,Disabled training when limit_train_batches=0 (#4371),,0
Fix typo of handling (#2625),0.49055296,Syntax changes are: ,,0
Add Transformers Question Answering to community examples (#2603),0.34530318,Add a notebook example to reach a quick baseline of ~94% accuracy on CIFAR10 using Resnet in Lightning (#4818), Update index.rst  Add QA Finetuning in community examples.  Update README.md,0
tests for val loop flow (#2605),1.0,tests for val loop flow (#2605),  add tests for single scalar return from training   add tests for single scalar return from training   add tests for single scalar return from training   fixing val step only   fixing val step only   fixing val step only   fixing val step only   fixing val step only   fixing val step only   fixing val step only   fixing val step only   fixing val step only   fixing val step only   fixing val step only   fixing val step only   fixing val step only   fixing val step only   fixing val step only   fixing val step only   fixing val step only   fixing val step only   fixing val step only   fixing val step only   fixing val step only   fixing val step only ,1
add simple CircleCI (#2609),0.37313873,Made type hints public (#17100),  add simple CircleCI   ignore some   note ,0
make it clear the example is under the hood (#2607),0.42912903,Simplify the PL examples structure (shallower and more readable) (#1247),,0
add tests for single scalar return from training (#2587),0.5982326,"Refactored training_batch + tests to verify correctness (#2327, #2328)",  add tests for single scalar return from training   add tests for single scalar return from training   add tests for single scalar return from training   add tests for single scalar return from training   add tests for single scalar return from training ,0
docs: add hint about register_buffer (#2577),0.4728979,warning_utils >> warnings,  register buffer hint   testcode   clarify init   code block   make it unambigous ,0
missing changelog (#2464),0.69372004,Full Changelog,  missing   miss   miss   new ,0
Typo (#2575),0.42827806,"In addition, we fixed:",  Typo   Update docs/source/performance.rst   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
0.8.5 (#2573),0.69833225,0.4.0,  remove grad scaling tpu   remove grad scaling tpu ,0
Ampt (#2572),0.43597442,        :param amp:,  remove grad scaling tpu   remove grad scaling tpu   remove grad scaling tpu   remove grad scaling tpu   remove grad scaling tpu   remove grad scaling tpu   remove grad scaling tpu   remove grad scaling tpu   remove grad scaling tpu ,0
Avoid zeros in dice and iou (#2567),0.45270476,Changed iou [func] to allow float input (#4704),  nones   fix   fix   test   test   test   fix   eps   tpu   eps   type   test tpu   Update init.py   Co-authored-by: William Falcon waf2107@columbia.edu,0
Fixes .test() for ddp (#2570),0.5730352,- fixed all the .test() calls,  enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint ,0
don't pass tpu weights back on test (#2566),0.50102174,TPU training (#2708),  enable none checkpoint   enable none checkpoint ,0
Add functional regression metrics (#2492),0.82215834,Regression metrics (#2221),  Add functional regression metrics   add functional tests   add docs   changelog   init   pep8   docs   docs   setup docs   docs   Apply suggestions from code review   Apply suggestions from code review   typo   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka jirka@pytorchlightning.ai,1
.fit() returns last not best weights in ddp_spawn (#2565),0.5172825,"Implemented {DDPShardedPlugin,DDPShardedSpawnPlugin}._setup_model_and_optimizers (#10028, #10064)",  added base tests for tpu   added base tests for tpu   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint ,0
Fix default value in documentation (#2452),0.48522612,Default: False,,0
save_dir fix for MLflowLogger + save_dir tests for others (#2502),0.6042842,MLflowLogger now uses the env variable MLFLOW_TRACKING_URI as default tracking URI (#7457),  mlflow rework   logger save_dir   folder   mlflow   simplify   fix test   add a test for file dir contents   new line   changelog   docs   Update CHANGELOG.md   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   test for comet logger   improve mlflow checkpoint test   prevent  commet logger error on pytest exit   test tensorboard save dir structure   wandb save dir test   skip test on windows   add mlflow to pickle tests   wandb   code factor   remove unused imports   remove unused setter   wandb mock   wip mock   wip mock   wandb tests with mocking   clean up   clean up   comments   include wandblogger in test   clean up   missing argument   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Start accumulate gradients schedule at epoch 0 (continued) (#2513),0.63055784,Separate the Gradient Accumulation Scheduler from Trainer (#16729),  Start accumulate gradients schedule at epoch 0   Undo change in #2375   Update test_trainer.py::test_gradient_accumulation_scheduling   Fix pep8 formatting   Remove 'Datasets/' folder   Split args for readability   Fix pep8 formatting ,0
Fix argparse default value bug (#2526),0.6730442,argparse_utils >> argparse,  Add failing test for bug   Fix bug ,0
added small eps to dice and iou to avoid NaN (#2545),0.43570822,Remove nan loss in manual optimization (#5121),,0
Fix CI crash on coverage upload timeout (#2548),0.38596416,- Code coverage (99%),  fix ci crash on codecov timeout   Update .github/workflows/tpu-testing.yml   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
remove parameterize from TPU tests (#2561),0.5152564,Changed the default of find_unused_parameters to False in DDP (#5185),  added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu ,0
Finish #2549 (#2557),0.47361284,"Cleaning (#5948, #5949, #5950)", removed spawns for test_converters and verified tests  Co-authored-by: Ananya Harsh Jha ahj265@nyu.edu Co-authored-by: zcain zcain@google.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka jirka@pytorchlightning.ai,0
Fix parameters and docs in metrics (#2473),0.7211072,Removed deprecated metrics (#8586),  Fix parameters and docs in metrics   doc improvements   whitespace   doc indentation   Apply suggestions from code review   Co-authored-by: Nicki Skafte skaftenicki@gmail.com   zero   drop defaults   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Jirka jirka@pytorchlightning.ai,1
PSNR metric (#2483),0.5716787,Updated SSIM metric (#4566)(#4656),"  Add stub PSNR metric   Fix linter   Add data range as parameter   Add tests   Add scikit-image   Add PSNR to regression metrics and add functional   Refactor to functional   Fix linter   Fix linter, again   Fix linter, again   Fix typo in test   Fix typo in another test   Add scikit-image to conda   Lift numpy requirement   Add random tests   Update CHANGELOG.md   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",0
flatten Wandb hyperparameters dict  (#2459),0.8003989,Flattening Wandb Hyperparameters (#2459),  wandb logging fix   Changelog fix   change test ,1
try remove pr (#2543),0.5151841,Renamed xxx_AVAILABLE as protected (#5082),,0
Fixed skipped horovod tests (#2514),0.5915213,horovod deprecation (#16141),  skip ckpt test on rank  > 0   fx test   add extra assert   code factor   add back removed   add old loading code   add back old   unused import   add same skip to run_model_without_loggers   test if horovod now works with python 3.8   test remove all 3.8 skips   remove spawn   fix   fix test   move load check up   fix test multigpu   rename   fix gpu mode   on gpu fix when on cpu   move ,0
Fix ddp tests + .test() (#2512),0.6870708,- fixed all the .test() calls,  added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   fix deprecation warnings   added base tests for tpu   added base tests for tpu   Update pytorch_lightning/trainer/trainer.py   Co-authored-by: Jeremy Jordan 13970565+jeremyjordan@users.noreply.github.com   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   Co-authored-by: Jirka jirka@pytorchlightning.ai Co-authored-by: Jeremy Jordan 13970565+jeremyjordan@users.noreply.github.com,0
use XLA base image for TPU testing (#2536),0.437635,TPU core selection,  drop py3.6   use base image   typo   skip extra   drop cache ,0
Docker: building XLA base image (#2494),0.49466103,Remove unnecessary intermediate layers in Dockerfiles (#5697),  refactor   add TPU base   wip   builds   typo   extras   simple   unzip   rename ,0
fix worker warning (#2504),0.51082146,"    num_workers=10,",  fix worker warning   improve tests   suggestion   Co-authored-by: Jirka jirka@pytorchlightning.ai,0
[tiny] Fix training_dataloader usage to be train_dataloader instead. (#2521),0.7931694,"Removed the deprecated Trainer.reset_train_val_dataloaders() in favor of Trainer.reset_{train,val}_dataloader (#16131)",Co-authored-by: Vijay Rajaram vrajaram3@gatech.edu,1
make loggers pickleable (#2518),0.62167466,Change default logger to a dedicated one (#1064),  state updates to logger   change log   changelog ,0
fix dtype conversion of example_input_array in model summary (#2510),0.53974175,"Deprecated DataModule properties: train_transforms, val_transforms, test_transforms, size, dims (#8851)",  fix dtype conversion   changelog ,0
add metrics to pl module (#2506),0.6242041,brand new Metrics package with built-in DDP support (by @justusschock and  @SkafteNicki),,0
Amp2 (#2505),0.5102812,        :param amp:,  fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang ,0
"Revert ""Revert ""join coverage (#2460)"" (#2499)"" (#2500)",0.43205726,Moved result teardown to the loops (#8245),This reverts commit 355918af8dcee6bb21cbdc497b7b33243fec8db3.,0
"Revert ""join coverage (#2460)"" (#2499)",0.4648553,Removed callback metrics from test results obj (#2994),This reverts commit 944ffba305418a3bb63071e1a64e051cf83c0490.,0
join coverage (#2460),0.556509,- Code coverage (99%),  join coverage   full TPU test   codecov   typo   report   docker   timeout   base   show   cd dir   req   docker   docker   docker   coverage   upload   drop main   report   report   python   upload   drone   drone   drone   drone   drone   drone   drone   drone   drone ,0
Hang (#2488),0.4943806,(#16002),  added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test ,0
DDp interpreter (#2482),0.68898475,DDP(2) backend (#2796),  interpreter   chlog ,0
Add link to TPU Pods tutorial. (#2477),0.49918443,TPU training (#2708),,0
Clean up (#2467),0.6931265,"Cleaning (#5948, #5949, #5950)",  Fixes #2455   Fixes #2455   Fixes #2455   Fixes #2455   Fixes #2455   Fixes #2455   Fixes #2455   Fixes #2455   Fixes #2455   Fixes #2455   Fixes #2455   Fixes #2455   Fixes #2455   Fixes #2455   Fixes #2455   Fixes #2455   Fixes #2455   Fixes #2455   Fixes #2455   Fixes #2455   Fixes #2455   Fixes #2455   Fixes #2455   Fixes #2455   Fixes #2455   Fixes #2455   Fixes #2455   added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test ,0
fix gpu example (#2466),0.6388513,refactored GPU backend __step (#3120),  fix gpu example   make cpu_template and gpu_template differnt   Co-authored-by: Adrian Wälchli adrian.waelchli@inf.unibe.ch,0
Fixes #2455 (#2463),0.6200373,"In addition, we fixed:",,0
removed auto val reduce (#2462),1.0000001,Removed auto val reduce (#2462),,1
Add Github Action to run TPU tests. (#2376),0.5406101,This means we fixed many TPU bugs we hadn’t caught before because we had no tests.,  Add Github Action to run TPU tests.   Trigger new Github Actions run.   Clean up more comments.   Use different fixed version of ml-testing-accelerators and update config to match.   use cluster in us-central1-a   Run 'gcloud logging read' directly without 'echo' to preserve newlines.   cat coverage.xml on the TPU VM side and upload xml on the Github Action side   Use new commit on ml-testing-accelerators so command runs fully.   Preserve newlines in the xml and use if: always() temporarily to upload codecov   Use pytorch_lightning for coverage instead of pytorch-lightning   Remove the debug cat of coverage xml   Apply suggestions from code review   jsonnet rename   name   add codecov flags   add codecov flags   codecov   codecov   revert codecov   Clean up after apt-get and remove old TODOs.   More codefactor cleanups.   drone   drone   disable codecov   cleaning   docker py versions   docker py 3.7   readme   bash   docker   freeze conda   py3.6   Stop using apt-get clean.   Dont rm pytorch-lightning   Update docker/tpu/Dockerfile   Longer timeout in the Github Action to wait for GKE to finish.   job1   job2   job3   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka jirka@pytorchlightning.ai,0
continue (#2450),0.48254573,(#16002),,0
try adding coverage (#2441),0.5666833,- Code coverage (99%),"  add coverage, test failing   fix test   badges   typo   freeze conda ",0
cleaning (#2449),0.8036107,"Cleaning (#5948, #5949, #5950)",,1
Warn user when IterableDataset has len defined (#2437),0.59330404,return iterabledataset,  add warning when getting checking len   added test   changelog   pep   do not show warning below 1.4   try version parse   comments   xfail   Update requirements/base.txt   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/trainer/data_loading.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  version  Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka jirka@pytorchlightning.ai,0
enabled no returns from eval (#2446),1.0,Enabled no returns from eval (#2446),  enabled no returns from eval   fixed docs   fixed docs   fixed docs   fixed docs   fixed docs   fixed docs   fixed docs   fixed docs   fixed docs   fixed docs   fixed docs   fixed docs ,1
Corrected typo python -m pip pre-commit install (#2447),0.50242203,Add missing python-multipart dependency (#17244),,0
missing changes in chlog (#2430),0.5206568,moved eval loop logging to loggers (#3408),  missing   miss ,0
Pure package & base tests (#2418),0.52623063,Updated app testing (#16000),  base tests   pil   wip   wip   wip   ignore   ignore   win   link   win   cpu   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
fix logging on rank 0 only (#2425),0.76835835,Removed restrictions in the Trainer that loggers can only log from rank 0; the existing logger behavior has not changed (#8608),  fix and test for ddp block logging rank > 0   rename   use the dummy logger   dummy logger test   set the logger in  model   decorator for rank zero experiment   simplify check   simplify   fix problem with None in checkpoint path   revert configure logger   unused import   offline   try rank 0 decorator in checkpoint   try fix test   imgs   add asserts to make sure log zero only saves checkpoints   add asserts to make sure log zero only saves checkpoints   add asserts to make sure log zero only saves checkpoints   add asserts to make sure log zero only saves checkpoints   add asserts to make sure log zero only saves checkpoints   fix tpu tests   fix tpu tests   Co-authored-by: William Falcon waf2107@columbia.edu,1
fix tpu tests,0.66494954,This means we fixed many TPU bugs we hadn’t caught before because we had no tests.,,0
Checking ipywidgets is installed for ensure tqdm working (#2417),0.53103983,Changed default TQDM to use tqdm.auto for prettier outputs in IPython notebooks (#752),  Adding importing ipywidgets before importing tqdm.auto to make sure ipywidgets is installed.   Updated CHANGELOG.md   Updated ipywidgets importing checks to @awaelchli comments.   Co-authored-by: William Falcon waf2107@columbia.edu,0
added reduce ddp results on eval (#2434),0.7044139,fix result obj DP auto reduce (#3013),  added reduce ddp results on eval   added reduce ddp results on eval   added reduce ddp results on eval   added reduce ddp results on eval   added reduce ddp results on eval   added reduce ddp results on eval   added reduce ddp results on eval   added reduce ddp results on eval   added reduce ddp results on eval   added reduce ddp results on eval   added reduce ddp results on eval ,1
Fix apex scaling with decoupled backward (#2433),0.4963472,Moved the gradient unscaling in NativeMixedPrecisionPlugin from pre_optimizer_step to post_backward (#9606),  fix outputs   fix outputs   fix outputs   fix outputs   fix outputs   fix outputs   fix outputs   fix outputs   fix outputs   fix outputs   fix outputs   fix outputs   fix outputs   fix outputs   fix outputs   fix outputs   fix outputs   fix outputs   fix outputs ,0
tests: drop CircleCI (#2412),0.4342375,Fully tested!,  drop CircleCI   add PT testing   fix   cpu   conda   conda   req   base   conda   conda   conda   conda   conda   conda   conda   name   req   info   tests   pt 1.6   drop 1.6   info ,0
Fixes train outputs (#2428),0.62196016,Removed deprecated TrainResult (#5323),  fix outputs   fix outputs ,0
continue (#2416),0.48243624,(#16002),,0
typo (#2415),0.44365644,Removed deprecated: (#2760),,0
release (#2414),0.47445154,This release includes:,,0
fix batch typo,0.5941723,"    batch_size=32,",,0
fix amp wrong call,0.5143217,enable_pl_optimizer=False by default to temporarily fix AMP issues (#5163),,0
missing changes (#2283),0.5312365,Noteworthy changes:,  missing   RC1   RC1   format ,0
Continue Jeremy's early stopping PR #1504 (#2391),0.38807744,Resuming from checkpoints (#16167),  add state_dict for early stopping   move best attr after monitor_op defined   improve early stopping and model checkpoint callbacks   fix formatting   fix attr init order   clean up setting of default_root_dir attr   logger needs default root dir set first   reorg trainer init   remove direct references to checkpoint callback   more fixes   more bugfixes   run callbacks at epoch end   update tests to use on epoch end   PR cleanup   address failing tests   refactor for homogeneity   fix merge conflict   separate tests   tests for early stopping bug regressions   small fixes   revert model checkpoint change   typo fix   fix tests   update train loop   cannot pass an int as default_save_path   refactor log message   fix test case   appease the linter   fix some doctests   move config to callback   fixes from rebase   fixes from rebase   chlog   docs   reformat   formatting   fix   fix   fixes from rebase   add new test for patience   Update pytorch_lightning/callbacks/model_checkpoint.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/callbacks/model_checkpoint.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update tests/callbacks/test_early_stopping.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   fix formatting   remove enable_early_stop attribute   add state_dict for early stopping   move best attr after monitor_op defined   improve early stopping and model checkpoint callbacks   fix formatting   fix attr init order   clean up setting of default_root_dir attr   logger needs default root dir set first   reorg trainer init   remove direct references to checkpoint callback   more fixes   more bugfixes   run callbacks at epoch end   update tests to use on epoch end   PR cleanup   address failing tests   refactor for homogeneity   fix merge conflict   separate tests   tests for early stopping bug regressions   small fixes   revert model checkpoint change   typo fix   fix tests   update train loop   fix test case   appease the linter   fix some doctests   move config to callback   fixes from rebase   fixes from rebase   chlog   docs   reformat   formatting   fix   fix   fixes from rebase   add new test for patience   Update pytorch_lightning/callbacks/model_checkpoint.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/callbacks/model_checkpoint.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update tests/callbacks/test_early_stopping.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   fix formatting   remove enable_early_stop attribute   fix test with new epoch indexing   fix progress bar totals   fix off by one error (see #2289) epoch starts at 0 now   added missing imports   fix hpc_save folderpath   fix formatting   fix tests   small fixes from a rebase   fix   tmpdir   tmpdir   tmpdir   wandb   fix merge conflict   add back evaluation after training   test_resume_early_stopping_from_checkpoint TODO   undo the horovod check   update changelog   remove a duplicate test from merge error   try fix dp_resume test   add the logger fix from master   try remove default_root_dir   try mocking numpy   try import numpy in docs test   fix wandb test   pep 8 fix   skip if no amp   dont mock when doctesting   install extra   fix the resume ES test   undo conf.py changes   revert remove comet pickle from test   Update CHANGELOG.md   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   Update weights_loading.rst   Update weights_loading.rst   Update weights_loading.rst   renamed flag   renamed flag   revert the None check in logger experiment name/version   add the old comments   _experiment   test chckpointing on DDP   skip the ddp test on windows   cloudpickle   renamed flag   renamed flag   parentheses for clarity   apply suggestion max epochs   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jeremy Jordan jtjordan@ncsu.edu Co-authored-by: Jirka jirka@pytorchlightning.ai Co-authored-by: Jeremy Jordan 13970565+jeremyjordan@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: William Falcon waf2107@columbia.edu,0
fix loading with hparams (#2403),0.5338294,    hparams_file='/path/to/hparams_file.yaml',  fix #2386   extra test   extra case   extra test   chlog   fix test ,0
fix when torchtext not installed (#2402),0.7478895,- Removed deprecated support for old torchtext versions ([#14375](https://github.com/Lightning-AI/lightning/pull/14375)),,1
fix loading past checpoints (#2405),0.42210084,    # put all logic related to loading a checkpoint here,  fix #2334   chlog ,0
updates teardown to account for ddp (#2389),0.58535993,DDP(2) backend (#2796),  remove warnings   remove warnings   added doc lines   added doc lines ,0
docs: dont mock imports when running sphinx doctest (#2396),0.52014256,Pinned sphinx-autodoc-typehints with <v1.15 (#11400),  skip if no amp   dont mock when doctesting   install extra ,0
move torchtext as optional (#2395),0.6188839,- Removed deprecated support for old torchtext versions ([#14375](https://github.com/Lightning-AI/lightning/pull/14375)),  torchtext   Update pytorch_lightning/utilities/apply_func.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update apply_func.py  Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
fix loading model with kwargs (#2387),0.60845965,Users who relied ontrainer.test(ckpt_path=None)to load the latest model need to change their code totrainer.test(model)` and pass the model reference directly.,  test   fix   fix ,0
Support torchtext on a single GPU (#2379),0.5992576,"Set PyTorch 1.4 as min requirements, also for testing and examples torchvision>=0.5 and torchtext>=0.5 (#5418)",  Handle torchtext.data.Batch on GPU   Update CHANGELOG.md   Apply code review requests   Correct the docs   Change requirements ,0
CI: partial move from CircleCI (#2378),0.37924895,reduced all simplified forward (#3126),  move from CircleCI   req   tex   tex   sudo   extra   recom   pic   dvipng ,0
fixes logger crash on ddp (#2388),0.7121587,DDP + loggers should be fixed,  remove warnings   remove warnings   remove warnings   remove warnings   remove warnings   remove warnings   remove warnings   remove warnings   remove warnings   remove warnings ,1
move Trains logger to Bolts (#2384),0.8703346,Moved TrainsLogger to Bolts (#2384),  move Trains logger   chlog ,1
add CLI test for examples (#2285),0.5013058,Introducing CLI commands for apps (#13602)!,  cli examples   ddp   CI   CI   req   tests   skip DDP   Co-authored-by: William Falcon waf2107@columbia.edu,0
fix docker builds (#2383),0.5534702,Remove unnecessary intermediate layers in Dockerfiles (#5697),,0
fix key typo (#2374),0.48279554,Key updates,,0
Fix ModelCheckpoint example (#2321),0.77291036,"Removed deprecated ModelCheckpoint arguments prefix, mode=""auto"" (#6162)",save_top_k should be an int and have been mentioned as save_top_k=True in the snippet provided under 'Saving and Loading Weights' docs. Changed it to its default value (1) to make it consistent. Signed-off-by: Kshitij Patil kshitijpatil98@gmail.com,1
native amp (#2373),0.48501804,Trainer now raises an exception when requesting amp_level with native amp_backend (#9755),  native amp   typo   imports   apex ,0
repair CI for Win (#2358),0.3828289,"    'path/to/checkpoint.ckpt',",  no cov   no cov   ReduceOp   group   reduce_op.sum   Update sklearns.py   formatting   horovod   Apply suggestions from code review   horovod   horovod   horovod   horovod   ci   print   ci   timeout   timeout   time   fix   distributed cpu   pipes   time   cpu   spawn   spawn   spawn   tp   separate   os   os   npm   Fix load_from_checkpoint() not working with URL on Windows   Update CHANGELOG   Update CHANGELOG.md   Co-authored-by: Peter Yu 2057325+yukw777@users.noreply.github.com   Apply suggestions from code review   fix   fix meta tags creating empty lines   pyright   node   fix httpserver address   drop tutils.default_trainer_options   imports   Better fix for load_from_checkpoint() not working with absolute path on Windows (#2294)   Fix load_from_checkpoint() not working with URL on Windows   Update CHANGELOG   Update CHANGELOG.md   Co-authored-by: Peter Yu 2057325+yukw777@users.noreply.github.com  Apply suggestions from code review  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Peter Yu 2057325+yukw777@users.noreply.github.com  drop duplicate  Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: airium airium@outlook.com Co-authored-by: Peter Yu 2057325+yukw777@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: AIRIUM 38249940+airium@users.noreply.github.com,0
fix get dataloader size (#2375),0.69119424,"Accessing dataloaders (#16726, #16800)",  get dataloader size   pyright ,0
Bugfix/_has_len (#2307),0.44248787,Truncated long version numbers in progress bar (#2594),  deal with NotImplementedError raised by torchtext   deal with NotImplementedError raised by torchtext   Added tests for dataloader which raise NotImplementedError in len()   Fixed some typos   enabled tests for dataloader raising NotImplementedError in len and corrected match string for raised exception   deleted empty line for style compliance   refactored CustomNotImplementedErrorDataloader to derive from CustomInfDataloader   enabled reduced number of not_implemented_error dataloader test to reduce runtime for continuous integration   reduced test number of not_implemented_error dataloader test further to reduce test time   reduced test number of not_implemented_error dataloader test to one to reduce test time   disabled all not_implemented_error dataloader test to see if test pass in time   added next with a reduced number (5) of elements after which CustomNotImplementedErrorDataloader stops to speedup test.   enabling all not_implemented_error dataloader test   added brief description of change and relation of torchtext   CustomNotImplementedErrorDataloader reduced number of batches served to 2.   Update CHANGELOG.md   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   Apply suggestions from code review   Update CHANGELOG.md   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Disable parallelism in dataloader  Suspect that it might cause pytest to hang more frequent   added max_steps=None to Trainer in not_implemented_error dataloader tests   rearranged not_implemented_error test in file to group them together   disabled parallel data loading Reason: testing if that stops the test framework from hanging.   Apply suggestions from code review   Apply suggestions from code review   Co-authored-by: Thomas Schaaf tschaaf@cs.cmu.edu Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
changed apex level (#2362),0.78940904,Changed default apex level to 'O2' (#2362),,1
making optimization steps for hooks (#2363),0.9433871,Made optimization steps for hooks (#2363),*simplified optimizer step and zero grad overriding,1
fix 2333 (#2360),0.48594117,"In addition, we fixed:",,0
adds tensorboard hparams logging test (#2342),0.72320926,Changed the default logger to TensorBoardLogger (#609),  fixes hparam logging   fixes hparam logging   fixes hparam logging   fixes hparam logging   fixes hparam logging   Apply suggestions from code review   skipif   rename   Update test_tensorboard.py   Update test_tensorboard.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka jirka@pytorchlightning.ai,1
swaps lr sched order (#2356),0.46525592,    --lr_scheduler=anneal_strategy=linear,  swaps lr sched order   Update optimizers.py   added amdim encoder choice ,0
[docs] add community example : pl + ms nni (#2340),0.4318826,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",https://github.com/PyTorchLightning/pytorch-lightning/issues/2329,0
remove wrong annotation (#2349),0.46584842,Removed deprecated TrainResult (#5323),,0
Python logging level docs (#2348),0.66691244,    logger.do_something(),  docs about Python logging   add link to Python logging docs ,0
corrected example usage of save_hyperparameters from List[str] to seperate str (#2353),0.7453758,Moved save_hyperparameters to its own function (#7119),Co-authored-by: David Waterworth david.waterworth@cim.io,1
Fix lost compatibility with custom datatypes implementing .to (#2335),0.4510888,Changed type checker with explicit cast of ref_model object (#4457),  generalize data transfer   added test   update docs   fix spelling error   changelog   update docs ,0
refactor training loop (#2336),0.97219926,Refactored training loop (#2336),  refactoring training epoch   refactored training epoch   refactored training epoch   refactored training epoch   refactored training epoch   refactored training epoch   fixes slurm weights saving   fixes slurm weights saving ,1
test (#2341),0.4971485,Tested pickling (#1636), fixes rank zero issue,0
fixes slurm weights saving (#2339),0.59096855,Fix an issue with the SLURM srun detection causing permission errors (#15485),,0
fix TPU parsing and TPU tests (#2094),0.52377796,This means we fixed many TPU bugs we hadn’t caught before because we had no tests.,  added tpu params test   added tests   removed xla imports   added test cases for TPU   fix pep 8 issues   refactorings and comments   add message to MisconfigurationException   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   test if device is set correctly   added TPU device check removed mark.spawn   removed device selection   remove xla_device call   readded spawn due to test failures   add TODO for tpu check   Apply suggestions from code review   Apply suggestions from code review   flake8   added tpu args to cli tests   added support for tpu_core selection via cli   fixed flake formatting   replaced default_save_path with default_root_dir   added check for data type for tpu_cores   fixed flake indent   protected   protected   added tpu params test   added tests   removed xla imports   test if device is set correctly   added support for tpu_core selection via cli   replaced default_save_path with default_root_dir   added check for data type for tpu_cores   chlog   fixed tpu cores error   rebased with latest changes   flake fix   Update pytorch_lightning/trainer/distrib_parts.py   added suggesstion Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka jirka@pytorchlightning.ai,0
"Add missing test for ""multiple dataloader + percent_check fix"" (#2226)",0.59601945,enabled multiple dataloaders for validation.    ,"  Init fix num_batches   Fix num_batches in case of multiple dataloaders   Apply suggestions from code review   Changes based on suggestions   Flake8   Add test to check num_batches   generalize dataloader percent check test   fix formatting   remove hparams   tests   CHANGELOG   Update CHANGELOG.md   max_batches can be int   conflict and rebase   add back the test   fix fix message 0.0 works Revert ""fix message"" This reverts commit 839cacf8b8610f4e697e654ef6f3d2501bf23984.   update changelog   Update CHANGELOG.md   Fix num batches in case of multiple dataloaders and percent_check (#1920)   git conflict   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   missing union   doc update suggestion by @rohitgr7   extend test   changelog   docs add note about multiple loaders   update changelog   remove unused variable   Co-authored-by: rohitgr7 rohitgr1998@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",0
Checking if the parameters are a DictConfig Object (#2216),0.47953725,Increased DeepDiff's verbose level to properly handle dict changes (#13960)," Checking if the parameters are a DictConfig Object  This is in reference to #2058 .  To be honest, I have no idea how I should go about writing a test for this.  Update pytorch_lightning/loggers/base.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  fix ...  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka jirka@pytorchlightning.ai",0
"update docs for ""overfit_batches"" (#2324)",0.6975119,overfit_pct in favour of overfit_batches,  update docs   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
refactored training_batch + tests to verify correctness (#2328),0.9860271,"Refactored training_batch + tests to verify correctness (#2327, #2328)",  refactored training_bath   refactored training_bath   refactored training_bath   refactored training_bath   refactored training_bath   refactored training_bath   refactored training_bath   refactored training_bath   refactored training_bath   refactored training_bath   refactored training_bath ,1
Fix ROC metric for CUDA tensors (#2304),0.58777094,  * The `PrecisionPlugin.backward` signature changed: The `closure_loss` argument was renamed to `tensor`," Fix ROC metric for CUDA tensors  Previously roc metric (and auroc) errors when passed in CUDA tensors, due to torch.tensor construction without specifying device. This fixes the error by using F.pad instead.   Update test_classification.py   Update test_classification.py   chlog   Update test_classification.py   Update test_classification.py   Update tests/metrics/functional/test_classification.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update test_classification.py  Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Jirka jirka@pytorchlightning.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",0
Fix average_precision metric (#2319),0.6322558,    precision=16,"  Fixed average_precision metric, parenthesis were missing. Added test test that failed with the old implementation   Modified CHANGELOG.md   Update CHANGELOG.md   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",0
fix typo in forward return (#2301),0.52633387,training forward refactor (#3134),,0
Fix summary hook handles not getting removed (#2298),0.579337,moved ___step_end hooks (#3130),  detach hooks after completion   detach hook   update docs   add test   docs   changelog ,0
devel version (#2292),0.49034917,[1.7.4] - 2022-08-31,,0
check omegaconf gpus (#2273),0.5531641,Support auto_select_gpus with the accelerator and devices API (#12608),  check omegaconf gpus   test   test   Apply suggestions from code review   Co-authored-by: Omry Yadan omry@fb.com Co-authored-by: Omry Yadan omry@fb.com,0
test CLI parsing gpus (#2284),0.737193,Parsing of GPU Argument,  cli gpus   test   test ,1
Update new project code sample (#2287),0.44176853,- Removed the deprecated code in:,,0
"Revert/Fix: epoch indexing from 1, to be from 0 (#2289)",0.9216331,Changed epoch indexing from 0 instead of 1 (#2289)," Revert ""deprecated: epoch indexing from 1 (#2206)""  This reverts commit f94b919b   chlog   grad index   Apply suggestions from code review   tests   fix   test ",1
Bugfix/_has_len (#2293),0.43373203,Truncated long version numbers in progress bar (#2594),  deal with NotImplementedError raised by torchtext   deal with NotImplementedError raised by torchtext   Added tests for dataloader which raise NotImplementedError in len()   Fixed some typos   Co-authored-by: Thomas Schaaf tschaaf@cs.cmu.edu,0
Update progress.py (#2268),0.6665726,"Removed legacy code to log or include metrics in the progress bar by returning them in a dict with the ""log""/""progress_bar"" magic keys. Use self.log instead (#6734)",Fixes a minor bug introduced in #2213,0
deprecated Trainer proc_rank (#2269),0.76568615,Deprecated Trainer argument print_nan_grads (#1097),  deprecated   test ,1
[refactor results 1] - refactor backward (#2276),0.65728784,Refactor Model backward (#2276),  move backward   refactor backward to remove 16 bit from user override   refactor backward to remove 16 bit from user override   Update pytorch_lightning/core/hooks.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Attempt to add broken test to mimic transformers use case (#2272),0.43744874,"Refactored training_batch + tests to verify correctness (#2327, #2328)",  Attempt to add broken test   use wandb logger   Update test_amp.py   Co-authored-by: William Falcon waf2107@columbia.edu,0
continue 0.8.x (#2264),0.49460238,[1.1.8] - 2021-02-08,  cleaning   docs   docs   types   mixins   mixins   docs   typo ,0
Release2 (#2262),0.505543,This release includes:,  fix missing arg   fix missing arg   fix missing arg   fix missing arg   fix missing arg   fix missing arg   fix missing arg ,0
updates to changelog (#2248),0.6316924,Full Changelog,  miss   miss   chlog   chlog ,0
fix missing arg,0.74111384,Args should come after the last positional argument (#1807),,1
remove tpu barrier (#2260),0.57365936,Enabled manual optimization for TPUs (#8458),,0
fallback to hparams str (#2259),0.56117994,Support **DictConfig for hparam serialization (#2519),,0
Barrier (#2257),0.36308968,"Cleaning (#5948, #5949, #5950)",  remove barriers   remove barriers   remove barriers   remove barriers   remove barriers   remove barriers   remove barriers   remove barriers   remove barriers   remove barriers ,0
decrease some training times (#2256),0.5678723,Refactored training loop (#2336),,0
remove frame inspection on self.hparams (#2253),0.43466288,Remove MetricsHolder (#7909),  remove frame inspection on self.hparams   remove frame inspection on self.hparams   remove frame inspection on self.hparams   remove frame inspection on self.hparams   remove frame inspection on self.hparams   remove frame inspection on self.hparams ,0
fix gpu template (#2255),0.64196974,refactored GPU backend __step (#3120),,0
fix setup and on fit calls (#2252),0.48895007,Trainer.fit hook clean up (#3198),,0
made fx public (#2247),0.44018102,Made type hints public (#17100),  made fx public   made fx public   made fx public ,0
added barrier (#2245),0.35293725,Made type hints public (#17100),  added barrier   blank line   added barrier   added barrier   made fx public   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Docs new section (#2236),0.57311577,Docs improvements,  chlog   docs   ver++   docs   url   docs   readme   docs --- ,0
Typo fix in metrics docs (#2237),0.65711176,Removed deprecated metrics (#6161),"There was a typo in metrics description: RMSE titled metric was actually RMSLE and vice-versa. So I've fixed it, changing two characters.",0
Fixed the load_from_checkpoint path detected as URL bug (#2244),0.7443006,Fix for load_from_checkpoint() not working with absolute path on Windows (#2294),  Fixed the load_from_checkpoint path detected as URL bug   Fixed the load_from_checkpoint path detected as URL bug   fixed Caps lock typo   Added .absolute() to checkpoint path to force hard drive prefix in string ,1
Change PR template (#2224),0.45021993,You can find a migration guide for this change in this PR's description.,  Change PR template   Update .github/PULL_REQUEST_TEMPLATE.md   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
[metrics] IoU Metric (#2062),0.58431363,"docs for all Metrics (#2184, #2209)",  add iou function   update stat scores   add iou class   add iou tests   chlog   Apply suggestions from code review   tests   docs   Apply suggestions from code review   docs   Co-authored-by: Jirka jirka@pytorchlightning.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Docs clean-up (#2234),0.65487635,Docs improvements,  update docs   update docs   update docs   update docs   update docs   update docs ,0
final cleanup for v0.8.0 (#2181),0.5510404,some minor cleaning,  final clean for v0.8.0   chlog   chlog   date   rename stage   date   missing ,0
Pid port + duplicate rank_zero logging (#2231),0.4478492,"Defines shared proc. rank, remove rank from instances (e.g. loggers) (#1408)",  init the port using a seed that matches process id for ddp   init the port using a seed that matches process id for ddp   init the port using a seed that matches process id for ddp   init the port using a seed that matches process id for ddp   init the port using a seed that matches process id for ddp   init the port using a seed that matches process id for ddp   init the port using a seed that matches process id for ddp   Co-authored-by: Zhaofeng Wu zfw7@cs.washington.edu,0
Tpu logging (#2230),0.5438411,Resolve TPU miss rendezvous (#6781),  add tpu view   add tpu view   add tpu view   add tpu view   add tpu view ,0
lightning typo (#2228),0.5205579,- Removed deprecated `LightningDistributed` ([#13549](https://github.com/Lightning-AI/lightning/pull/13549)),,0
adds setup+teardown hook (#2229),0.65795314,Updated hooks arguments - breaking for setup and teardown (#2850),  allow regression metrics to import   allow regression metrics to import   allow regression metrics to import   allow regression metrics to import   allow regression metrics to import   allow regression metrics to import   allow regression metrics to import   allow regression metrics to import   allow regression metrics to import ,0
allow regression metrics to import (#2225),0.5900085,Removed deprecated metrics (#8586),  allow regression metrics to import   allow regression metrics to import   docs   Apply suggestions from code review   Co-authored-by: Jirka jirka@pytorchlightning.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
fixing docs (#2227),0.6329457,Docs improvements,,0
Regression metrics (#2221),0.9999999,Regression metrics (#2221),  add regression metrics   solve tests   add docs ,1
replace train_percent_check with limit_train_batches (#2220),0.7097108,Disabled training when limit_train_batches=0 (#4371),  drop train_percent_check   drop train_percent_check   drop train_percent_check   drop train_percent_check   drop train_percent_check   drop train_percent_check   drop train_percent_check   drop train_percent_check   drop train_percent_check   drop train_percent_check   drop train_percent_check   drop train_percent_check   drop train_percent_check   drop train_percent_check   drop train_percent_check   drop train_percent_check   drop train_percent_check   chlog   deprecated   deprecated   deprecated   tests   tests   Apply suggestions from code review   tests   hydra support   tests   hydra support   hydra support   hydra support   tests   typo   typo   Update test_dataloaders.py   docs   docs   docs   docs   Co-authored-by: Jirka jirka@pytorchlightning.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
doc fixes (#2222),0.5950574,"In addition, we fixed:",  hydra support   hydra support   hydra support   hydra support   hydra support   hydra support   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Enable gradients at train start (#2200),0.59526443,"trainer = Trainer(callbacks=GradientAccumulationScheduler({""1"": 5, ""10"": 3}))",  Enable gradients at train start   Update training_loop.py   Co-authored-by: William Falcon waf2107@columbia.edu,0
hydra ddp support + generalized ddp gpu flags (#2212),0.6549655,Made DDP the default if no backend specified with multiple GPUs (#1789),  hydra ddp support   hydra ddp support   hydra ddp support   hydra support   hydra support   hydra support ,0
[WIP] Rename overfit_pct to overfit_batches (and fix) and val_percent_check and test_percent_check (and fix) (#2213),0.67586184,"Removed deprecated trainer flags: overfit_pct, log_save_interval, row_log_interval (#3969)",  fixed percent check for val/test   fixed percent check for val/test   fixed percent check for val/test   fixed percent check for val/test   overfit_pct now uses train loaders for val and test and does not shuffle   overfit_pct now uses train loaders for val and test and does not shuffle   overfit_pct now uses train loaders for val and test and does not shuffle   overfit_pct now uses train loaders for val and test and does not shuffle   overfit_pct now uses train loaders for val and test and does not shuffle   overfit_pct now uses train loaders for val and test and does not shuffle   overfit_pct now uses train loaders for val and test and does not shuffle   overfit_pct now uses train loaders for val and test and does not shuffle   overfit_pct now uses train loaders for val and test and does not shuffle   overfit_pct now uses train loaders for val and test and does not shuffle   overfit_pct now uses train loaders for val and test and does not shuffle   overfit_pct now uses train loaders for val and test and does not shuffle   overfit_pct now uses train loaders for val and test and does not shuffle   overfit_pct now uses train loaders for val and test and does not shuffle   overfit_pct now uses train loaders for val and test and does not shuffle   overfit_pct now uses train loaders for val and test and does not shuffle   overfit_pct now uses train loaders for val and test and does not shuffle   overfit_pct now uses train loaders for val and test and does not shuffle   overfit_pct now uses train loaders for val and test and does not shuffle   overfit_pct now uses train loaders for val and test and does not shuffle   overfit_pct now uses train loaders for val and test and does not shuffle   overfit_pct now uses train loaders for val and test and does not shuffle   overfit_pct now uses train loaders for val and test and does not shuffle   add on fit_start on fit_end hooks   add on fit_start on fit_end hooks   add on fit_start on fit_end hooks   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
"Revert ""Misleading exception raised during batch scaling (#1973)"" (#2219)",0.6719518,Reset epoch progress with batch size scaler (#13846),This reverts commit f8103f9c7dfc35b4198e951a1789cae534c8b1db.,0
Misleading exception raised during batch scaling (#1973),0.6626879,Reset epoch progress with batch size scaler (#13846), Misleading exception raised during batch scaling  Use batch_size from model.hparams.batch_size instead of model.batch_size   Improvements considering #1896   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
add on fit_start on fit_end hooks (#2217),0.5729263,moved ___step_end hooks (#3130),  add on fit_start on fit_end hooks   add on fit_start on fit_end hooks   add on fit_start on fit_end hooks ,0
Metric docs fix (#2209),0.68360925,"docs for all Metrics (#2184, #2209)",  fix docs   Update docs/source/metrics.rst   Update docs/source/metrics.rst   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   Update docs/source/metrics.rst   Update docs/source/metrics.rst   Update metrics.rst   title   fix   fix for num_classes   chlog   nb classes   hints   zero division   add tests   Update metrics.rst   Update classification.py   Update classification.py   prune doctests   docs   Apply suggestions from code review   Apply suggestions from code review   flake8   doctests   formatting   cleaning   formatting   formatting   doctests   flake8   docs   rename   rename   typo   Co-authored-by: Nicki Skafte nugginea@gmail.com Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka jirka@pytorchlightning.ai Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com,0
Configure isort (#2136),0.35150328,Split profilers module (#6261),"  Configure isort   Fix whitespace   Line length, make THIRDPARTY the default ",0
Metrics docs  (#2184),0.8645927,"docs for all Metrics (#2184, #2209)",  fixes   fixes   fixes   fixes   fixes   fixes   fixes   fixes   fixes   fixes   fixes   fixes   fixes   fixes   fixes   fixes   fixes   fixes   fixes   fixes   Apply suggestions from code review   Co-authored-by: Nicki Skafte skaftenicki@gmail.com   add workers fix   add workers fix   add workers fix   add workers fix   add workers fix   add workers fix   add workers fix   add workers fix   add workers fix   add workers fix   Update docs/source/metrics.rst   Co-authored-by: Nicki Skafte skaftenicki@gmail.com  Update docs/source/metrics.rst  Co-authored-by: Nicki Skafte skaftenicki@gmail.com  Update docs/source/metrics.rst  Co-authored-by: Nicki Skafte skaftenicki@gmail.com  Update docs/source/metrics.rst  Co-authored-by: Nicki Skafte skaftenicki@gmail.com   add workers fix   add workers fix   add workers fix   doctests   add workers fix   add workers fix   fixes   fix docs   fixes   fixes   fixes   fixes   fixes   fixes   fixes   fixes   fixes   fixes   fixes   fixes   fixes   fixes   fixes   fixes   fixes   Apply suggestions from code review   Co-authored-by: Nicki Skafte skaftenicki@gmail.com   add workers fix   Update docs/source/metrics.rst   Co-authored-by: Nicki Skafte skaftenicki@gmail.com   doctests   add workers fix   fix docs   fixes   fixes   fix doctests   Apply suggestions from code review   fix doctests   fix examples   bug   Update docs/source/metrics.rst   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update docs/source/metrics.rst  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update docs/source/metrics.rst  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   fixes   fixes   fixes   fixes   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Jirka jirka@pytorchlightning.ai Co-authored-by: Nicki Skafte nugginea@gmail.com,1
test: save hparams to yaml (#2198),0.7534858,    hparams_file='/path/to/hparams_file.yaml',  save hparams to yaml   import   resolves   req   Update requirements/base.txt   Co-authored-by: Omry Yadan omry@fb.com Co-authored-by: Omry Yadan omry@fb.com,1
deprecated: epoch indexing from 1 (#2206),0.77005184,Changed epoch indexing from 0 instead of 1 (#2289),  epoch indexing from 1   chlog   fix tests   fix tests   self.min_epochs ,1
reduce test warnings (#2202),0.5805519,Removed no return warning from val/test step (#6139),  reduce test warnings   Update test_trainer.py   Update test_trainer.py   Co-authored-by: William Falcon waf2107@columbia.edu,0
cleaning tests (#2201),0.66190284,"Cleaning (#5948, #5949, #5950)",,0
README typo (#2177),0.51036173,Pkg: fix parsing versions by @Borda in https://github.com/Lightning-AI/lightning/pull/15401,,0
template label typo (#2195),0.40132055,"In addition, we fixed:",,0
Refactor model summary + generalize example input array (#1773),0.55922073,Refactor Model backward (#2276)," squash  variant a variant b add test revert rename add changelog docs move changelog entry to top use hooks wip wipp layer summary clean up, refactor type hints rename remove obsolete code rename unused imports simplify formatting of table and increase readability doctest superclass object update examples print unknown sizes more docs and doctest testing unknown layers add rnn test remove main restore train mode test device wip device constant simplify model forward transfer return summary object in method extend tests fix summary for empty module extend tests refactor and added hook variant a variant b add test revert rename add changelog docs move changelog entry to top remove hardcoded string simplify test unknown shapes and all others comments for tests fix hparams attribute   update default   unused import   clean up   replace hardcoded strings   fix doctest   fix top/full   black   fix rnn test   fix rnn   update debugging docs   update docs typo update docs update docs   add changelog   extract constant   setter and getter   move parity models to test folder   parameterize mode ",0
HenryJia: auto-move data decorator (#1905),0.5822073,Removed deprecated auto_move_data decorator (#9231),"  First attempt at auto-moving data for inference   Correct my copypaste errors   Correct for if device is CPU   Get rid of the WIP code I accidentally added   Add tests   Make tests more foolproof   Make sure we stick with pep8 formatting   Clarify docs a little   Apply suggestions from code review   Get everything working again hopefully   refactor and added hook   variant a variant b add test revert rename add changelog docs   move changelog entry to top   Move data transfer to utilities   Add back in warnings for autotransfer   Get rid of the test code I ended up accidentally commiting again   Add docs any changelog   Correct PR number in Changelog   Correct changelog   Update data.py   Update test_cpu.py   make a decorator   type hint   changelog   changelog   remove old function   import   test for decorator   fix test   remove old test   doctest   apply decorator directly   convert doctest to code block   prevent side effects in tests   fix merge   update forward docs   update docs   added docs in section ""deployment / prediction""   update changelog   Co-authored-by: Hengjian Jia henryjia18@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: William Falcon waf2107@columbia.edu",0
hide doctest decoration (#2194),0.3681756,# 2. Remove the `hiddens` argument,  hide doctest decoration   Update docs/source/conf.py   Update docs/source/_static/copybutton.js   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Add ckpt_path option to LightningModule.test() (#2190),0.59613454,Removed the exp_save_path property from the LightningModule (#7266)," Add ckpt_path option to LightningModule.test()  If ckpt_path is ""best"" (default), it loads the best weights saved by ModelCheckpoint for the test loop. If ckpt_path is a path to a checkpoint file, it loads the weights from the file for the test loop. If ckpt_path is None, it uses the weights from the end of training for the test loop. If model parameter is set, ckpt_path is ignored.  Update test_set.rst  Co-authored-by: William Falcon waf2107@columbia.edu",0
Performance docs (#2191),0.6822295,Speed/memory optimizations.,  add workers fix   add workers fix ,0
Handle KeyboardInterrupt during training (#2134),0.8037956,"Support graceful training cleanup after Keyboard Interrupt (#856, #1019)", Handle KeyboardInterrupt during training  Fixes #2079.   chlog   Fix whitespace   Update callback_hook.py   Update base.py   Update training_loop.py   Update test_trainer.py   Update CHANGELOG.md   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   Update CHANGELOG.md   on_keyboard_interrupt   Co-authored-by: Jirka jirka@pytorchlightning.ai Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Fast defaults (#2185),0.44755757,Select the strategy with good defaults,  log row might be a bottleneck depending on network. 50 unblocks this and is small enough for small datasets   log row might be a bottleneck depending on network. 50 unblocks this and is small enough for small datasets ,0
Fix for accuracy calculation (#2183),0.6209837,    precision=16,  accuracy_fix   fix line length   Apply suggestions from code review   Update test_classification.py   Co-authored-by: Nicki Skafte nugginea@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: William Falcon waf2107@columbia.edu,0
past checkpoints (#2160),0.75660115,Resuming from checkpoints (#16167),  past checkpoints   omegaConf save   enforce type   resolve=True   Co-authored-by: Omry Yadan omry@fb.com   test omegaconf   tests   test past   Co-authored-by: Omry Yadan omry@fb.com,1
Docs & Changelog (#2176),0.6846971,Docs improvements,  missed   format   math   req   notes   fix CI   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Finish Ananthsub patch 1 (enable prepare_data from correct processes). clarify local vs global rank (#2166),0.69273865,Enabled prepare_data from correct processes - clarify local vs global rank (#2166),  [trainer] Call prepare_data once per node in DDP/DDP2 training   refactored DDP routes   renamed proc_rank to local_rank   renamed proc_rank to local_rank   renamed proc_rank to local_rank   renamed proc_rank to local_rank   renamed proc_rank to local_rank   renamed proc_rank to local_rank   renamed proc_rank to local_rank   renamed proc_rank to local_rank   renamed proc_rank to local_rank   renamed proc_rank to local_rank   renamed proc_rank to local_rank   renamed proc_rank to local_rank   spawn message   spawn message   spawn message   fixes   fixes   fixes   fixes   fixes   Update trainer.py   Co-authored-by: ananthsub ananth.subramaniam@gmail.com,0
Docs/template (#2152),0.573933,Docs improvements,  typo   typo ,0
clean requirements (#2128),0.6440229,"Cleaning (#5948, #5949, #5950)",  clean requirements   missing   missing   req   min   default >> base   base.txt ,0
fix loss param order (#2169),0.62414813,        :param loss: Loss is already scaled by accumulated grads,  fix loss param order   valid loss ,0
Minor warning message fix (#2173),0.6077641,Silenced some warnings. verified ddp refactors (#3483),,0
Native torch metrics (#1488),0.9750705,"Native torch metrics (#1488, #2062)",  Create metric.py   Create utils.py   Create init.py   Create init.py   Create init.py   add tests for metric utils   add tests for metric utils   add docstrings for metrics utils   add docstrings for metrics utils   add function to recursively apply other function to collection   add function to recursively apply other function to collection   add tests for this function   add tests for this function   add tests for this function   update test   update test   Update pytorch_lightning/metrics/metric.py   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   update metric name   remove example docs   fix tests   fix tests   add metric tests   fix to tensor conversion   fix to tensor conversion   fix apply to collection   fix apply to collection   Update pytorch_lightning/metrics/metric.py   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   remove tests from init   remove tests from init   add missing type annotations   rename utils to convertors   rename utils to convertors   rename utils to convertors   rename utils to convertors   Update pytorch_lightning/metrics/convertors.py   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/metrics/convertors.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/metrics/convertors.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/metrics/convertors.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/metrics/convertors.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/metrics/convertors.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/metrics/metric.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update tests/utilities/test_apply_to_collection.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update tests/utilities/test_apply_to_collection.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update tests/utilities/test_apply_to_collection.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update tests/utilities/test_apply_to_collection.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update tests/metrics/convertors.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update tests/metrics/convertors.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Apply suggestions from code review  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Apply suggestions from code review  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Apply suggestions from code review  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   add doctest example   rename file and fix imports   rename file and fix imports   added parametrized test   added parametrized test   replace lambda with inlined function   rename apply_to_collection to apply_func   rename apply_to_collection to apply_func   rename apply_to_collection to apply_func   Separated class description from init args   Apply suggestions from code review   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   adjust random values   suppress output when seeding   remove gpu from doctest   Add requested changes and add ellipsis for doctest   Add requested changes and add ellipsis for doctest   Add requested changes and add ellipsis for doctest   forgot to push these files...   forgot to push these files...   forgot to push these files...   add explicit check for dtype to convert to   add explicit check for dtype to convert to   fix ddp tests   fix ddp tests   fix ddp tests   remove explicit ddp destruction   remove explicit ddp destruction   New metric classes (#1326)   Create metrics package   Create metric.py   Create utils.py   Create init.py   add tests for metric utils   add docstrings for metrics utils   add function to recursively apply other function to collection   add tests for this function   update test   Update pytorch_lightning/metrics/metric.py   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   update metric name   remove example docs   fix tests   add metric tests   fix to tensor conversion   fix apply to collection   Update CHANGELOG.md   Update pytorch_lightning/metrics/metric.py   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   remove tests from init   add missing type annotations   rename utils to convertors   Create metrics.rst   Update index.rst   Update index.rst   Update pytorch_lightning/metrics/convertors.py   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/metrics/convertors.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/metrics/convertors.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/metrics/metric.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update tests/utilities/test_apply_to_collection.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update tests/utilities/test_apply_to_collection.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update tests/metrics/convertors.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Apply suggestions from code review  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   add doctest example   rename file and fix imports   added parametrized test   replace lambda with inlined function   rename apply_to_collection to apply_func   Separated class description from init args   Apply suggestions from code review   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   adjust random values   suppress output when seeding   remove gpu from doctest   Add requested changes and add ellipsis for doctest   forgot to push these files...   add explicit check for dtype to convert to   fix ddp tests   remove explicit ddp destruction   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   add function to reduce tensors (similar to reduction in torch.nn)   add functionals of reduction metrics   add functionals of reduction metrics   add more metrics   pep8 fixes   rename   rename   add reduction tests   add first classification tests   bugfixes   bugfixes   add more unit tests   fix roc score metric   fix tests   solve tests   fix docs   Update CHANGELOG.md   remove binaries   solve changes from rebase   add eos   test auc independently   fix formatting   docs   docs   chlog   move   function descriptions   Add documentation to native metrics (#2144)   add docs   add docs   Apply suggestions from code review   formatting   add docs   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka jirka@pytorchlightning.ai   Rename tests/metrics/test_classification.py to tests/metrics/functional/test_classification.py   Rename tests/metrics/test_reduction.py to tests/metrics/functional/test_reduction.py   Add module interface for classification metrics   add basic tests for classification metrics' module interface   pep8   add additional converters   add additional base class   change baseclass for some metrics   update classification tests   update converter tests   update metric tests   Apply suggestions from code review   tests-params   tests-params   imports   pep8   tests-params   formatting   fix test_metrics   typo   formatting   fix dice tests   fix decorator order   fix tests   seed   dice test   formatting   try freeze test   formatting   fix tests   try spawn   formatting   fix   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: J. Borovec jirka.borovec@seznam.cz Co-authored-by: Xavier Sumba c.uent@hotmail.com Co-authored-by: Jirka jirka@pytorchlightning.ai Co-authored-by: Nicki Skafte nugginea@gmail.com,1
sets default ddp mode to spawn (#2168),0.76876634,"When using multiple devices, the strategy now defaults to ""ddp"" instead of ""ddp_spawn"" when none is set (#16388)",  set ddp_spawn as default   spawn message   spawn message   spawn message   spawn message   spawn message   spawn message   spawn message   spawn message ,1
Fix docs for 0.8.0 (#2162),0.524737,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",  multi_gpu.rst   update lr_finder   Update docs/source/lr_finder.rst   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update docs/source/multi_gpu.rst  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update docs/source/multi_gpu.rst  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update docs/source/multi_gpu.rst  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update docs/source/multi_gpu.rst  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update docs/source/multi_gpu.rst  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update docs/source/multi_gpu.rst  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
get fullpath before splitting (#2153),0.39827344,expand eval loop out (#3165),,0
remove deprecated API for v0.8 (#2073),0.86272883,Removed deprecated API (#2073),  remove deprecated API   chlog   times   missed   formatting check   missing   missing   miss   fix docs build error   fix pep whitespace error   docs   wip   amp_level   amp_level   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
Fix some pyright member access errors in training module (#2121),0.5640329,- Removed deprecated `TrainerModelHooksMixin.is_function_implemented` and `TrainerModelHooksMixin.has_arg` ([#10322](https://github.com/PyTorchLightning/pytorch-lightning/pull/10322)),  Fix pyright member access errors in training module   Fix Trainer instantiation error due to inheritence order   Add GH workflow for pyright   Fix more pyright errors in trainer module   Add pyrightconfig and setup python environment in type-check workflow   Exclude pyrightconfig.json   suggestions   Co-authored-by: Jirka jirka@pytorchlightning.ai,0
Fix typo in contributing docs (#2076),0.5298141,"In addition, we fixed:",,0
Fix DataParallel typo (#2154),0.55470014,Moved ignore_scalar_return_in_dp warning suppression to the DataParallelPlugin class (#7421),,0
Allow loading checkpoints from urls (#1667),0.6225044,    # put all logic related to loading a checkpoint here,  allow loading checkpoints from urls   tmpdir_server fixture   test cases for loading checkpoints from url   dir => root_dir   default map_location to None   test case for resume_from_checkpoint   changelog   doc update   monkeypatch TORCH_HOME to avoid caching   Use a threading server with random ports so that it is easier to clean up   test fixes   pep8 fix   ThreadingHTTPServer support in 3.6   pep8 fix   fix changelog   separate tests for urls   typo   Co-authored-by: Peter Yu 2057325+yukw777@users.noreply.github.com  Apply suggestions from code review  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Rework of Sklearn Metrics (#1327),0.7969402,Sklearn metrics classes (#1327),  Create utils.py   Create init.py   redo sklearn metrics   add some more metrics   add sklearn metrics   Create init.py   redo sklearn metrics   New metric classes (#1326)   Create metrics package   Create metric.py   Create utils.py   Create init.py   add tests for metric utils   add docstrings for metrics utils   add function to recursively apply other function to collection   add tests for this function   update test   Update pytorch_lightning/metrics/metric.py   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   update metric name   remove example docs   fix tests   add metric tests   fix to tensor conversion   fix apply to collection   Update CHANGELOG.md   Update pytorch_lightning/metrics/metric.py   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   remove tests from init   add missing type annotations   rename utils to convertors   Create metrics.rst   Update index.rst   Update index.rst   Update pytorch_lightning/metrics/convertors.py   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/metrics/convertors.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   add doctest example   rename file and fix imports   added parametrized test   replace lambda with inlined function   rename apply_to_collection to apply_func   Separated class description from init args   Apply suggestions from code review   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   adjust random values   suppress output when seeding   remove gpu from doctest   Add requested changes and add ellipsis for doctest   forgot to push these files...   add explicit check for dtype to convert to   fix ddp tests   remove explicit ddp destruction   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   add sklearn metrics   start adding sklearn tests   fix typo   return x and y only for curves   fix typo   add missing tests for sklearn funcs   imports   all   imports   fix sklearn arguments   fix imports   update requirements   Update CHANGELOG.md   Update test_sklearn_metrics.py   formatting   formatting   format   fix all warnings and formatting problems   Update environment.yml   Update requirements-extra.txt   Update environment.yml   Update requirements-extra.txt   fix all warnings and formatting problems   Update CHANGELOG.md   docs   inherit   docs inherit.   docs   Apply suggestions from code review   Co-authored-by: Nicki Skafte skaftenicki@gmail.com   docs   req   min   Apply suggestions from code review   Co-authored-by: Tullie Murrell tulliemurrell@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka jirka@pytorchlightning.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Tullie Murrell tulliemurrell@gmail.com,1
test cloudpickle (#2105),0.44953212,Tested pickling (#1636),  cloudpickle   ci tests ,0
Docs/changelog (#2125),0.70788413,Full Changelog,  miss chlog   miss chlog   docs   miss   formatting ,1
setup py 3.8 (#2135),0.5282445,python script.py \,,0
[docs] Add Cotratron to community examples (#2130),0.41112506,Implemented ready for components (#16129),  [docs] Add Cotratron to community examples   Apply suggestions from code review   Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com,0
document: fix  callback signature (#2113),0.63410515,Removed deprecated callbacks (#3979),,0
Update README.md (#2117),0.5455695,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",,0
Add docs about example dependencies (#2122),0.49732977,Docs improvements,  Add torchvision and gym dependencies   Add pl_examples/requirements.txt to the list of dependencies for running local tests ,0
Remove explicit flush from tensorboard logger (#2126),1.0000002,Remove explicit flush from tensorboard logger (#2126),  Remove explicit flush from tensorboard logger   Update changelog ,1
temporarily fixes early stopping bug (#2119),0.67155254,"As we continue to strengthen the codebase with more tests, we’re finally getting rid of annoying bugs that have been around for a bit now. Mostly around the inconsistent checkpoint and early stopping behaviour (amazing work @awaelchli  @jeremyjordan )",  fixes early stopping bug   fixes early stopping bug   fixes early stopping bug   fixes early stopping bug   fixe docs   fixe docs   fixe docs   fixe docs   fixe docs   fixe docs   fixe docs   fixe docs   fixe docs   fixe docs   fixe docs   fixe docs   fixe docs   fixe docs   fixe docs   fixe docs   fixe docs   fixe docs   fixe docs   fixe docs   added test ,0
fixe docs,0.64192903,Docs improvements,,0
Adds back the slow spawn ddp implementation that people want (#2115),0.78434724,much faster ddp implementation. Old one was renamed ddp_spawn,  training batch clean up   training batch clean up   training batch clean up   training batch clean up   training batch clean up   training batch clean up   training batch clean up   training batch clean up   training batch clean up   training batch clean up   training batch clean up   training batch clean up   training batch clean up   training batch clean up   training batch clean up   training batch clean up   training batch clean up   training batch clean up   training batch clean up   training batch clean up   adding spawn   adding spawn   adding spawn   adding spawn   adding spawn   adding spawn   adding spawn   adding spawn ,1
Fixes CPU and hanging GPU crash (#2118),0.53626275,"Deprecated log_gpu_memory, gpu_metrics, and util funcs in favor of DeviceStatsMonitor callback (#9921)",  training batch clean up   training batch clean up   training batch clean up ,0
Update Readme with tunning overhead time (#2082),0.48541462,Increased TPU check timeout from 20s to 100s (#5598),,0
update readme with conda installation instruction (#2099),0.422392,  * Deprecated the `pytorch_lightning.utilities.device_parser.is_cuda_available` in favor of `lightning_lite.accelerators.cuda.is_cuda_available`,  update readme with conda installation instruction   fix team header   bibtex spelling   Update README.md   Co-authored-by: William Falcon waf2107@columbia.edu,0
"update hparams, allow OmegaConf (#2047)",0.5126172,Support **DictConfig for hparam serialization (#2519),  DictConf   inits   Apply suggestions from code review   Co-authored-by: Omry Yadan omry@fb.com   wip   wip   wip   wip   wip   wip   wip   wip   wip   wip   atrib   wip   wip   wip   added hparams test   wip   wip   wip   wip   wip   wip   wip   wip   wip   wip   wip   wip   wip   wip   wip   wip   wip   wip   wip   wip   wip   Update test_hparams.py   added hparams test   added hparams test   pep8   pep8   pep8   docs   wip   wip   clean   review @omry   Update docs/source/hyperparameters.rst   Co-authored-by: Omry Yadan omry@fb.com Co-authored-by: Omry Yadan omry@fb.com Co-authored-by: William Falcon waf2107@columbia.edu,0
cleaning (#2030),0.79447895,"Cleaning (#5948, #5949, #5950)",  cleaning   optim imports   fix   typo   on   mergify ,1
correct trainer.fit production example (#2068),0.7821847,trainer.fit(model),"trainer.fit uses the parameter val_dataloaders but in the documentation it is val_dataloader, which is invalid.",1
Update the documentation of configure_optimizers() (#2071),0.7535938,    def configure_optimizers(self):, Explain the default value for scheduler  Co-authored-by: Qinru Li q4li@eng.ucsd.edu,1
testing new speed (#1587),0.50046533,"Updated DeepSpeed ZeRO (#6546, #6752, #6142, #6321)","  fixed new amp bugs   fixed new amp bugs   fixed new amp bugs   try exit   larger dataset   full mnist   full mnist   trainer   assert   .05   .10, #4   5   5   5   refactor   abs diff   speed   speed   speed   speed   Co-authored-by: J. Borovec jirka.borovec@seznam.cz Co-authored-by: Jirka jirka@pytorchlightning.ai",0
Fix local variables being collected into module_arguments dict (#2048),0.5091285,"Alternatively, you can add the argparse arguments you want manually:python","  do not include local vars in auto collection   add test   add test for model with ""self"" renamed to ""obj""   skip decorator   changelog   changelog   update docs   remove obsolete child collection   generalize args, kwargs names   docs   also update varargs passed in   Revert ""also update varargs passed in""   This reverts commit 3d7a30dbee07a513ee13e1cc3e08ca5ccdb85734.  update test",0
Added black formater for the code with code-checker on pull (#1610),0.40599796,- Removed the deprecated code in:," black  Added throught black.toml other options are hard so far No caching for black github action Moved from black.toml to pyproject.toml Exclude not only yml but also yaml Update pyproject.toml Co-authored-by: Thomas Johansen thomasjo@gmail.com Update .github/workflows/code-formatting-check.yml mergify Remove formating check E231 error ignoring because of black formating Updated CONTRIBUTING to the master   Update .github/workflows/code-formatting-check.yml   Bump black to 19.10b0 version   resolved incorrect merge of CONTRIBUTING,   Black skipping string normalization   Minor fixes in CONTRIBUTING, two typos   Update setup.cfg   chlog   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka jirka@pytorchlightning.ai",0
Tests/drop macos py38 (#2061),0.46474344,python script.py test,  tests drop macOS py38   ignore single test   try freeze env   drop   drop   drop   drop   drop skips   imports   fix ,0
increase acc (#2039),0.37821084,Renamed several Trainer atributes:  (#567),  increase acc   try 0.45   @pytest   @pytest   try .50   duration   pytest ,0
tests drop macOS py38 (#2054),0.47654256,python script.py test,  tests drop macOS py38   ignore single test   try freeze env   drop   drop   drop   drop   drop skips   drop macOS py38   imports ,0
data transfer model hook (+ refactor) (#1756),0.60963225,refactored dataloader process hook (#3139), refactor and added hook  variant a variant b add test revert rename add changelog docs   resolve merge duplication   overridden typo   fix test   tpu id   raise if TPU not available   re-use apply_to_collection function for parsing collections   comment   make utility function available to user   documentation   move changelog entry to top   fix tpu transfer call   fix call   remove hardcoded string   improve test   call model hook by default   Apply suggestions from code review   rename utility function   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Raise an error when lightning replaces an existing sampler (#2020),0.99999976,Raise an error when lightning replaces an existing sampler (#2020)," Raise an error when lightning replaces an existing sampler  Currently, Trainer replaces the existing sampler with DistributedSampler if running distributing training and replace_sampler_ddp=True (default behaviour). If a user has configured an existing sampler, this would lead to widely different results if running a distributed vs non-distributed training. This PR fixes this by raising an Error if user has configured a sampler and uses replace_sampler_ddp=True. The recommended behavior from now on is to either remove the sampler or set replace_sampler_ddp=False   Fix tests   Simpler fix   Fix tests   Make inner method protected   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",1
Mistake in parameters' grad norm tracking (#2012),0.7162062,Gradient norm tracking (#16745),  fix grad norm formula   grad-norm tracker test   fixed seed and explicit rtol in grad norm tracking test   a docstring for grad-norms and forced cast to float of norm_type   support for inf-norm   renamed the grad norm test   docs   fixed language in docstring   Apply suggestions from code review   Co-authored-by: Jirka jirka@pytorchlightning.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
Update/merge multi-gpu docs (#2021),0.66882557,"For even more memory savings and model sharding advice, check out stage 2 & 3 as well in our multi-GPU docs.",  merge multi-gpu docs   extend slurm docs   update links to elastic   format docs and type hints in distrib parts   reference multi-gpu/slurm in trainer args docs   fix doctest   typo   doctest   Apply suggestions from code review   Co-authored-by: Lucas Vazquez lucasgouvaz@gmail.com   wall time   Update docs/source/slurm.rst   Co-authored-by: Lucas Vazquez lucasgouvaz@gmail.com   fix title   update docs for weights summary   update changelog   Co-authored-by: Lucas Vazquez lucasgouvaz@gmail.com,0
Add Open MPI installation details for horovod (#2050),0.46527743,"refactored Horovod backend (#3121, #3122)",,0
slow tpu train (#2033),0.65140504,TPU training (#2708),"  use parallel loader   Revert ""use parallel loader""   This reverts commit ed6e7583   select tpu id for pl   condition if tpu_id is None   added info to changelog   Revert ""condition if tpu_id is None""   This reverts commit 1fb6e586  Apply suggestions from code review  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",0
fix bug_report template (#2052),0.4851157,"In addition, we fixed:",  fix bug_report template   article ,0
notes on Bug fixing (#2053),0.6442151,"At last, lots of bug fixes (see below).",  import   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
fix(wandb): use same logger on multiple training loops (#2055),0.8141211,Allow use of same WandbLogger instance for multiple training loops (#2055), fix(wandb): use same logger on multiple training loops  New training loops reset step to 0 which would previously try to overwrite logs fix #2015  docs(changelog.md): add reference to PR 2055,1
Fix domain_template scripts (#2014),0.37629846,Silenced some warnings. verified ddp refactors (#3483),  Fix domain_templates   Fix type of fake labels   type   args ,0
Replaces ddp .spawn with subprocess (#2029),0.6622273,"decoupled DDP, DDP spawn (#3733, #3766, #3767, #3774, #3802, #3806)",  replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix ,0
sooner CI testing (#2037),0.556996,Updated app testing (#16000),,0
"Revert ""Fixes EarlyStopping With Precision=16 (#1996)"" (#2032)",0.5551525,Removed deprecated early_stop_callback (#3982),This reverts commit bf39cb26c57f1fd5968420e99ba351a9e0df9541.,0
Fixes EarlyStopping With Precision=16 (#1996),0.5825007,EarlyStopping now runs at the end of the training epoch by default (#8286),"  Patch for issue 1815, which will allow EarlyStopping to work on precision=16   Added a whitespace to the end of the line so CICD can rerun. No reason for the latest macos test to have been cancelled.   Format. ",0
Keep track of the best model's path saved by ModelCheckpoint (#1799),0.85227394,Attribute best_model_path to ModelCheckpoint for storing and later retrieving the path to the best saved model file (#1799) ," Add an additional attribute to ModelCheckpoint to keep track of the best model's path  Currently, only the best metric value is directly tracked. This new attribute will help in uses cases where the trained model needs to be used or tracked right after training.   Add small description and usage example to docs   Fix PEP8 issues   Fix doctest example   Fix expected output in doctest   Apply suggestions from code review   Show example as code block instead of doctest   Apply suggestions from code review   Update CHANGELOG.md   Rename ModelCheckpoint.best to ModelCheckpoint.best_model_score   Also rename ModelCheckpoint.best_model (added in this PR) to ModelCheckpoint.best_model_path, for consistency, and kth_best_model to kth_best_model_path.  Update pytorch_lightning/trainer/training_io.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Apply suggestions from code review  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Add warning when loading checkpoint from an old version  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",1
Bugfix/fix gan example (#2019),0.5636958,We have fixed GAN training - supporting multiple optimizers.,"  :bug: fixed fake example type assigning and hparams arg   fixed GAN example to work with dp, ddp., ddp_cpu   Update generative_adversarial_net.py   Co-authored-by: William Falcon waf2107@columbia.edu",0
hotfix to unblock hparams and OmniConf - removes auto_register_init_args by default (#2025),0.54283524,Fixing critical bugs in newly added hooks and hparams assignment.,  ogc install   cleaned up tests   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix ,0
fix changelog (#1864),0.57987916,"In addition, we fixed:",  fix chlog   test for #1729   hist   update   Document use case of passing test dataloaders to Trainer.test() (#1992)   Issue 1990 Doc patch.   Codeblock directive.   Update to reflect current state of pytorch-lightning   Final grammar cleaning. I hope these commits are squashed.   Apply suggestions from code review   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: authman uapatira@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Few typo correction (#2011),0.5743902,"In addition, we fixed:",,0
docs/fix_CONTRIBUTING.md (#1984),0.42980757,"In addition, we fixed:", Update CONTRIBUTING.md  typos  Apply suggestions from code review  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Graceful shutdown on python interpreter exit (#1631),0.67996824,Run graceful training teardown on interpreter exit (#1631),"  Fraceful shutdown on python interpreter exit   Update CHANGELOG.md   Update training_loop.py   Update training_loop.py   Update CHANGELOG.md   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   pep8, move to constant   Update training_loop.py   Update training_loop.py   Update training_loop.py   pep8, move to constant   pep8   timeout   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz",0
Add toma comments to auto_scale_batch_size (#1994),0.60803074,tuner.scale_batch_size(...),  Add source comments   Update training_tricks.rst ,0
Minor typo (#1987),0.47299695,"In addition, we fixed:",,0
code guideline (#1949),0.45802668,Deprecation warning (#3844),  code rule   Apply suggestions from code review   Co-authored-by: Jeremy Jordan 13970565+jeremyjordan@users.noreply.github.com  chlog  Co-authored-by: Jeremy Jordan 13970565+jeremyjordan@users.noreply.github.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com,0
Fix root node resolution (#1954),0.5282049,    root_node = '127.0.0.2',,0
unify tests (#1940),0.5088784,Fully tested!,  unify tests   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
LearningRateLogger in multi-scheduler setting (#1944),0.67970353,Simplified logic for updating the learning rate for schedulers (#7682),  fixed undesired behaviour due to dict.fromkeys   a test for log length consistency   runtime-warn if no schedulers are configured   chlog   move   Co-authored-by: Jirka jirka@pytorchlightning.ai,0
Removing unecessary early stopping calls (#1863),0.5260351,Deprecate early_stop_callback Trainer argument (#3845),  Removing unecessary early stopping calls   Update CHANGELOG.md   Co-authored-by: Mateusz Pieniak mateusz.pieniak@evidenceprime.com Co-authored-by: William Falcon waf2107@columbia.edu,0
fix default arg (#1927),0.72709453,Args should come after the last positional argument (#1807),  fix default   formatting errors   update   flake8 ,1
"Revert ""Remove unused param tpu_core_idx (#1948)"" (#1963)",0.6036074,moved TPU xxx_step to backend (#3118),This reverts commit d0ec11b9d656a185b10a113a08dfba3042b4da01.,0
Gen ddp support (#1961),0.676726,DDP(2) backend (#2796),  updated docs   added mixed   added mixed ,0
Update unet.py (#1955),0.3978679,Do not override PYTHONWARNINGS (#4700),,0
Remove unused param tpu_core_idx (#1948),0.57744485,  * Removed the `Trainer(tpu_cores=...)` argument,,0
handle unknown args passed to Trainer.from_argparse_args (#1932),0.7983574,parser = Trainer.add_argparse_args(parser),  filter valid args   error on unknown manual args   added test   changelog   update docs and doctest   simplify   doctest   doctest   doctest   better test with mock check for init call   fstring   extend test   skip test on 3.6 not working   Co-authored-by: William Falcon waf2107@columbia.edu,1
updated docs (#1941),0.60770833,Docs improvements,,0
early stopping checks on_validation_end (#1458),0.9999997,Early stopping checks on_validation_end (#1458)," Fixes PyTorchLightning/pytorch-lightning#490  EarlyStopping should check the metric of interest on_validation_end rather than on_epoch_end.  In a normal scenario, this does not cause a problem, but in combination with check_val_every_n_epoch>1 in the Trainer it results in a warning or in a RuntimeError depending on strict.   Highlighted that ES callback runs on val epochs in docstring   Updated EarlyStopping in rst doc   Update early_stopping.py   Update early_stopping.rst   Update early_stopping.rst   Update early_stopping.rst   Update early_stopping.rst   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   Update docs/source/early_stopping.rst   fix doctest indentation warning   Train loop calls early_stop.on_validation_end   chlog   Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka jirka@pytorchlightning.ai",1
protect progress bar callback (#1855),0.5982101,Deprecated ProgressBar callback in favor of TQDMProgressBar (#10134),  wip protected progress bar settings   remove callback attr from LRfinder   whitespace   changelog ,0
Adds the option of saving the last model on checkpoint (#1908),0.67052877,    # put all logic related to saving a checkpoint here,  saves model every epoch   implement test for save_last   Update CHANGELOG.md   Update CHANGELOG.md   changes test description   Co-authored-by: Jeremy Jordan 13970565+jeremyjordan@users.noreply.github.com Co-authored-by: Jeremy Jordan 13970565+jeremyjordan@users.noreply.github.com,0
Fix logger bug and prepare data bug (#1933),0.6186559,Un-balanced logging properly supported (#5119),"  tests, fix logger bug and prepare data bug   add CHANGELOG.md   Co-authored-by: Nicki Skafte nugginea@gmail.com",0
update min req (#1934),0.46879473,Removed auto val reduce (#2462),,0
Re-Enable Import Errors (#1938),0.7465867,Re-Enable Logger's ImportErrors (#1938),  update logger imports   pep8 fixes   pep8 ,1
replace Hparams by init args (#1896),0.65248406,hparams can now be anything! (call self.save_hyperparameters() to register anything in the _init_,  remove the need for hparams   remove the need for hparams   remove the need for hparams   remove the need for hparams   replace self.hparams   replace self.hparams   replace self.hparams   replace self.hparams   replace self.hparams   replace self.hparams   replace self.hparams   replace self.hparams   replace self.hparams   replace self.hparams   replace self.hparams   replace self.hparams   replace self.hparams   replace self.hparams   replace self.hparams   replace self.hparams   replace self.hparams   replace self.hparams   replace self.hparams   replace self.hparams   replace self.hparams   replace self.hparams   replace self.hparams   replace self.hparams   replace self.hparams   replace self.hparams   replace self.hparams   replace self.hparams   replace self.hparams   replace self.hparams   replace self.hparams   replace self.hparams   fixed   fixed   fixed   fixed   fixed   fixed   fixed   fixed   fixed   fixed   fixed   fixed   fixed   fixed   finished moco   basic   testing   todo   recurse   hparams   persist   hparams   chlog   tests   tests   tests   tests   tests   tests   review   saving   tests   tests   tests   docs   finished moco   hparams   review   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   hparams   overwrite   transform   transform   transform   transform   cleaning   cleaning   tests   examples   examples   examples   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   chp key   tests   Apply suggestions from code review   class   updated docs   updated docs   updated docs   updated docs   save   wip   fix   flake8   Co-authored-by: Jirka jirka@pytorchlightning.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
readme (#1931),0.5282229,[1.2.0] - 2021-02-18,  readme   add governance.rst   Co-authored-by: Nicki Skafte nugginea@gmail.com,0
Fix user warning produced by apex + scheduler combination (#1873),0.5665586,Do not warn when the name key is used in the lr_scheduler dict (#5057),  fix user error produced by apex + scheduler combination   add changelog   added reinit to every configure_apex call   fix styling   Co-authored-by: Nicki Skafte nugginea@gmail.com,0
set min PT 1.3 (#1917),0.44779652,"min_nb_epochs to min_epochs,",  set min PT 1.3   circleCI   mergify   min   chlog   skip ,0
Raise RTD doc build warnings as errors (#1823),0.53837866,warning_utils >> warnings,  suppress epub warn   fail on warn   suppress epub warn   fail on warn   disable epub   typo   remove obsolete suppress ,0
Allow dataloaders without sampler field present (#1907),1.0,Allow dataloaders without sampler field present (#1907)," Allow dataloaders without sampler field present  Sometimes we have a custom dataloader that doesn't have a sampler, better to check that the field is there before reading it.  chlog  Co-authored-by: Jirka jirka@pytorchlightning.ai",1
fix syntax issue (#1900),0.65137446,Syntax changes are: ,,0
Changed order of update_learning_rates() and run_training_teardown(). (#1891),0.65626526,# 3. Remove the `optimizer_idx` argument from `training_step`,,0
New metric classes (#1326) (#1877),0.72572833,Sklearn metrics classes (#1327),  New metric classes (#1326)   Create metrics package   Create metric.py   Create utils.py   Create init.py   add tests for metric utils   add docstrings for metrics utils   add function to recursively apply other function to collection   add tests for this function   update test   Update pytorch_lightning/metrics/metric.py   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   update metric name   remove example docs   fix tests   add metric tests   fix to tensor conversion   fix apply to collection   Update CHANGELOG.md   Update pytorch_lightning/metrics/metric.py   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   remove tests from init   add missing type annotations   rename utils to convertors   Create metrics.rst   Update index.rst   Update index.rst   Update pytorch_lightning/metrics/convertors.py   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/metrics/convertors.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/metrics/convertors.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/metrics/metric.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update tests/utilities/test_apply_to_collection.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update tests/utilities/test_apply_to_collection.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update tests/metrics/convertors.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Apply suggestions from code review  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   add doctest example   rename file and fix imports   added parametrized test   replace lambda with inlined function   rename apply_to_collection to apply_func   Separated class description from init args   Apply suggestions from code review   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   adjust random values   suppress output when seeding   remove gpu from doctest   Add requested changes and add ellipsis for doctest   forgot to push these files...   add explicit check for dtype to convert to   fix ddp tests   remove explicit ddp destruction   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   move dtype device mixin to more general place   refactor to general device dtype mixin   add initial metric package description   change default to none for mac os   pep8   fix import   Update index.rst   Update ci-testing.yml   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   Update CHANGELOG.md   Update pytorch_lightning/metrics/converters.py   readme   Update metric.py   Update pytorch_lightning/metrics/converters.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka jirka@pytorchlightning.ai,1
Remove NaNs from loss in LRFinder (#1862),0.8099256,Removed non-finite values from loss in LRFinder (#1862),  Remove NaNs from loss in LRFinder   np.isfinite   chlog   add test   chlog   Co-authored-by: Jirka jirka@pytorchlightning.ai,1
fix codecov reports (#1867),0.3957937,"In addition, we fixed:",  fix codecov   upgrade codecov   upgrade codecov ,0
fix porgressbar postfix order (#1874),0.41635093,Removed reorder parameter of the auc metric (#5004),,0
add warning for shuffling in test/val (#1865),0.7105688,Disabled val and test shuffling (#1600),,1
Allow user to select individual TPU core to train on (#1729),1.0000002,Allow user to select individual TPU core to train on (#1729)," added tpu_id  added tpu_id to mixins   train on individual tpu   parallel loader if tpu_id is None   removed progress_bar_refresh_rate   chlog   replaced num_tpu_cores with tpu_cores   set tpu_id to None if int   changed num_tpu_cores to tpu_cores in docs   updated docs   updated init.py removed self.tpu_id for ParallelLoader   Update pytorch_lightning/trainer/init.py   check if tpu_cores is a list   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   xla device conditional   num_tpu_cores deprecation   removed duplicate warning   fixed pep8 error   Revert ""removed duplicate warning""   This reverts commit 8adb0a9b   deprecated api update   fixed recursion error   fixed tests   fixed flake errors   removed current_tpu_index   Update CHANGELOG.md   Update trainer.py   Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: William Falcon waf2107@columbia.edu",1
add test for trainer.test() (#1858),0.8861351,trainer.test(),  fix trainer.test()   Update trainer.py   Co-authored-by: William Falcon waf2107@columbia.edu,1
Fixed the default value of auto_lr_find in docs (#1854),0.5525612,move lr_finder (#3434),  Fixed the default value of auto_lr_find in docs   Update lr_finder.rst   Co-authored-by: William Falcon waf2107@columbia.edu,0
Adding a new section to the docs: Example Lightning Project Structures (#1851),0.49276397,Docs improvements,  Adding a new section to the docs: Example Lightning Project Structures   Update index.rst   Update index.rst   Co-authored-by: William Falcon waf2107@columbia.edu,0
Fix save_weights_only flag in ModelCheckpoint (#1780),0.69368356,Changed defaults of save_top_k and save_last to None in ModelCheckpoint (#3680), Add flag to dump_checkpoint for only including weights  ModelCheckpoint then passes self.save_weights_only to the save function.   Fix tests and add changelog entry   Add check and descriptive message when training state is restored from a weights only checkpoint   Also add a test for making sure ModelCheckpoint.save_weights_only works as expected.   Fix weights-only test to properly match expected exception   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
remove extra kwargs from Trainer init (#1820),0.63431656,trainer.ckpt_path = None,  remove kwargs   remove useless test   rename unknown trainer flag   trainer inheritance and test   blank line   test for unknown arg   changelog ,0
continue devel (#1793),0.39976948,DDP(2) backend (#2796),  miss   miss   miss   update   format ,0
Fix test configuration check and testing (#1804),0.64653075,Refactored setup_training and remove test_mode (#5388),  Fix test configuration check and testing   Fix test configuration check and testing   Remove check_testing_configuration during test   Fix docstring   fix function name   remove conflicts ,0
remove obsolete self._device in Trainer (#1849),0.9581709,Removed obsolete self._device in Trainer (#1849),  remove unused device attribute   dtype   move on_gpu to model ,1
enable any dict and namespace in hparams (#1847),0.69479585,Support **DictConfig for hparam serialization (#2519),,0
release 0.7.6 (#1813),0.65554893,[1.7.6] - 2022-09-13,  release 0.7.6rc2   release 0.7.6   include img   smaller image   missing   miss   miss   miss   up ,0
Enable non-blocking for gpu device transfer (#1843),0.9776194,Enable non-blocking for device transfers to GPU (#1843),  Update distrib_parts.py   Update CHANGELOG.md ,1
extend arg parser (#1842),0.63394195,The brittle argument parsing utilities (#16708),  extend arg parser   flake8   tests   example   fix test ,0
"Update args, kwargs doc for load_from_checkpoint() (#1839)",0.7627568,"def load_checkpoint(self, path):",,1
docs dpp warn (#1835),0.66761696,Silenced some warnings. verified ddp refactors (#3483),  add warn   Apply suggestions from code review ,0
dummy logger (#1836),0.6765337,"Mocking loggers (#3596, #3617, #3851, #3859, #3884, #3853, #3910, #3889, #3926)",Co-authored-by: Nicki Skafte nugginea@gmail.com,0
Fix failing docs (#1821),0.47774404,Docs improvements,  missing pkg   update CI   strict RTD   strict RTD   make   missing   ignore   ignore   mock   typo ,0
fix bugs in semantic segmentation example (#1824),0.66480184,Updated semantic segmentation example with custom u-net and logging (#1371),  Update unet.py   Update semantic_segmentation.py ,0
fixes ddp bugs (#1819),0.7462815,Silenced some warnings. verified ddp refactors (#3483),  debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug ,1
:sparkles: Use store_true for bool args (#1822),0.41105568,Changed the Trainer's checkpoint_callback argument to allow only boolean values (#7539),  :sparkles: Use store_true for bool args   debug   Co-authored-by: Nate Raw nxr9266@g.rit.edu,0
args should come after the last positional argument (#1807),1.0,Args should come after the last positional argument (#1807),,1
Add ElasticTraining documentation (#1818),0.5553642,Adds LightningTrainingComponent. LightningTrainingComponent orchestrates multi-node training in the cloud (#13830),,0
[checkpoint logic] Fix bug which doesn't account for NoneType for model.hparams (#1817),0.6856493,Enable None model checkpoint default (#3669),The intention of the code is to output a warning message when hparams is null or not set. Instead the code now fatals when model.hparams = None. Prevent that.,0
Bugfix: accumulation and suggestion for learning rate finder (#1801),0.73340875,Learning Rate Finder (#13802),  fix suggestion being too naive   fix accumulation error and added new tests   fix styling   update CHANGELOG.md   update based on review   fix tests   Apply suggestions from code review   Apply suggestions from code review   Apply suggestions from code review   Apply suggestions from code review   Co-authored-by: Nicki Skafte nugginea@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
[ddp] Support multi-node distributed execution under torchelastic (#1811),0.5929456,Enabled traditional/manual launching of DDP processes through LOCAL_RANK and NODE_RANK environment variable assignments (#7480),"The changes are quite local and limited in nature -- viz., checking for some indicator environment variables. We check for (SLURM_LOCALID, NODE_RANK, GROUP_RANK) in order. If multiple are found set, a warning is logged. This patch also fixes a minor bug with comparing the WORLD_SIZE environment variable. This can be a string type.",0
Update README.md (#1798),0.5181587,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",  Update README.md   Update README.md   committed suggestion Co-authored-by: William Falcon waf2107@columbia.edu  Update README.md  Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com  Update README.md  Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,0
Replace meta_tags.csv with hparams.yaml (#1271),0.9014925,Replace mata_tags.csv with hparams.yaml (#1271),  Add support for hierarchical dict   Support nested Namespace   Add docstring   Migrate hparam flattening to each logger   Modify URLs in CHANGELOG   typo   Simplify the conditional branch about Namespace   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update CHANGELOG.md  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   added examples section to docstring   renamed _dict -> input_dict   mata_tags.csv -> hparams.yaml   code style fixes   add pyyaml   remove unused import   create the member NAME_HPARAMS_FILE   improve tests   Update tensorboard.py   pass the local test w/o relavents of Horovod   formatting   update dependencies   fix dependencies   Apply suggestions from code review   add savings   warn   docstrings   tests   Apply suggestions from code review   saving   Apply suggestions from code review   use default   remove logging   typo fixes   update docs   update CHANGELOG   clean imports   add blank lines   Update pytorch_lightning/core/lightning.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update pytorch_lightning/core/lightning.py  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   back to namespace   add docs   test fix   update dependencies   add space   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
added override for hparams in load_from_ckpt (#1797),0.60481966,    hparams_file='/path/to/hparams_file.yaml',  added override for hparams in load_from_ckpt   override hparams   override hparams   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   update doctest   typo   chlog   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka jirka.borovec@seznam.cz,0
device property (#1791),0.7497223,"device parser (#3400, #3405)",  device property   add/copy properties   inherit   rename   Apply suggestions from code review   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com   dtype   prop   pt api   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,1
add missing flag (#1805),0.5709241,Deprecated flags: (#2213),,0
Missing profiler attribute in add_argparse_args() ArgumentParser (#1794),0.6280695,"Alternatively, you can add the argparse arguments you want manually:python",  Fixed typing annotation by adding boolean type. After that Profiler flag will be added to argparse.   Updated CHANGELOG.md   Updated git_init_arguments_and_types() to pass doctests.   Added doctest example to add_argparse_parser() ,0
Option to provide seed to random generators to ensure reproducibility (#1572),0.58486795,seed_everything now fails when an invalid seed value is passed instead of selecting a random seed (#8787)," Option to provide seed to random generators to ensure reproducibility  I added small function in utilities which imports torch, numpy, python random and sets seed for all of the libraries to ensure reproducibility of results.   Apply recommendations from core contributors on seeding   Moved the seeding code to another file  Make deterministic as a parameter for trainer class Add assertions for seeding numpy Added warnings  torch.manual_seed should be enough for seeding torch   Revert ""Apply recommendations from core contributors on seeding""   This reverts commit a213c8e6882eec8a9e7408b9418926d2db7c5461.  Revert ""Revert ""Apply recommendations from core contributors on seeding""""  This reverts commit 59b2da53c62878de7aab0aa3feb3115e105eea06.   Change in test, for correct seeding   Allow seed equal to 0   Allow seed to be uint32.max   Added deterministic to benchmarks   Cuda manual seed as in benchmark seeding   Seeding should be done before model initialization   cuda manual_seed is not necessary   Fixing seed test_cpu_lbfgs   On some seeds seems like lbfgs doesn't converge. So I fixed the seed during testing.   rebasing issue with old reproducibility.py   Improved documentation and ability to seed before initializing Train class   Change in docs   Removed seed from trainer, update for documentation   Typo in the docs   Added seed_everything to all   Fixing old changes   Model initialization should be earlier then Trainer   Update pytorch_lightning/trainer/init.py   From Example to testcode Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   Fixing according to the contributors suggestions   Moving horovod deterministic to Trainer class   deterministic flag affects horovod docs update   Improved static typing   Added deterministic to test runners of horovod   It is failing on some versions, not very predictable   static seeds for horovod tests   Change for reset_seed function in tests   Seeding horovod using reset_seed from tutils   Update pytorch_lightning/trainer/init.py   chlog   Update trainer.py   change ""testcode"" to ""Example"" in trainer init documentation   Update pytorch_lightning/trainer/seed.py, first line in comment   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: William Falcon waf2107@columbia.edu",0
Bug fix hparam logging with metrics (#1647),0.86487347,Allow logging of metrics together with hparams (#1630),  add metric logging   Use pytorch built-in method   Update tensorboard.py   Update tensorboard.py ,1
Fix build Docker releases (#1783),0.5244495,Pkg: fix parsing versions by @Borda in https://github.com/Lightning-AI/lightning/pull/15401,  gh act - if   gh act - if   gh act - steps   gh act - steps   gh act - steps   name   name   reorder   docker   timeout   repo   show   show   ver   rc   tag ,0
made ddp the default if no backend specified with multiple GPUs (#1789),0.9999999,Made DDP the default if no backend specified with multiple GPUs (#1789),  made ddp the default if no backend specified with multiple GPUs   fix   spawn   Co-authored-by: Jirka jirka.borovec@seznam.cz,1
Join Horovod workers at the end of trainer.fit() to prevent race conditions following training (#1786),0.6557744,Deprecated the Trainer.train_loop property in favor of Trainer.fit_loop (#8025),  Join Horovod workers at the end of trainer.fit() to prevent race conditions following training   flake8   flake8   Co-authored-by: Jirka jirka.borovec@seznam.cz,0
fixed native amp + ddp (#1788),0.50338197,decoupled DDP2 (#3816),  fixed native amp + ddp   fixed native amp + ddp ,0
set logger level for package (#1718),0.66196936,Change default logger to a dedicated one (#1064),  move logging config to trainer class init   alternate logging config ,0
Device (#1790),0.6819531,    devices=1,  added self.device   added docs ,0
Removed test_dataloader call in check_testing_model_configuration (#1670),0.6734942,Removed test_dataloaders parameter from Trainer.fit() (#1434),  Removed test_dataloader call   Check if test_dataloader is actually overriden   Fixed method spelling   Replaced lambdas   Replaced None with super method   Fixed testpass ,0
dataloaders with fast_dev_run (#1787),0.6326803,Changed warnings and recommendations for dataloaders in ddp_spawn (#6762),  dataloaders with fast_dev_run   dataloaders with fast_dev_run   dataloaders with fast_dev_run   fix   pep 8 ,0
RC & Docs/changelog (#1776),0.63208824,Full Changelog,  missing   RC   tol   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  test  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Fix saving native AMP scaler state (#1777),0.5495416,training AMP scaling refactor (#3135),Saving was introduced in #1561.,0
enable fast_dev_run without a validation loop (#1779),0.72928774,"Disabled checkpointing, earlystopping and logging with fast_dev_run (#5277)",  fix val dataloader   Update evaluation_loop.py ,1
Update progress.py,0.66577405,"Removed legacy code to log or include metrics in the progress bar by returning them in a dict with the ""log""/""progress_bar"" magic keys. Use self.log instead (#6734)",,0
fixes no val loader,0.47047174,"Removed automatic patching of {train,val,test,predict}_dataloader() on the LightningModule (#9764)",,0
Fix lr key name in case of param groups (#1719),0.44471574,Do not warn when the name key is used in the lr_scheduler dict (#5057),  Fix lr key name in case of param groups   Add tests   Update test and added configure_optimizers__param_groups   Update CHANGELOG ,0
Fix Docker Pipeline (#1765),0.5808573,Remove unnecessary intermediate layers in Dockerfiles (#5697),  Update and rename docker_builds.yml to docker_nightly_builds.yml   Update and rename docker_nightly_builds.yml to docker_builds.yml   Update docker_builds.yml   Update .github/workflows/docker_builds.yml   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Fix NeptuneLogger to work in ddp mode (#1753),0.7979492,Enable NeptuneLogger to work with distributed_backend=ddp (#1753),,1
Fixed error message and test docstring (#1698),0.55499476,Removed no return warning from val/test step (#6139),training_dataloader -> train_dataloader Co-authored-by: Alexander Kreuzer alexander.kreuzer@sap.com,0
Group argument wandb (#1760),0.43363863,1. Add the arguments you need,  group argument wandb   formatting fix ,0
Tests: refactor cleanup (#1744),0.5845146,"Refactored training_batch + tests to verify correctness (#2327, #2328)",  wip   cleaning   optim imports   -   default hparams   fix restore   fix imports ,0
Feature: auto scale batch size (#1638),0.68333066,tuner.scale_batch_size(...),  auto batch finder   fix styling   add description   add different modes   fix copy paste error   better organised code   fix styling   add tests   fix   fix   add some documentation   added CHANGELOG.md   some documentation   update based on review   Update trainer.py   Update docs/source/training_tricks.rst   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update tests/trainer/test_trainer_tricks.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update tests/trainer/test_trainer_tricks.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Apply suggestions from code review  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   use EvalModelTemplate   param tests   rename   wrap params   rename function   rename   rename param   fix   abs   rename   refactor code   add docs   try   arg   loop   exept   loop   drop bool   docs   docs   added check and test for passing dataloader to fit   styling fix   update based on review   Co-authored-by: Nicki Skafte nugginea@gmail.com Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz,0
Also update progress_bar in training_epoch_end (#1724),0.86315143,The progress bar metrics now also get updated in training_epoch_end (#1724),  update prog. bar metrics on train epoch end   changelog   wip test   more thorough testing   comments   update docs   move test   Co-authored-by: Jirka jirka.borovec@seznam.cz,1
added warning for None dataloader (#1745),0.6780959,"Raised MisconfigurationException when total length of dataloader across ranks is zero, and give warning when total length is non-zero, but only local rank length is zero. (#9827)",  added warning for None dataloader   fixed variable style   updated warning message   remove unused import   Co-authored-by: ybrovman ybrovman@ebay.com,0
Fix typo (#1750),0.5251856,"In addition, we fixed:",,0
lr_finder: Fix typo in docstring (#1746),0.6254439,move lr_finder (#3434),,0
Attach version_ to checkpoint path only if version is int (#1748),0.61872244,Removed deprecated checkpoint argument filepath (#5321),,0
Mock packages for RTD docs build (follow up to doctests) (#1739),0.40225285,- Full tests that test specific functionality in trainer.,  mock all packages on RTD   update ,0
fix _reset_eval_dataloader() for IterableDataset (#1560),0.5949352,Disabled sampler replacement when using IterableDataset (#11507),  removed if dl from _reset_eval_dataloader()   changed to if dl != None to be more safe   hints from pep8speaks   Co-authored-by: ybrovman ybrovman@ebay.com,0
improve pickle tests for callbacks (#1717),0.52281004,  callbacks:,  improve pickle tests for callbacks   set mode dict as a class attr ,0
complete test (#1705),0.48611152,Tested pickling (#1636),,0
Fixing logic (#1734),0.47885135,refactored inner eval loop (#3141),,0
Tests: refactor trainer dataloaders (#1690),0.7090222,"Standardized the dataloaders arguments of trainer.{fit,valdiate,test,tune} (#7431)",  refactor default model   drop redundant seeds   refactor dataloaders tests   fix multiple   fix conf   flake8   Apply suggestions from code review   Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: William Falcon waf2107@columbia.edu,1
doctest for .rst files (#1511),0.38294327,Docs improvements,"  add doctest to circleci   Revert ""add doctest to circleci""   This reverts commit c45b34ea911a81f87989f6c3a832b1e8d8c471c6.  Revert ""Revert ""add doctest to circleci""""  This reverts commit 41fca97fdcfe1cf4f6bdb3bbba75d25fa3b11f70.   doctest docs rst files   Revert ""doctest docs rst files""   This reverts commit b4a2e83e3da5ed1909de500ec14b6b614527c07f.   doctest only rst   doctest debugging.rst   doctest apex   doctest callbacks   doctest early stopping   doctest for child modules   doctest experiment reporting   indentation   doctest fast training   doctest for hyperparams   doctests for lr_finder   doctests multi-gpu   more doctest   make doctest drone   fix label build error   update fast training   update invalid imports   fix problem with int device count   rebase stuff   wip   wip   wip   intro guide   add missing code block   circleci   logger import for doctest   test if doctest runs on drone   fix mnist download   also run install deps for building docs   install cmake   try sudo   hide output   try pip stuff   try to mock horovod   Tranfer -> Transfer   add torchvision to extras   revert pip stuff   mlflow file location   do not mock torch   torchvision   drone extra req.   try higher sphinx version   Revert ""try higher sphinx version""   This reverts commit 490ac28e46d6fd52352640dfdf0d765befa56988.   try coverage command   try coverage command   try undoc flag   newline   undo drone   report coverage   review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   remove torchvision from extras   skip tests only if torchvision not available   fix testoutput torchvision   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",0
Move generated RST files to subfolder (#1555),0.35633475,move lr_finder (#3434),  move generated files to subfolder   remove if exists   reformat argv   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   update rebase   rebase yml   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: J. Borovec jirka.borovec@seznam.cz,0
Tests: refactor callbacks (#1688),0.6339773,Removed callback metrics from test results obj (#2994),  refactor default model   drop redundant seeds   path   refactor callback tests   update   fix sch   wip   fix return   review ,0
Tests: refactor trainer (#1728),0.7648473,trainer.test(...),  lr   optim   wip   wip   fix mean   flake8 ,1
Fix disabling progress bar on non-zero ranks using Horovod backend (#1709),0.5571422,Changed progress_bar_refresh_rate trainer flag to disable progress bar when setting to 0. (#1108),  Fix Horovod backend to disable progress bar on all ranks except 0   Add join barriers   Added changelog   Make protected and add verbosity   Refactor to disable progress bar callback in train   Removed vebose setting   Add cache check for Horovod   Test run again   Updated comment   Always skip cache for Horovod   Only reinstall when necessary   Added separate step   Fixed spacing   Skip Python 3.8 ,0
Fix example argument parser in docs (#1692),0.6111331,The brittle argument parsing utilities (#16708),parser.parse_known_args() actually returns a tuple of the Namespace of known args and a list of unknown args. We only want the former.,0
Bugfix/lr finder (#1676),0.64718485,move lr_finder (#3434),  fix early stopping bug   allow val dataloader   update CHANGELOG.md   fix early stopping bug   allow val dataloader   update CHANGELOG.md   Co-authored-by: Nicki Skafte nugginea@gmail.com,0
Tests: refactor models (#1691),0.6703007,Refactor Model backward (#2276),  refactor default model   drop redundant seeds   drop redundant seeds   refactor models tests   refactor models tests   imports   fix conf   Apply suggestions from code review ,0
Update type hints for multiple dataloaders in .fit() and .test() (#1723),0.59796417,"Standardized the dataloaders arguments of trainer.{fit,valdiate,test,tune} (#7431)",  update typehints   change log ,0
Tests: refactor loggers (#1689),0.77619284,Cleaning up stale logger tests (#3490),  refactor default model   drop redundant seeds   path   refactor loggers tests   imports ,1
specify cache matrix (#1725),0.3824482,"Deprecated the use of Trainer(gpus=""i"") (string) for selecting the i-th GPU; from v1.5 this will set the number of GPUs instead of the index (#6388)",,0
refactor trainer checks (#1651),0.69521785,training forward refactor (#3134),  refactor trainer checks   opt   none   Apply suggestions from code review   imports   fix tensors ,0
[WIP] Reduction when batch size < num gpus (#1609),0.88362956,Reduction when batch_size < num_gpus (#1609),  reduce if <= num_gpus   add test with explanation   chlog   fix changelog   Co-authored-by: J. Borovec jirka.borovec@seznam.cz,1
Transfer learning example (#1564),0.52917147,Refactored training loop (#2336),  Fine tuning example.   Fix (in train method) + Borda's comments (added argparse + fixed docstrings).   Updated CHANGELOG.md   Fix + updated docstring.   Fixes (awaelchli's comments) + docstrings.   Fix train/val loss.   Fix. ,0
Fix typo in progress bar docs (#1680),0.5865021,- Renamed `ProgressBarBase` to `ProgressBar` ([#17058](https://github.com/Lightning-AI/lightning/pull/17058)),  fix typo   Typo   typo Borda   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
wandb logger 'global_step' affects other logger (#1492),0.6896161,WandbLogger does not force wandb reinit arg to True anymore and creates a run only when needed (#4648),"  Removed unnecessary 'global_step' from wandb logger.   Fixed wrong step implementation in wandb and missing metric skipping in logger base.   simplified metric check in base logger   Added Fix Description in CHANGELOG.md   Updated wandb logger tests.   udpate test, step=3   Moved Fix Description in CHANGELOG.md to unreleased.   Update CHANGELOG.md   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",0
Change lightning module params to dict when loading (#1639),0.553452,- `DataParallelStrategy.get_module_state_dict()` and `DDPStrategy.get_module_state_dict()` now correctly extracts the state dict without keys prefixed with 'module' ([#16487](https://github.com/Lightning-AI/lightning/pull/16487)),  change module params to dict   tiny change   reverse ,0
fix LightningTemplateModel (#1577),0.7001418,Updated LightningTemplateModel to look more like Colab example (#1577),  fix LightningTemplateModel   update CHANGELOG.md   update LightningTemplate   update changelog   update changelog   loss fix ,1
Docker release (#1613),0.44731346,Remove unnecessary intermediate layers in Dockerfiles (#5697),  Update docker_builds.yml   Update docker_builds.yml   nightly   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
refactor default model (#1652),0.80987906,Refactor Model backward (#2276),  refactor default model   drop redundant seeds   formatting   path   formatting   rename ,1
Trigger automatic rebase on issue comment (#1695),0.31370535,Allow easy trainer re-instantiation (#7508)," Trigger automatic rebase on issue comment  Instead of pull_request event (created, closed, etc.). Fixes https://github.com/cirrus-actions/rebase/issues/43  Removed workaround",0
Fix Horovod distributed backend to set the root_gpu property (#1669),0.58526754,"refactored Horovod backend (#3121, #3122)",  params   drop acc   Fix Horovod distributed backend to set the root_gpu   Fixed test   Fixed tests   Fixed lint   Set root_gpu during initialization   chlog   Co-authored-by: Jirka jirka.borovec@seznam.cz,0
move unnecessary dict trainer_options (#1469),0.5577582,Removed obsolete self._device in Trainer (#1849),  move unnecessary dict trainer_options   fix tests   fix tests   formatting   missing ,0
fixing LBFGS test (#1678),0.45007944,Remove beta arg from F1 class and functional (#5076),  params   drop acc   acc ,0
Learning rate log callback (#1498),0.69166005,Learning Rate Finder (#13802),  base implementation   docs + implementation   fix styling   add lr string   renaming   CHANGELOG.md   add tests   Apply suggestions from code review   Co-Authored-By: Adrian Wälchli aedu.waelchli@gmail.com   Apply suggestions from code review   Update pytorch_lightning/callbacks/lr_logger.py   Update pytorch_lightning/callbacks/lr_logger.py   add test for naming   base implementation   docs + implementation   fix styling   add lr string   renaming   CHANGELOG.md   add tests   Apply suggestions from code review   Co-Authored-By: Adrian Wälchli aedu.waelchli@gmail.com   Apply suggestions from code review   Update pytorch_lightning/callbacks/lr_logger.py   Update pytorch_lightning/callbacks/lr_logger.py   add test for naming   Update pytorch_lightning/callbacks/lr_logger.py   Co-Authored-By: Adrian Wälchli aedu.waelchli@gmail.com   suggestions from code review   fix styling   rebase   fix tests   Co-authored-by: Nicki Skafte nugginea@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
Don't convert namedtuple to tuple (#1589),0.61067635,Don't convert namedtuple to tuple when transferring the batch to target device (#1589),  Don't convert namedtuple to tuple   Test namedtuples sent to device correctly ,0
added warning to crash (#1625),0.54709846,Deprecation warning (#3844),  added warning to crash   formatting   Co-authored-by: J. Borovec jirka.borovec@seznam.cz,0
Update new-project.rst (#1655),0.42623413,Set version as today (#13906),fix a typo,0
Add log output for slurm (#1657),0.5588639,Read more about our SLURM integration here.,  add log output for slurm   change log levels   formatting   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
call on_load_checkpoint() when resuming from checkpoint (#1666),0.8189779,"-def on_load_checkpoint(self, checkpoint):",,1
Fixed broken link in PR template (#1675),0.49654478,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",  Fixed broken link in PR template.   Updated CHANGELOG.md ,0
None check for filepath in ModelCheckpoint (#1654),0.84261715,Deprecated filepath in ModelCheckpoint (#4213),Check if the optional filepath is None before checking if it exists Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,1
Update docs (#1656),0.55053407,Docs improvements, edit doc  mentioned in #646   edit doc   underline   class reference   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
changelog (#1643),0.66008765,Full Changelog,,0
Merge pull request #1645 from danpcampbell/bugfix/1637_slurm-fix,0.61787546,Fix an issue with the SLURM srun detection causing permission errors (#15485),Only call load_spawn_weights if COLAB_GPU or KAGGLE_URL_BASE,0
Only call load_spawn_weights if COLAB_GPU or KAGGLE_URL_BASE,0.5255659,"Disabled strict loading in multiprocessing launcher (""ddp_spawn"", etc.) when loading weights back into the main process (#16365)",environment variables are set,0
Merge pull request #1642 from PyTorchLightning/new-master,0.61333907,Full commit list: https://github.com/PyTorchLightning/pytorch-lightning/compare/1.5.0...1.6.0,Propper master...,0
Merge branch 'master' into new-master,0.3740939,"merge backends (#3476, #3477, #3478, #3480, #3482)",,0
releasing,0.59107316,Generalization release,,0
test pickling (#1636),0.96983534,Tested pickling (#1636),fix hparams issue cache cache cache cache ddp pickle ddp pickle ddp pickle ddp pickle ddp pickle ddp pickle ddp pickle ddp fix pep8,1
Fixes CPU DDP breaking change and DDP change (#1635),0.6366874,Silenced some warnings. verified ddp refactors (#3483),ddp pickle ddp pickle ddp pickle ddp pickle ddp pickle ddp pickle ddp pickle,0
Allow metrics logged together with hparams (#1630),1.0,Allow metrics logged together with hparams (#1630),Update tensorboard.py Update CHANGELOG.md Update tensorboard.py Update test_tensorboard.py Update test_tensorboard.py tests pep8,1
Fix ModelCheckpoint not being fixable (#1632),0.71152484,"Removed deprecated ModelCheckpoint arguments prefix, mode=""auto"" (#6162)",,1
Remove warning (#1634),0.6476027,Removed Warning from trainer loop (#1634),Remove warning Update CHANGELOG.md,0
fix hparams issue (#1623),0.60116637,Fixing critical bugs in newly added hooks and hparams assignment.,fix hparams issue fix hparams issue,0
"try GH actions cache (#1558, #1624)",0.40437207,Changed overwrite to True (#16009),spec cache spec cache trigger extras revert refactor cache cache cache cache,0
changelog (#1616),0.68700594,Full Changelog,  changelog   warning   pull   typo   typo ,0
pin actions/checkout version to v2 (#1617),0.4013213,Set version as today (#13906),,0
Merge pull request #1636 from PyTorchLightning/callback,0.5958906,Full commit list: https://github.com/PyTorchLightning/pytorch-lightning/compare/1.5.0...1.6.0,test pickling,0
pep8,0.31853294,py,,0
ddp fix,0.6887178,DDP Debugging Improvements,,0
ddp pickle,0.52717793,Tested pickling (#1636),,0
Merge pull request #1624 from PyTorchLightning/tests/cache,0.61335266,Full commit list: https://github.com/PyTorchLightning/pytorch-lightning/compare/1.5.0...1.6.0,resume GH action,0
Merge pull request #1635 from PyTorchLightning/pkl,0.60239595,Full commit list: https://github.com/PyTorchLightning/pytorch-lightning/compare/1.6.0...1.7.0,Fixes CPU DDP breaking change and DDP change,0
Merge pull request #1630 from PyTorchLightning/hparams_logger,0.5753162,- Deprecated `LightningLoggerBase.agg_and_log_metrics` in favor of `LightningLoggerBase.log_metrics` ([#11832](https://github.com/PyTorchLightning/pytorch-lightning/pull/11832)),Allow metrics logged together with hparams,0
Merge pull request #1632 from quinor/patch-1,0.4504759,"merge backends (#3476, #3477, #3478, #3480, #3482)",Fix ModelCheckpoint not being picklable.,0
Merge pull request #1634 from PyTorchLightning/remove_warning,0.64513636,Removed deprecated code in pytorch_lightning.utilities.meta (#16038),Remove warning,0
Update CHANGELOG.md,0.6989172,Full Changelog,,0
Remove warning,0.6513011,warning_utils >> warnings,,0
Fix ModelCheckpoint not being fixable.,0.681201,"Removed deprecated ModelCheckpoint arguments prefix, mode=""auto"" (#6162)",,0
tests pep8,0.45078483,Fully tested!,,0
Update test_tensorboard.py,0.67391145,Improved the error message for installing tensorboard or tensorboardx (#17053),,0
Update tensorboard.py,0.7391108,Improved the error message for installing tensorboard or tensorboardx (#17053),,1
cache,0.38238853,loading,,0
refactor,0.8787566,Refactoring,,1
Merge pull request #1623 from PyTorchLightning/hparams,0.58981013,Full commit list: https://github.com/PyTorchLightning/pytorch-lightning/compare/1.6.0...1.7.0,Hparams,0
revert,0.6132324,    return,,0
fix hparams issue,0.5986594,    hparams_file='/path/to/hparams_file.yaml',,0
try GH actions cache (#1558),0.38801575,Changed overwrite to True (#16009),  spec cache   spec cache   trigger   extras   checkout ,0
checkout,0.30244136,Now:,,0
extras,0.45701522,Highlights,,0
trigger,0.30856073,NOTE,,0
spec cache,0.43462503,Speed/memory optimizations.,,0
Merge branch 'master' of https://github.com/williamFalcon/pytorch-lightning,0.6731251,Full commit list: https://github.com/PyTorchLightning/pytorch-lightning/compare/1.5.0...1.6.0,,0
fixed warning,0.627866,warning_utils >> warnings,,0
fixed val_loss for early stopping,0.59491974,Checkpoint and early stopping now work without val. step (#1041),,0
docs clean up,0.595743,Docs,,0
clean up docs (#1614),0.57687795,Docs improvements,  fixed hparams section   docs clean up ,0
Clean up Argparse interface with trainer (#1606),0.81682694,Improved Argparse usability with Trainer,  fixed distutil parsing   fixed distutil parsing   Apply suggestions from code review   log   fixed distutil parsing   fixed distutil parsing   fixed distutil parsing   fixed distutil parsing   doctest   fixed hparams section   fixed hparams section   fixed hparams section   formatting   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: J. Borovec jirka.borovec@seznam.cz,1
Create GH action for automated docker builds on releases (#1559),0.5000766,"Show a message when BuildConfig(dockerfile=""..."") is passed but a Dockerfile file is already present in the Work (#15799)",  Create docker_builds.yml   Update docker_builds.yml   Update docker_builds.yml   Update docker_builds.yml   Update docker_builds.yml ,0
diable val and test shuffling (#1600),0.79326594,Disabled val and test shuffling (#1600),  diable val and test shuffling   diable val and test shuffling   diable val and test shuffling   diable val and test shuffling   log   condition   shuffle   refactor   Co-authored-by: J. Borovec jirka.borovec@seznam.cz,1
slurm job id (#1605),0.67698884,# use slurm job id for the port number,,0
Create Dockerfile (#1569),0.534844,Remove unnecessary intermediate layers in Dockerfiles (#5697),  Create Dockerfile   add readme   Update MANIFEST.in   Update Dockerfile   Co-authored-by: J. Borovec jirka.borovec@seznam.cz,0
Clean docs (#1604),0.61964774,Docs improvements,  spacing   slurm docs ,0
multi processing warnings (#1602),0.53870285,Updated Multinode Warning (#16091),  multi processing warnings   multi processing warnings   multi processing warnings   multi processing warnings   multi processing warnings   multi processing warnings ,0
model checkpint on rank_zero_only & global rank state (#1408),0.5425525,Rank-zero only EarlyStopping messages,  try delete in async or DDP us0-ecase   changelog   add model chekpoint rank   simple delete   flake8   use global rank   chnagelog   fix review   fix import   proposal   proposal   proposal   improve proposal (fix problems with method call self)   cleaning   Co-authored-by: Adrian Wälchli adrian.waelchli@students.unibe.ch Co-authored-by: William Falcon waf2107@columbia.edu,0
fixed dataset stuff + docs (#1599),0.4937852,Docs improvements, Fixed dataset docs and disabled auto-sampler for iterable dataset,0
update contributers list (#1597),0.48529783,"If we forgot someone due to not matching the commit email with the GitHub account, let us know :]",,0
fix depreated call (#1596),0.7861456,Removed deprecated callbacks (#3979),  fix parity   update deprecated call ,1
missing change (#1591),0.51890844,Changed,,0
fix(wandb): allow use of sweeps (#1512),0.8145834,Allow use of sweeps with WandbLogger (#1512), fix(wandb): allow use of sweeps  overwrite run config parameters due to precision error fix #1290   docs(wandb): update changelog   test(wandb): update config test   Co-authored-by: William Falcon waf2107@columbia.edu,1
fixed new amp bugs (#1593),0.62236834,enable_pl_optimizer=False by default to temporarily fix AMP issues (#5163),,0
Fixes automatic parser bug (#1585),0.52979434,Syntax changes are: ,  fixes gpu parsing   fixes gpu parsing ,0
Progress bar callback (#1450),0.7246598,progress bar, squash and rebase  sanity check hooks sanity check callback hook finish moved core progress bar functionality into callback wip remove duplicate merge clean up imports docs sanity check progress bar main sanity move callback calls init progrss bar callback configuration and docs changelog rate decorator pass process_position disable on rank > 0 position index is_enabled remove decorator refactor init tqdm bars callback method ordering  cannot reset when disabled sequence -> list default values fix has no attr _time()  move on_val_end to proper place fix the pickle issue update warning properties check for None remove old comment switch order pull out non-tqdm functionality into base class documentation for the base class docs fix refresh rate issue in validation restrict type hint of trainer arg more docs update trainer docs rst docs fix lines too long fix test add missing type hints fix typo move docstring to init solves doctest failures remove doctest :(( can't fix the pickle error fix example simplify by saving trainer reference fix docs errors move docstring initial value multiple val checks per epoch simpler handling of inf dataset sizes update inf docs renamed training_tqdm_dict rename get_tqdm_dict rename occurences of tqdm  update changelog fix doctest fix formatting errors added callback tests progress bar on off test more tests for progress bar weird test fix? add ignored property disable default progress bar in LR finder change enable/disable behavior trying doctest in CI again undo doctest pickle error undo doctest pickle error :(( remove progress_bar_callback Trainer arg and fix tests restore progress bar after auto lr find update docs fix rebase fix wrong negation   fix fast dev run total   more thorough testing   remove old args   fix merge   fix merge   separate tests   type hint total batches   reduce if   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  is_disabled  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  is_enabled  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   rename enabled/disabled   move deprecated api   remove duplicated test from merge   fix rename is_disabled   newline   test also testprogress for fast dev run   Co-authored-by: J. Borovec jirka.borovec@seznam.cz Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
Fixing a small issue in trainer logging  (#1563),0.7402479,Rename Trainer arguments row_log_interval >> log_every_n_steps and log_save_interval >> flush_logs_every_n_steps (#3748),"  The epoch was being logged to metrics, which isn't read, rather than to current_metrics.   Updated the tests to account for the epoch arriving at the logger. ",1
test deprecation warnings (#1470),0.7606492,Deprecation warning (#3844),  check deprecation warnings   extend warning test   try   unimport modules   update ,1
Nested metrics dictionaries now can be passed to the loggers (#1582),0.6442045,Integrated metrics API with self.log (#3961),  now func merge_dicts works with nested dictionaries   CHANGELOG.md upd ,0
fix changelog (#1583),0.594229,"In addition, we fixed:",,0
Amp2 (#1580),0.5102557,        :param amp:,  fixed new amp bugs   fixed new amp bugs ,0
why copy? (#1579),0.3350778,(#16002),,0
support for native amp (#1561),0.5163824,Trainer now raises an exception when requesting amp_level with native amp_backend (#9755),  adding native amp suppport   adding native amp suppport   adding native amp suppport   adding native amp suppport   autocast   autocast   autocast   autocast   autocast   autocast   removed comments   removed comments   added state saving   added state saving   try install amp again   added state saving   drop Apex reinstall   Co-authored-by: J. Borovec jirka.borovec@seznam.cz Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Don't copy the batch when training on a single gpu (#1576),0.67219716,"where you can use the outputs of training_step (performed on each GPU with a portion of the batch),",  fix   whitespace   Co-authored-by: Josh Karlin karlinjf@gmail.com,0
Tests/docker (#1573),0.4556194,Remove unnecessary intermediate layers in Dockerfiles (#5697),  devel image   try parallel   new image ,0
Default value for ModelCheckpoint filepath (#1548),0.8182292,Deprecated filepath in ModelCheckpoint (#4213),  allow determine of filepath at runtime   typing   Co-authored-by: Nicki Skafte nugginea@gmail.com,1
fix boolean argparse (#1571),0.62308097,import argparse,  fix boolean argparse #1570   update change log ,0
check for kaggle env variable (#1568),0.4661628,- Deprecated the `on_colab_kaggle` function ([#14247](https://github.com/Lightning-AI/lightning/pull/14247)),  check for kaggle env variable   added changelog ,0
Added Horovod distributed backend (#1529),0.65847,"refactored Horovod backend (#3121, #3122)",  Initial commit of Horovod distributed backend implementation   Update distrib_data_parallel.py   Update distrib_data_parallel.py   Update tests/models/test_horovod.py   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update tests/models/test_horovod.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   Fixed tests   Added six   tests   Install tox for GitHub CI   Retry tests   Catch all exceptions   Skip cache   Remove tox   Restore pip cache   Remove the cache   Restore pip cache   Remove AMP   Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: J. Borovec jirka.borovec@seznam.cz,0
tests for pytorch 1.5 (#1552),0.6870338,PyTorch 1.5  support,  tests for pytorch 1.5   up Win   win   win   win   win   win   win ,0
default test logger (#1478),0.71644294,Change default logger to a dedicated one (#1064),  default test logger   fix tests   spawn   try   simplify tests   simplify tests   formatting   loggers   loggers   revert to TestTube   default   default   wraps   world size   optim imports ,1
Replace GPU device idx with current process index (#1541),0.6088967,"Deprecated the use of Trainer(gpus=""i"") (string) for selecting the i-th GPU; from v1.5 this will set the number of GPUs instead of the index (#6388)",,0
Revert namespace package search to normal package search (#1545),0.45387033,Re-enabled naming metrics in ckpt name (#3060),  Revert this   typos   version++   Co-authored-by: J. Borovec jirka.borovec@seznam.cz,0
Prepare Namespace package (#1543),0.4952049,"    name=""my-package"",",  Update init.py   Update setup.py ,0
fix changelog (#1452),0.60812217,"In addition, we fixed:",  fix changelog   formatting   add ddp_cpu   docs   add another ,0
Update learning rate on each backward pass instead of each forward pass. (#1477),0.6229327,Tune learning rate,  change lr scheduler step interval to update every backwards pass instead of every forwards pass   update CHANGELOG   fix spacing   Add TODO to lr schedule update   remove trailing whitespace   Co-authored-by: William Falcon waf2107@columbia.edu,0
Fix callback default (horror bug!) (#1534),0.7122443,Removed deprecated callbacks (#3979),  fix horror bug   update changelog   fix doctest   liine too long ,1
skip warning test (#1533),0.6194852,Removed no return warning from val/test step (#6139),,0
Add SLURM check in ddp_train() and init_ddp_connection() (#1387),0.6462164,separate SLURM from DDP (#3809),  slurm check in ddp_train and init_ddp_connection   Remove code example in init_ddp_connection   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  remove blank line  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  improve for test coverage  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   update changelog   Default values and warnings for DDP env variables   fix merge artifacts   update localhost value   change to NODE_RANK   Co-authored-by: Alexander Reshytko areshytko@Alexanders-MacBook-Pro.local Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: William Falcon waf2107@columbia.edu,0
CI: Docs build preview in each PR (#1494),0.4279855,The point of this release is more bug fixes ahead of v 1.0.0. We now have CI tests on TPU thanks to @zcain117 from Google! :slightly_smiling_face:,  store html artifacts in circle ci   add note to contributing.md ,0
DDP sampler (#1513),0.60770553,DDP(2) backend (#2796),  Add explicit flag for ddp sampler replacement   Add flag for sampler replacement in ddp   Update data_loading.py   Update CHANGELOG.md   pep8 fixes   pep8 ,0
fixed memory leak from opt return (#1528),0.7551891,Resolve memory leak for evaluation (#6326),  fixed memory leak from opt return   fixed memory leak from opt return   fixed memory leak from opt return   fixed memory leak from opt return   fixed memory leak from opt return   fixed memory leak from opt return   fixed memory leak from opt return   fixed memory leak from opt return   fixed memory leak from opt return   fixed memory leak from opt return   fixed memory leak from opt return   fixed memory leak from opt return   fixed memory leak from opt return   fixed memory leak from opt return   fixed memory leak from opt return   fixed memory leak from opt return   fixed memory leak from opt return   fixed memory leak from opt return   fixed memory leak from opt return ,1
Fix Mixing hparams and arguments in LightningModule (#1505),0.786106,Allow passing hparams as a keyword argument to LightningModule when loading from checkpoint (#1639),"  Attempt to fix #1468   Remove the if statement, it doesn't actually make any difference   Update docs   Correct warnings I caused in the last commit   Add to changelog   Actually add to changelog   Clarify documentation and examples   Update CHANGELOG.md   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",1
Removed redundant computations in clip_gradients that slowed down the gradient clipping. (#1523),0.72058326,    gradient_clip_algorithm,Fixes #1522,1
Allow Trainer's gpus arg type to be subclass of currently accepted types (#1423),0.68929356,  * Removed the `Trainer(gpus=...)` argument, Fixed Trainer gpus arg type issue  Fixes #1388  Disallow boolean gpus parameter  Co-Authored-By: Adrian Wälchli aedu.waelchli@gmail.com  Fixed missing paranthesis  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
add QA to docs (#1374),0.5728008,Docs improvements,  add QA to docs   not about doc updates by @awaelchli   Apply suggestions from code review   Co-Authored-By: Adrian Wälchli adrian.waelchli@students.unibe.ch  help  Co-authored-by: Adrian Wälchli adrian.waelchli@students.unibe.ch,0
change name (#1519),0.57392097,| Old name                     | New name                       |,,0
feat: save checkpoint before deleting old ones (#1453),0.71415764,    # put all logic related to deleting a checkpoint here,  feat: save checkpoint before deleting old ones   fix: make sure that the new model is not deleted   changelog   Co-authored-by: J. Borovec jirka.borovec@seznam.cz Co-authored-by: William Falcon waf2107@columbia.edu,1
Improved docs for pytorch_lightning.core (continued) (#1483),0.792078,  * ([#10403](https://github.com/PyTorchLightning/pytorch-lightning/pull/10403)), improved docs for core  update links add references to hooks lifecycle wip continue with init.py improve docs for memory.py improve docs for saving.py simpler links fix formatting   move hooks lifecycle to top of file   fix doctest import problem   add missing hook in lifecycle ,1
Improved docs for Loggers (#1484),0.7663509,Refactored logging,"  improve init   improve logger base   improve comet logger docs   improved docs for mlflow   improved nepune logger docs   fix matplotlib import issue   improve tensorboard docs   improve docs for test tube   improved trains logger docs   improve wandb logger docs   improved docs in experiment_logging.rst   added MLflow to the list of loggers   fix too long lines   fix trains doctest   fix neptune doctest   fix mlflow doctest   Apply suggestions from code review   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   Apply suggestions from code review   fix whitespace   try bypass mode for neptune (fix doctest api key error)   try ""test"" as api key   Revert ""try ""test"" as api key""   This reverts commit fd77db26d551f08b4b4a12bb93cbd8f7a0814f29.   try test as api key   update neptune docs   bump neptune minimal version   revert unnecessary bypass code   test if CI runs doctests in .rst files   Revert ""test if CI runs doctests in .rst files""   This reverts commit a45aeb460a8c4b7445a35dd7b49265f48d11c485.   add doctest directive   neptune demo links   added tutorial link for W&B   fix line too long   fix merge error   fix merge error   add instructions how to install loggers   add instructions how to install the loggers   hide _abc_impl property from docs   review Borda, 4 spaces   indentation in example sections   blank   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",1
Call on_before_zero_grad model hook (#1493),0.55387133,The default value of accumulate_grad_batches has changed from 1 to None (#9652).,  call on_before_zero_grad   update changelog   add note about overriding both hooks   added test   move test_hooks.py to models folder ,0
feat(semseg): allow model customization (#1371),0.53366107,Direct support for compiled models (#15922),  feat(semantic_segmentation): allow customization of unet   feat(semseg): allow model customization   style(semseg): format to PEP8   fix(semseg): rename logger   docs(changelog): updated semantic segmentation example   suggestions   suggestions   flake8   Co-authored-by: J. Borovec jirka.borovec@seznam.cz,0
Add ddp_cpu backend for testing ddp without GPUs (#1158),0.7414612,Made DDP the default if no backend specified with multiple GPUs (#1789),"  Add tests for distributed backend config   Refactor set_distributed_mode   Use gloo backend on cpu   Use 127.0.0.1 instead of 127.0.0.2   Not totally clear on why this is necessary, but it seemt to work   Update LightningDDP so that it works with CPU   Add ddp_cpu backend and num_processes Trainer arg   PEP8   Fix test skipping. Inequalities are hard :/   Skip ddp_cpu test on Windows   Make a few more cases fall back to ddp_cpu   New function name   Flake8   Don't test distributed on MacOS with torch < 1.3   Support for distributed in MacOS was added in Torch 1.3.0   Add ddp_cpu and num_processes to docs   Parametrize trainer config tests   Tweak warning   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   Remove redundant test   Replace pass branches with comments   Add missing warnings import   save_path -> root_dir   Use new rank_zero_warn   Whitespace   Apply suggestions from code review   formatting   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: J. Borovec jirka.borovec@seznam.cz",1
Remove error when test dataloader used in test (#1495),0.63094807,"Raised MisconfigurationException when total length of dataloader across ranks is zero, and give warning when total length is non-zero, but only local rank length is zero. (#9827)",  remove error when test dataloader used in test   remove error when test dataloader used in test   remove error when test dataloader used in test   remove error when test dataloader used in test   remove error when test dataloader used in test   remove error when test dataloader used in test   fix lost model reference   remove error when test dataloader used in test   fix lost model reference   moved optimizer types   moved optimizer types   moved optimizer types   moved optimizer types   moved optimizer types   moved optimizer types   moved optimizer types   moved optimizer types   added tests for warning   fix lost model reference   fix lost model reference   added tests for warning   added tests for warning   refactoring   refactoring   fix imports   refactoring   fix imports   refactoring   fix tests   fix mnist   flake8   review   Co-authored-by: J. Borovec jirka.borovec@seznam.cz,0
neptune online (#1499),0.49262595,Changed to the NeptuneLogger (#16761):,,0
fix flushing loggers (#1459),0.6352512,DDP + loggers should be fixed,  flushing loggers   flushing loggers   flushing loggers   flushing loggers   changelog   typo   fix trains   optimize imports   add logger test all   add logger test pickle   flake8   fix benchmark   hanging loggers   try   del   all   cleaning ,0
attempting to remove some speed issues (#1482),0.49520448,"Updated DeepSpeed ZeRO (#6546, #6752, #6142, #6321)",  removed some .items   added speed tests   added speed tests   Update benchmarks/test_rnn_parity.py   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update benchmarks/test_trainer_parity.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   fix lost model reference   added speed tests   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Merge pull request #1473 from aiyolo/patch-3,0.4503535,Pkg: fix parsing versions by @Borda in https://github.com/Lightning-AI/lightning/pull/15401,docs typo,0
Replace automatic nan check with optional flag (#1475),0.55973595,Changed the default behaviour to no longer include a NaN check with each training iteration. (#1475),  Replace automatic nan check with optional flag   Update CHANGELOG.md ,0
typing error,0.427473,Made type hints public (#17100),it should be val_batch here.,0
fix lr scheduler docs (#1446),0.68712974,lr_scheduler now activated after epoch    ,Co-authored-by: Nicki Skafte nugginea@gmail.com,0
Learning Rate finder (#1347),0.99101424,Learning Rate Finder (#13802),"  initial structure   rebase   incorporate suggestions   update CHANGELOG.md   initial docs   fixes based on reviews   added trainer arg   update docs   added saving/restore of model state   initial tests   fix styling   added more tests   fix docs, backward compatility and progressbar   fix styling   docs update   updates based on review   changed saving to standard functions   consistent naming   fix formatting   improve docs, added support for nested fields, improve codecov   update CHANGELOG.md   Update lr_finder.rst   Update pytorch_lightning/trainer/trainer.py   Update trainer.py   Update CHANGELOG.md   Update path   restoring   test   attribs   docs   doc typo   Co-authored-by: Nicki Skafte nugginea@gmail.com Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: J. Borovec jirka.borovec@seznam.cz",1
fix deprecated default_save_path (#1449),0.5612707,Deprecated filepath in ModelCheckpoint (#4213),,0
continues develop (#1419),0.4359342,Resolved a bug where the work statuses will grow quickly and be duplicated (#13970),  continues develop   changelog   typo ,0
Fix weights path (#1445),0.58846015,- Removed the deprecated `weights_save_path` Trainer argumnent and `Trainer.weights_save_path` property ([#14424](https://github.com/Lightning-AI/lightning/pull/14424)),  renamed default path to actual root_dir   added default weights path   added default weights path   added default weights path ,0
Add automatic GPU choice to trainer (#1426),0.79747367,  * Removed the `Trainer(auto_select_gpus=...)` argument," Add automatic GPU choice to trainer  This commit adds the gpu_choice parameter to Trainer. By default, this parameter is set to 'manual' which causes no observable difference in behavior. When gpu_choice is set to ""auto"" and gpus is an int, then the trainer will automatically allocate the first available GPU. This is especially useful when GPUs are configured to be in ""exclusive mode"", which means that only one process at a time can use them.  Rename gpu_choice -> auto_select_gpus",1
Add test_dataloaders to test method (#1434),0.6463127,Removed test_dataloaders parameter from Trainer.fit() (#1434),  Add test_dataloaders to test method   Remove test_dataloaders from .fit()   Fix code comment   Fix tests   Add test_dataloaders to test method (#1393)   Fix failing tests   Update docs (#1393) ,0
"Fixed configure optimizer from dict without ""scheduler"" key (#1443)",0.63187844,"        return {""optimizer"": optimizer, ""lr_scheduler"": scheduler}","  configure_optimizer from dict with only ""optimizer"" key. bug fixed   autopep8   pep8speaks suggested fixes   CHANGELOG.md upd ",0
fix pretty print (#1441),0.4063213,switched from print to logging,  grid sample   grid sample   grid sample   grid sample   grid sample   changelog   version   Co-authored-by: J. Borovec jirka.borovec@seznam.cz,0
Fix gradient clipping (#1438),0.7326361,def configure_gradient_clipping(,  Fix gradient clipping   Relax accuracy constraint ,1
fix retruning returns (#1431),0.47899252,Moved result teardown to the loops (#8245),  returns   changelog ,0
workers warning not on windows (#1433),0.5127274,"    num_workers=10,",,0
add rank warning (#1428),0.5830646,- Removed deprecated support for passing the `rank_zero_warn` warning category positionally ([#14470](https://github.com/Lightning-AI/lightning/pull/14470)),  add rank warning   changelog   use rank_zero_warn   user trainer_init   replace warnings   fix test   flake8   docs   changelog   bug lol ,0
workers warning not on windows (#1430),0.51657516,"    num_workers=10,",,0
fixed default sampler (#1425),0.82117236,Did not interfere with a default sampler (#1318),,1
added slurm doc (#1418),0.4780861,Read more about our SLURM integration here.,  added slurm doc   added slurm doc ,0
Fix TrainsLogger doctest failing (switch to bypass mode in GitHub CI) (#1379),0.60472286,  * Removed the `LoggerConnector.on_train_split_start` method,  Fix TrainsLogger doctest failing (switch to bypass mode in GitHub CI)   fix   test ci   debug   debug CI   Fix CircleCI   Fix Any CI environment switch to bypass mode   Removed debug prints   Improve code coverage   Improve code coverage   Reverted   Improve code coverage   Test CI   test codecov   Codecov fix   remove pragma   Co-authored-by: bmartinn <>,0
Print test results only if prog_bar_metrics is not empty (#1411),0.447864,Removed callback metrics from test results obj (#2994),  Print test results only if prog_bar_metrics is not empty   Update evaluation_loop.py   Co-authored-by: vitor-guizilini vitor.guizilini@tri.global Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Loader docs (#1416),0.6471484,"Accessing dataloaders (#16726, #16800)",  added multiple loader docs   added multiple loader docs   added multiple loader docs   added multiple loader docs   added multiple loader docs   Apply suggestions from code review   added multiple loader docs   added build docs script   typo   added build docs script   added build docs script   added build docs script   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: J. Borovec jirka.borovec@seznam.cz,0
updated early stopping docs (#1410),0.60930717,Deprecate early_stop_callback Trainer argument (#3845),  remove incorrect comment in training_step   added comment for on_batch_start in hooks.py   update early stopping docs   typo fix   whitespace fix   Apply suggestions from code review   Update docs/source/early_stopping.rst   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Added accumulation of loggers' metrics for the same steps (#1278),0.6568357,Metric reduction with Logging (#5150),"  add_argparse_args method fixed (argument types added)   autopep8 fixes   --gpus=0 removed from test (for ci tests)   Update pytorch_lightning/trainer/trainer.py   Co-Authored-By: Joe Davison joe@huggingface.co   test_with_accumulate_grad_batches added   agg_and_log_metrics logic added to the base logger class   small format fix   agg metrics strategies removed (not to complicate stuff)   agg metrics: handle zero step   autopep8   changelog upd   flake fix   metrics aggregators factored out, metrics_agg.py added + tests   metrics agg default value added   Update pytorch_lightning/loggers/metrics_agg.py   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   metrics aggregators factored out, metrics_agg.py added + tests   metrics agg default value added   Update pytorch_lightning/loggers/metrics_agg.py   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   remove .item which causes sync issues (#1254)   remove .item which causes sync issues   fixed gradient acc sched   fixed gradient acc sched   test_metrics_agg.py removed (all tested in doctrings), agg metrics refactored   test_metrics_agg.py removed (all tested in doctrings), agg metrics refactored   autopep8   loggers base.py types fixed   test   test   metrics aggregation for loggers: each key now has a specific function (or default one)   metrics aggregation for loggers: each key now has a specific function (or default one)   docstrings upd   manual typehints removed from docstrings   batch_size decreased for test test_with_accumulate_grad_batches   extend running accum   refactor   fix tests   fix tests   allowed_types generator scoped   trainer.py distutils was imported twice, fixed   TensorRunningAccum refactored   TensorRunningAccum added to change log (Changed)   change log pull link added   Co-authored-by: Joe Davison joe@huggingface.co Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: J. Borovec jirka.borovec@seznam.cz",0
Temporary Fix for docs build failure (#1413),0.51640964,Release LAI docs as stable (#14250),  Update requirements.txt   fix SG typo   Co-authored-by: J. Borovec jirka.borovec@seznam.cz,0
fix SVG images (#1409),0.2933834,Removed the deprecated TrainerTrainingTricksMixin class (#8679),,0
fix missing images on pypi (#1407),0.41284862,Add missing python-multipart dependency (#17244),  formatting   fix missing image on pypi   fix pypi push ,0
release v0.7.2rc5,0.6077019,[1.7.4] - 2022-08-31,,0
release update (#1405),0.6079816,This release includes:,  exclude tests   compress image   compress image   update Manifest   update action ,0
release 0.7.2rc4 (#1402),0.6722137,[1.7.4] - 2022-08-31,  instructions for changelog   instructions for changelog   on ,0
add pypi user (#1401),0.42174637,"If we forgot somebody or you have a suggestion, find us on Discord :zap:",  add pypi user   changelog   changelog ,0
update Docs/changelog (#1398),0.63373995,Full Changelog,  update docs/changelog   fix   Co-authored-by: William Falcon waf2107@columbia.edu,0
Update optimizers.py (#1383),0.752912,    def configure_optimizers(self):,,1
Tensorboard logger check if lightning_logs directory exists (#1377),0.6981945,Changed the directory for tensorboard logging to be the same as model checkpointing (#706),  tensorboard logger version if root_dir not exist   update changelog   resolve comments   Co-authored-by: Alexander Reshytko areshytko@Alexanders-MacBook-Pro.local Co-authored-by: William Falcon waf2107@columbia.edu,0
Fix unimplemented type() on TPU (#1396),0.5189028,  * Removed the `Trainer(tpu_cores=...)` argument,  Fix unimplemented type() on TPU   Add changelog entry   Add quotation marks ,0
load_spawn_weights only in proc rank 0 (#1385),0.66212904,"Disabled strict loading in multiprocessing launcher (""ddp_spawn"", etc.) when loading weights back into the main process (#16365)",Co-authored-by: Alexander Reshytko areshytko@Alexanders-MacBook-Pro.local,0
Set precision=16 when use_amp is passed as True (#1145),0.72993803,Apex mixed precision gets replaced with AMP (#16149),  Set precision=16 when use_amp is passed as True   Update CHANGELOG.md   add use_amp to deprecated API   Update trainer.py   Update trainer.py   move the use_amp attribute to deprecated API   move use_amp deprecation back to Trainer's init   drop unsed   drop deprecated   reorder imports   typing   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: J. Borovec jirka.borovec@seznam.cz,1
Improved docs for LightningModule (#1389),0.6333905,Update the Lightning App docs (#13537),  improve docs for LightingModule   fix typos   revert a doctest   more fixes ,0
Pin sphinx version (fix for current docs build errors) (#1382),0.765472,Pinned sphinx-autodoc-typehints with <v1.15 (#11400),  Update requirements.txt   Update docs/requirements.txt   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   freeze sphinx version   Update docs/source/conf.py   Co-Authored-By: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: J. Borovec jirka.borovec@seznam.cz,1
add trainer attribute to denote if interrupted (#1368),0.68043566,    --trainer.callbacks=EarlyStopping \,  add trainer attribute to denote if interrupted   bugfix and formatting ,0
Adding Intersphinx documentation links (#1369),0.5918285,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",  added intersphinx links for base packages   remove comment ,0
fix docs on saving checkpoints manually (#1373),0.7275369,    # put all logic related to saving a checkpoint here,,1
Add warning for few workers (#1378),0.5472653,"    num_workers=10,",  Add warning for few workers   Fix style issue   Update CHANGELOG.md   Update test   formatting   formatting   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
add forgotten change logs (#1380),0.66417336,Full Changelog,  forgot change logs   more missing   more missing ,0
model_checkpoint to save all models (#1359),0.80429655,Removed the deprecated model argument from ModelCheckpoint.save_checkpoint (#8688),  model_checkpoint to save all models   changelog   rise if   Co-authored-by: jamesjjcondon jamesjjcondon@gmail.com Co-authored-by: J. Borovec jirka.borovec@seznam.cz,1
Improved docs for callbacks (#1370),0.79560995,Support for user-defined callbacks (#889 and #950),  improved docs for callbacks   class references   make doctest pass   doctests   fix lines too long   fix line too long   fix permission error in doctest   Apply suggestions from code review   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   fix doctest   fix default   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
simplify examples structure (#1247),0.71636426,Simplify the PL examples structure (shallower and more readable) (#1247),  simplify examples structure   update changelog   fix imports   rename example   rename scripts   changelog ,1
Shubhamagarwal92 master (#1349),0.4750183,"@ananthsub, @rohitgr7",  SA: for #958: set torch cuda device when finding root   SA: for #958: removing root gpu hack in trainer/evaluation_loop   SA: setting torch cuda device   comment line too long   check if root gpu exists or available   Incorporating suggestions on #1094   since root gpu returns none instead of -1 for cpu   undo changes   fixed dp memory thing   Co-authored-by: Shubham Agarwal shubhamagarwal92@gmail.com,0
generalize reinstantiation of dataloader (#1346),0.73752433,"Did not always create a DataLoader during reinstantiation, but the same type as before (if a subclass of DataLoader) (#1346)",  generalize reinstantiation of dataloader   fix condition   add test   update changelog   fix changelog   Co-authored-by: J. Borovec jirka.borovec@seznam.cz,1
added warnings to unimplemented methods (#1317),0.6907762,Gave warnings for unimplemented required lightning methods (#1317),  added warnings and removed default optimizer   opt   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Wandb bug/wandb multi (#1360),0.65558326,WandbLogger does not force wandb reinit arg to True anymore and creates a run only when needed (#4648),"  Allow reinits in sub procs   Dont create an experiment on pickle, name, or project   Comments consistency   Fix test   Apply suggestions from code review   Co-authored-by: Chris Van Pelt vanpelt@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",0
Borisdayma: fix(wandb) - fix watch method (#1361),0.57945013,Removed the deprecated sync_step argument from WandbLogger (#8763),  fix(wandb): fix watch method   rebased   Apply suggestions from code review   Co-authored-by: Boris Dayma boris.dayma@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Doc fixes (#1362),0.59574366,"In addition, we fixed:",  Doc fixes from #1357 (awaelchli's comments) + changelog.   Fix indentation.   Add blank line to fix doc build? ,0
Example docs formatting (#1364),0.49217302,Docs improvements,  update basic examples   update domain examples   reinforse -> reinforce   update full examples   update multi node examples   update examples readme   fix copy paste   fix line too long ,0
Fix fast_dev_run running validation twice (#1365),0.64446557,Updated fast_dev_run to accept integer representing num_batches (#4629),,0
Change min max gpu memory to be on their own plots (#1358),0.95556253,Changed min-max GPU memory to be on their own plots (#1358),,1
Add highlighting to the BibTeX entry in README (#1356),0.30292416,Implemented ready for components (#16129),,0
Add parity test for simple RNN (#1351),0.4725868,    self.my_rnn = ...,  Add parity test for simple RNN   Update test_rnn_parity.py   Co-authored-by: William Falcon waf2107@columbia.edu,0
Fix for incorrect run on the validation set with overwritten validation_epoch_end and test_end (#1353),0.7592082,Changed the behavior of on_epoch_start to run at the beginning of validation & test epoch (#6498),  reorder if clauses   fix wrong method overload in test   fix formatting   update change_log   fix line too long ,1
Make training_epoch_end behave like validation_epoch_end (#1357),0.988402,Made training_epoch_end behave like validation_epoch_end (#1357),  Make training_epoch_end behave like validation_epoch_end + minor fixes in docstrings.   Minor fixes (Borda's comments).   Detach tensors in batch_output (to avoid possible memory leak) + doc fix.   Co-authored-by: Jean-Baptiste SCHIRATTI jean-baptisteschiratti@MacBook-Pro-de-Jean-Baptiste.local,1
quick patch code (#1352),0.5046563,"Updated DeepSpeed ZeRO (#6546, #6752, #6142, #6321)",  quick patch   testing fix   testing fix   testing fix   testing fix   testing fix   testing fix   testing fix   testing fix   testing fix   testing fix   testing fix   testing fix   testing fix ,0
Fix docs typo (#1355),0.49874407,"In addition, we fixed:",  Fix typo   Fix typo ,0
Simplify progress bar args (#1108),0.6612029,Better progress bar (#16695),"  show progress bar dependent on refresh_rate   test progress_bar_refresh control show bar   remove show_progress_bar from other tests   borda fixes   flake8 fix   changelog update prog bar refresh rate   move show_progress_bar to deprecated 0.9 api   rm show_progress_bar references, test deprecated   Update pytorch_lightning/trainer/init.py   fix test   changelog   minor CHANGELOG.md format   Update pytorch_lightning/trainer/init.py   Update pytorch_lightning/trainer/trainer.py   Co-authored-by: Gerard Bentley gbkh2015@mymail.pomona.edu Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: J. Borovec jirka.borovec@seznam.cz",0
faster CI testing (#1323),0.49501747,We have upgrade Continues Integration to speed up the automatic testing. ,  MNIST digits   increase test acc   smaller parity   drone builds   increase GH action timeout   drone format   fix paths   drone cache   circle cache   fix test   lower nb epochs   circleCI   user orb   fix test   fix test   circle cache   circle cache   circle cache   comment caches   benchmark batch size   cache dataset   smaller dataset   smaller dataset   fix nb samples   batch size   fix test ,0
add python 3.8 testing (#915),0.6183901,"This release aims at fixing particular issues and improving the user development experience via extending docs, adding typing and supporting python 3.8. In particular, some of the release highlights are:",  add python 3.8 test   update info   py38 >> torch 1.2   skip py38 minimal   changelog   Co-authored-by: William Falcon waf2107@columbia.edu,0
Add useful errors when model is not configured correctly (#1199),0.68394923,- Added model configuration checking before it runs,  add check_model_configuration method   trying to fix errors   trying to fix tests   added test_epoch_end to lightning template   fix tests   fix new test after rebase   fix spelling   added more checks   updated formating   added tests   fixed CHANGELOG   Apply suggestions from code review   move test to new module   change check on configure_optimizers   Co-authored-by: Nicki Skafte nugginea@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Fix iterable dataset docs (#1342),0.614122,Disabled sampler replacement when using IterableDataset (#11507),,0
"Remove default optimizer, add None optimizer option (#1279)",0.7358837,Changed default behaviour of configure_optimizers to use no optimizer rather than Adam. (#1279),"  Add warning when using default optimizer   Refactor optimizer tests to test_optimizers   Remove default optimizer, add option to use no optimizer   Update CHANGELOG.md   Update pytorch_lightning/trainer/optimizers.py   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Fix style  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",1
Automatic PyPi Release (#1324),0.4358456,py,  Add Workflow for Automated pypi releases   Rename pypi_release to pypi_release.yml   Update .github/workflows/pypi_release.yml   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update .github/workflows/pypi_release.yml  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Add releases to test.pypi  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Removes need to unsqueeze from dp (#1319),0.775503,On DP and DDP2 unsqueeze is automated now (#1319),  removes need to unsqueeze from dp   removes need to unsqueeze from dp   fixed examples   added auto unsqueeze   added auto unsqueeze   added auto unsqueeze   added auto unsqueeze   Update pytorch_lightning/overrides/data_parallel.py   Co-Authored-By: Adrian Wälchli adrian.waelchli@students.unibe.ch   fixed dp parse   fixed dp parse   Co-authored-by: Adrian Wälchli adrian.waelchli@students.unibe.ch,1
feat(wandb): save models on wandb (#1339),0.48780638,Allow uploading models on W&B (#1339),  feat(wandb): save models on wandb   docs(changelog): allow to upload models on W&B ,0
fixed extra dataloader bug (#1196),0.6879521,"Did not always create a DataLoader during reinstantiation, but the same type as before (if a subclass of DataLoader) (#1346)",  fixed extra dataloader bug   Update pytorch_lightning/trainer/training_loop.py   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   updated CHANGELOG   Small non-repetition change   self.get_model() => model as it was already defined   Update CHANGELOG.md   changed argument name to reload_train_dataloader_every_epoch   fixed doc underline too short   reverted to reload_dataloaders_every_epoch   fixed val and test reloading   fixed val and test reloading   Co-authored-by: TevenLeScao teven.lescao@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Sampler (#1328),0.55121356,Did not interfere with a default sampler (#1318),  sampler   check for dataloader type   check for dataloader type   fixed sampler cases ,0
Sampler (#1318),0.55400884,Did not interfere with a default sampler (#1318),  sampler   sampler   sampler   check for dataloader type   check for dataloader type ,0
"Optimizer Frequencies logic, and new configure_optimizers (#1269)",0.70364577,- Extended optimizer support with particular frequency,"  init_optimizers accepts Dict, Sequence[Dict] and returns optimizer_frequencies. optimizer_frequencies was added as a member of Trainer.   Optimizer frequencies logic implemented in training_loop. Description added to configure_optimizers in LightningModule   optimizer frequencies tests added to test_gpu   Fixed formatting for merging PR #1269   Apply suggestions from code review   Apply suggestions from code review   Co-Authored-By: Asaf Manor 32155911+asafmanor@users.noreply.github.com   Update trainer.py   Moving get_optimizers_iterable() outside.   Update note   Apply suggestions from code review   formatting   formatting   Update CHANGELOG.md   formatting   Update CHANGELOG.md   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",1
reorder deprecated args (#1230),0.65914416,Args should come after the last positional argument (#1807),,0
Move some tests to correct subfolder/file (#1312),0.4697922,"Refactored training_batch + tests to verify correctness (#2327, #2328)",  move some tests to trainer file   fix imports ,0
Profiler summary (#1259),0.6656256,Split profilers module (#6261),"  refactor and add types   add Prorfiler summary   fix imports   Revert ""refactor and add types""   This reverts commit b4c552fa   changelog   revert rename   fix test   mute verbose ",0
fixed type hint for weights_summary arg (#1313),0.5213402,Changed the default value of the Trainer argument weights_summary from full to top (#2029),Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,0
Early stopping when validation is disabled (#1235),0.77978194,Early stopping checks on_validation_end (#1458),  early stop fallback to train epoch   added test   fix imports   update docs   update changelog   fix typo ,1
Replace Wandb callback's finalize with no-op (#1193),0.71457267,Removed wandb logger's finalize method (#1193),  Replace Wandb callback's finalize with no-op   Update pytorch_lightning/loggers/wandb.py   Update wandb.py   remove wandb logger's finalize and update tests   update changelog   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: William Falcon waf2107@columbia.edu,1
Error on zero length dataloaders (#1280),0.73454195,"Raised MisconfigurationException when total length of dataloader across ranks is zero, and give warning when total length is non-zero, but only local rank length is zero. (#9827)",  error_on_zero_length   update CHANGELOG.md   added test   Update pytorch_lightning/trainer/data_loading.py   Co-authored-by: Nicki Skafte nugginea@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
Checkpointing interval (#1272),0.65006596,    # put all logic related to saving a checkpoint here,  formatting   formatting   fix interval   fix train loop   fix test   parametrize test   Apply suggestions from code review   Co-Authored-By: Adrian Wälchli adrian.waelchli@students.unibe.ch   fix calling   flake8   add types   Co-authored-by: Adrian Wälchli adrian.waelchli@students.unibe.ch Co-authored-by: William Falcon waf2107@columbia.edu,0
auto add reviewrs via Mergify (#1257),0.3186731,Allow passing model hyperparameters as complete kwarg list (#1896) ,  update mergify nb checks   lover merge action   draft auto review   fix rule ,0
clear skipping tests (#1285),0.53562605,- Code coverage (99%),  clear skipping tests   fix simple/multi GPU   review: simplify ,0
made load from checkpoint flexible (#1307),0.8283946,Refactor load in checkpoint connector (#4593),  made load from checkpoint flexible   made load from checkpoint flexible   made load from checkpoint flexible ,1
fix incomplete RunningMean (#1309),0.546856,| Enum RunningStage.TUNING                                    | 1.10             | No longer supported         |,"  fix RunningMean   changelog   fix none   Update supporters.py   just needed to multiply by zero for init  Revert ""Update supporters.py""  This reverts commit 7e0da6c6   fix NaN   formatting   Co-authored-by: William Falcon waf2107@columbia.edu",0
Add MNIST dataset & drop torchvision dep. from tests (#986),0.8483672,Dropped torchvision dependency in tests and added own MNIST dataset class instead (#986),"  added custom mnist without torchvision dep   move files so it does not conflict with mnist gitignore   mock torchvision for tests   fix line too long   fix line too long   fix ""module level import not at top of file"" warning   move mock imports to init.py   simplify MNIST a lot and download directly the .pt files   further simplify and clean up mnist   revert import overrides   make as before   drop  PIL requirement   move mnist.py to datasets subfolder   use logging instead of print   choose same name as in torchvision   remove torchvision and pillow also from yml file   refactor if train   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   capitalized class attr   moved mnist to models   re-added datsets ignore   better name for file variable   Update mnist.py   move dataset classes to datasets.py   new line   update   update   fix automerge   move to base folder   adapt testingmnist to new mnist base class   remove temporal fix   fix datatype   remove old testingmnist   readable   fix import   fix whitespace   docstring   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update tests/base/datasets.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   changelog   added types   Update CHANGELOG.md   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  exist->isfile  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   index -> idx   temporary fix for trains error   better changelog message   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",1
Parity test (#1284),0.4126405,- Code coverage (99%),  adding test   adding test   added base parity model   added base parity model   added parity test   added parity test   added parity test   added parity test   added parity test   added parity test   added parity test   added parity test   added parity test   added parity test   added parity test   added parity test   added parity test   added parity test   added parity test   move parity to benchmark   formatting   fixed gradient acc sched   move parity to benchmark   formatting   fixed gradient acc sched   skip for CPU   call last   Co-authored-by: J. Borovec jirka.borovec@seznam.cz,0
make evaluate private (#1260),0.6312278,Made evaluate method private >> Trainer._evaluate(...). (#1260),  make evaluate private   changelog ,0
validation and training loops run the partial dataset (#1192),0.68481845,Refactored training loop (#2336),  auto_add_sampler() fix   auto_add_sampler() fix   Co-authored-by: seth seth@duckpapa.com,0
update readme typo (#1292),0.5430843,Pkg: fix parsing versions by @Borda in https://github.com/Lightning-AI/lightning/pull/15401,,0
remove .item which causes sync issues (#1254),1.0,Remove .item which causes sync issues (#1254),  remove .item which causes sync issues   fixed gradient acc sched   fixed gradient acc sched ,1
Fix training resuming docs (#1265),0.64832103,1. Remove resume_from_checkpoint from the Trainer,,0
Update docs iterable datasets (#1281),0.56690997,    ds = IterableDataset(...),  Updated Sequencial Data docs   Sequntial Data section now contains info on using IterableDatasets    Undid reformatting of bullet points     added information about val_check_interval    Co-authored-by: Donal Byrne Donal.Byrne@xperi.com,0
Add support for iterable datasets when val_check_interval=1.0 (#1283),0.5327904,Disabled sampler replacement when using IterableDataset (#11507),  Add support for iterable datasets when val_check_interval=1.0   Update CHANGELOG.md ,0
fix logging config and add profiler test (#1267),0.6619854,Change default logger to a dedicated one (#1064),,0
Example: Simple RL example using DQN/Lightning (#1232),0.47007257,Example using the LightningCLI:,"  Example: Simple RL example using DQN/Lightning   DQN RL Agent using Lightning   Uses Iterable Dataset for Replay Buffer   Buffer is populated by agent as training is carried out, updating the dataset   Applied autopep8 fixes    Updated line length from 120 to 110    Update pl_examples/domain_templates/dqn.py   simplify get_device method Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update pl_examples/domain_templates/dqn.py  Re-ordered imports Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   CI: split tests-examples (#990)   CI: split tests-examples   tests without template   comment depends   CircleCI typo   add doctest   update test req.   CI tests   setup macOS   longer train   lover pred acc   fix model   rename default model   lower tests acc   typo   imports   fix test optimizer   update calls   fix Win   lower Drone image   fix call   pytorch image   fix test   add dev image   add dev image   update image   drone volume   lint   update test notes   rename tests/models >> tests/base   group models   conftest   optim imports   typos   fix import   fix tests   install AMP   tests   fix import   Clean up   added module docstring   renamed variables to be more descriptive   Added missing docstrings and type annotations   Added gym to example requirements   Added note to changelog   updated example image   update types   rename script   Update CHANGELOG.md   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   another rename   Disable validation when val_percent_check=0 (#1251)   fix disable validation   add test   update changelog   update docs for val_percent_check   make ""fast training"" docs consistent   calling self.forward() -> self() (#1211)   self.forward() -> self()   update changelog   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   Fix requirements-extra.txt Trains package to release version (#1229)   Fix requirement-extra use released Trains package   Update README.md add Trains and links to the external Visualization section   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Remove unnecessary parameters to super() in documentation and source code (#1240)  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   update deprecation warning (#1258)   update docs for progress bat values (#1253)   lower timeouts for inactive issues (#1250)   update contrib list (#1241)   Co-authored-by: William Falcon waf2107@columbia.edu   Fix outdated docs (#1227)   Fix typo (#1224)   drop unused Tox (#1242)   system info (#1234)   system info   update big info   test script   update config   rename script   import path   Changed smoothing in tqdm to decrease variability of time remaining between training / eval (#1194)   Example: Simple RL example using DQN/Lightning   DQN RL Agent using Lightning   Uses Iterable Dataset for Replay Buffer   Buffer is populated by agent as training is carried out, updating the dataset   Applied autopep8 fixes    Updated line length from 120 to 110    Update pl_examples/domain_templates/dqn.py   simplify get_device method Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update pl_examples/domain_templates/dqn.py  Re-ordered imports Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   Clean up   added module docstring   renamed variables to be more descriptive   Added missing docstrings and type annotations   Added gym to example requirements   Added note to changelog   update types   rename script   Update CHANGELOG.md   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  another rename  Co-authored-by: Donal Byrne Donal.Byrne@xperi.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: Adrian Wälchli adrian.waelchli@students.unibe.ch Co-authored-by: Jeremy Jordan 13970565+jeremyjordan@users.noreply.github.com Co-authored-by: Martin.B 51887611+bmartinn@users.noreply.github.com Co-authored-by: Tyler Yep tyep@stanford.edu Co-authored-by: Shunta Komatsu 59395084+skmatz@users.noreply.github.com Co-authored-by: Jack Pertschuk jackpertschuk@gmail.com",0
Update Contributors (#1274),0.5833983,Contributors,  Update Contributors in Readme   Update Governance ,0
Fix outdated docs follow up (#1266),0.5754645,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",  save_top_k   num_gpus   print_nan_grads fix leftovers   undo doctest removal ,0
Changed smoothing in tqdm to decrease variability of time remaining between training / eval (#1194),1.0,Changed smoothing in TQDM to decrease variability of time remaining between training/eval (#1194),,1
system info (#1234),0.48201,"device parser (#3400, #3405)",  system info   update big info   test script   update config   rename script   import path ,0
drop unused Tox (#1242),0.39324492,Remove MetricsHolder (#7909),,0
Fix typo (#1224),0.5415957,Syntax changes are: ,,0
Fix outdated docs (#1227),0.5397528,Release LAI docs as stable (#14250),,0
update contrib list (#1241),0.4246922,"Updated DeepSpeed ZeRO (#6546, #6752, #6142, #6321)",Co-authored-by: William Falcon waf2107@columbia.edu,0
lower timeouts for inactive issues (#1250),0.58147556,Increased TPU check timeout from 20s to 100s (#5598),,0
update docs for progress bat values (#1253),0.53032684,Changed the default progress bar to print to stdout instead of stderr (#531),,0
update deprecation warning (#1258),0.88083977,Deprecation warning (#3844),,1
Remove unnecessary parameters to super() in documentation and source code (#1240),0.6313004,    super().__init__(),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Fix requirements-extra.txt Trains package to release version (#1229),0.5824842,Setup: added requirement freeze for the next major version (#14480),  Fix requirement-extra use released Trains package   Update README.md add Trains and links to the external Visualization section   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
calling self.forward() -> self() (#1211),0.76993513,Updated references to self.forward() to instead use the __call__ interface. (#1211),  self.forward() -> self()   update changelog   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
Disable validation when val_percent_check=0 (#1251),0.59935826,val_percent_check in favour of limit_val_batches,"  fix disable validation   add test   update changelog   update docs for val_percent_check   make ""fast training"" docs consistent ",0
updated example image,0.45130765,Minor changes,,0
CI: split tests-examples (#990),0.53239346,- Full tests that test specific functionality in trainer.,  CI: split tests-examples   tests without template   comment depends   CircleCI typo   add doctest   update test req.   CI tests   setup macOS   longer train   lover pred acc   fix model   rename default model   lower tests acc   typo   imports   fix test optimizer   update calls   fix Win   lower Drone image   fix call   pytorch image   fix test   add dev image   add dev image   update image   drone volume   lint   update test notes   rename tests/models >> tests/base   group models   conftest   optim imports   typos   fix import   fix tests   install AMP   tests   fix import ,0
Custom argparser extension with Trainer arguments (argument types added)  (#1147),0.8154872,Trainer.add_argparse_args classmethod fixed. Now it adds a type for the arguments (#1147),  add_argparse_args method fixed (argument types added)   CHANGELOG.md upd   autopep8 fixes   --gpus=0 removed from test (for ci tests)   typo fixed   reduce on plateau scheduler fixed   Trainer cli related tests moved to test_trainer_cli.py   refactored: get_init_arguments_and_types is a public classmethod of the Trainer now   test_get_init_arguments_and_types added   autopep8 fixes   Trainer cli related tests moved to test_trainer_cli.py   refactored: get_init_arguments_and_types is a public classmethod of the Trainer now   test_get_init_arguments_and_types added   autopep8 fixes   Trainer cli related tests moved to test_trainer_cli.py   refactored: get_init_arguments_and_types is a public classmethod of the Trainer now   test_get_init_arguments_and_types added   autopep8 fixes   Trainer cli related tests moved to test_trainer_cli.py   test_get_init_arguments_and_types added   autopep8 fixes   Apply suggestions from code review   cosmetics   cosmetics   Update pytorch_lightning/trainer/trainer.py   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   Trainer.get_init_arguments_and_types now returns arg types wrapped in tuples (not in sets)   deprecated args are now ignored in argparser   get_deprecated_arg_names small refactor   get_deprecated_arg_names bug fixed   Trainer cli related tests moved to test_trainer_cli.py   refactored: get_init_arguments_and_types is a public classmethod of the Trainer now   test_get_init_arguments_and_types added   autopep8 fixes   Trainer cli related tests moved to test_trainer_cli.py   autopep8 fixes   Trainer cli related tests moved to test_trainer_cli.py   Trainer cli related tests moved to test_trainer_cli.py   test_get_init_arguments_and_types added   autopep8 fixes   autopep8 fixes   Apply suggestions from code review   cosmetics   cosmetics   Update pytorch_lightning/trainer/trainer.py   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   Trainer.get_init_arguments_and_types now returns arg types wrapped in tuples (not in sets)   deprecated args are now ignored in argparser   get_deprecated_arg_names small refactor   get_deprecated_arg_names bug fixed   Update pytorch_lightning/trainer/trainer.py   Co-Authored-By: Joe Davison joe@huggingface.co  Update pytorch_lightning/trainer/trainer.py  Co-Authored-By: Joe Davison joe@huggingface.co Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Joe Davison joe@huggingface.co Co-authored-by: William Falcon waf2107@columbia.edu,1
update chnagelog (#1169),0.5300969,Removed LoggerStages (#5673),  update chnagelog   formatting ,0
Fix dataloaders in TPU example (#1174),0.66427106,"Accessing dataloaders (#16726, #16800)",,0
Pretty test results with pprint (#1176),0.45080432,- Code coverage (99%),,0
update header links in docs (#1184),0.5091131,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",  update links in docs   fix ,0
Mergify: configuration update (#1200),0.39958453,- Deprecated `LightningLoggerBase.update_agg_funcs` ([#11871](https://github.com/PyTorchLightning/pytorch-lightning/pull/11871)),  Mergify: configuration update   Update .mergify.yml   Update .mergify.yml   Update .mergify.yml ,0
Docs badges / images (#1202),0.51162034,Docs,  move docs images   click GH badges   fixed docs build error   Co-Authored-By: Adrian Wälchli adrian.waelchli@inf.unibe.ch Co-authored-by: Adrian Wälchli adrian.waelchli@inf.unibe.ch,0
add missing overfit_pct docs (#1204),0.65722597,"Removed deprecated trainer flags: overfit_pct, log_save_interval, row_log_interval (#3969)",  add missing overfit_pct docs   move arg to old position   move arg docs ,0
Fixed typos in README (#1219),0.56804407,"In addition, we fixed:",,0
increase profiler test coverage (#1208),0.5910767,Moved profilers to their own file (#7822),  increase profiler test coverage   fix line length   tests for valueerror assertions ,0
test deprecated - model (#1074),0.59948087,"Deprecated DataModule properties: has_prepared_data, has_setup_fit, has_setup_validate, has_setup_test, has_setup_predict, has_teardown_fit, has_teardown_validate, has_teardown_test, has_teardown_predict (#7657)",  pylint   model API   update test   formatting   disable logger   fix checking overwrite   fix test   typo   deprecated model   fix for DDP   drop Flake8 in GH actions   Update pytorch_lightning/trainer/evaluation_loop.py   fix imports   Co-authored-by: Nic Eggert nic@eggert.io,0
CI: Force docs warnings to be raised as errors (+ fix all) (#1191),0.54963607,warning_utils >> warnings,"  add argument to force warn   fix automodule error   fix permalink error   fix indentation warning   fix warning   fix import warnings   fix duplicate label warning   fix bullet point indentation warning   fix duplicate label warning   fix ""import not top level"" warning   line too long   fix indentation   fix bullet points indentation warning   fix hooks warnings   fix reference problem with excluded test_tube   fix indentation in print   change imports for trains logger   remove pandas type annotation   Update pytorch_lightning/core/lightning.py   include bullet points inside note   remove old quick start guide (unused)   fix unused warning   fix formatting   fix duplicate label issue   fix duplicate label warning (replaced by class ref)   fix tick   fix indentation warnings   docstring ticks   remove obsolete docstring typing   Revert ""remove old quick start guide (unused)""   This reverts commit d51bb40695442c8fa11bc9df74f6db56264f7509.   added old quick start guide to navigation   remove unused  tutorials file   ignore some modules that got deprecated and are not used anymore   fix duplicate label warning   move examples doc and exclude pl_examples from autodoc   fix formatting for configure_optimizer   fix no blank line warnings   fix ""see also"" labels and add paramref extension   fix more reference problems   fix multi-gpu reference   fix weird warning   fix indentation and unrecognized characters in code block   fix warning ""... not included in toctree""   fix PIL import error   fix duplicate target ""here"" warning   fix broken link   revert accidentally moved pl_examples   changelog   stdout   note some things to know   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: J. Borovec jirka.borovec@seznam.cz Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",0
nan detection and intervention  (#1097),0.57033014,nan,"  check for nan values   test nan detection on loss   sys.exit   whitespace   detect nan and inf values in loss and params   update   added documentation   moved detect nan to training loop, remove flag for print   blank line   test   rename   deprecate print_nan_grads   deprecated print_nan_grads   remove unused imports   update changelog   fix line too long   correct deprecated version   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  raise exception instead of sysexit  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  raise exception instead of sysexit  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/trainer/training_tricks.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/trainer/training_tricks.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  fix test  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",0
ensure reset method works in notebooks (#1093),0.548203,clean up data reset (#3161),,0
Fix Configuring Learning Rate Schedulers (#1177),0.7661621,"If you need to customize the learning rate scheduler configuration, you can do so by overriding:",  Update docs so users know the desired manner of configuring learning rate schedulers.   update list   as note   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
Support hierarchical dict (#1152),0.531268,+def state_dict(self):,  Add support for hierarchical dict   Support nested Namespace   Add docstring   Migrate hparam flattening to each logger   Modify URLs in CHANGELOG   typo   Simplify the conditional branch about Namespace   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update CHANGELOG.md  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   added examples section to docstring   renamed _dict -> input_dict   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
improve partial Codecov (#1172),0.42415684,Trainer code became harder to follow,  ignore in setup   show report   abs imports   abstract pass   cover loggers   doctest trains   locals   pass   revert tensorboard   use tensorboardX   revert tensorboardX   fix trains   Add TrainsLogger.set_credentials (#1179)   Add TrainsLogger.set_credentials to control trains server configuration and authentication from code. Sync trains package version. Fix CI Trains tests   Add global TrainsLogger set_bypass_mode (#1187)   Add global TrainsLogger set_bypass_mode skips all external communication   Co-authored-by: bmartinn <>  rm some no-cov  Co-authored-by: Martin.B 51887611+bmartinn@users.noreply.github.com,0
Backward compatibility for checkpoint loading (#1132),0.7818366,all the checkpoint issues should be gone now (including backward support for old checkpoints),  check if hparams_type exists in checkpoint dictionary for backward compatibility   concisely maintain backward compatibility for hparams type   Bug fix in checkpoint loading (#1132) ,1
change default logger to dedicated one (#1064),0.99432945,Change default logger to a dedicated one (#1064),Fix test Fix format Update pytorch_lightning/init.py Separate imports,1
coverage increase (#1167),0.44196543,- Code coverage (99%),  fixed docs   Docs (#1164)   fixed docs   fixed docs   fixed docs   fixing Win failed import (#1163)   version   try fix distrib   update try import   fixed docs   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
fixing Win failed import (#1163),0.45343742,Module imports are now catching ModuleNotFoundError instead of ImportError (#9867),  version   try fix distrib   update try import ,0
Docs (#1164),0.72005737,Docs,  fixed docs   fixed docs   fixed docs ,1
prune codecov exceptions (#1107),0.39959243,Sanitize None params during pruning (#6836),,0
ReduceLROnPlateau bug fix (#1126),0.60067326,We have also added support for the ReduceLROnPlateau scheduler with shorthand notation:,  bug fix and test   update CHANGELOG.md   Co-authored-by: Nicki Skafte nugginea@gmail.com,0
Fix docs - missing Trainer (#1159),0.687251,Removed trainer.reset_*_dataloader() methods (#16726),  drop pandas   formatting ,0
Neptune Logger Improvements (#1084),0.77129966,"It now supports neptune-client 0.16.16 and neptune >=1.0, and we have replaced the log() method with append() and extend().","  removed project and experiment from getstate   added tests for closing experiment, updated token in example to user neptuner   updated teoken   Update neptune.py   added a link to example experiment   added exmaple experiment link   dropped duplication   flake fixes   merged with master, added changes information to CHANGELOG ",1
Fix examples that uses type_as (#1129),0.4880657,Trainer.add_argparse_args classmethod fixed. Now it adds a type for the arguments (#1147),,0
Add TRAINS experiment manager support (#1122),0.5065845,adding Trainer.tune() (#3293),"  Add allegro.ai TRAINS experiment manager support   improve docstring and type hinting, fix the bug in log_metrics, add support torch.Tensor to input into log_image   complete missing docstring of constructor's arguments   fix docs   pep8   pep8   remove redundant typing use logging fix typing and pep8   remove deprecated interface   add TrainsLogger test   add TrainsLogger PR in CHANGELOG   add id/name property documentation   change logging as log   Co-authored-by: bmartinn <> Co-authored-by: Sou Uchida s.aiueo32@gmail.com",0
Add Support for Non-primitive types in TensorboardLogger (#1130),0.72182465,We have added own custom Tensorboard logger as default logger. ,  Added support for non-primitive types to tensorboard logger   added EOF newline   PEP8   Updated CHANGELOG for PR #1130. Moved _sanitize_params to base logger. Cleaned up _sanitize_params   Updated CHANGELOG for PR #1130. Moved _sanitize_params to base logger. Cleaned up _sanitize_params   changed convert_params to static method   PEP8   Cleanup Doctest for _sanitize_params   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   Removed OrderedDict import   Updated import order to conventions   Co-authored-by: Manbir Gulati manbirgulati@Manbirs-MBP.hsd1.md.comcast.net Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
enable Codecov (#1133),0.44360116,Enabled cp (upload) at project level (#16631),  update config   try Drone cache   drop Drone cache   move import   remove token ,0
Type Hints for Lightning Core (#946),0.61077446,Made type hints public (#17100),"  first pass for LightningModule typehints   fix return types   add missing types   add type annotations to grads.py   add type annotations to hooks.py   add type annotation to memory.py   proper docstring quotation marks   add type annotations to saving.py   fix cyclic import problem   fix cyclic import problem   add missing whitespace   finish type hints for load_from_ methods   docs: prepare_data does not return anything   fix auto types in docs   revert typehint for trainer in hook   remove unnecessary return docs   some fixes for memory docs   revert typing for args kwargs   added all missing None return types   remove unused import   add more details to dict/list return types   fix line too long   optimize imports   linted   Revert ""linted""   This reverts commit 85559611e84e312bce64f4e73b638d4999a8439e.   remove whitespace   update   update   update   update   update   changelog   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: William Falcon waf2107@columbia.edu",0
fix tmpdir (#1012),0.4098913,- all the file path errors with loggers (txs @awaelchli),  fix tmpdir   just str path ,0
Add support for IterableDatasets everywhere (#1104),0.6935401,return iterabledataset,"  Add support for IterableDatasets everywhere   Added type hints, simplified code and improved coverage in data_loading.py   Update CHANGELOG.md ",0
fix max_epochs setup in basic example (#1105),0.6853918,"max_nb_epochs to max_epochs,",tested only for the CPU version,0
Fix typo (#1118),0.5157781,"In addition, we fixed:",,0
cleaning imports (#1032),0.59902287,Wrapped imports for traceability (#13924),,0
set the default value of progress_bar_refresh_rate to 1 (#1100),0.7625741,Changed progress_bar_refresh_rate trainer flag to disable progress bar when setting to 0. (#1108),,1
update info for GPU (#1021),0.62384295,refactored GPU backend __step (#3120),,0
Run on_validation_end only on main process in DDP (#1125),0.64780575,Early stopping checks on_validation_end (#1458),Co-authored-by: xingzhaolee xingzhaolee@users.noreply.github.com,0
fix typo (#1124),0.52674687,"In addition, we fixed:",,0
prune changelog (#1123),0.51255095,Pruned requirements duplicity (#13739),,0
suspend PR greeting,0.34024587,A message informs about the changed settings:,,0
add Drone CI (#1115),0.42238948,"If we forgot somebody or you have a suggestion, find us on Discord :zap:",  add Drone config   update Drone config   add Drone config   list GPUs   add type   native torch   native torch   fix image   update   SLURM_LOCALID   add badge   simple test ,0
update chnagelog (#1091),0.48691517,Removed LoggerStages (#5673),  update chnagelog   update chnagelog ,0
reduce nb CI builds (#1078),0.4719062,Reduction when batch_size < num_gpus (#1609),,0
release v0.7.1,0.62412304,This release includes:,,0
Fixes print issues and data_loader (#1080),0.75161076,Monir bug fix with print issues and data_loader (#1080),  print issue   print issue   print issue   print issue   print issue ,1
release v0.7.0,0.6311247,This release includes:,,0
release v0.6.4.dev1,0.6341014,[1.6.2] - 2022-04-27,,0
remove deprecated data_loader (#1077),0.7897377,Deprecated @data_loader decorator  (#926),  change version in CHangelog   warning   remove der data_loader   Co-authored-by: William Falcon waf2107@columbia.edu,1
removed decorators (#1079),0.61466676,Removed deprecated auto_move_data decorator (#9231),,0
dev release 1,0.57012165,This release includes:,,0
release v,0.66342795,This release includes:,,0
"cleaned docs, fixed argparse generator (#1075)",0.7357674,- Removed legacy argparse utilities ([#16708](https://github.com/Lightning-AI/lightning/pull/16708)),  Update README.md   Update README.md   Update README.md   Update README.md   Update README.md   Update README.md   Update README.md   Test deprecated API for 0.8.0 and 0.9.0 (#1071)   till 0.8   refactor   fix tests   fix tests   deprx till 0.9   Update trainer.py   Apply suggestions from code review   Co-authored-by: William Falcon waf2107@columbia.edu   updated test   updated test   updated test   updated test   updated test   updated test   updated test   updated test   updated test   updated test   updated test   updated test   updated test   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
updated test (#1073),0.60539573,Updated app testing (#16000),,0
Test deprecated API for 0.8.0 and 0.9.0 (#1071),0.5490754,Removed deprecated API (#2073),  till 0.8   refactor   fix tests   fix tests   deprx till 0.9   Update trainer.py   Apply suggestions from code review   Co-authored-by: William Falcon waf2107@columbia.edu,0
Fix Inconsistencies after introducing _step_end and _epoch_end (#1072),0.67836493,test_end >> test_epoch_end,  fix copy-paste errors after renaming *_end methods   line too long   update   Update lightning.py   Update lightning.py   Update lightning.py   Co-authored-by: William Falcon waf2107@columbia.edu,0
updated test,0.6662904,Updated app testing (#16000),,0
extend docs notes (#1004),0.68690485,Docs improvements,  wip   notes   WIP   example   notes ,0
updated examples,0.57072103,Noteworthy changes:,,0
fix deprecated warnings (#1048),0.7962852,Deprecation warning (#3844),  fix deprecated warnings   deprecated ,1
new image (#1070),0.48264536,New,,0
Docs2 (#1069),0.69575346,Docs,  new image   new image ,0
Docs (#1068),0.7534047,Docs,  prepare_data assignment   prepare_data assignment   prepare_data assignment   prepare_data assignment   prepare_data assignment   new image ,1
prepare_data before spawns (#1067),0.55861145,"    * The hooks/callbacks `prepare_data`, `setup`, `configure_sharded_model` and `teardown` now run under initialized process group for spawn-based plugins just like their non-spawn counterparts",  prepare_data assignment   prepare_data assignment   prepare_data assignment   prepare_data assignment   prepare_data assignment   prepare_data assignment ,0
Update experiment_logging.rst (#1066),0.5097053,Rename Trainer arguments row_log_interval >> log_every_n_steps and log_save_interval >> flush_logs_every_n_steps (#3748),fix import in documentation sample code,0
fix Trainer docs example indent and warnings (#1065),0.6077982,Removed Warning from trainer loop (#1634),  fix example indent and warnings   appease the linter ,0
add docs title (#1063),0.5224316,Docs improvements,,0
remove extra whitespace (#1060),0.34374362,"Cleaning (#5948, #5949, #5950)",,0
fix GH CI (#1052),0.45743763,Removed auto val reduce (#2462),,0
added image,0.36515072,Removed,,0
changed root image (#1061),0.52157885,Moved base req. to root (#4219),,0
docs (#1059),0.7111783,Docs,,1
Coverage (#1058),0.5480913,- Code coverage (99%),  docs   docs   docs   docs ,0
Examples: using new API (#1056),0.56604636,Removed deprecated API (#2073),  using new API   typo ,0
Docs12 (#1057),0.6348936,Docs,  docs   docs ,0
Docs format - Trainer & LModule (#1055),0.5514585,"Standardized the dataloaders arguments of trainer.{fit,valdiate,test,tune} (#7431)",  format Trainer   format LModule   format LModule   linted   Update lightning.py   Co-authored-by: William Falcon waf2107@columbia.edu,0
Docs11 (#1054),0.6464211,Docs,  training_end to training_step_end   training_end to training_step_end   training_end to training_step_end   training_end to training_step_end   training_end to training_step_end ,0
Steps (#1051),0.60704684,Renames model steps (#1051),  training_end renamed to training_step_end   training_end renamed to training_step_end   training_end renamed to training_step_end   training_end renamed to training_step_end   training_end to training_step_end   training_end to training_step_end   training_end to training_step_end   training_end to training_step_end   fix lost model reference   training_end to training_step_end   training_end to training_step_end   training_end to training_step_end   training_end to training_step_end   training_end to training_step_end   training_end to training_step_end   training_end to training_step_end   training_end to training_step_end   training_end to training_step_end   training_end to training_step_end   training_end to training_step_end   training_end to training_step_end   training_end to training_step_end   training_end to training_step_end   training_end to training_step_end   training_end to training_step_end   training_end to training_step_end   training_end to training_step_end   training_end to training_step_end   training_end to training_step_end   training_end to training_step_end   training_end to training_step_end   training_end to training_step_end   training_end to training_step_end   training_end to training_step_end   training_end to training_step_end   training_end to training_step_end   training_end to training_step_end ,0
Learning rate stepping option (#941),0.69635093,Remove deprecated args to learning rate step function (#890),  remove deprecated args to learning rate step function   step based scheduler   mixing models for testing   fix styling   tests   update documentation   smaller fix   update to dict structure   updated test   update documentation   update CHANGELOG.md   fix styling   fix problems with trainer io   fix tests   simplification of code   fix styling   change from batch to step   update to tests   fix styling   fixed some logic   Update pytorch_lightning/core/lightning.py   duplicated test   fix test on amp   small update to tests   added monitor key for ReduceLROnPlateau   Update trainer.py   Update training_loop.py   fix test after introducing monitor keyword   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: William Falcon waf2107@columbia.edu,0
proper checkpoint implementation (#1043),0.7471249,Asynchronous Checkpointing,  enabled early stopping/checkpooiunt even  without val step   enabled early stopping/checkpooiunt even  without val step   enabled early stopping/checkpooiunt even  without val step   enabled early stopping/checkpooiunt even  without val step   enabled early stopping/checkpooiunt even  without val step   enabled early stopping/checkpooiunt even  without val step   enabled early stopping/checkpooiunt even  without val step   enabled early stopping/checkpooiunt even  without val step   enabled early stopping/checkpooiunt even  without val step   enabled early stopping/checkpooiunt even  without val step   enabled early stopping/checkpooiunt even  without val step   enabled early stopping/checkpooiunt even  without val step   enabled early stopping/checkpooiunt even  without val step   enabled early stopping/checkpooiunt even  without val step   enabled early stopping/checkpooiunt even  without val step   enabled early stopping/checkpooiunt even  without val step   enabled early stopping/checkpooiunt even  without val step   enabled early stopping/checkpooiunt even  without val step   enabled early stopping/checkpooiunt even  without val step   enabled early stopping/checkpooiunt even  without val step   enabled early stopping/checkpooiunt even  without val step   enabled early stopping/checkpooiunt even  without val step   enabled early stopping/checkpooiunt even  without val step   enabled early stopping/checkpooiunt even  without val step   enabled early stopping/checkpooiunt even  without val step   enabled early stopping/checkpooiunt even  without val step   enabled early stopping/checkpooiunt even  without val step   enabled early stopping/checkpooiunt even  without val step   name formatting   version   testing   add test   fix test   Update model_checkpoint.py   doctests   pylint   tests   debug   debug   enabled early stopping/checkpooiunt even  without val step   fix MNIST download (#1044)   fix MNIST download   simple   name formatting   version   testing   add test   fix test   doctests   tests   debug   debug   rebased 1041   rebased 1041   tests   rebased 1041   rebased 1041   rebased 1041   rebased 1041   rebased 1041   rebased 1041   rebased 1041   rebased 1041   rebased 1041   rebased 1041   rebased 1041   rebased 1041   rebased 1041   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
fix MNIST download (#1044),0.47608918,Dropped torchvision dependency in tests and added own MNIST dataset class instead (#986),  fix MNIST download   simple ,0
hparams as dict [blocked by 1041] (#1029),0.66400075,Support **DictConfig for hparam serialization (#2519),  hparams as dict   hparams as dict   fixing   fixing   fixing   fixing   typing   typing   chnagelog   update set hparams   use setter   simplify   chnagelog   imports   pylint   typing   Update training_io.py   Update training_io.py   Update lightning.py   Update test_trainer.py   Update init.py   Update base.py   Update utils.py   Update test_trainer.py   Update training_io.py   Update test_trainer.py   Update test_trainer.py   Update test_trainer.py   Update test_trainer.py   Update callback_config.py   Update callback_config.py   Update test_trainer.py   Co-authored-by: William Falcon waf2107@columbia.edu,0
lint,0.546113,Fabric,,0
Docs5 (#1036),0.66379535,Docs,  changed path   changed path   changed path   changed path   changed path   changed path   changed path   changed path   changed path   changed path   changed path   added cv   added cv   added cv   added cv   added cv   added cv   added cv   added cv   added cv   added cv   added cv   added cv   added cv   added cv   added cv   added cv   added cv   added cv ,0
Docs5 (#1033),0.65287226,Docs,  changed path   changed path   changed path   changed path   changed path   changed path   changed path   changed path   changed path   changed path   changed path   added cv   added cv   added cv   added cv   added cv   added cv   added cv   added cv   added cv   added cv   added cv   added cv   added cv   added cv   added cv   added cv   added cv ,0
Fix spelling :P (#1035),0.5017347,"In addition, we fixed:",,0
update checkpoint docs (#1016),0.77221817,all the checkpoint issues should be gone now (including backward support for old checkpoints),  update checkpoint docs   fix tests   fix tests   formatting   typing   filename   fix tests   fixing tests   fixing tests   fixing tests   unique name   fixing   fixing   Update model_checkpoint.py   Co-authored-by: William Falcon waf2107@columbia.edu,1
changed path (#1031),0.56540877,Changed Checkpoint path parameter from filepath to dirpath (#1016),,0
added community examples (#1030),0.50598633,Made type hints public (#17100),,0
Fix typo (#1027),0.5310093,"In addition, we fixed:",,0
Docs2 (#1028),0.6828951,Docs,  added community examples   added community examples ,0
[Docs] hparams is not a dictionary for now (#1026),0.6280512,    hparams_file='/path/to/hparams_file.yaml',Co-authored-by: peterAsapp 31011756+peterAsapp@users.noreply.github.com,0
Docs (#1024),0.73423576,Docs,  added checkpoint defaults   added checkpoint defaults   added checkpoint defaults   added checkpoint defaults   added checkpoint defaults   added checkpoint defaults   added checkpoint defaults   added checkpoint defaults   added checkpoint defaults   added checkpoint defaults   added checkpoint defaults   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   added community examples   added community examples ,1
Skepticleo trainer argparser (#1023),0.5178275,  trainer:,"  Added default parser for trainer and class method to construct trainer from default args   Removed print statement   Added test for constructing Trainer from command line args   Removed extra line   Removed redundant imports, removed whitespace from empty lines   Fixed typo   Updated default parser creation to get class attributes automatically   Updated default parser creation to get class attributes automatically   Added method to get default args for trainer   Trimmed trainer get default args method   Updated from argparse method to not return trainer with static arguments   Update trainer get default args to classmethod   adjustment   fix   Fixed variable name   Update trainer.py   Update test_trainer.py   Update trainer.py   Update tests/trainer/test_trainer.py   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   Update trainer.py   Update test_trainer.py   Update trainer.py   Update test_trainer.py   Update tests/trainer/test_trainer.py   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/trainer/trainer.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   Update trainer.py   Update test_trainer.py   Co-authored-by: Mudit Tanwani mudittanwani@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",0
disabled early stopping by default (#1022),0.6634641,EarlyStopping now runs at the end of the training epoch by default (#8286),,0
fix 16 bit for TPU (#1020),0.5420673,Default to precision=bf16 on CPU when precision=16 is passed (#10033),  tpu 16 bit   tpu 16 bit   tpu 16 bit ,0
consolidate callbacks and hooks (#950),0.78864396,Callback hooks,  consolidate callbacks and hooks   ensure callbacks recieve proper arg types   remove model from init callback events   clean up early stopping event   update changelog   remove on_fit_start and on_fit_end   fix args for on_init_start and on_init_end   handle case where early stopping is not used   show all callback methods   wrap checkpoint callback logic into proper class   fix check for main process in checkpoint callback   move callbacks test to separate file   refactor arg checks   get model and call hook on same line   define trainer_options dict in one call   add more asserts to callback test ,1
handle keyboard interrupt for ddp .test() (#1019),0.575104,Deprecated on_keyboard_interrupt callback hook in favor of new on_exception hook (#9260),  updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs ,0
Fix #997 (#1018),0.55537474,"In addition, we fixed:",,0
fixes test issues on ddp (#1017),0.7143419,DDP Debugging Improvements,  updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs ,1
Merge load functions (#995),0.5445439,"        return CombinedLoader(iterables, mode=""min_size"")","  Update README.md   Update README.md   Use callable object for patching dataloaders (#971)   Use callable object for patching dataloaders   Add test for ddp with dataloaders passed to fit()   Update pytorch_lightning/trainer/trainer.py   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/trainer/trainer.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   merge load functions   update tests   fix documentation warnings   fix line too long   fix line too long   print deprecation warning   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   move tags_csv argument to end of signature   fix typo, update version numbers   fix line too long   add typing as requested   update changelog   Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: Sho Arora sho854@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",0
Logger tests and fixes (#1009),0.7250019,Cleaning up stale logger tests (#3490),  Refactor logger tests   Update and add tests for wandb logger   Update and add tests for logger bases   Update and add tests for mlflow logger   Improve coverage   Updates   Update CHANGELOG   Updates   Fix style   Fix style   Updates ,1
drop logging level (#1015),0.58388495,Change default logger to a dedicated one (#1064),,0
No auto load weights (#985),0.62057936,Loading Model Weights,  remove autoload   remove autoload   added weights loading docs   checkpoint loading saving docs   checkpoint loading saving docs   checkpoint loading saving docs   docs (#1010)   remove autoload   remove autoload   added weights loading docs   checkpoint loading saving docs   checkpoint loading saving docs   checkpoint loading saving docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs ,0
docs (#1010),0.707507,Docs,,1
Use callable object for patching dataloaders (#971),0.6236825,"Refactor dataloading, supports infinite dataloader (#955)",  Use callable object for patching dataloaders   Add test for ddp with dataloaders passed to fit()   Update pytorch_lightning/trainer/trainer.py   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/trainer/trainer.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
docs (#1001),0.724946,Docs,,1
Tut (#1000),0.47892767,(#16002),  docs   docs   docs   docs ,0
Tutorials (#998),0.5042727,1. Create the Trainer,  removed abstract requirement so LightningModule == nn.Module   default adam   default adam   default adam   default adam   default adam   default adam   default adam   default adam   default adam   default adam   default adam   default adam   default adam   docs   docs   docs ,0
update PR guidelines (#993),0.5121459,Updated governance docs,  update PR guidelines   update tests   add CircleCI   Update CONTRIBUTING.md   Co-authored-by: William Falcon waf2107@columbia.edu,0
Fix load_from_checkpoint docs (#978),0.73866445,Refactor load in checkpoint connector (#4593),We don't (yet) support storing hparams a a dict. It must be an argparse.Namespace for checkpoint saving and loading to work.,1
fix docs (#982),0.57226753,Docs improvements,,0
update contributors in README (#974),0.50328624,Contributors,,0
clean docs (#967),0.56939006,Docs improvements,  clean docs   clean docs   clean docs ,0
Trainer cleanup (#934),0.6633619,Renamed several Trainer atributes:  (#567),  Trainer cleanup   update abstract   remove ...   remove init   update mixin types   update callbacks   fix   lower test acc ,0
resolving documentation warnings (#833),0.630487,Deprecation warning (#3844),  add more underline   fix LightningMudule import error   remove unneeded blank line   escape asterisk to fix inline emphasis warning   add PULL_REQUEST_TEMPLATE.md   add init.py and import imagenet_example   fix duplicate label   add noindex option to fix duplicate object warnings   remove unexpected indent   refer explicit LightningModule   fix minor bug   refer EarlyStopping explicitly   restore exclude patterns   change the way how to refer class   remove unused import   update badges & drop Travis/Appveyor (#826)   drop Travis   drop Appveyor   update badges   fix missing PyPI images & CI badges (#853)   docs - anchor links (#848)   docs - add links   add desc.   add Greeting action (#843)   add Greeting action   Update greetings.yml   Co-authored-by: William Falcon waf2107@columbia.edu   add pep8speaks (#842)   advanced profiler describe + cleaned up tests (#837)   add py36 compatibility   add test case to capture previous bug   clean up tests   clean up tests   Update lightning_module_template.py   Update lightning.py   respond lint issues   break long line   break more lines   checkout conflicting files from master   shorten url   checkout from upstream/master   remove trailing whitespaces   remove unused import LightningModule   fix sphinx bot warnings   Apply suggestions from code review   just to trigger CI  Update .github/workflows/greetings.yml  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: Jeremy Jordan 13970565+jeremyjordan@users.noreply.github.com,0
Fix loggers and update docs (#964),0.6883309,DDP + loggers should be fixed,  Fix loggers and update docs   Update trainer.py ,0
TPU gradient clipping.  (#963),0.71759975,Update Gradient Clipping for the TPU Accelerator (#6576),  clip   Update pytorch_lightning/trainer/training_tricks.py   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/trainer/training_tricks.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   pull out epsilon   add fp16 case   Update pytorch_lightning/trainer/training_tricks.py   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
clean docs (#966),0.5678168,Docs improvements,,0
split trainer tests (#956),0.7040682,trainer.test(...),  split trainer tests   Apply suggestions from code review   format string   add CI timeout ,1
fixes tpu data loader bug (#957),0.5951449,Ensured process_dataloader is called when tpu_cores > 1 to use Parallel DataLoader (#6015),  fixes tpu data loader bug   fixes tpu data loader bug ,0
Refactor dataloading (#955),0.8307098,"Refactor dataloading, supports infinite dataloader (#955)",  Refactor dataloading   Refactor dataloading   Refactor dataloading   Add shuffle to test ,1
Callbacks [wip] (#889),0.7456865,  callbacks:,  Add callback system + associated test   Add trainer and pl_module args to callback methods   typing   typo in docstring   Switch to on_.*_start()   fix on_test_start   fix the mess after rebasing ,1
added docs (#944),0.6028185,Docs improvements,,0
feat(trainer): add enable_benchmarking option (#803),0.7517153,Removed the Trainer(move_metrics_to_cpu=True) argument (#16358)," feat(trainer): add enable_benchmarking option  closes #370   Update trainer.py   Update trainer.py   Update trainer.py   Update trainer.py   Update trainer.py   Update trainer.py   Update trainer.py   Update trainer.py   add test   try to make the lint work   fix typo   add test, verify torch.backends.cudnn.benchmark   make lint happy   make lint happy   Co-authored-by: William Falcon waf2107@columbia.edu",1
Add support for multiple loggers (#903),0.82697564,Logging with multiple loggers,"  Add support for multiple loggers   Fix PEP   Cleanup   Cleanup   Add typing to loggers   Update base.py   Replace duck typing with isinstance check   Update CHANGELOG.md   Update comet experiment type, Switch to abstractmethod in logging.py   Fix test   Add passes to LightningLoggerBase   Update experiment_logging.rst ",1
use log no print (#940),0.64695156,switched from print to logging,,0
Fixing tests (#936),0.64683175,- fixed all the .test() calls,  abs import   rename test model   update trainer   revert test_step check   move tags   fix test_step   clean tests   fix template   update dataset path   fix parent order ,0
relax hparams (#919),0.42554927,Fixing critical bugs in newly added hooks and hparams assignment.,relax model loading hparams test wip wip fix warning finish test remove unused import,0
add epoch option (#933),0.63235545,Changed epoch indexing from 0 instead of 1 (#2289),,0
Update tests README to point to tests/requirements.txt (#935),0.45173836,Refactored setup_training and remove test_mode (#5388), Update tests README  Point to tests/requirements.txt as part of instructions  Update requirements to dependencies,0
Caching MNIST dataset for testing (#917),0.5293529,Dropped torchvision dependency in tests and added own MNIST dataset class instead (#986),  Caching MNIST dataset for testing   Added MNIST datset to the tests directory   Caches dataset based off hash of the test.pt file   Cleaned Up yml file   Cleaned Up yml file   Removed MNIST Data from framework   Set cache key for dataset to 'mnist'   Apply suggestions from code review   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
fix tests (#938),0.68688494,- fixed all the .test() calls,  fix tests   fix tests ,0
Fix/test pass overrides (#918),0.55024505,- fixed all the .test() calls,  Fix test requiring both test_step and test_end   Add test   Co-authored-by: William Falcon waf2107@columbia.edu,0
Tpu features (#932),0.5735866,TPU core selection,  added guide   added self.print()   added self.print() ,0
Clean up dataloader logic (#926),0.6566385,refactored dataloader process hook (#3139),  added get dataloaders directly using a getter   deleted decorator   added prepare_data hook   refactored dataloader init   refactored dataloader init   added dataloader reset flag and main loop   added dataloader reset flag and main loop   added dataloader reset flag and main loop   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   fixed bad loaders   fixed bad loaders   fixed bad loaders   fixed bad loaders   fixed bad loaders   fixed bad loaders   fixed bad loaders   fixed bad loaders   fixed bad loaders   fixed error in .fit with loaders   fixed error in .fit with loaders   fixed error in .fit with loaders   fixed error in .fit with loaders   fixed error in .fit with loaders   fixed error in .fit with loaders   fixed error in .fit with loaders   fixed error in .fit with loaders   fixed error in .fit with loaders   fixed error in .fit with loaders   fixed error in .fit with loaders   fixed error in .fit with loaders   fixed error in .fit with loaders   fixes #909   fixes #909   bug fix   Fixes #902 ,0
Update docs for map_location (#920),0.42303056,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",  update docs for map location   update return description ,0
Add tags to the rendezvous calls for TPU.  (#921),0.6530215,Resolve TPU miss rendezvous (#6781),  Update data_loading.py   Update training_io.py   Update trainer.py ,0
Split callbacks (#849),0.7634746,Split callbacks in multiple files (#849),  add .vscode in .gitignore   Split callbacks in individual files + add a  property to Callback for easy trainer instance access   formatting   Add a conda env file for quick and easy env setup to develop on PL   Adress comments   add fix to kth_best_model   add some typing to callbacks   fix typo   add autopep8 config to pyproject.toml   format again   format   fix toml   fix toml again   consistent max line length in all config files   remove conda env file   Update pytorch_lightning/callbacks/early_stopping.py   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/callbacks/model_checkpoint.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   docstring   Update pytorch_lightning/callbacks/model_checkpoint.py   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/callbacks/model_checkpoint.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   fix logic error   format   simplify if/else   format   fix linting issue in changelog   edit changelog about new callback mechanism   fix remaining formating issue on CHANGELOG   remove lambda function because it's compatible with pickle (used during ddp)   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
Type Hints for Trainer (#912),0.6278783,  trainer:, typehints for trainer   fix type links in docs fix types in docs type hints for trainer methods fix fit docs switch to comments readability added sphinx typehints extension wip remove typehints from docstring more type annotations fix spaces  Update trainer.py  Co-authored-by: William Falcon waf2107@columbia.edu,0
"extract training teardown into method, catch KeyboardInterrupt (#856)",0.6586454,"Support graceful training cleanup after Keyboard Interrupt (#856, #1019)",Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
Update training_loop.py (#913),0.74406403,Deprecated the Trainer.train_loop property in favor of Trainer.fit_loop (#8025),,1
Fix comet logger to log after train (#892),0.6410213,Using .comet.config file for CometLogger (#1913),  Fix comet logger to log after train   Add clarifying comment to COmetLogger code   Explains the need to use CometExistingExperiment in the CometLogger class after CometLogger.finalize.,0
finished dist (#911),0.42786196,Better progress bar (#16695),,0
Fixes resuming checkpoints rerunning last epoch (#866),0.68684334,all the checkpoint issues should be gone now (including backward support for old checkpoints),  Properly restore current epoch and global step on resume   Add test   Move increment to saving rather than loading   Fix other tests that refer to current epoch   Formatting   Add warning for mid-epoch resuming   Formatting   Fix warning check for accumulated batches   Add variable to init   Formatting   Add check for 0 training steps   Make check more readable ,0
add Sphinx Check (#844),0.6039033,Pinned sphinx-autodoc-typehints with <v1.15 (#11400),  add sphinx bot   source   typo   Make a change to the docs (#2)   Make a change to the docs  Introduce an error Install git before building docs Apply suggestions from code review  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   Update docs/source/apex.rst   Update docs/source/apex.rst   Co-authored-by: Ammar Askar ammar_askar@hotmail.com,0
Add conda env setup (#898),0.4243851,from setuptools import setup,  add a conda env file for easy PL conda env setup   Update environment.yml   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update environment.yml  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update environment.yml  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update environment.yml  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update environment.yml  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update environment.yml  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
update Loggers (#818),0.52893573,"Mocking loggers (#3596, #3617, #3851, #3859, #3884, #3853, #3910, #3889, #3926)",  add warnings   fix link ,0
separate requirements for logger dependencies (#792),0.6188594,Removed collisions with logger versions by tying it to job id.,"  added file that contains information on the minimal versions needed for the supported loggers   copied minimal version, combined files, deleted duplicates   sorted functions in tests/test_loggers.py to be consistent   expanded wandb logging test; added minimal versions for requirements-extra.txt; increased the amount of training data that is used for tests   formatting   added requirements-extra.txt to MANIFEST.in   reverted wandb test; ensured minimal version for dependencies in requirements-extra.txt in ci-testing.yml ",0
Fix backwards compatibility for optional logging dependencies (#900),0.65695375,Removed logger_connector legacy code (#6733),,0
add Stale action (#905),0.40927625,moved ___step_end hooks (#3130),,0
update CHANGELOG (#897),0.6655544,Full Changelog,add info about TPU and segmentation,0
typo JB,0.33431512,"In addition, we fixed:",typo in my name lol,0
update contributors (#895),0.57393026,Contributors,  updated governance docs   added maintainers to readme   added governance docs   added governance docs ,0
clean up tests/test_profiler.py (#867),0.6125097,Moved profilers to their own file (#7822),"  cleanup docstrings, _get_total_cprofile_duration in module   relax profiler overhead tolerance ",0
remove deprecated args to learning rate step function (#890),1.0000001,Remove deprecated args to learning rate step function (#890),,1
update governance docs (#894),0.8997779,Updated governance docs,  updated governance docs   added maintainers to readme   added governance docs ,1
new way of passing dataloaders (#759),0.7682002,"Redesigned multi-dataloader support (#16743, #16784, #16939)",  new way of passing dataloaders   fixed docs   fixed codestyle to follow flake8   allow val/test be list of dataloaders and smarter checking   added test   fix flake error   fix linking to new test model   split into multiple test   fix naming and typo   minor documentation changes   remove random file   Update trainer.py   Update trainer.py   Update trainer.py   Update trainer.py   Update trainer.py   Update trainer.py   better error/warning message   final adjustments   update CHANGELOG.md   Co-authored-by: William Falcon waf2107@columbia.edu,1
Updates theme Sphinx configuration (#893),0.61186147,Pinned sphinx-autodoc-typehints with <v1.15 (#11400),"I am updating the project's Sphinx documentation to fix (#819). The issue is related to a library the Sphinx extension nbsphinx (to load Jupyter Notebooks) loads into the docs context (RequireJS). That library conflicts with other theme libraries, causing the latter to be not loaded. This would result in several crashes, the most obvious of them the lack of anchors. The fix above solves all errors -- and now anchors work.",0
Fix docs for early stopping (#865),0.60892075,Deprecate early_stop_callback Trainer argument (#3845),  updated docs   updated docs   upd ,0
Added max number of steps in Trainer (#728),0.81088644,Changed default value of the max_steps Trainer argument from None to -1 (#9460),  Added max number of steps in Trainer   Added docstring   Fix flake8 errors   Clarified docstrings   Fixed flake8 error   Added min_steps to Trainer   Added steps and epochs test   flake8   minor fix   fix steps test in test_trainer   Split steps test into 2 tests   Refactor steps test   Update test_trainer.py   Minor in test_trainer.py   Update test_trainer.py   Address PR comments   Minor   Co-authored-by: William Falcon waf2107@columbia.edu,1
fix tpu docs (#886),0.59796,Resolve TPU miss rendezvous (#6781),,0
fix tpu transfer bug 2,0.56117874,Resolve TPU miss rendezvous (#6781),,0
fix tpu transfer bug,0.55549884,Resolve TPU miss rendezvous (#6781),,0
Enable TPU support (#868),0.66570425,Enabled manual optimization for TPUs (#8458),  added tpu docs   added tpu flags   add tpu docs + init training call   amp   amp   amp   amp   optimizer step   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   fix test pkg create (#873)   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added test return and print   added test return and print   added test return and print   added test return and print   added test return and print   Update pytorch_lightning/trainer/trainer.py   Co-Authored-By: Luis Capelo luiscape@gmail.com   Fix segmentation example (#876)   removed torchvision model and added custom model   minor fix   Fixed relative imports issue   Fix/typo (#880)   Update greetings.yml   Update greetings.yml   Changelog (#869)   Create CHANGELOG.md   Update CHANGELOG.md   Update CHANGELOG.md   Update PULL_REQUEST_TEMPLATE.md   Update PULL_REQUEST_TEMPLATE.md   Add PR links to Version 0.6.0 in CHANGELOG.md   Add PR links for Unreleased in CHANGELOG.md   Update PULL_REQUEST_TEMPLATE.md   Fixing Function Signatures (#871)   added tpu docs   added tpu flags   add tpu docs + init training call   amp   amp   amp   amp   optimizer step   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added test return and print   added test return and print   added test return and print   added test return and print   added test return and print   added test return and print   added test return and print   added test return and print   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Luis Capelo luiscape@gmail.com Co-authored-by: Akshay Kulkarni akshayk.vnit@gmail.com Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk Co-authored-by: Shikhar Chauhan xssChauhan@users.noreply.github.com,0
updated fast training docs with latest usage (#884),0.5940975,"In this release, we've made some large changes to achieve that goal. Not to worry, though! The only users affected by these changes are those who use custom implementations of Accelerator and Strategy (TrainingTypePlugin) as well as certain Plugins. In particular, we want to highlight the following changes:",,0
changed to absolute imports and added docs (#881),0.5135399,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",,0
Fixing Function Signatures (#871),0.4944651,Changed signatures:,,0
Changelog (#869),0.7186208,Full Changelog,  Create CHANGELOG.md   Update CHANGELOG.md   Update CHANGELOG.md   Update PULL_REQUEST_TEMPLATE.md   Update PULL_REQUEST_TEMPLATE.md   Add PR links to Version 0.6.0 in CHANGELOG.md   Add PR links for Unreleased in CHANGELOG.md   Update PULL_REQUEST_TEMPLATE.md ,1
Fix/typo (#880),0.557786,"In addition, we fixed:",  Update greetings.yml   Update greetings.yml ,0
Fix segmentation example (#876),0.46705785,Updated semantic segmentation example with custom u-net and logging (#1371),  removed torchvision model and added custom model   minor fix   Fixed relative imports issue ,0
fix test pkg create (#873),0.45110917,Refactored setup_training and remove test_mode (#5388),,0
added initial semantic segmentation example (#751),0.647287,Updated semantic segmentation example with custom u-net and logging (#1371),  added initial semantic segmentation example   removed unnecessary lines.   changed according to reviews   minor changes   Added some documentation for Dataset class   Fixed some long lines   added docstring for LightningModule ,0
drop duplicated guides (#864),0.5572416,Drop duplicate metrics (#5014),  drop duplicated guides   prevent copy to git ,0
Refactor callbacks (#776),0.794324,  callbacks:,"  Refactor callbacks   flake8   Update docstrings   Simplified callback, protected trainer   .set_trainer() check   update docs   missed super().ini()   Updated tests   Use uppercase   refine checkpoint callback tests   Added test_begin() and test_end() ",1
Fix global_step when gradient accumulation > 1 (#832),0.6733577,- global_step += 1,,0
advanced profiler describe + cleaned up tests (#837),0.58147717,Moved profilers to their own file (#7822),  add py36 compatibility   add test case to capture previous bug   clean up tests   clean up tests ,0
Allow user to specify 'step' key while logging metrics (#808),0.6327621,Allow logging of metrics together with hparams (#1630),  allow to specify 'step' key   add test   docs to log_metrics   fix test   rename   also rename ,0
add pep8speaks (#842),0.45711985,"If we forgot somebody or you have a suggestion, find us on Discord :zap:",,0
add Greeting action (#843),0.34858572,Changed to the NeptuneLogger (#16761):,  add Greeting action   Update greetings.yml   Co-authored-by: William Falcon waf2107@columbia.edu,0
add Auto rebase action (#845),0.42538506,Removed deprecated model hooks (#3980),,0
docs - anchor links (#848),0.49371302,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",  docs - add links   add desc. ,0
fix missing PyPI images & CI badges (#853),0.3654831,Renamed and moved core/step_result.py to trainer/connectors/logger_connector/result.py (#7736),,0
add autopep8 to Contributions guide (#852),0.42874247,Deprecated mode='auto' from ModelCheckpoint and EarlyStopping (#4695),  add autopep8 to Contrib.   simplify cmd   update GH templates   add pytest-flake8   update GH template ,0
update badges & drop Travis/Appveyor (#826),0.38236675,| API                                                                                                                      | Removal version | Alternative                                     |,  drop Travis   drop Appveyor   update badges ,0
Add GitHub Actions build (#823),0.39725253,@akihironitta @awaelchli @Borda @carmocca @kaushikb11 @krshrimali @mauvilsa @otaj @pre-commit-ci @rohitgr7 @semaphore-egg @tkonopka @wayi1,  init GH building   try run   circleci: install   add cache   add artifacts   fix cache   update ,0
update Docs req. (#824),0.53897965,Docs improvements,  upgrade sphinx   badge   Update README.md   Co-authored-by: William Falcon waf2107@columbia.edu,0
pin virtualenv (#822),0.38414347,  * `env_prefix`,https://github.com/tox-dev/tox/issues/1516,0
Use standard Comet env variable names in docstring (#816),0.6436448,Using .comet.config file for CometLogger (#1913), COMET_KEY -> COMET_API_KEY COMET_REST_KEY -> COMET_REST_API_KEY,0
set html_add_permalinks for docs (#812),0.32375622,Docs improvements,ref: https://www.sphinx-doc.org/en/master/usage/configuration.html#confval-html_add_permalinks,0
update deprecated messages (#810),0.70703006,Deprecation warning (#3844),  update deprecated messages   formatting   fix docs tags ,1
drop sklearn dependency (#801),0.79046535,Removed dependency on scikit-learn (#801),,1
Fix backwards compatibility for logging (#799),0.66945744,Removed logger_connector legacy code (#6733),,0
Docs (#813),0.72825706,Docs,  added outline of all features   updated common use cases doc   updated common use cases doc   updated common use cases doc   updated common use cases doc   updated common use cases doc   updated common use cases doc   updated common use cases doc   updated common use cases doc   updated common use cases doc   updated common use cases doc   updated common use cases doc   updated common use cases doc   updated common use cases doc   updated docs ,1
"drop torchvision, tests only (#797)",0.5355561,Removed dependency on torchvision (#797),"  drop torchvision, tests only   manifest   move test utils ",0
Tensorboard path generalisation (#804),0.5153633,Switch from tensorboard to tensorboardx in TensorBoardLogger (#15728),"  Allow experiment versions to be overridden by passing a string value. Allow experiment names to be empty, in which case no per-experiment subdirectory will be created and checkpoints will be saved in the directory given by the save_dir parameter.   Document tensorboard api changes   Review comment fixes plus fixed test failure for minimum requirements build   More format fixes from review ",0
fix test for profiler (#800),0.6911715,Moved profilers to their own file (#7822),  fix test for profiler   use allclose   user relative tol ,0
update Docs [links & formatting] (#769),0.60696197,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",  wip   wip   debug imports docs formatting   WIP   formatting   fix setup ,0
update license (#809),0.3972362,Key updates,  update license   Update LICENSE   Co-authored-by: William Falcon waf2107@columbia.edu,0
new feature for profiling training runs (#782),0.66441053, - Profiling: `Trainer(profiler=...)`,"  initial implementation   formatting, pass through profiler, docstring   call profiler during training   add initial tests   report stats when training is done   fix formatting   error handling, bugfix in passthroughprofiler   finish documenting profiler arg in Trainer   relax required precision for profiling tests   option to dump cProfiler results to text file   use logging, format with black   include profiler in docs   improved logging and better docs   appease the linter   better summaries, wrapper for iterables   fix typo   allow profiler=True creation   more documentation   add tests for advanced profiler   Update trainer.py   make profilers accessible in pl.utilities   reorg profiler files   change import for profiler tests   Co-authored-by: William Falcon waf2107@columbia.edu",0
Added Wandb entity attribute (#783),0.51186097,Removed wandb logger's finalize method (#1193),,0
Make default tqdm dict overridable (#749),0.7301318,"Moved the default tqdm_dict definition from Trainer to LightningModule, so it can be overridden by the user (#749)",  overridable tqdm_dict   Slim down default tqdm_metrics   gpu fix ,1
Set warnings : Unify epoch numbers to be zero-based : #675 (#786),0.6322893,"Renamed step_idx to step, epoch_idx to epoch, max_num_epochs to max_epochs and min_num_epochs to min_epochs (#589)",  [update] : #675 : set warnings   [fix] : #675 : remove white space ,0
Fixed broken link for 9 key lightning tricks. (#787),0.51491207,- Removed the deprecated ([#14471](https://github.com/Lightning-AI/lightning/pull/14471)),"""Research seed""  seed still missing as the repo seems to be not existing any longer",0
update CodeFactor badge (#779),0.4048155,"Updated DeepSpeed ZeRO (#6546, #6752, #6142, #6321)",,0
Create single file in TensorBoardLogger (#777),1.0000001,Create single file in TensorBoardLogger (#777),  write to single file   fix import ,1
Resolve some codefactor issues (#756),0.46219638,Porting fixes to autoscaler component (#16249),"  remove unnecessary pass statements   use isinstance for type checks   remove unnecessary else/elif after return   remove unnecessary return statements   move doc string to top   merge isinstance calls   remove unnecessary else/elif after raise   use list comprehension   do not use len without comparison   add missing shebang   revert isinstance check back to type   broke tests, because bool is actually subclass of int   add missing period to doc string   remove unnecessary pass statements   use isinstance for type checks   remove unnecessary else/elif after return   remove unnecessary return statements   move doc string to top   merge isinstance calls   remove unnecessary else/elif after raise   use list comprehension   do not use len without comparison   add missing shebang   revert isinstance check back to type   broke tests, because bool is actually subclass of int   add missing period to doc string   Fix default ckpt path when logger exists (#771)   rename logging -> loggers (#767)   move logging >> loggers   add warning   fix tests   logging alias   formatting   formatting   use isinstance for type checks   revert isinstance check back to type   broke tests, because bool is actually subclass of int   add more detail to tbptt example (#755)   add more detail to tbptt example   warn user about new arg in training_step   Co-authored-by: Vadim Bereznyuk kuynzereb@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jeremy Jordan 13970565+jeremyjordan@users.noreply.github.com",0
update package info (#768),0.54630554,Parsed local package versions (#13933),  update info   fix duplicate #733   update Appveyor budge #626   budge to master ,0
add more detail to tbptt example (#755),0.48477393,- Simplify the PL examples structure (shallower and more readable),  add more detail to tbptt example   warn user about new arg in training_step ,0
rename logging -> loggers (#767),0.6811391,Change default logger to a dedicated one (#1064),  move logging >> loggers   add warning   fix tests   logging alias   formatting   formatting ,0
Fix default ckpt path when logger exists (#771),0.59320027,Change default logger to a dedicated one (#1064),,0
"Removed dependency on pandas, instead use generic csv (#736)",0.72987974,Removed dependency on pandas (#736),"  removed dependency on pandas, instead use generic csv   remove mnist files, pushed by accident   added docstring and small fixes   Update memory.py   fixed path   Co-authored-by: William Falcon waf2107@columbia.edu",1
"for #330, use tqdm.auto in trainer (#752)",0.6481487,adding Trainer.tune() (#3293)," use tqdm.auto in trainer  This will import the ipywidgets version of tqdm if available. This works nicely in notebooks by not filling up the log. In the terminal it will use the same old tqdm. We might also want to consider passing in the tqdm we want as an argument since there may be some edge cases where ipywidgets is available but the interface doesn't support it (e.g. vscode?) or isn't working. In which case people will get a warning message, but may want to configure it themselves.   use from tqdm.auto in eval loop   indents ",0
Move logger initialization (#750),0.7655375,Change default logger to a dedicated one (#1064),,1
fix links to Docs (#744),0.5186639,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",  update log   fix links   formatting   fixing docs path   formatting ,0
early stopping check_val_every_n_epoch fix (#743),0.6757047,Deprecated ModelCheckpoint(every_n_val_epochs) in favor of ModelCheckpoint(every_n_epochs) (#8383),,0
Added optimizer_idx to backward call (#733),0.79461855,  * Removed `opt_idx` argument from `Callback.on_before_optimizer_step` callback method,,1
update logger init (#727),0.73528916,Change default logger to a dedicated one (#1064),  update logger init   formatting ,1
Check early stopping metric in the beginning of the training (#542),0.5425625,Removed training loop explicitly calling EarlyStopping.on_validation_end if no validation is run (#7069),  Early stopping fix   Update trainer.py   Don't force validation sanity check   fix tests   update   Added early_stopping check_metrics   Updated docs   Update docs   Do not call early stopping when validation is disabled   Co-authored-by: William Falcon waf2107@columbia.edu,0
release v0.6.0,0.6418257,This release includes:,,0
release v0.5.3.3,0.6499896,0.5.1,,0
added .md,0.40819728,"If we forgot somebody or you have a suggestion, find us on Discord :zap:",,0
doc reqs,0.6788341,Docs,,0
Issue #657 - Call on_train_end after early stopping (#723),0.6721006,Changed EarlyStopping callback from by default running EarlyStopping.on_validation_end if only training is run. Set check_on_train_epoch_end to run the callback at the end of the train epoch instead of at the end of the validation epoch (#7069),,0
implement forward and update args (#709) (#724),0.5469917,Args should come after the last positional argument (#1807)," implement forward and update args (#709)  Fixes the following issues as discussed in issue #709 1) Implement forward method wrapped. 2) Set default value for seed. ""None"" breaks tensorboard. 3) Update redundant hparams.data to new hparams.data_path. 4) Update 'use-16bit' to 'use_16bit' to maintain consistency.   Fix failing GPU tests (#722)   Fix distributed_backend=None test   We now throw a warning instead of an exception. Update test to reflect this.   Fix test_tube logger close when debug=True   Clean docs (#725)   updated gitignore   updated gitignore   updated links in ninja file   updated docs   finished callbacks   finished callbacks   finished callbacks   fixed left menu   added callbacks to menu   added direct links to docs   added direct links to docs   added direct links to docs   added direct links to docs   added direct links to docs   added direct links to docs   updated gitignore   updated links in ninja file   updated docs   finished callbacks   finished callbacks   finished callbacks   fixed left menu   added callbacks to menu   added direct links to docs   added direct links to docs   added direct links to docs   added direct links to docs   added direct links to docs   added direct links to docs   finished rebase   making private  members   making private  members   making private  members   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   set auto dp if no backend   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   fixed lightning import   cleared  spaces   cleared  spaces   cleared  spaces   cleared  spaces   cleared  spaces   cleared  spaces   cleared  spaces   cleared  spaces   cleared  spaces   cleared  spaces   finished lightning module   finished lightning module   finished lightning module   finished lightning module   added callbacks   added loggers   added loggers   added loggers   added loggers   added loggers   added loggers   added loggers   added loggers   set auto dp if no backend   added loggers   added loggers   added loggers   added loggers   added loggers   added loggers   flake 8   flake 8   fix docs path   updated gitignore   updated gitignore   updated links in ninja file   updated docs   finished callbacks   finished callbacks   finished callbacks   fixed left menu   added callbacks to menu   added direct links to docs   added direct links to docs   added direct links to docs   added direct links to docs   added direct links to docs   added direct links to docs   updated gitignore   updated docs   finished callbacks   finished callbacks   finished callbacks   fixed left menu   added callbacks to menu   added direct links to docs   added direct links to docs   added direct links to docs   added direct links to docs   added direct links to docs   added direct links to docs   finished rebase   making private  members   making private  members   making private  members   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   set auto dp if no backend   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   fixed lightning import   cleared  spaces   cleared  spaces   cleared  spaces   cleared  spaces   cleared  spaces   finished lightning module   finished lightning module   finished lightning module   finished lightning module   added callbacks   added loggers   added loggers   added loggers   added loggers   added loggers   added loggers   added loggers   added loggers   added loggers   added loggers   added loggers   flake 8   flake 8   fix docs path   flake 8   Update theme_variables.jinja   implement forward and update args (#709)   Fixes the following issues as discussed in issue #709 1) Implement forward method wrapped. 2) Set default value for seed. ""None"" breaks tensorboard. 3) Update redundant hparams.data to new hparams.data_path. 4) Update 'use-16bit' to 'use_16bit' to maintain consistency.  use self.forward for val step (#709)  Co-authored-by: Nic Eggert nic@eggert.io Co-authored-by: William Falcon waf2107@columbia.edu",0
Clean docs (#725),0.58535457,Docs improvements,  updated gitignore   updated gitignore   updated links in ninja file   updated docs   finished callbacks   finished callbacks   finished callbacks   fixed left menu   added callbacks to menu   added direct links to docs   added direct links to docs   added direct links to docs   added direct links to docs   added direct links to docs   added direct links to docs   updated gitignore   updated links in ninja file   updated docs   finished callbacks   finished callbacks   finished callbacks   fixed left menu   added callbacks to menu   added direct links to docs   added direct links to docs   added direct links to docs   added direct links to docs   added direct links to docs   added direct links to docs   finished rebase   making private  members   making private  members   making private  members   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   set auto dp if no backend   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   fixed lightning import   cleared  spaces   cleared  spaces   cleared  spaces   cleared  spaces   cleared  spaces   cleared  spaces   cleared  spaces   cleared  spaces   cleared  spaces   cleared  spaces   finished lightning module   finished lightning module   finished lightning module   finished lightning module   added callbacks   added loggers   added loggers   added loggers   added loggers   added loggers   added loggers   added loggers   added loggers   set auto dp if no backend   added loggers   added loggers   added loggers   added loggers   added loggers   added loggers   flake 8   flake 8   fix docs path   updated gitignore   updated gitignore   updated links in ninja file   updated docs   finished callbacks   finished callbacks   finished callbacks   fixed left menu   added callbacks to menu   added direct links to docs   added direct links to docs   added direct links to docs   added direct links to docs   added direct links to docs   added direct links to docs   updated gitignore   updated docs   finished callbacks   finished callbacks   finished callbacks   fixed left menu   added callbacks to menu   added direct links to docs   added direct links to docs   added direct links to docs   added direct links to docs   added direct links to docs   added direct links to docs   finished rebase   making private  members   making private  members   making private  members   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   set auto dp if no backend   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   fixed lightning import   cleared  spaces   cleared  spaces   cleared  spaces   cleared  spaces   cleared  spaces   finished lightning module   finished lightning module   finished lightning module   finished lightning module   added callbacks   added loggers   added loggers   added loggers   added loggers   added loggers   added loggers   added loggers   added loggers   added loggers   added loggers   added loggers   flake 8   flake 8   fix docs path   flake 8   Update theme_variables.jinja ,0
Fix failing GPU tests (#722),0.55525666,"Deprecated log_gpu_memory, gpu_metrics, and util funcs in favor of DeviceStatsMonitor callback (#9921)", Fix distributed_backend=None test  We now throw a warning instead of an exception. Update test to reflect this.  Fix test_tube logger close when debug=True,0
passing experiment to wandb (#720),0.48322618,Flattening Wandb Hyperparameters (#2459),,0
Update requirements.txt,0.5541601,Show a message when BuildConfig(requirements=[...]) is passed but a requirements.txt file is already present in the Work (#15799),,0
remove unnecesarry gradient freeze/unfreeze for single optimizer setup (#719),0.5936105,Moved the gradient unscaling in NativeMixedPrecisionPlugin from pre_optimizer_step to post_backward (#9606),,0
Added atomic checkpoint creation (#689),0.63730085,    # put all logic related to saving a checkpoint here,  Added atomic checkpoint creation   Added documentation for _atomic_checkpoint ,0
Fix issue_703: backward compatibility with python3.6 (#715),0.7879393,"Following Python's end-of-life, support for Python 3.6 has been removed.",,1
update org paths & convert logos (#685),0.33194202,Changed signatures:,  fix typos   update org paths   update links from READMe to docs   add svg logo   add svg logo-text   update logos   testing temp paths   prune links from readme   optimize imports   update logo   update paths in README   missing imports ,0
add version_ prefix to log_dir (#706),0.59935385,Removed collisions with logger versions by tying it to job id.,  add version_ prefix to log_dir   add version_ prefix ,0
fix docs path,0.46561283,Avoid relpath bug on Windows (#16164),,0
replace obj.copy() with copy.copy(obj) (#701),0.42583346,Changed overwrite to True (#16009),,0
clean v2 docs (#691),0.57717013,"Cleaning (#5948, #5949, #5950)",  updated gitignore   Update README.md   updated gitignore   updated links in ninja file   updated docs   Update README.md   Update README.md   finished callbacks   finished callbacks   finished callbacks   fixed left menu   added callbacks to menu   added direct links to docs   added direct links to docs   added direct links to docs   added direct links to docs   added direct links to docs   fixing TensorBoard (#687)   flake8   fix typo   fix tensorboardlogger drop test_tube dependence   formatting   fix tensorboard & tests   upgrade Tensorboard   test formatting separately   try to fix JIT issue   add tests for 1.4   added direct links to docs   updated gitignore   updated links in ninja file   updated docs   finished callbacks   finished callbacks   finished callbacks   fixed left menu   added callbacks to menu   added direct links to docs   added direct links to docs   added direct links to docs   added direct links to docs   added direct links to docs   added direct links to docs   finished rebase   making private  members   making private  members   making private  members   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   set auto dp if no backend   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   fixed lightning import   cleared  spaces   cleared  spaces   cleared  spaces   cleared  spaces   cleared  spaces   cleared  spaces   cleared  spaces   cleared  spaces   cleared  spaces   cleared  spaces   finished lightning module   finished lightning module   finished lightning module   finished lightning module   added callbacks   added loggers   added loggers   added loggers   added loggers   added loggers   added loggers   added loggers   added loggers   set auto dp if no backend   added loggers   added loggers   added loggers   added loggers   added loggers   added loggers   flake 8   flake 8   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
unify model test acc (#696),0.45812523,Ensure we set the eval/train flag correctly on accelerator model (#6877),,0
fixing TensorBoard (#687),0.67743593,Switch from tensorboard to tensorboardx in TensorBoardLogger (#15728),  flake8   fix typo   fix tensorboardlogger drop test_tube dependence   formatting   fix tensorboard & tests   upgrade Tensorboard   test formatting separately   try to fix JIT issue   add tests for 1.4 ,0
default logger is now tensorboard (#609),0.9322995,Changed the default logger to TensorBoardLogger (#609),  refactor   refactor   refactor   made tensorboard the default not test-tube ,1
log named parameters (#660),0.5591185,Changed gradient logging to use parameter names instead of indexes (#660),,0
Clearer disable validation logic (#650),0.76742995,"Simplified ""should run validation"" logic (#7682)",  Clearer disable validation logic   fix for fast_dev_run   flake8 fix   Test check fix   update error message ,1
Update Readme so that .test will work. (#659),0.6290453,- fixed all the .test() calls,"When one follows the Readme, the example will fail once we call trainer.test() because the methods are not overridden. Fixes https://github.com/williamFalcon/pytorch-lightning/issues/428",0
Feature: wandb logger (#627),0.68788934,Removed wandb logger's finalize method (#1193),  Basic wandb support   refactor(wandb): remove unused variables and document logger   docs(wandb): explain how to use WandbLogger   test(wandb): add tests for WandbLogger   feat(wandb): add save_dir   fix(wandb): allow pickle of logger   fix(wandb): save logs in custom directory   test(wandb): test import   docs(wandb): simplify docstring and use doctest   test: increase number of epochs for satisfactory accuracy   test(test_load_model_from_checkpoint): ensure we load last checkpoint   Co-authored-by: Chris Van Pelt vanpelt@wandb.com Co-authored-by: William Falcon waf2107@columbia.edu,0
fix deprecated tng and abstract ligntning (#644),0.5377251,Deprecation warning (#3844),,0
Neptune integration (#648),0.4482999,"It now supports neptune-client 0.16.16 and neptune >=1.0, and we have replaced the log() method with append() and extend().","  added neptune integration   added tests for NeptuneLogger, added neptune to docs   updated link to neptune support   fixed docstrings, fixed try/except in tests, changed append_tags input   fixed docstrings line lenght   bumped epoch nr in model restore tests   added tags support for single strings   fixed passing neptune token to backend   fixed project name in offline mode   added save_top_k=-1 to checkpoint callback   reformated initialization of neptune in online mode   bumped epoch nr to 4 in test_load_model_from_checkpoint   bumped epoch nr to 5   Co-authored-by: William Falcon waf2107@columbia.edu",0
Fix GAN training. (#603),0.61603695,We have fixed GAN training - supporting multiple optimizers., fix dangling gradients  make sure only the gradients of the current optimizer's paramaters are calculated in the training step.   add note about multiple optimizer gradient update   Update training_loop.py ,0
Remove extraneous f character from f-string. (#679),0.497465,Remove beta arg from F1 class and functional (#5076),"Makes tracking experiment names confusion, especially when using uuids.",0
CI pass (#671),0.36628956,TPU training (#2708),  fix pillow in test   test acc   update version in deprecated msg ,0
Fix the number of training batches used in the training loop (#653),0.71446306,Integrated TrainingEpochLoop.total_batch_idx (#8598),  Fix the number of processed training batches   Fix tests   fix tests   fix tests   One more attempt   Fix another test ,1
Fix percent_checks (#649),0.47508776,test_percent_check in favour of limit_test_batches,  fix percent_checks   Added _percent_range_check   remove max ,0
Update requirements.txt (#664),0.60410875,Setup: added requirement freeze for the next major version (#14480),Fix typo 'buildins' -> 'builtins',0
Fix amp tests (#661),0.6205561,enable_pl_optimizer=False by default to temporarily fix AMP issues (#5163)," Run AMP tests in their own process  With opt_level=""O1"" (the default), AMP patches many torch functions, which breaks any tests that run afterwards. This patch introduces a pytest extension that lets tests be marked with @pytest.mark.spawn so that they are run in their own process using torch.multiprocessing.spawn so that the main python interpreter stays un-patched. Note that tests using DDP already run AMP in its own process, so they don't need this annotation.  Fix AMP tests  Since AMP defaults to O1 now, DP tests no longer throw exceptions. Since AMP patches torch functions, CPU inference no longer works. Skip prediction step for AMP tests.  typo",0
Fix ci xos (#647),0.37967348,Changed iou [func] to allow float input (#4704),  upgrade python 3.7   upgrade python 3.7 ,0
fix of issue 600 (#625),0.55913126,This release fixes that core issue,,0
fix metric name to work with default earlystopping (#628),0.6150299,"Removed experimental Metric API (#3868, #3943, #3949, #3946), listed changes before final removal:",,0
Fix #618 Change papi to api (#619),0.48548505,This release has breaking API changes. See #124 for all details. ,  Change papi to api   Added try catch for old/new api reference ,0
Fix early stopping off by 2 (min_epochs) (#617),0.6354751,Deprecated max_nb_epochs and min_nb_epochs (#567),  fix early stopping off by 2   add min_epochs example in docs ,0
Allow for multiple example inputs when creating summary (#543),0.43356937,Simplify the PL examples structure (shallower and more readable) (#1247),,0
"Change nb to num in ABCs, comments, and tqdm logging (#613)",0.47716725,and nb_val_batches to num_val_batches (#567),"  Change nb to num in ABCs, comments, and tqdm logging   Fix warnings text   Make warnings one line   Change num to number in comments ",0
update GitHub templates (#612),0.48244467,Included app templates to the lightning and app packages (#13731),,0
"Fix logger, tensorboard (#610)",0.87312174,Changed the default logger to TensorBoardLogger (#609),  fix logger tests   fix missing flush   fix tensorboard   fix namespace   fix flush   fix add_hparams ,1
Implement TensorboardLogger (#607),0.7834049,We have added own custom Tensorboard logger as default logger. ,  Implement TensorboardLogger   Pass default_save_path to trainers   Update tensorboard.py ,1
Make sure train doesn't crash when called at max_epoch (#608),0.73384213,def on_train_epoch_end(self):,,1
fix Logger tests for Win (#605),0.6865991,Cleaning up stale logger tests (#3490),  fix mlflow test   fix mlflow test   update logger / mlflow   flake8   fix appveyor ,0
Update logging.py (#602),0.6322872,    logger.do_something(),,0
Additional hooks (#598),0.80101544,New Hooks,  Renamed on_sanity_check_start to on_train_start and added on_train_end to ModelHooks   changed tests to use on_train_start instead of on_sanity_check_start ,1
"Simplify variables: step, epoch, max_epochs, min_epochs (#589)",0.7225994,"Renamed step_idx to step, epoch_idx to epoch, max_num_epochs to max_epochs and min_num_epochs to min_epochs (#589)",,1
prevent Travis caching (#590),0.3802316,- Added profiling of `LightningDataModule` hooks ([#12971](https://github.com/Lightning-AI/lightning/pull/12971)),  change CI install   change CI install   change CI install ,0
"Docs and Tests for ""gpus"" Trainer Argument (#593)",0.69021434,"| Arguments Trainer(num_processes=..., gpus=..., tpu_cores=..., ipus=...)                                    | 2.0             | Trainer(accelerator=..., devices=...)         |","  add table for gpus argument   fix typo in error message   tests for supported values   tests for unsupported values   fix typo   add table for gpus argument   fix typo in error message   tests for supported values   tests for unsupported values   fix typo   fix typo list->str   fix travis warning ""line too long"" ",0
Fix number of total steps shown in progress bar during sanity validation check when number of validation dataloaders >= 2 (#597),0.5971857,enabled multiple dataloaders for validation.    , type: debug  Calculate the adequate number of steps to run during sanity_check. This fixes the bug when there are two or more validation dataloaders.  Before: total=self.num_sanity_val_steps  After: total=self.num_sanity_val_steps*len(self.get_val_dataloaders())   type: refactor   Put total=... in the next line  type: refactor  run flake8,0
Fix CometML tests (#585),0.5324149,- fixed all the .test() calls,  monkeypatch atexit.register to fix problem with cometml logging   Use experiment id for version in cometml ,0
extend documentation (#569),0.673385,Docs improvements,  extend documentation   update index   fix list ,0
update GitHub templates (#601),0.46282312,Included app templates to the lightning and app packages (#13731),,0
add slack badge (#583),0.38599628,"If we forgot someone or have any suggestion, let us know in Slack :zap:",  add slack badge   Update README.md ,0
"rename trainer modules, drop _mixin (#571)",0.6303044,Remove model.trainer call inside of dataloading mixin (#7317),"  rename trainer modules, drop _mixin   fix imports ",0
Abstract Mixin classes (#572),0.50559133,Removed unused mixin attributes (#6487),  make partial Trainer classes as abstract   add empty attributes/methods   flake8   fix mixin order   update abstact   reorder ,0
fixed gan template (#528),0.5953266,We have fixed GAN training - supporting multiple optimizers.,  fixed gan template   Update gan.py ,0
inspect training_step for opt_idx (#573),0.7175662,# 3. Remove the `optimizer_idx` argument from `training_step`,,1
use print for INFO and lower levels summarize() (#580),0.43360662,Model summary: add 1 decimal place (#4745),  use print for INFO and lower levels summarize()   use logging.INFO instead of magic number   bring logging.info back for other cases   move logging config to init.py   prepend the model summary with a newline ,0
filter param with no grad (#579),0.50397694,Sanitize None params during pruning (#6836),,0
fix logging error (#575),0.6294857,Removed logger_connector legacy code (#6733),  fix logging error   no need for the '+' sign   move space to beginning of next line ,0
fix defecation warnings (#570),0.507455,Silenced some warnings. verified ddp refactors (#3483),  fix defecation warnings   flake8   update deprecations ,0
rename variables nb -> num (#567),0.5764561,and nb_val_batches to num_val_batches (#567),"  rename nb -> num   flake8   batch_nb, epoch_nb, gpu_nb, split_nb   add _num deprecations ",0
prune tests (#564),0.54554117,Pruned requirements duplicity (#13739),  format docstring in tests   prune unused vars   optimize imports   drop duplicated var ,0
Use pytest tmpdir fixture  (#482),0.4203736,Remove pytest as a requirement to run app by @manskx in https://github.com/Lightning-AI/lightning/pull/15449,  Use pytest tmpdir   Switch to tmpdir fixtures   Switch to tmpdir fixture   tmpdir fixture   Fix more conflicts ,0
Support torch.optim.lr_scheduler.ReduceLROnPlateau (#320),0.78297126,        scheduler = torch.optim.lr_scheduler.OneCycleLR(,  feat: add reducelronplateau callback   feat: use reducelronplateau callback in trainer   feat: only on unsupported lr schedulers   feat: last but not the least merge of master   feat: merge master   feat: support only on scheduler in reduceLrOnPlateauScheduler   refactor: code style   Update pt_callbacks.py   Update trainer.py   Update train_loop_mixin.py   Update trainer.py   Update train_loop_mixin.py ,1
fix for pyTorch 1.1 (#552),0.8057413,Enable PyTorch 1.7 compatibility (#3541),  min pyTorch 1.1   try fixed test-tube   try fixed test-tube   try fixed test-tube   cleaning   Update requirements.txt ,1
Add resuming from specific checkpoint (#516),0.836607,Resuming from checkpoints (#16167),  Add resume_from_checkpoint   Fix variable name   515 Remove did_restore   515 Simplify code   515 Update doc for resume_from_checkpoint   515 Add on_gpu ,1
Correct behavior for argument gpus in Trainer (#561),0.7995447,  * Removed the `Trainer(gpus=...)` argument,,1
fixed tests,0.53854066,This means we fixed many TPU bugs we hadn’t caught before because we had no tests.,,0
Dp default (#560),0.5779028,DDP(2) backend (#2796),  set auto dp if no backend   fix imagenet example   run flake8 first to fail build on syntax first ,0
Sphinx generated documentation (#521),0.5093102,Pinned sphinx-autodoc-typehints with <v1.15 (#11400),"  upgrade req.   move MkDocs   create Sphinx   init Sphinx   move md from MkDocs to Sphinx   CI: build docs   build Sphinx   formatting move docs from MD to docstring in particular package/modules formatting add Sphinx ext. rename root_module to core drop implicit name ""_logger"" drop duplicate name ""overwrite"" fix imports use pytorch theme add sample link mapping try fix RTD build use forked template fix some docs warnings fix paths add deprecation warnings fix flake8 fix paths revert refactor revert MLFlowLogger   revert example import   update link   Update lightning_module_template.py ",0
speed-up testing (#504),0.52109146,Speed/memory optimizations.,"  extend CI timeout   add short MNIST   lower dataset and stop thr   refactor imports   formatting   early stop   play params   play params   minor refactoring   Conflicts: pytorch_lightning/testing/init.py pytorch_lightning/testing/lm_test_module.py pytorch_lightning/testing/lm_test_module_base.py pytorch_lightning/testing/lm_test_module_mixins.py pytorch_lightning/testing/model.py pytorch_lightning/testing/model_base.py pytorch_lightning/testing/model_mixins.py pytorch_lightning/testing/test_module.py pytorch_lightning/testing/test_module_base.py pytorch_lightning/testing/test_module_mixins.py  typo  Co-Authored-By: Ir1dXD sirius.caffrey@gmail.com  Revert ""refactor imports""  This reverts commit b86aee92  update imports",0
Refactor: name modules (#548),0.6430452,Renamed utils modules (#5199),  refactor: rename some modules   add deprecation warnings   fix paths ,0
Move model to cuda before creating optimizer (#554),0.5595016,"Case 3: Model fits into GPU memory. No action required, use any strategy you want. ",,0
fix for pyTorch 1.2 (#549),0.7761569,Enable PyTorch 1.7 compatibility (#3541),  min pytorch 1.2   fix IterableDataset   upgrade torchvision   fix msg ,1
fixing bug in testing for IterableDataset (#547),0.72047544,Disabled sampler replacement when using IterableDataset (#11507),,1
CI buils with minimal and latest requirements (#500),0.36662403,The point of this release is more bug fixes ahead of v 1.0.0. We now have CI tests on TPU thanks to @zcain117 from Google! :slightly_smiling_face:,  install nim req.   update requirements   drop Cython ,0
Copy batch for local forward (#532),0.5094202,"          input, target = batch",,0
Remove unneeded filename print (#540),0.43387684,Rename failed -> error in tables (#15608),,0
Avoid race condition in creating checkpoint directories (#530),0.6061781,Asynchronous Checkpointing," Avoid race condition in creating checkpoint directories  In multi-GPU training, several processes run the code that creates checkpoint dirs. This fix avoids a probably rare situation (but it happened to me) where another process created a dir between the exists check and the makedirs call.  Remove the now unneeded check for dir existence",0
Write progress bar to stdout (#531),0.7490409,Changed the default progress bar to print to stdout instead of stderr (#531),  Default write progress bar to stdout   Change validation progress too ,1
change Checkpoint callback's save_best_only to save_top_k (#128),0.7215117,"-def on_save_checkpoint(self, checkpoint):","  docs: enable syntax highlight   feat: change Checkpoint callback's save_best_only to save_top_k   fix #70   docs: update docs for save_top_k   revert other files   style: lint for travis-ci   fix typo   make flake8 happy   update according to review   add tests   rename func to private   add doc on save_top_k == 0   make flake8 happy   update according to PR comments   change some f-strings   Update pt_callbacks.py   Update test_models.py   update options   create folders   Update test_models.py   change epoch num   support calling multiple times, add docs and tests   update docs   roll back changes in earlystopping   clean test files   make flake8 happy   fix epoch number   update tests about epoch numbers   clean debugging code   fix testing utils codes   fix testing utils codes   fix testing utils codes   fix testing utils codes   change save_dir to tests/tests according to previous lines   remove unused overwrite option   make flake8 happy   change var name as per review   make flake8 happy   update property name to work on master   elaborate in the docs   update docs as per review   revert previous commit   accidentally pressed wrong button when solving conflicts",1
Fix incorrect handling of on_batch_end edge cases in run_training_batch (#509),0.7131725,"Overriding the on_train_batch_{start,end} hooks in conjunction with taking a dataloader_iter in the training_step no longer errors out and instead shows a warning (#16062)"," Fix returning only 2 values on an early exit.   This fixes a bug  ValueError: not enough values to unpack (expected 3, got 2)   Update train_loop_mixin.py   Change to return dict   The return value was actually a dict even though that variable is initialized as a list.",1
fix failing on pip (#503),0.5069238,pip install rich,,0
Update methods.md (#507),0.50847274,Changed ModelCheckpoint version suffixes to start at 1 (#5008),,0
Escape percentage symbol in argparse (#499),0.5534978,import argparse,,0
Add circle CI for building PyTorch 1.1/1.2/1.3 (#502),0.65455973,PyTorch 1.5  support,  add CircleCI config   fix CircleCI   fix CircleCI ,0
Comet fix (#481),0.52953625,Using .comet.config file for CometLogger (#1913),"  Fixing comet ml bug and adding functionality   Updating documents   Fixing code style issues in comet_logger   Changing comet_logger experiment to execute lazily   Adding tests for comet_logger and addressing comments from @Borda   Setting step_num to optional keyword argument in log_metrics() to comply to other loggers   Adding offline logging mode for comet_ml, updating tests and docs   Switching to MisconfigurationException ",0
fixed issue where callback_metrics was replaced instead of updated (#492),0.72392845,Removed callback metrics from test results obj (#2994),,1
Enable apex O2 + dp (#493),0.6894685,Changed default apex level to 'O2' (#2362),  remove O2 crash   remove O2 crash   bananas ,0
bananas (#494),0.4951059,        self.bananas = [],,0
release v0.5.3.2,0.6549046,0.5.1,,0
fix imagenet example,0.4627802,Updated semantic segmentation example with custom u-net and logging (#1371),,0
ImageNet Example (#476),0.5291911,Updated semantic segmentation example with custom u-net and logging (#1371),  ImageNet example   cleanup   cleanup   Minor changes from feedback   More cleanup ,0
Fix setup-doc for pypi (#472),0.4694108,Add missing python-multipart dependency (#17244),  add Twine to CI   freeze Twine   freeze Twine   minor refactoring   try another   fix req.   update README   fix doc   fix multiple req. test-tube ,0
Fix tbptt docs (#484),0.47400767,"Removed support for self.log(tbptt_reduce_fx) and self.log(tbptt_pad_token). Please, open a discussion explaining your use-case if you relied on these. (#7644)",,0
Remove errnonoeous comma in logging call (#474),0.47667304,Re-Enable Logger's ImportErrors (#1938),,0
make checkboxes (#473),0.34248888,Try it out:,,0
release v0.5.3.1,0.65166587,0.5.1,,0
fix install,0.45697942,"Fixing missing packaging package in dependencies, which was affecting the only installation to a very blank system.",,0
Update setup.py,0.6216215,from setuptools import setup,,0
need to fix readme for pypi,0.44231963,Add missing python-multipart dependency (#17244),,0
release v0.5.3,0.65634245,0.5.1,,0
Release (#467),0.5432498,This release includes:,  smurf ethics   smurf ethics   removed auto ddp fix   removed auto ddp fix   removed auto ddp fix   removed auto ddp fix   removed auto ddp fix   removed auto ddp fix ,0
Update CONTRIBUTING.md,0.40653342,"If we forgot someone due to not matching the commit email with the GitHub account, let us know :]",,0
Fix ModelCheckpoint default paths (#413),0.7797768,Deprecated filepath in ModelCheckpoint (#4213),  Make name and version properties required   Warn before deleting files in checkpoint directory   Get default checkpoint path from any logger   Fix typos   Uncomment logger tests   Whitespace   Update callback_config_mixin.py   checkpoints and version file names would just have a number. it's easy to tell what you're looking at with version_ prepended   Address comments   Fix broken tests ,1
Ddp2 fix (#448),0.73075485,decoupled DDP2 (#3816),  added training_end   added training_end   added training_end   added training_end   added training_end   added training_end   added training_end   added training_end   added training_end   added training_end   added training_end   added training_end   allow ddp and apex to be configured   allow ddp and apex to be configured   bananas   bananas   bananas   bananas   bananas   bananas   bananas   bananas   bananas   bananas   bananas   bananas   bananas   bananas   bananas   added eval and train for redundancy   added eval and train for redundancy   added training_end   added training_end   added training_end   added training_end   added training_end   added training_end   added training_end   added training_end   added training_end   added training_end   added training_end   added training_end   allow ddp and apex to be configured   allow ddp and apex to be configured   bananas   bananas   bananas   bananas   bananas   bananas   bananas   bananas   bananas   bananas   bananas   bananas   bananas   bananas   bananas   added eval and train for redundancy   added eval and train for redundancy ,1
added eval and train for redundancy (#464),0.5782069,training forward refactor (#3134),,0
Fix min_max gpu memory logging bug (#453),0.7152694,"Deprecated log_gpu_memory, gpu_metrics, and util funcs in favor of DeviceStatsMonitor callback (#9921)",  452 Fix ValueError   452 Use subprocess.run   452 Simplify code for gpu_memory_map   452 Simplify code for min max memory   452 Add test for get_memory_profile   452 Use os.sep   452 Use os.linesep ,1
change print to logging (#457),0.8197847,switched from print to logging,  change print to logging   always use logging.info   use f-strings   update code style   set logging configs   remove unused code ,1
add coding styleguide (#460),0.40719807,Syntax changes are: , Update CONTRIBUTING.md  add coding styleguide  Update CONTRIBUTING.md,0
fixing test (#451),0.5721128,- fixed all the .test() calls,,0
Split progress bar (#449),0.73273623,Better progress bar (#16695),  Splitted progress bars   Iterable dataset total batches fix   Use dynamic ncols and use batch as units   Count epochs from 1 in progress bar   Fix for disabled progress bar   Code simplifications ,1
packed sequence clarification in train_dataloader (#443),0.5775185,def train_dataloader(...):,  packed sequence clarification in train_dataloader   moved changes to training loop   removed changes from required interface   added index entry ,0
[WIP] Fix wrong example paths in README.md (#444),0.51339483,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",  Fix wrong example paths   correct dataloading wrong condition in Readme ,0
Add tbptt (#429),0.437822,"@Borda, @justusschock, @kandluis, @mauvilsa, @shuyingsunshine21, @tchaton","  Add truncated bptt   Fix rebase error   AutoPep8   Address comments, incl default bptt_split impl   Add tbptt test   Add default split for lists/tuples   Add tbptt docs   Fix trainer spacing   Update RequiredTrainerInterface.md ",0
Update Docs for current checkpointing behaviour (#445),0.7462238,all the checkpoint issues should be gone now (including backward support for old checkpoints),"Related issue #432 The old documentation suggested that the way to restore a training session is to use a test_tube Experiment. Trainer no longer takes an experiment as a parameter, so it seems the current way to restore a training session is to pass an experiment via a TestTubeLogger. Even if this is not the most elegant solution, updating the docs will at least point new users in the right direction.",1
Set total number of batches in progress bar while testing (#425),0.63356763,set validation to a fix number of batches,,0
Fixed total number of batches (#439),0.7050971,set validation to a fix number of batches,  Fixed total number of batches   Fixed flake8 warning   Update train_loop_mixin.py   Update train_loop_mixin.py ,1
mem clear (#440),0.5043624,"Cleaning (#5948, #5949, #5950)",  mem clear   mem clear ,0
Catch exceptions when optional dependencies are missing (#442),0.5587151,Improved support for running apps when dependencies aren't installed (#15711),,0
add package info (#395),0.5319866,"    name=""my-package"",",  add package info #358   Update init.py   wrap package info   update CI   fix package info   fix for #388   prune duplicated configs   fix install   use req from file   move info to sep. module drop comments from req   add setup req.   add setup req.   update get info   refactor init   update pip   fix failing on buildins   fix failing open   fix test imports   fix tests   fix pep8 ,0
makes checkpoint process safe (#431),0.69408107,    # put all logic related to deleting a checkpoint here,,0
Back hook (#424),0.548809,New Hooks,  Fixes #356   Fixes #356   Fixes #356   Fixes #356   Fixes #356   Fixes #356 ,0
Loaders (#422),0.5406281,@ptl.data_loader,  refactor dataloading   refactor dataloading   refactor dataloading   refactor dataloading   refactor dataloading   refactor dataloading   refactor dataloading   refactor dataloading ,0
hpc restore takes priority over non hpc weights (#419),0.46954396,Deprecated the use of CheckpointConnector.hpc_load() in favor of CheckpointConnector.restore() (#7652),  hpc restore takes priority over non hpc weights   hpc restore takes priority over non hpc weights   hpc restore takes priority over non hpc weights   hpc restore takes priority over non hpc weights   hpc restore takes priority over non hpc weights   hpc restore takes priority over non hpc weights   hpc restore takes priority over non hpc weights ,0
clear memory cache before train starts (#418),0.56387365,"This ""silent"" accumulation of memory lead to many users scratching their head when they found that after training for 20 hours the epoch crashed randomly with an out-of-memory error. This also meant that avoiding this behaviour required code changes, as simply overriding this hook would force this behaviour, regardless of whether you used the outputs. This was exacerbated by users not knowing the difference between the on_train_epoch_end (does not store outputs) and training_epoch_end (does store outputs) hooks.",  clear memory cache before train starts   clear memory cache before train starts ,0
Move global_step incrementing (#412),0.74794185,+ global_step += 1,"  Move global_step incrementing to the end of a batch loop, per https://github.com/williamFalcon/pytorch-lightning/issues/411   Move met_batch_limit condition to the end   cleanup whitespace   Update train_loop_mixin.py ",1
refactored tests (#417),0.61583436,Updated app testing (#16000),  refactored tests   refactored tests   refactored tests   refactored tests   refactored tests   refactored tests   refactored tests   refactored tests   refactored tests ,0
parse_gpu_ids fix (#382),0.67657524,  * Deprecated the `pytorch_lightning.utilities.device_parser.parse_gpu_ids` in favor of `lightning_lite.utilities.device_parser.parse_gpu_ids`,"  Unit tests for num_gpu property as proxy for __parse_gpu_ids.   Refactoring __parse_gpu_ids   Moved the function outside the class as it is an utility function and did not depend on class in any way.   Added unit tests for it.   Mocked torch.cuda.device_count function in tests.   This allows the tests to be run on machines that do not have gpus.  Fixed the parse_gpu_ids function to handle -1 case.  Function now handles -1 the same way as it does for '-1'.  Unit tests for root_gpu added.  Added backend as a parameter as currently depending on backend set or not, code fails with exception in certain circumstances, before giving a wrong answer.  Moved __set_root_gpu function out of the class.  This function does not depend on the class and can be tested more easily this way. Also added unit tests for this function. They simply reuse data for the root_gpu property.   determine_root_gpu_device passes unit tests.   num_gpus passes unit tests.   Also added a None test for this function.  parse_gpu_ids tests changed to reflect desired state after refactoring.  Planning to refactor parse_gpu_ids to always return list of ints. This will simplify code that use output of this function.    parse_gpu_ids always returns lists   parse_gpu_ids checks given ids against available ids parse_gpu_ids raises exception for non existant ids parse_gpu_ids returns None when no gpus are available cleaned up determine_root_gpu_device cleaned up num_gpus property  Updated unit tests to reflect changes in the functions   Flake8 fixes   Moved fixture code up before where it is used.   Updated documentation.   Changed tests to match the API:  gpus=-1 or gpus='-1' should use all available gpu devices gpus=N N=0: no gpus should be used. N>0: N gpus should be used    gpus=list of ints or a comma separated string of numbers:     Use the gpus indicated by the list or the string.   Fixed code to pass all the changed tests for parsing gpus param.   Refactoring parse_gpu_ids function.   flake8 fixes.   Updating documentation.   flake8 fixes.   flake8 fixes.   flake8 fixes   Update trainer.py   Update dp_mixin.py   Make reduce_distributed_output a stand alone function. Fix imports. Fix flake8.   Add comet_ml dependency to tests requirements.txt   Revert ""Make reduce_distributed_output a stand alone function. Fix imports. Fix flake8.""   This reverts commit eac0338  Merge with master.",0
Save / Load Hyperparameters with checkpoint (#415),0.7326307,Enhanced load_from_checkpoint to also forward params to the model (#1307),  Save and load hparams from checkpoints   Update docs   Add warning when not saving hparams   Missing import   Update .run_local_tests.sh   Update lm_test_module_mixins.py   Update lightning_module_template.py ,1
Merge branch 'hparams_from_checkpoint' of https://github.com/neggert/pytorch-lightning,0.68968344,Full commit list: https://github.com/PyTorchLightning/pytorch-lightning/compare/1.6.0...1.7.0,,0
Update lightning_module_template.py,0.677462,Use correct python version in lightning component template (#13790),,0
Update lm_test_module_mixins.py,0.5415484,Removed unused mixin attributes (#6487),,0
Update .run_local_tests.sh,0.44225276,python script.py test,,0
moved env var to before import (#414),0.5348879,Module imports are now catching ModuleNotFoundError instead of ImportError (#9867),,0
Missing import,0.49117482,Module imports are now catching ModuleNotFoundError instead of ImportError (#9867),,0
Add warning when not saving hparams,0.69146454,Don't raise a warning when nn.Module is not saved under hparams (#12669),,0
Update docs,0.66718185,Docs improvements,,0
Save and load hparams from checkpoints,0.80780303,checkpoints now store hparams,,1
fixed bag callback=False or None at trainer_io.py (#409),0.6280016,Changed callbacks argument in Trainer to allow Callback input (#5446),,0
moved COMET_DISABLE_AUTO_LOGGING out of modeule for flake8 compliance (#410),0.4607772,Using .comet.config file for CometLogger (#1913),  moved COMET_DISABLE_AUTO_LOGGING out of modeule for flake8 compliance   Update init.py ,0
Update BECOMING_A_CORE_CONTRIBUTOR.md,0.4352089,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",,0
Rename CORE_CONTRIBUTOR_GUIDELINES to BECOMING_A_CORE_CONTRIBUTOR.md,0.5172671,"All documentation and examples are now recommending the new, less ambiguous names.",,0
Create CORE_CONTRIBUTOR_GUIDELINES,0.42301822,Contributors,,0
Minor imports cleaning (#402),0.60333574,Wrapped imports for traceability (#13924),  code cleaning   drop unused imports   optimize imports ,0
added comet testing dep,0.5256839,Using .comet.config file for CometLogger (#1913),,0
flake8,0.39054424,"@airglow, @akshaykvnit, @AljoSt, @AntixK, @awaelchli, @baeseongsu, @bobkemp, @Borda, @calclavia, @Calysto, @djbyrne, @ethanwharris, @fdelrio89, @hadim, @hanbyul-kim, @jeremyjordan, @kuynzereb, @luiscape, @MattPainter01, @neggert, @onkyo14taro, @peteriz, @shoarora, @SkafteNicki, @smallzzy, @srush, @theevann, @tullie, @williamFalcon, @xeTaiz, @xssChauhan, @yukw777",,0
Fixed val interval (#405),0.47880867,Trainer(val_check_interval=100),  added fixed frequency val batch check   added fixed frequency val batch check   Finished IterableDataset support   flake8   flake8   flake8 ,0
Logger consistency (#397),0.6758657,Un-balanced logging properly supported (#5119),"  added comet logger   bug fix in cases where comet was not imported before torch   fixed mlflow logger to be consistent with docs, updated cometLogger and cometLoggers docs + flake 8 compliance ",0
Refactor (#407),0.72472143,Refactoring,"  moved dp, ddp outside of trainer   added main mixins   finished major mixin refactor   flake8   finished major mixin refactor   finished major mixin refactor   finished major mixin refactor   finished major mixin refactor   finished major mixin refactor   finished major mixin refactor   finished major mixin refactor ",1
updated test runner,0.59071434,Updated app testing (#16000),,0
Provide backward compatibility for #124 (#400),0.57629263,Backward Incompatible Changes,  Provide backward compatibility for e681253   typo fix ,0
Fix testing for mac OS (#399),0.5190246,Updated app testing (#16000),  fix test for MacOS   formatting   fix pkg names ,0
Fixes #347 (#393),0.62417203,"In addition, we fixed:",,0
changes examples to pl_examples for name connflict,0.55011445,- Simplify the PL examples structure (shallower and more readable),,0
Fixes #361 (#391),0.61728275,"In addition, we fixed:",,0
Test fx (#390),0.47052985,Fully tested!,  changes to test fx   changes to test fx   changes to test fx   changes to test fx   changes to test fx   changes to test fx   changes to test fx   changes to test fx   changes to test fx   changes to test fx ,0
working on dp state fix,0.5134529,DDP(2) backend (#2796),,0
removed mlflow and custom logger  tests (#389),0.6352063,Removed logger_connector legacy code (#6733),  changes to seed for tests   changes to seed for tests   changes to seed for tests   changes to seed for tests   changes to seed for tests   changes to seed for tests   changes to seed for tests   changes to seed for tests   changes to seed for tests   changes to seed for tests   changes to seed for tests   changes to seed for tests   changes to seed for tests   changes to seed for tests   changes to seed for tests ,0
changes to seed for tests,0.5982862,seed_everything now fails when an invalid seed value is passed instead of selecting a random seed (#8787),,0
Fixed link to trainer.py github code (#386),0.64624155,Removed pytorch_lightning/trainer/training_loop.py (#7985),,0
Fixes #380,0.6331239,"In addition, we fixed:",,0
Loss keys (#387),0.5858995,        return loss0,  any key in logs or progress bar is a candidate for callback metric   any key in logs or progress bar is a candidate for callback metric ,0
Fix off-by-one epoch length (#377),0.6752541,Changed epoch indexing from 0 instead of 1 (#2289),,0
"Revert ""add package info (#359)"" (#384)",0.5353829,Moved accelerators and plugins to its legacy pkg (#5645),This reverts commit a7f26a67ac1b28f2ddd47eaec96ca7e92befbfc8.,0
add package info (#359),0.5244242,"    name=""my-package"",",  add package info #358   Update init.py   Update init.py ,0
update appveyor badge (#378),0.4362154,Updated app testing (#16000),"@williamFalcon needs to create own appveyor ""account"" and update the badge ",0
changed lbfgs test min acc,0.37417665,    --optimizer.lr=0.01 \,,0
changed lbfgs test,0.40759152,Cleaning up stale logger tests (#3490),,0
dp tests,0.51704323,"Hmm, I wonder, does this only apply to DDP?",,0
updated gitignore,0.42645717,Pkg: fix parsing versions by @Borda in https://github.com/Lightning-AI/lightning/pull/15401,,0
fix demo,0.44306976,- fixed all the .test() calls,,0
fixing tests (#372),0.6606427,- fixed all the .test() calls,  fixing tests   fixing tests   fixing tests   fixing tests   fixing tests   fixing tests   fixing tests   fixed tests   fixed tests   fixed tests   fixed tests   fixed tests   fixed tests   fixed tests   fixed tests   fixed tests ,0
fix val logging (#362),0.66089904,bug fix with logging val epoch end + monitor (#3812),  fix test   fix test   fix test   fix test   fix test   fix test   fix test   fix test   fix test   fix test   fix test   fix test   fix test   no warnings always   no warnings always   no warnings always   no warnings always ,0
fix domain_templates (#365),0.3703758,Unification of app template: moved app.py to root dir for lightning init app <app_name> template (#13853),,0
"Allow disabling default logger, checkpoint_callback, and early_stop_callback (#360)",0.7260274,"Disabled checkpointing, earlystopping and logging with fast_dev_run (#5277)","  Allow disabling logger, early stopping, and checkpoints   Typo   Get tests passing   Update trainer.py ",1
Pad experiment version with zero for easier listing (#355),0.39943367,Disabled val and test shuffling (#1600),,0
release v0.5.2.1,0.6657455,0.5.1,,0
Update trainer.py,0.7095771,Deprecated the Trainer.train_loop property in favor of Trainer.fit_loop (#8025),,1
bug free release,0.5856216,This release includes:,,0
Update test_models.py,0.5925708,Users who relied ontrainer.test(ckpt_path=None)to load the latest model need to change their code totrainer.test(model)` and pass the model reference directly.,,0
fixed ckpt tests (#352),0.55441016,Re-enabled naming metrics in ckpt name (#3060),  fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests ,0
fixes Flake8,0.4470085,"In addition, we fixed:",,0
add tags argument to MLFlowLogger (#349),0.68004763,Updated mlflow with using resolve_tags (#6746),,0
Logger default (#351),0.7704755,Change default logger to a dedicated one (#1064),  weights go into default logger folder   weights go into default logger folder   weights go into default logger folder   weights go into default logger folder   weights go into default logger folder   weights go into default logger folder   ckpt callback in pretrain routine so exp already has version   ckpt callback in pretrain routine so exp already has version   ckpt callback in pretrain routine so exp already has version ,1
Logger default (#350),0.77943695,Change default logger to a dedicated one (#1064),  weights go into default logger folder   weights go into default logger folder   weights go into default logger folder   weights go into default logger folder   weights go into default logger folder   weights go into default logger folder ,1
fixed callback metrics ddp bug,0.6143805,callback system and init DDP (#3836),,0
removed pdb,0.4296715,Removed,,0
Fixed tests (#340),0.5991485,Updated app testing (#16000),  removed hparam calls   removed hparam calls   removed hparam calls   removed hparam calls   removed hparam calls   Update test_models.py ,0
fixes non python type callback metrics and fast_dev_run (#345),0.62968546,Removed legacy code to include step dictionary returns in callback_metrics. Use self.log_dict instead. (#6682),  fixes non python type callback metrics   fixed fast dev run   fixed fast dev run   fixed fast dev run   fixed fast dev run   fixed fast dev run   fixed fast dev run   fixed fast dev run ,0
Finalize logger (#337),0.651602,Removed logger_connector legacy code (#6733),  Ensure logger.finalize is called   Call logger.finalize   Update mlflow_logger.py   Update test_logging.py   Update trainer.py ,0
Ports (#338),0.6253257,    # all ports should be in the 10k+ range,  remove os.exit from early stopping   remove os.exit from early stopping   fixed weight summary   fixed weight summary   fixed weight summary   fixed weight summary   fixed weight summary   fixed weight summary   fixed weight summary ,0
Early stopping (#332),0.4739066,EarlyStopping now runs at the end of the training epoch by default (#8286),  callbacks use all other keys in return dict   callbacks use all other keys in return dict   callbacks use all other keys in return dict   callbacks use all other keys in return dict   remove os.exit from early stopping ,0
Param printing (#336),0.5578856,Args should come after the last positional argument (#1807),"  print thousands as K, M, B, T, ...   add option to print top-level modules only   added doc string and added spacing   do not print summary if neither ""full"" nor ""top""   updated docs showing summary print options   fix line length for travis ",0
default to O1 (#334),0.5529113,Changed default apex level to 'O2' (#2362),,0
fix CONTRIBUTING link and silence checkpoint callback message (#325),0.5553467,Removed deprecated checkpoint argument filepath (#5321),,0
Fix broken link in Examples Readme (#327),0.6095011,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",It now points to the current examples folder.,0
release v0.5.1.3,0.67957616,0.5.1,,0
Fixes lack of logging in logger (#319),0.7331518,Change default logger to a dedicated one (#1064),  changed rank 0   changed rank 0   changed rank 0   changed rank 0   changed rank 0   changed rank 0   changed rank 0   changed rank 0   changed rank 0   changed rank 0   changed rank 0   changed rank 0   changed rank 0   changed rank 0   changed rank 0   changed rank 0   changed rank 0   changed rank 0   changed rank 0   changed rank 0   changed rank 0   changed rank 0   changed rank 0   changed rank 0   models wait to restore weights   models wait to restore weights ,1
Docs (#315),0.724043,Docs,  cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up docs   cleaned up test_tube logger   cleaned up test_tube logger   cleaned up test_tube logger ,1
readme,0.4118037,[1.4.0] - 2021-07-27,,0
release v0.5.1,0.6612947,0.5.1,,0
updated readme,0.55015016,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",,0
cleaning up demos (#313),0.54382586,"Cleaning (#5948, #5949, #5950)",  cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos ,0
cleaning up demos (#312),0.5239029,"Cleaning (#5948, #5949, #5950)",  cleaning up demos   Update job_submit.sh   Update README.md ,0
Fixes #234 (#311),0.6402792,"In addition, we fixed:",  Fixes #234   default logger version is now slurm job id   default logger version is now slurm job id ,0
cleaned up demos,0.5332582,some minor cleaning,,0
Update single_gpu_node_ddp_template.py,0.5785712,refactored GPU backend __step (#3120),,0
Update single_gpu_node_16bit_template.py,0.568573,refactored GPU backend __step (#3120),,0
Update single_cpu_template.py,0.5077747,"trainer = pl.Trainer(callbacks=DeviceStatsMonitor(cpu_stats=True), accelerator=""cpu"")",,0
decouple returns from each step (#307),0.5685284,        # 1. Create a list to hold the outputs of *_step,  decoupled training metrics from logging metrics   decoupled validation metrics from log metrics   updated docs   updated docs   updated docs   Fixed test   merged master   merged master   merged master   merged master   merged master   merged master   merged master   merged master   merged master   merged master   merged master   merged master   merged master   merged master   merged master   merged master   merged master   merged master   merged master   merged master   merged master   merged master   merged master   merged master   merged master ,0
Gpu mem (#308),0.6197343,GPU training (#2704),  Fixes #289   Fixes #289   added lbfgs support   Fixes #280 (#309)   added test seeds (#306)   added test seeds   added test seeds   updated docs   added lbfgs support (#310)   added lbfgs support   added lbfgs support   added lbfgs support   Fixes #280 (#309)   added test seeds (#306)   added test seeds   added test seeds   updated docs   added lbfgs support   added lbfgs support   added lbfgs support   added lbfgs support   added lbfgs support   added lbfgs support   added lbfgs support   added lbfgs support   Fixes #289   Fixes #289   merged master   merged master ,0
added lbfgs support (#310),0.43125814,support for LBFGS. If you pass in LBFGS Lightning handles the closure for you automatically.,  added lbfgs support   added lbfgs support   added lbfgs support   Fixes #280 (#309)   added test seeds (#306)   added test seeds   added test seeds   updated docs   added lbfgs support   added lbfgs support   added lbfgs support   added lbfgs support   added lbfgs support   added lbfgs support   added lbfgs support   added lbfgs support ,0
added test seeds (#306),0.4200133,Tested pickling (#1636),  added test seeds   added test seeds   updated docs ,0
Fixes #280 (#309),0.5963325,"In addition, we fixed:",,0
added lbfgs support,0.48961842,BFloat16 Support,,0
Fixes #292 (#303),0.62231225,"In addition, we fixed:",  early stopping callback is not default   added a default logger   added default checkpoint callback   added default checkpoint/loggers   added default checkpoint/loggers   updated docs   cleaned demos   cleaned demos   cleaned demos   clean up docs around loggers   clean up docs around loggers   clean up docs around loggers   clean up docs around loggers   clean up docs around loggers   clean up docs around loggers   clean up docs around loggers   clean up docs around loggers   clean up docs around loggers   clean up docs around loggers   clean up docs around loggers   clean up docs around loggers   clean up docs around loggers ,0
clean up docs around loggers (#304),0.6242655,Cleaning up stale logger tests (#3490),,0
tests fix,0.679924,- fixed all the .test() calls,,0
Gpu load (#302),0.5575863,refactored GPU backend __step (#3120),  Update root_module.py   Update root_module.py   Update root_module.py   tests fix   tests fix   tests fix ,0
disable auto gpu loading when restoring weights to avoid OOM (#242),0.67210567,"Loads all models on CPU when restoring weights to avoid OOM issues in PyTorch. User now needs to move to GPU manually. However, if using Lightning, lightning will move to correct GPUs automatically.   ",  Update root_module.py   Update root_module.py   Update root_module.py   tests fix   tests fix ,0
Mem crash (#299),0.425111,Resolve memory leak for evaluation (#6326),  fixes memory crash   fixes memory crash ,0
Use getter instead of python property for the dataloaders (#275),0.69825584,               self._tng_dataloader = DataLoader(...),  Use getter instead of python property for the dataloaders   Fix lint   Update trainer.py ,0
Ddp2 (#261),0.7978848,DDP(2) backend (#2796),  adds ddp2 option where on each node a single  process  uses all gpus   added ddp2  test   added ddp2 docs   Update Distributed training.md   delete ref to old update_training_log_metrics   delete ref to old update_training_log_metrics   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   debug   debug   debug   debug   debug   debug   debug   debug   cheesecake ,1
Update multi_node_cluster_auto_slurm.py,0.55699515,Deprecated ClusterEnvironment.creates_children() in favor of ClusterEnvironment.creates_processes_externally (property) (#10106),,0
Better error message if no loss was returned from model.training_step() (#294),0.64518225,Move training_output validation to after train_step_end (#7868),,0
tiny spelling error (#295),0.5044315,"In addition, we fixed:",,0
WIP: Moved grad_norm tracking code to __run_tng_batch  (#278),0.6307153,The default value of accumulate_grad_batches has changed from 1 to None (#9652).,  Moved grad_norm tracking code to __run_tng_batch + added norms to tqdm_metrics   Update trainer.py   Update trainer.py   Update trainer.py   Update trainer.py ,0
Initialize loggers only once (#270),0.73046184,Change default logger to a dedicated one (#1064), Create underlying loggers lazily  This avoids creating duplicate experiments or run in multi-node DDP.   Save hyperparameters automatically   Update docs for snapshotting hyperparams   Fix test tube   Fix test tube pickling ,1
Hacky fix for mlflow logger (#277),0.70232457,MLFlowLogger now accepts run_name as an constructor argument (#7622)," Hacky fix for mlflow logger  It dies when ""created_at"" is logged  Log warning",1
Allow newer torch versions (#269),0.7385081,- Fixed an issue with comparing torch versions when using a version of torch built from source ([#17030](https://github.com/Lightning-AI/lightning/pull/17030)),,1
Make test_tube optional (#274),0.52404016,Removed no return warning from val/test step (#6139),,0
Enable any ML experiment tracking framework (#223),0.4917928,MLflowLogger now uses the env variable MLFLOW_TRACKING_URI as default tracking URI (#7457),  Implement generic loggers for experiment tracking   Add tests for loggers   Get model tests passing   Test and fix logger pickling   Expand pickle test and fix bug   Missed exp -> logger conversion   Remove commented code   Add docstrings   Update logging docs   Add mlflow to test requirements   Make linter happy   Fix mlflow timestamp   Update Logging.md   Update test_models.py   Update test_models.py   Update test_models.py   Update properties.md   Fix tests   Line length ,0
delete ref to old update_training_log_metrics (#262),0.5763283,Removed the deprecated TrainerLoggingMixin class (#8609),,0
always calls the lr scheduler  with epoch nb. Fixes #98 (#252),0.8167496,lr_scheduler now activated after epoch    ,  always calls the lr scheduler  with epoch nb   added docs for cluster grid search   added docs for cluster grid search   undo test changes   undo test changes ,1
undo test changes,0.48461318,Refactored setup_training and remove test_mode (#5388),,0
added docs for cluster grid search,0.52967596,customize it for your cluster,,0
Update Distributed training.md,0.61184955,Trainer(distributed_backend='ddp2')  ,,0
fix typo in early stopping (#260),0.5030357,Deprecate early_stop_callback Trainer argument (#3845),,0
enables samplers which don't need set epoch (or when ppl don't need a sampler) (#254),0.64721686,The dataloader wrapper returned from .setup_dataloaders() now calls .set_epoch() on the distributed sampler if one is used (#16101),  enables samplers which dont need set epoch   added docs for single gpu ddp   added docs for single gpu ddp   added docs for cluster grid search   added docs for cluster grid search   added docs for cluster grid search   added docs for cluster grid search   added docs for cluster grid search   added docs for cluster grid search   added docs for cluster grid search   added docs for cluster grid search   added docs for cluster grid search   added docs for cluster grid search   added docs for cluster grid search   added docs for cluster grid search   added docs for cluster grid search   added docs for cluster grid search   added docs for cluster grid search   added docs for cluster grid search   added docs for cluster grid search   added docs for cluster grid search   added docs for cluster grid search   added docs for cluster grid search ,0
Dim 0 warning (#256),0.456814,"Raised MisconfigurationException when total length of dataloader across ranks is zero, and give warning when total length is non-zero, but only local rank length is zero. (#9827)",  added ignore warnings module   added ignore warnings module   Fixes #249   Update ignored_warnings.py ,0
release v0.5.0,0.66106385,0.5.1,,0
expanded apex install (#255),0.48388216,Setup: added requirement freeze for the next major version (#14480),,0
Fixes #250 (#253),0.64165527,"In addition, we fixed:",,0
Rename variables (#124),0.5318545,moved eval loop (#3412[#3408), data_batch → batch batch_i → batch_idx dataloader_i → dataloader_idx tng → training training_dataloader → train_dataloader add_log_row_interval → row_log_interval gradient_clip → gradient_clip_val prog → progress tqdm_dic → tqdm_dict,0
Add EarlyStop documentation (#245),0.5971717,EarlyStopping now runs at the end of the training epoch by default (#8286),  Update Training Loop.md   Update index.md   Update README.md   Update Training Loop.md   Update Training Loop.md ,0
Added missing parameters (#237),0.55637085,Args should come after the last positional argument (#1807), Added missing parameters  added missing distributed_backend parameter and added the parameter to step 4 Init Trainer.  Update single_gpu_node_dp_template.py,0
"changed hard coded paramater, and moved it to parent_parser (#238)",0.5651171,parser = Trainer.add_argparse_args(parser)," changed hard coded paramater, and moved it to parent_parser  ```python # ------------------------ # 4 INIT TRAINER # ------------------------ trainer = Trainer(     experiment=exp,     checkpoint_callback=checkpoint,     early_stop_callback=early_stop,     gpus=hparams.gpus,     distributed_backend=hparams.dist_bak_end )   parent_parser.add_argument('--dist_bak_end', type=str, default='ddp',                             help='When using multiple GPUs set Trainer(distributed_backend=dp) (or ddp)')  ```  Update single_gpu_node_ddp_template.py",0
Update trainer.py (#233),0.7758587,adding Trainer.tune() (#3293),,1
release v0.4.9,0.67605036,0.4.0,,0
Metrics load (#228),0.60250294,Removed deprecated metrics (#8586),  load from metrics defaults to CPU   load from metrics defaults to CPU   load from metrics defaults to CPU ,0
"added set_epoch for distributed sampler, fix for #224 (#225)",0.74973917,The dataloader wrapper returned from .setup_dataloaders() now calls .set_epoch() on the distributed sampler if one is used (#16101),,1
added load on CPU first (#221),0.55130196,remove weight loading hack for ddp_cpu (#3808),  added load on CPU first   added load on CPU first   added load on CPU first   added load on CPU first   added load on CPU first   added load on CPU first   added load on CPU first   added load on CPU first   added load on CPU first   added load on CPU first   added load on CPU first   added load on CPU first   added load on CPU first   added load on CPU first   added load on CPU first   added load on CPU first   added load on CPU first   added load on CPU first   added load on CPU first   added load on CPU first   added load on CPU first   added load on CPU first   added load on CPU first   added load on CPU first   added load on CPU first   added load on CPU first   added load on CPU first   added print logs   added print logs   changed close order   changed close order ,0
changed examples scripts,0.4653501,Syntax changes are: ,,0
Sai prasanna master (#219),0.4509939,Lightning Trainer, Fix incorrect warning for DistributedSampler.  Check whether dataloader.sampler is an instance of DistributedSampler instead of checking the dataloader.   Update trainer.py   merged ,0
Update multi_node_own_slurm_script.py,0.5462804,Fix an issue with the SLURM srun detection causing permission errors (#15485),,0
enable single gpu per node (#218),0.658712,"Deprecated the use of Trainer(gpus=""i"") (string) for selecting the i-th GPU; from v1.5 this will set the number of GPUs instead of the index (#6388)",  enable single gpu per node   enable single gpu per node   enable single gpu per node   enable single gpu per node   enable single gpu per node   enable single gpu per node ,0
Updated distributed Demos (#215),0.45840025,Sampler replacement in distributed strategies (#16829),  added simple cluster template   added simple cluster template   added simple cluster template   added simple cluster template   added simple cluster template   added simple cluster template   added simple cluster template   added simple cluster template   added simple cluster template   added simple cluster template   sets correct backend for possible combinations of gpu inputs   sets correct backend for possible combinations of gpu inputs   simple slurm example   simple slurm example   simple slurm example ,0
Update tox.ini,0.35187036,xla_device_utils >> xla_device,,0
Simplified gpu api. No NVIDIA flag managing by lightning for cluster (#213),0.6336253,"- `accelerator=""gpu""` now automatically selects an available GPU backend (CUDA and MPS currently) ([#13642](https://github.com/Lightning-AI/lightning/pull/13642))",  added nvidia flag set   added nvidia flag set   added nvidia flag set   added nvidia flag set   added nvidia flag set   added nvidia flag set   added nvidia flag set   added nvidia flag set   added simple cluster template   sets correct backend for possible combinations of gpu inputs   sets correct backend for possible combinations of gpu inputs   sets correct backend for possible combinations of gpu inputs   sets correct backend for possible combinations of gpu inputs   sets correct backend for possible combinations of gpu inputs   sets correct backend for possible combinations of gpu inputs   sets correct backend for possible combinations of gpu inputs   sets correct backend for possible combinations of gpu inputs   sets correct backend for possible combinations of gpu inputs   sets correct backend for possible combinations of gpu inputs   sets correct backend for possible combinations of gpu inputs   sets correct backend for possible combinations of gpu inputs   sets correct backend for possible combinations of gpu inputs   sets correct backend for possible combinations of gpu inputs   sets correct backend for possible combinations of gpu inputs   sets correct backend for possible combinations of gpu inputs ,0
Update multi_node_cluster_template.py,0.54204273,Deprecated ClusterEnvironment.creates_children() in favor of ClusterEnvironment.creates_processes_externally (property) (#10106),,0
Make print_nan_grads print grad (#208),0.6212153,Deprecated Trainer argument print_nan_grads (#1097),This seems more useful for debugging.,0
fixed demo,0.45141566,Fully tested!,,0
Weights path (#211),0.50487137,"Deprecated Trainer attribute ckpt_path, which will now be set by weights_save_path (#2681)",  added docs. removed options. added weights_save option   removed old restore   cleaned up save path   cleaned up save path   flake8 ,0
Fixes #120 (#210),0.6022227,"In addition, we fixed:",,0
split trainer mixins (#209),0.58665264,Barebones Trainer mode (#16854),  split trainer mixins   Update multi_node_cluster_template.py   Update single_cpu_template.py   Update single_gpu_node_16bit_template.py   Update single_gpu_node_ddp_template.py   Update single_gpu_node_dp_template.py   Update trainer_cpu_template.py   Update trainer_io.py   split trainer mixins   Update multi_node_cluster_template.py   deconflicted   deconflicted   deconflicted ,0
Moves hpc auto-resubmit to trainer from test-tube (#207),0.4884627,Allow easy trainer re-instantiation (#7508),  added slurm signal handler   added restore weight functions   set slurm signal handling inside process   added resubmit docs   added resubmit docs   fixed missing param   Update trainer.py   fixed missing param   fixed missing param   debugging tests   debugging tests   debugging tests   debugging tests   debugging tests   debugging tests   debugging tests ,0
add PR template (#204),0.37075537,You can find a migration guide for this change in this PR's description.,  add PR template   Update PULL_REQUEST_TEMPLATE.md ,0
Pass outputs from all dataloaders to test_end and validation_end (#203),0.6048706,Passing 1 dataloader per stage,  Pass outputs from all dataloaders to test_end and validation_end   Update tests   Update docs   Update trainer.py   Update test_models.py ,0
extend pip install info (#194),0.48148972,pip install rich,  extend pip install info   Update README.md   Update README.md ,0
refactored init (#206),0.57472533,      init_args:,,0
DOC Adds reference to test-tube (#205),0.4880023,support for latest test-tube logger optimized for PT 1.2.0.   ,,0
add osx to Travis (#202),0.280463,"adding compute environments (#3837, [#3842)",  add CI macOS   add CI Windows   update CI   drop Win   update CI   update CI ,0
Implement correct transfer to GPU for batches (#200),0.65672296,Reduction when batch_size < num_gpus (#1609),,0
STY Minor flake8 fix (#197),0.48225388,Resolve bug with Finetuning (#5744),,0
Allow to deactivate GPU memory logging in Trainer (#190),0.6823759,- Removed the deprecated `log_gpu_memory` argument from the `Trainer` constructor ([#12657](https://github.com/Lightning-AI/lightning/pull/12657)), Allow to deactivate GPU memory logging in Trainer  Adds the flag log_gpu_memory to Trainer to deactivate logging of GPU memory utilization. On some servers logging the GPU memory usage can significantly slow down training.   Update Logging.md   Update trainer.py ,0
fix import in Tensorboard example (#193),0.71739304,Move tensorboardX to extra dependencies. Use the CSVLogger by default (#16349),,1
DOC Minor import fix (#192),0.39748508,Replace mata_tags.csv with hparams.yaml (#1271),,0
Refactor test modules (#180),0.51153165,Refactored setup_training and remove test_mode (#5388),"  Expectopatronum implement #89 (#182)   rename validate -> evaluate; implement test logic; allow multiple test_loaders   add test_step and test_end to LightningModule   add in_test_mode to pretraining to implement case 2 (test pretrained model)   fix code style issues   LightningTestModel: add optional second test set, implement test_step and test_end   implemented test for multiple test_dataloaders; fixed typo   add two test cases for #89   add documentation for test_step, test_end; fix computation of loss in validation_step example   Update trainer.py   Update trainer.py   Update trainer.py   Update trainer.py   Update trainer.py   Update trainer.py   Added proper dp ddp routing calls for test mode   Update trainer.py   Update test_models.py   Update trainer.py   Update trainer.py   Update override_data_parallel.py   Update test_models.py   Update test_models.py   Update trainer.py   Update trainer.py   Update trainer.py   Update test_models.py   Update test_models.py   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   Update trainer.py   Update override_data_parallel.py   Update debug.py   Update lm_test_module.py   Update test_models.py   release v0.4.8   Update README.md   add training loop docs   testing loop docs   testing loop docs   Convert __dataloader to _dataloader   This will let inherited classes use it   Factor common test model setup into base class   Specialized test modules inherit from LightningTestModelBase   Fix __is_overriden so that it works with more complicated inheritance   Use mixins to add functionality to test models   Fix test with no val_dataloader   Remove unused imports   Get rid of wild card import   Update trainer.py   Update lm_test_module.py ",0
testing loop docs,0.6104918,Read our comprehensive introduction to loops,,0
add training loop docs,0.6412688,"Refactored internal loop interface; added new classes FitLoop, TrainingEpochLoop, TrainingBatchLoop (#7871, #8077)",,0
release v0.4.8,0.67491925,0.4.0,,0
Expectopatronum implement #89 (#182),0.4852898,Removed no return warning from val/test step (#6139),"  rename validate -> evaluate; implement test logic; allow multiple test_loaders   add test_step and test_end to LightningModule   add in_test_mode to pretraining to implement case 2 (test pretrained model)   fix code style issues   LightningTestModel: add optional second test set, implement test_step and test_end   implemented test for multiple test_dataloaders; fixed typo   add two test cases for #89   add documentation for test_step, test_end; fix computation of loss in validation_step example   Update trainer.py   Update trainer.py   Update trainer.py   Update trainer.py   Update trainer.py   Update trainer.py   Added proper dp ddp routing calls for test mode   Update trainer.py   Update test_models.py   Update trainer.py   Update trainer.py   Update override_data_parallel.py   Update test_models.py   Update test_models.py   Update trainer.py   Update trainer.py   Update trainer.py   Update test_models.py   Update test_models.py   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   Update trainer.py   Update override_data_parallel.py   Update debug.py   Update lm_test_module.py   Update test_models.py ",0
Gradient accumulation callback (#150),0.606782,"trainer = Trainer(callbacks=GradientAccumulationScheduler({""1"": 5, ""10"": 3}))","  Gradient accumulation callback   little test case   typo   import fix   method name fix   fix epochs indexing from 1   better code style   code style fix v2 :/   change interface   fix Trainre new api in tests   trainer api bug fix   new raising error, new update method   extentions tests   a little better tests   typo fix   flack8 better   using scheduler for int and dict   typo   firs epoch bug fix   test update   empty dict exception   floats check   codestyle fix   grad counting test   someday, i will install normal linter   add more checks   Update test_models.py   Update test_models.py   Update test_models.py   Update test_models.py   Update test_models.py   Update test_models.py   Update test_models.py ",0
feat(val_sanity): enable skipping validation sanity  (#176),0.60760117, - Sanity checking: `Trainer(num_sanity_val_steps>0)`,  feat(val_sanity): enable skipping validation sanity when self.nb_sanity_val_steps is 0   docs: elaborate on skipping ,0
Update setup.py (#174),0.55997336,from setuptools import setup,,0
docs: add repo_name in the upright corner (#171),0.37392777,Pkg: fix parsing versions by @Borda in https://github.com/Lightning-AI/lightning/pull/15401,,0
move GH docs (#168),0.46982524,move lr_finder (#3434),,0
enable highlight (#170),0.48085958,This is how you enable it:,,0
fix python syntax in code blocks to be consistent (#166),0.48289645,```python,"A couple code blocks used ""{.python}"" instead of just ""python"" for the syntax highlighting, which doesn't render properly in GitHub markdown.",0
cleaned up progbar (#165),0.51691633,"Cleaning (#5948, #5949, #5950)",  cleaned up progbar   cleaned up progbar   cleaned up progbar   cleaned up progbar   cleaned up progbar   cleaned up progbar   cleaned up progbar   updated base files   updated base files   updated base files   updated base files   updated base files   updated base files   updated base files   updated base files   updated base files   updated base files   updated base files   updated base files   updated base files   updated base files   updated base files   updated base files   updated base files   updated base files   updated base files   updated base files   updated base files   flake 8 ,0
Guard against AttributeError in dataloaders. (#161),0.6324933,def train_dataloader(...):,"A solution for https://github.com/williamFalcon/pytorch-lightning/issues/142. Since hasattr ""calls getattr(object, name) and to see whether it raises an AttributeError or not"", I replaced it with a single call to getattr. See also https://stackoverflow.com/questions/24971061/python-hasattr-vs-getattr",0
Cleaned up val/tng/test nb batches (#163),0.56452245,and nb_val_batches to num_val_batches (#567),Set all to be 0 instead of  None.  Cleaned up val batch,0
train = False in test_dataloader (#162),0.80444443,def train_dataloader(self):,A small change to the CoolModel example. Now test_dataloader returns the MNIST test dataset.,1
bug fix for #157 (#158),0.59233224,"In addition, we fixed:",  Separate condition list/tuple case into separated cases   Add test for tuple of tensor list and list of tensor dict   Update test_models.py ,0
fixes #154 (#155),0.62050253,"In addition, we fixed:",  fixes #154   Update trainer.py   Update trainer.py ,0
bug fix for #138 (#143),0.5792264,"In addition, we fixed:",  bug fix for #138   split if for readability ,0
Set val_check_interval default to 1.0. (#145),0.5958913,Trainer(val_check_interval=100),See discussion in https://github.com/williamFalcon/pytorch-lightning/issues/139.,0
add Codecov info (#144),0.38061896,- Removed the deprecated code in:,,0
"F.cross_entropy(y_hat, y)(y_hat, y) typo. (#137)",0.46596405,"Changed the order of backward, step, zero_grad to zero_grad, backward, step (#6147)",This seems to be a typo. Throws TypeError: 'Tensor' object is not callable.,0
tensorboarX to tensorboardX (#136),0.7361621,Switch from tensorboard to tensorboardx in TensorBoardLogger (#15728),  tensorboarX to tensorboardX   Update properties.md ,1
Removed redundant line. (#140),0.5381073,Syntax changes are: ,,0
Error if dataset size = 1 batch. (#141),0.6996615,"    batch_size=32,",Fix for the bug mentioned in https://github.com/williamFalcon/pytorch-lightning/issues/139,0
use val_percent_check in validation step (#135),0.6725011,val_percent_check in favour of limit_val_batches,,0
elaborate on the correlation between overfit_pct and xxx_percent_check (#132),0.54315436,overfit_pct in favour of overfit_batches,  Update Training Loop.md   update docs and elaborate on the correlation ,0
fixed str crash err,0.40807423,Replaced _DataModuleWrapper with __new__ (#7289),,0
fix typo in docs (#129),0.55546343,Syntax changes are: ,  fix typo   fix typo   fix typo   fix list ,0
allow loss to be used for early stopping (#127),0.53375584,    # Returning loss in manual optimization is not needed,,0
release v0.4.6,0.6698669,0.4.0,,0
update Win CI req. (#123),0.37537727,Key updates,,0
enhanced optimizer return options (#120),0.72021234,"    # 2. Return multiple optimizers, same as before",  added smarter optimizer options   added smarter optimizer options   added smarter optimizer options tests   added smarter optimizer options tests   added smarter optimizer options tests   added smarter optimizer options tests   added smarter optimizer options tests   added smarter optimizer options tests   added smarter optimizer options tests   added single gpu data transfer recursive   added single gpu data transfer recursive   added single gpu data transfer recursive   added single gpu data transfer recursive   added single gpu data transfer recursive ,1
fix appveyor (#69),0.4807682,Updated app testing (#16000),  fix appveyor   fix appveyor ,0
enable recursive parsing for single gpu inputs (#121),0.62931776,Parsing of GPU Argument,  added tests   added single gpu data transfer recursive   added single gpu data transfer recursive   added single gpu data transfer recursive   added single gpu data transfer recursive   added single gpu data transfer recursive   added single gpu data transfer recursive ,0
Update gan.py,0.52352244,We have fixed GAN training - supporting multiple optimizers.,,0
enable returning only opt list (#114),0.57534283,Enabled no returns from eval (#2446),,0
ommit templates folder,0.42379916,Included app templates to the lightning and app packages (#13731),,0
added gan template (#115),0.43747842,We have fixed GAN training - supporting multiple optimizers.,  added gan template   ommit templates folder ,0
docs: enable syntax highlight (#109),0.43105412,This is how you enable it:,,0
updated optimizer_step docs,0.6860013,          optimizer.step(),,0
release v0.4.5,0.69043374,0.4.0,,0
updated multiple val dataset docs,0.5170562,val_dataset = trainer.val_dataloaders.dataset,,0
Val idx optional in validation_step (#108),0.66696167,"validation_step, val_dataloader are now optional.   ",  made dataset_i only available with multiple datasets   updated interface signature   updated tests ,0
allow user to control optimizer step for every optimizer,0.64832973,Working with multiple optimizers (#16539),  added custom hook for user defined optimizer step   refactored to allow multiple optimizers different training_step   refactored to allow multiple optimizers different training_step   refactored to allow multiple optimizers different training_step   refactored to allow multiple optimizers different training_step   refactored to allow multiple optimizers different training_step   pep8 ,0
Update issue templates,0.40321666,Included app templates to the lightning and app packages (#13731),,0
Update RequiredTrainerInterface.md,0.5255619,Changed Trainer connectors to be protected attributes:,,0
release v0.4.4,0.6866692,0.4.0,,0
LR scheduler + train refactor (#103),0.6327445,Simplified logic for updating the learning rate for schedulers (#7682),  split __train up for clarity   split __train up for clarity   added lr scheduler after epoch completes ,0
Support for multiple val_dataloaders (#97),0.72012955,"Redesigned multi-dataloader support (#16743, #16784, #16939)","  Added support for multiple validation dataloaders   Fix typo in README.md   Update trainer.py   Add support for multiple dataloaders   Rename dataloader_index to dataloader_i   Added warning to check val_dataloaders   Added a warning to ensure that all val_dataloaders were DistributedSamplers if ddp is enabled   Updated DistributedSampler warning   Fixed typo   Added multiple val_dataloaders   Multiple val_dataloader test   Update lightning_module_template.py   Added dataloader_i to validation_step parameters   Update trainer.py   Reverted template changes   Create multi_val_module.py   Update no_val_end_module.py   New MultiValModel   Rename MultiValModel to MultiValTestModel   Revert to LightningTestModel   Update test_models.py   Update trainer.py   Update test_models.py   multiple val_dataloaders in test template   Fixed flake8 warnings   Update trainer.py   Fix flake errors   Fixed Flake8 errors   Update lm_test_module.py   keep this test model with a single dataset for val   Update trainer.py   Update trainer.py   Update trainer.py   Update trainer.py   Update trainer.py   Update test_models.py   Update trainer.py   Update trainer.py   Update trainer.py   Update trainer.py   Update trainer.py   Update trainer.py   Update RequiredTrainerInterface.md   Update RequiredTrainerInterface.md   Update test_models.py   Update trainer.py   dont need the else clause, val_dataloader is either a list or none because of get_dataloaders()  Update trainer.py  fixed flake errors  Update trainer.py",1
Create CODE_OF_CONDUCT.md (#96),0.328601,"docs for all Metrics (#2184, #2209)",,0
val and test are optional now (#95),0.6164106,Removed no return warning from val/test step (#6139),  made validation step optional   added no val model   val_step can be implemented but not validation_end   added no val end model   added tests   added tests   remove class   remove class   remove class   remove class   remove class   remove class   remove class   remove class   remove class   remove class   remove class   updated docs   updated docs   updated test   updated test   updated test   updated test   updated test   updated test   updated test   updated test   updated test   fix pep8 ,0
"When running DDP without DistributedSampler, throw warning instead of exception (#91)",0.5851603,It is now possible to use custom samplers in a distributed environment without the need to set replace_ddp_sampler=False and wrap your sampler manually with the DistributedSampler.,,0
Update github url for new project template (#90),0.47292578,Updated app URLs to the latest format (#16568),Previous url requested html,0
release v0.4.3,0.6863505,0.4.0,,0
fix accumulated grad norm fixes #87 (#88),0.47797108,Moved track_and_norm_grad into training loop and called only when optimizer_step is being called (#4439),  Update trainer.py   Update trainer.py   Update trainer.py   Update trainer.py   Update trainer.py   Update trainer.py   Update trainer.py ,0
Update Checkpointing.md (#83),0.68358094,all the checkpoint issues should be gone now (including backward support for old checkpoints), Update Checkpointing.md  Modified import for ModelCheckpoint.  Update Checkpointing.md,0
docs(trainer): fix gradient clipping entry (#85),0.6702058,def configure_gradient_clipping(, replace copy and paste error write brief description add link to pytorch docs for specific clipping implementation add example configuration,0
release v0.4.2,0.6868036,0.4.0,,0
updated support for 1.2.0 (#80),0.71369886,"Since 2.0 is a major release, we took the opportunity to take our APIs to the next level and make considerable changes to reduce the backwards incompatible changes in the future. To alleviate this, we will commit to continue supporting the 1.9.x line of releases by doing bug-fix releases with any important fixes that are necessary.",,1
release v0.4.1,0.6723525,0.4.0,,0
fix py 1.1.0 for now,0.5463539,Drop Python 3.6 support,,0
release v0.4.0,0.6889035,0.4.0,,0
removed reduce on non-loss outputs from dp (#78),0.6263591,fix result obj DP auto reduce (#3013),  removed reduce on non-loss outputs from dp   fixed val reduce   fixed val reduce   fixed val reduce   fixed val reduce ,0
make experiment param in trainer optional (#77),0.7526833,No need for experiment object in trainer.   ,  removed forced exp   modified test to also run without exp ,1
Load fix (#74),0.46906337,Called on_load_checkpoint before loading state_dict (#4057),  skip weight load without callback   added simple cpu test   fixed pep ,0
fix loading pkg while setup (#71),0.5169837,"Fixing missing packaging package in dependencies, which was affecting the only installation to a very blank system.",  fix loading package while setup #56   Update setup.py   Update init.py   Update setup.py   use complete req.   Update setup.py   Update setup.py ,0
Merge pull request #68 from williamFalcon/williamFalcon-patch-1,0.43674695,"merge backends (#3476, #3477, #3478, #3480, #3482)",Update setup.py,0
Merge pull request #63 from sidhanthholalkere/readme,0.43500823,@akihironitta @awaelchli @Borda @carmocca @kaushikb11 @krshrimali @mauvilsa @otaj @pre-commit-ci @rohitgr7 @semaphore-egg @tkonopka @wayi1,Fix typo in README.md,0
Merge pull request #66 from williamFalcon/no_back,0.54269147,"merge backends (#3476, #3477, #3478, #3480, #3482)",No back,0
added single gpu train doc,0.6811705,GPU training (#2704),,0
added single gpu train test,0.6340782,GPU training (#2704),,0
added single gpu train,0.63005906,  * Removed the `Trainer(gpus=...)` argument,,0
Merge pull request #64 from williamFalcon/cov,0.46833503,"merge backends (#3476, #3477, #3478, #3480, #3482)",added test model to do also,0
moved badge,0.40829152,Changed,,0
last config try,0.5141579,and run the command that was configured:,,0
moved badge image,0.32826686,Changed,,0
added test model to do also,0.58296585,trainer.test(model),,0
Update MANIFEST.in,0.3946578,"    version=""0.0.1"",",,0
Fix typo in README.md,0.5057077,Pkg: fix parsing versions by @Borda in https://github.com/Lightning-AI/lightning/pull/15401,,0
Create CONTRIBUTING.md,0.40727693,Contributors,,0
added badge,0.33825064,Removed,,0
Merge pull request #62 from williamFalcon/imports,0.4511476,"merge backends (#3476, #3477, #3478, #3480, #3482)",Imports,0
cleaned up pep8 issues,0.47338846,some minor cleaning,,0
made imports absolute,0.60598767,Wrapped imports for traceability (#13924),,0
Merge pull request #52 from alok/ptl-pl,0.42190748,Release LAI docs as stable (#14250),Rename ptl to pl,0
Merge pull request #48 from williamFalcon/readme-patch,0.46606553,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",Update README.md,0
Merge pull request #55 from williamFalcon/continue,0.432091,"merge backends (#3476, #3477, #3478, #3480, #3482)",add training restore,0
Merge pull request #44 from Borda/extend-CI,0.44804728,@akihironitta @awaelchli @Borda @carmocca @kaushikb11 @krshrimali @mauvilsa @otaj @pre-commit-ci @rohitgr7 @semaphore-egg @tkonopka @wayi1,Extend CI,0
Merge branch 'extend-CI' of https://github.com/Borda/pytorch-lightning into extend-CI,0.6781767,- Include the `pytorch_lightning` version as a header in the CLI config files ([#12532](https://github.com/Lightning-AI/lightning/pull/12532)),,0
update codecov,0.3858376,"Updated DeepSpeed ZeRO (#6546, #6752, #6142, #6321)",,0
debug,0.5862367,DDP Debugging Improvements,,0
added hook on_sanity_check_start,0.7455702,Module hook on_sanity_check_start and loading load_from_metrics,,1
fixed restore location,0.4243493,move backends back to individual files (#3712),,0
removed bad hook call,0.6167358,moved ___step_end hooks (#3130),,0
fixed none name,0.5010254,Renamed several Trainer atributes:  (#567),,0
updated tests and docs,0.543911,Docs improvements,,0
added auto restore,0.40490848,and run the command that was configured:,,0
fix appveyor - install pytorch,0.6072694,- Removed `AcceleratorConnector.devices` property ([#12435](https://github.com/PyTorchLightning/pytorch-lightning/pull/12435)),,0
Rename ptl to pl,0.40680945,- Simplify the PL examples structure (shallower and more readable),Closes #46.,0
Update .codecov.yml,0.46846253,Replace mata_tags.csv with hparams.yaml (#1271),,0
review changes #44,0.5608707,Noteworthy changes:,,0
drop CircleCI,0.25690398,Remove MetricsHolder (#7909),,0
fix imports in examples,0.54167485,Wrapped imports for traceability (#13924),,0
update by flake8,0.36606565,[1.4.8] - 2021-09-22,,0
add CircleCI,0.33389252,"adding compute environments (#3837, [#3842)",,0
fix prints for py3.5,0.49669784,Compatibility for Python 3.10,,0
MIT -> apache 2 license,0.22843921,Avoid using the deprecated LooseVersion (#16162),,0
apply PEP8,0.37022263,"In addition, we fixed:",,0
fix rltv imports,0.38894957,Wrapped imports for traceability (#13924),,0
add missing req.,0.4411189,Moved base req. to root (#4219),,0
fix MANIFEST,0.39039457,"    version=""0.0.1"",",,0
update setup,0.518453,Key updates,,0
update README,0.51206064,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",use stick MD syntax,0
rename LICENSE,0.40216264,| Old name                     | New name                       |,,0
add Codecov,0.33422363,try:,,0
fix tests req.,0.6281676,- fixed all the .test() calls,,0
use tox,0.31746146,xla_device_utils >> xla_device, update Travis add Appveyor,0
Delete COPYING,0.4159789,rm: Delete files from your Cloud Platform Filesystem,,0
Create LICENSE,0.25608388,2. Create the Tuner,,0
updated example doc links,0.6299827,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",,0
Merge pull request #42 from williamFalcon/warning,0.47865134,Deprecation warning (#3844),fixed clip grad warning,0
Merge pull request #39 from Borda/cutout-examples,0.40146032,"merge backends (#3476, #3477, #3478, #3480, #3482)",Cut-out examples,0
Update arg_parse.py,0.7456979,args = parser.parse_args(),,1
Update debugging.py,0.48916423,Renamed and moved core/step_result.py to trainer/connectors/logger_connector/result.py (#7736),,0
Merge branch 'warning' of https://github.com/williamFalcon/pytorch-lightning into warning,0.69686025,  * `pytorch_lightning.utilities.warnings.LightningDeprecationWarning` in favor of `pytorch_lightning.utilities.rank_zero.LightningDeprecationWarning`,,0
fixed clip grad warning,0.50126624,Ensure that clip gradients is only called if the value is greater than 0 (#6330),,0
pkg relative imports,0.48488823,Wrapped imports for traceability (#13924), split requirements.txt pytest verbose,0
cutout examples,0.42140436,Here are some examples:,,0
cleaned up examples,0.53020555,some minor cleaning,,0
cleaned up imports for examples,0.6150364,Wrapped imports for traceability (#13924),,0
removing unused imports,0.50115186,Wrapped imports for traceability (#13924),,0
formatting,0.45371044,Syntax changes are: ,,0
updated doc links,0.5851949,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",,0
Delete overview.jpg,0.36686054,Overview,,0
Merge pull request #32 from Separius/patch-1,0.45173964,Release LAI docs as stable (#14250),fix typo in readme,0
fix typo in readme,0.53902394,Syntax changes are: ,,0
Merge pull request #31 from williamFalcon/examples,0.4216766,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",fixed drop prob import,0
fixed drop prob import,0.39619517,Wrapped imports for traceability (#13924),,0
release v0.3.6.9,0.6247795,This release includes:,,0
fixed import,0.52135396,Wrapped imports for traceability (#13924),,0
set tt version,0.54377884,Set version as today (#13906),,0
back 1 tt version,0.40745834,moved TPU xxx_step to backend (#3118),,0
updated to latest tt version,0.4731102,This section outlines notable changes that are not backward compatible with previous versions. The full list of changes and removals can be found in the CHANGELOG below.,,0
release v0.3.6.8,0.6147263,This release includes:,,0
proc 0 only for save hpc. all procs for hpc load,0.5322921,Used checkpoint_connector.hpc_save in SLURM (#4217),,0
release v0.3.6.7,0.62645084,This release includes:,,0
fix broken opt link,0.42171362,Changed calling of untoggle_optimizer(opt_idx) out of the closure function (#7563),,0
only proc 0 can submit a continuation slurm job,0.6656599,Attempted SLURM auto resume call when non-shell call fails (#6002),,0
:q,0.5329484,):,erge branch 'master' of https://github.com/williamFalcon/pytorch-lightning,0
release v0.3.6.6,0.6289301,This release includes:,,0
Merge pull request #24 from williamFalcon/keys,0.45287222,"merge backends (#3476, #3477, #3478, #3480, #3482)",Keys,0
updated output of test models,0.5493177,- Full tests that run multiple models in different configs,,0
updated dict keys,0.58371615,Increased DeepDiff's verbose level to properly handle dict changes (#13960),,0
updated doc indexes,0.5925807,Docs improvements,,0
release v0.3.6.5,0.63575387,This release includes:,,0
Merge pull request #23 from williamFalcon/lkhphuc-lr_sched,0.45190954,Release LAI docs as stable (#14250),Lkhphuc lr sched,0
allow optimizer fx to return 1 or 2 lists,0.6022545,"    # 2. Return multiple optimizers, same as before",,0
fixed lr scheduler tests,0.6151601,          lr_scheduler.step(),,0
running tests,0.5515624,- Full tests that test specific functionality in trainer.,,0
removed file,0.5289439,Removed,,0
merged,0.46301448,"merge backends (#3476, #3477, #3478, #3480, #3482)",,0
Update hooks.md,0.6151676,New Hooks,,0
added hooks docs,0.6461436,New Hooks,,0
added tb docs,0.4706726,Docs,,0
removed old template,0.44613662,Removed attributes and methods:,,0
release v0.3.6.4,0.63296103,This release includes:,,0
added clean slurm save load test,0.5447693,Read more about our SLURM integration here.,,0
release v0.3.6.3,0.6339072,This release includes:,,0
Merge pull request #22 from williamFalcon/loading,0.43090898,"merge backends (#3476, #3477, #3478, #3480, #3482)",Loading,0
"fixed hpc save, load. cleaned apu",0.56036854,Fix hanging in DDP HPC accelerators (#5157),,0
added model save load test,0.5896597,- Added model configuration checking before it runs,,0
remove state_dict,0.7912885,def state_dict(self):,,1
auto state-dict and remove the way the model is loaded during hpc,0.5685016,"+def load_state_dict(self, state):",,0
release v0.3.6.1,0.63620013,This release includes:,,0
Merge pull request #21 from williamFalcon/r,0.39130393,"merge backends (#3476, #3477, #3478, #3480, #3482)",R,0
updated test-tube dep number,0.48416832,support for latest test-tube logger optimized for PT 1.2.0.   ,,0
added global rank var name,0.50006443,- Fixed environment variable priority for global rank determination ([#11406](https://github.com/PyTorchLightning/pytorch-lightning/pull/11406)),,0
Update lm_test_module.py,0.48550737,python script.py test,,0
Merge pull request #20 from williamFalcon/test2,0.43772715,"merge backends (#3476, #3477, #3478, #3480, #3482)",Test2,0
added saving tests to cpu,0.4967042,CPU stats monitoring,,0
added checkpoint test on cpu,0.6000576,"-def on_load_checkpoint(self, checkpoint):",,0
Support any lr_scheduler,0.7346647,    --lr_scheduler=OneCycleLR \,,1
removed exception crashing from val,0.49227172,Removed no return warning from val/test step (#6139),,0
cleaned up some if statements,0.53206813,Refactoring,,0
removed save model logging,0.5100585,Removed logger_connector legacy code (#6733),,0
release v0.3.6,0.6358732,This release includes:,,0
removed hparams req,0.4786321,Removed deprecated LightningModule hparams setter (#6207),,0
release v0.3.51,0.6060305,This release includes:,,0
testing multiple calles,0.5109593,- Full tests that run multiple models in different configs,,0
switched cpu amp order,0.37764263,        Override to init AMP your own way,,0
fixed root node addr,0.71946883,# figure out the root node addr,,1
updated test models with lazy decorators,0.46185663,- Full tests that run multiple models in different configs,,0
added lazy decorator,0.46379146,Removed deprecated auto_move_data decorator (#9231),,0
cleaned readme,0.4873404,some minor cleaning,,0
added downloads badge,0.3560789,progress bar,,0
release v0.3.5,0.6506953,This release does the following: ,,0
added init to test folder,0.41249883,      init_args:,,0
added instructions to test,0.56580734,Fully tested!,,0
removed deps,0.48832494,Removed support for the DDP2 strategy,,0
removed dep,0.49088246,Removed dependency on torchvision (#797),,0
added travis,0.41942966,"@Borda, @carmocca, @hemildesai, @rohitgr7, @s-rog, @tarepan, @tchaton",,0
added mkdocs config,0.37108925,and run the command that was configured:,,0
Merge pull request #18 from williamFalcon/tests,0.4530214,Updated app testing (#16000),removed dep,0
Merge pull request #17 from williamFalcon/tests,0.44556165,Updated app testing (#16000),added coverage badge,0
Merge pull request #16 from williamFalcon/tests,0.4535938,Updated app testing (#16000),Tests,0
added coverage badge,0.41400132,- Code coverage (99%),,0
added testing for metrics,0.6158532,brand new Metrics package with built-in DDP support (by @justusschock and  @SkafteNicki),,0
added dp reduce out test,0.5479944,fix result obj DP auto reduce (#3013),,0
testing map location,0.30632502,Updated app testing (#16000),,0
dp doesnt support amp with any setting,0.568985,enable_pl_optimizer=False by default to temporarily fix AMP issues (#5163),,0
pt dpp some ignores,0.39262196,"Hmm, I wonder, does this only apply to DDP?",,0
ignoring dist parallel forward,0.4759977,Sequence Parallelism,,0
added cpu + amp error,0.46564066,  * Removed the `Trainer(amp_backend=...)` argument,,0
added cpu 16 bit,0.40396821,No longer fallback to CPU with no devices,,0
testing -1 gpu option,0.48226833,Support auto_select_gpus with the accelerator and devices API (#12608),,0
made root note address individually testable,0.46716183,    Use the first node as the root address,,0
moved slurm flag resolution to init,0.562343,Fix an issue with the SLURM srun detection causing permission errors (#15485),,0
running ddp tests,0.65964276,DDP Debugging Improvements,,0
fixed correct module on hpc save,0.5774222,Don't raise a warning when nn.Module is not saved under hparams (#12669),,0
testing hpc save load,0.4636134,CPU stats monitoring,,0
test memory printing,0.3585544,switched from print to logging,,0
remove exception line,0.59622514,except Exception as e:,,0
ignore test module model,0.5510722,- Full tests that run multiple models in different configs,,0
ignore tests file,0.42175585,Cleaning up stale logger tests (#3490),,0
ignore argparse from example for tests,0.6494876,argparse_utils >> argparse,,0
added multiple outputs to LightningTestModel,0.52622443,- Full tests that run multiple models in different configs,,0
refactor tests,0.5999778,Refactoring,,0
added test for no dist sampler,0.5806523,Did not interfere with a default sampler (#1318),,0
added model for tests,0.5897142,trainer.test(model),,0
ignoring multi-node flag,0.56479144,The MultiNode components now warn the user when running with num_nodes > 1 locally (#15806),,0
added sample input for summary,0.46842474,Model summary: add 1 decimal place (#4745),,0
removed dead code in grads,0.47531408,Deprecated automatically detaching returned extras with grads (#7994),,0
added coverage file,0.5499319,- Code coverage (99%),,0
removed coverage file,0.46311158,Moved profilers to their own file (#7822),,0
removed dead code in model save,0.5966802,Removed the deprecated save_function property in ModelCheckpoint (#8680),,0
removed opt check,0.4569796,Removed default Adam optimizer (#1317),,0
removed forkedpdb,0.45151556,Removed teardown from ParallelPlugin (#8943),,0
removed old files,0.44868177,clean up data reset (#3161),,0
added auto port find,0.5699502,    default_port = 12910,,0
auto port kill before starting ddp,0.6388125,        Override to init DDP in a different way or use your own wrapper.,,0
moved port name,0.53893226,    default_port = 12910,,0
fixed amp bug,0.59196883,enable_pl_optimizer=False by default to temporarily fix AMP issues (#5163),,0
refactored model tests,0.6509071,- Full tests that run multiple models in different configs,,0
fixed multi-gpu tests,0.51534444,DDP is recommended for multi-GPU training,,0
added test for model loading and predicting,0.5381721,Loading Model Weights,,0
added safeguards for callbacks in loading saving,0.64283085,Callback hooks for loading and saving checkpoints,,0
removed dummy d,0.5103531,Removed,,0
added debugging util,0.5029315,DDP Debugging Improvements,,0
updated test docs,0.60210943,Updated app testing (#16000),,0
updated reqs,0.42052644,Changed,,0
added test docs,0.50954753,Updated app testing (#16000),,0
added min accuracy to models test,0.4513397,Model summary: add 1 decimal place (#4745),,0
added gpu check for each gpu test,0.5759942,Log just the GPU stats,,0
"added cpu, gpu tests",0.5086439,CPU stats monitoring,,0
added cpu model test,0.5624195,The CPU stats are gathered using the psutil package.,,0
adding tests,0.5763371,- Full tests that test specific functionality in trainer.,,0
release v0.3.4.1,0.6644846,0.4.0,,0
fixed dp + amp bug,0.5650414,enable_pl_optimizer=False by default to temporarily fix AMP issues (#5163),,0
find_unused_parameters=True,0.9999998,            find_unused_parameters=True,,1
release v0.3.4,0.65534276,0.4.0,,0
fixed ddp crash,0.6860715,DDP Debugging Improvements,,0
added on_after_backward,0.59638506,The on_after_backward hook is now called on accumulating iterations. Use the on_before_optimizer_step hook to mimic the old behaviour (#8328),,0
release v0.3.3,0.6453385,This release includes:,,0
added grad hook,0.48615286,moved ___step_end hooks (#3130),,0
release v0.3.2,0.655539,This release includes:,,0
added analysis notebook,0.36052105,Split profilers module (#6261),,0
release v0.3.1,0.6486742,This release includes:,,0
updated auto ddp for > 1 node,0.648214,DDP2 acts as DP in the node and DDP across nodes. ,,0
release v0.3,0.65801275,This release includes:,,0
release v0.2.6,0.6576294,This release includes:,,0
removed logging,0.7073166,Removed LoggerStages (#5673),,1
added slurm managed flag catch for non-slurm peeps,0.5810433,Read more about our SLURM integration here.,,0
release v0.2.5.2,0.65768236,0.5.1,,0
release v0.2.5.1,0.65818775,0.5.1,,0
removed printing. added auto process gen if slurm tasks do not match,0.5316416,Control SLURM's re-queueing,,0
added slurm no process warning,0.6239612,Fix an issue with the SLURM srun detection causing permission errors (#15485),,0
removed print lines,0.53283465,switched from print to logging,,0
testing single process ddp,0.63948214,DDP Debugging Improvements,,0
added epoch flag back,0.6549308,Changed epoch indexing from 0 instead of 1 (#2289),,0
release v0.2.5,0.6702723,This release includes:,,0
added arg docs,0.60349643,Args should come after the last positional argument (#1807),,0
set dp as default backend,0.66464674,Made DDP the default if no backend specified with multiple GPUs (#1789),,0
added training router,0.56168455,moved accelerator router (#3309),,0
added dp and ddp flag,0.6159007,DDP(2) backend (#2796),,0
added option and flag,0.4019802,Deprecated flags: (#2213),,0
release v0.2.4.1,0.6755713,0.4.0,,0
updated required deps,0.48966312,Setup: added requirement freeze for the next major version (#14480),,0
Merge pull request #13 from cinjon/on_tng_metrics,0.600358,Removed deprecated metrics (#8586),add a hook for on_tng_metrics so that users get access to the grad_no…,0
add a hook for on_tng_metrics so that users get access to the grad_norm and mem_map dicts.,0.5149039,Deprecated TrainerLoggingMixin in favor of a separate utilities module for metric handling (#7180),,0
accept dist sampler classes,0.5639797,Allow dataloaders without sampler field present (#1907),,0
release v0.2.4,0.672233,0.4.0,,0
early stop starts counting once min epochs met,0.62533265,EarlyStopping now runs at the end of the training epoch by default (#8286),,0
added summary flag,0.4548059,Deprecated flags: (#2213),,0
removed validation call,0.5639582,  validation_if_necessary(),,0
made early stop checkpoint optional,0.69416475,all the checkpoint issues should be gone now (including backward support for old checkpoints),,0
made checkpoint callback optional,0.7225106,Saved checkpoints will no longer use the type of a Callback as the key to avoid issues with unpickling (#6886),,1
fixed metrics request not forced anymore,0.71627283,Change Metrics persistent default mode to False (#4685),,1
fixed none bug,0.5144145,"In addition, we fixed:",,0
release v0.2.3,0.6713431,This release includes:,,0
removed print statements,0.5609503,switched from print to logging,,0
working on single gpu init speed,0.52074176,Train on 2 GPUs in a Jupyter notebook,,0
modified single gpu init,0.5679343,refactored GPU backend __step (#3120),,0
ddp flag change,0.5870303,        Override to init DDP in a different way or use your own wrapper.,,0
merge,0.54954636,"merge backends (#3476, #3477, #3478, #3480, #3482)",,0
Merge branch 'nccl' of https://github.com/williamFalcon/pytorch-lightning into nccl,0.63426965,from pytorch_lightning.cli import LightningCLI,,0
enabling gpu size = 1 to run without data parallel,0.5355929,Enable non-blocking for device transfers to GPU (#1843),,0
added fallback local init,0.5094919,      init_args:,,0
fixed nccl init,0.5445106,"The default remains ""nccl"", and you should choose ""gloo"" only for debugging purposes.",,0
Merge pull request #11 from cinjon/modulefix,0.41283485,"merge backends (#3476, #3477, #3478, #3480, #3482)",trainer: module fix.,0
testing env init,0.5441944,      init_args:,,0
Merge pull request #12 from cinjon/commafix,0.43830085,"merge backends (#3476, #3477, #3478, #3480, #3482)",root_module: fix comma splits.,0
testing file init,0.50531745,      init_args:,,0
trainer: module fix.,0.72446275,Trainer code became harder to follow,,1
root_module: fix comma splits.,0.38383353,    root_node = '127.0.0.2',,0
reset master,0.48849887,clean up data reset (#3161),,0
testing master_Addr flag,0.46905622,os.environ['MASTER_ADDR'] = root_node,,0
release v0.2.2,0.685105,This release includes:,,0
simplify trainer output,0.65413296,Trainer code became harder to follow,,0
added clarifying comments,0.39852923,Details changes,,0
updated amp use,0.5209394,        :param use_amp: Whether amp was requested or not,,0
release v0.21,0.6311337,This release does the following: ,,0
release v0.2,0.69445825,This release includes:,,0
removed from_lightning flag,0.5315526,Deprecated flags: (#2213),,0
removed dead code,0.55667925,- Removed the deprecated code in:,,0
adjusted imports,0.45032984,Wrapped imports for traceability (#13924),,0
scaled batch size,0.8400097,Tune batch size,,1
added dist sampler exception,0.61633646,Did not interfere with a default sampler (#1318),,0
updated dist sampler,0.6057421,Did not interfere with a default sampler (#1318),,0
moved sampler,0.6561143,Did not interfere with a default sampler (#1318),,0
auto distribute datasets across nodes,0.4898857,Automatic distributed samplers,,0
added cpu example,0.5333868,The CPU stats are gathered using the psutil package.,,0
moved dataloaders after amp and optimizers,0.5288309,"Redesigned multi-dataloader support (#16743, #16784, #16939)",,0
amp now supports multiple optimizers,0.6516082,"Training with multiple optimizers is now restricted to the ""manual optimization mode"":",,0
added single node example,0.49232385,# figure out the root node addr,,0
testing new pretrain order,0.5108897,- Full tests that test specific functionality in trainer.,,0
cleaning up demo file,0.4267727,clean up data reset (#3161),,0
updated demo name,0.48408327,"All documentation and examples are now recommending the new, less ambiguous names.",,0
updated parser help,0.5999443,Syntax changes are: ,,0
using slurm flag to fine node nb,0.511146,Moves SLURM resubmit from test-tube to PL (which removes the need for cluster parameter).,,0
added multi-node locked ip search,0.49647307,Slightly safer multi node (#15538),,0
testing slurm ddp,0.6896294,separate SLURM from DDP (#3809),,0
moved cuda flags inside trainer,0.6377868,"- Trainer queries the CUDA devices through NVML if available to avoid initializing CUDA before forking, which eliminates the need for the `PL_DISABLE_FORK` environment variable introduced in v1.7.4 ([#14631](https://github.com/Lightning-AI/lightning/pull/14631))",,0
added multi-node proc 0 ip reading,0.46027794,The MultiNode components now warn the user when running with num_nodes > 1 locally (#15806),,0
easy import for lightningModule,0.63900673,import lightning as L,,0
checkpoint only on rank=0 now,0.661454,all the checkpoint issues should be gone now (including backward support for old checkpoints),,0
clean up dead code,0.5020514,Refactoring,,0
added single node distdataparallel,0.42364866,"adding compute environments (#3837, [#3842)",,0
added on_hpc_load and on_hpc_save hooks,0.568942,Fixing critical bugs in newly added hooks and hparams assignment.,,0
remove default tensor,0.5103799,Auto convert tensors to contiguous format when gather_all (#4907),,0
release v0.122,0.6148622,This release includes:,,0
fix dataparallel,0.5896122,Implemented DataParallelPlugin._setup_model (#10010),,0
release v0.121,0.6492206,This release includes:,,0
release v0.12,0.6605299,This release includes:,,0
release vusing pytorch summarywriter now,0.6023167,Deprecated pytorch_lightning.logging (#767),,0
added demo tfx images,0.2869513,Improved the error message for installing tensorboard or tensorboardx (#17053),,0
added module properties,0.5281362,New properties,,0
0.113,0.5444436,[0.5.5] - 2022-08-9,,0
release v0.113,0.62258637,This release includes:,,0
verified tfx support,0.37368116,The point of this release is more bug fixes ahead of v 1.0.0. We now have CI tests on TPU thanks to @zcain117 from Google! :slightly_smiling_face:,,0
fixed multiprocessing import,0.51553476,"Disabled strict loading in multiprocessing launcher (""ddp_spawn"", etc.) when loading weights back into the main process (#16365)",,0
integrated tensorboardx test-tube,0.6114581,Switch from tensorboard to tensorboardx in TensorBoardLogger (#15728),,0
required tensorflow for tensorboardx install,0.6846502,Improved the error message for installing tensorboard or tensorboardx (#17053),,0
added module properties docs,0.44385472,New properties,,0
release v0.112,0.638422,This release includes:,,0
added gradient clipping,0.80538905,Gradient Clipping Customization,,1
added lightning docs,0.55739915,Update the Lightning App docs (#13537),,0
removed checkpoint save_function option,0.7294208,Removed support for the deprecated on_save_checkpoint signature. The hook now takes a checkpoint positional parameter (#8697),,1
distributed docs,0.5697989,Distributed Backend,,0
release v0.111,0.652999,This release is a buffer in case 1.0 breaks any compatibility for people who upgrade. 0.10.0 has all the bug fixes and features of 1.0 but is 100% backward compatible. The 1.0 release following in the next 24 hours. ,,0
changed read me,0.754773,Changed,,1
debugging and gpu guide,0.5868575,"When everything works, switch back to GPU by changing only the accelerator. Check our documentation for more useful debugging tricks.",,0
added val loop options,0.60747576,Enabling val/test loop disabling (#2692),,0
renamed options,0.49279794,Changed,,0
prog bar option,0.61869633,This is how the bar looked in versions before 2.0:,,0
adding docs,0.63314456,Docs improvements,,0
added trainer docs,0.62349397,adding Trainer.tune() (#3293),,0
added lightning model docs,0.562979,Update the Lightning App docs (#13537),,0
added docs page,0.59806484,Docs improvements,,0
Create index.md,0.28829503,Docs improvements,,0
Create mkdocs.yml,0.48048186,    hparams_file='/path/to/hparams_file.yaml',,0
release v0.11,0.65677476,This release includes:,,0
finished data parallel,0.8304624,Data Parallelism,,1
removed self.model refs,0.6153063,model reference not provided,,0
updated args,0.62851465,Args should come after the last positional argument (#1807),,0
fixed basic trainer,0.7276119,Barebones Trainer mode (#16854),,1
adding framework level dp,0.57497406,"Enables DP, but with many limitations",,0
release v0.1.dev21,0.6489828,This release includes:,,0
adding support for interrupt signals,0.45454,Deprecated on_keyboard_interrupt callback hook in favor of new on_exception hook (#9260),,0
adding dataparallel,0.6385037,Implemented DataParallelPlugin._setup_model (#10010),,0
dev2 release,0.589629,This release includes:,,0
added amp level option,0.6832796,        :param amp_level:,,0
fixed alternating loss,0.49605817,        return loss1,,0
tng and val steps now have batch nbs,0.58288723,and nb_val_batches to num_val_batches (#567),,0
fixed error with shorter batch cycles,0.59003305,Tune batch size,,0
release v0.1.dev182,0.64586174,This release includes:,,0
release v0.1.dev18,0.66147935,This release includes:,,0
added 16 bit training support with --use_amp flag,0.61818266,Deprecated the Trainer(amp_backend=...) argument,,0
added option to change default tensor,0.51148033,Fix reset TensorRunningAccum (#5106),,0
fixed gpu map location,0.41604853,Support auto_select_gpus with the accelerator and devices API (#12608),,0
release v0.1.dev16,0.64529395,This release includes:,,0
fixed epoch continuation from checkpoint,0.64392436,"Versioning of ""last"" checkpoints",,0
added log saving when early epoch stop,0.61888653,epoch can now log independently (#3843),,0
release v0.1.dev15,0.66045094,This release includes:,,0
early epoch stopping,0.63192546,training_end >> training_epoch_end,,0
pointer to trainer in model,0.68702066,trainer = Trainer(),,0
running new CE then DDT,0.40039456,"Hmm, I wonder, does this only apply to DDP?",,0
fixed hooks,0.7461847,New Hooks,,1
if return -1 from a hook that loop stopps,0.5578946,final inner eval loop hooks (#3154),,0
fixed imports,0.6223302,Wrapped imports for traceability (#13924),,0
fixex imports,0.5067426,Wrapped imports for traceability (#13924),,0
fixing setup,0.54417896,setup(,,0
trainer updates,0.68870884,API changes to the trainer,,0
added example and verified,0.43271282,Have a look at the full example here.,,0
added early epoch stopping hook,0.7915115,# 3. Rename the hook to `on_*_epoch_end`,,1
Merge pull request #9 from Derek-Wds/master,0.40466714,"merge backends (#3476, #3477, #3478, #3480, #3482)",Fix some link bugs in the README.md,0
Merge pull request #8 from shreyasbapat/further_changes,0.5014934,@akihironitta @awaelchli @Borda @carmocca @kaushikb11 @krshrimali @mauvilsa @otaj @pre-commit-ci @rohitgr7 @semaphore-egg @tkonopka @wayi1,Some more fixes,0
Fix pip install too,0.5518103,pip install rich,,0
Some more fixes,0.6588219,"At last, lots of bug fixes (see below).",,0
fixed os missing,0.31055376,"Fixing missing packaging package in dependencies, which was affecting the only installation to a very blank system.",,0
removed bilstm,0.5146396,Removed,,0
Merge pull request #6 from shreyasbapat/management,0.48424977,"merge backends (#3476, #3477, #3478, #3480, #3482)","Add src, docs and other important folders",0
Fix merge conflicts,0.5224236,"merge backends (#3476, #3477, #3478, #3480, #3482)",,0
"Add src, docs and other important folders",0.4214946,Docs improvements,,0
fixes #4,0.60184497,"In addition, we fixed:",,0
Update embeddings.py,0.43205088,Updated references to self.forward() to instead use the __call__ interface. (#1211),,0
fixes #5,0.58301353,"In addition, we fixed:",,0
removed .egg,0.6596761,Removed,,0
release v0.0.2,0.66692245,This release includes:,,0
updated required packages,0.47577095,Backward Incompatible Changes,,0
added .gitignore,0.44053298,"If we forgot someone due to not matching the commit email with GitHub account, let us know :]",,0
reqs,0.31978008,Removed,,0
beta release to pypi,0.45071316,"This release aims at fixing particular issues and improving the user development experience via extending docs, adding typing and supporting python 3.8. In particular, some of the release highlights are:",,0
updated lib name,0.5568104,Renamed utils modules (#5199),,0
initial commit,0.4785635,@akihironitta @awaelchli @Borda @carmocca @kaushikb11 @krshrimali @mauvilsa @otaj @pre-commit-ci @rohitgr7 @semaphore-egg @tkonopka @wayi1,,0
Initial commit,0.4785635,@akihironitta @awaelchli @Borda @carmocca @kaushikb11 @krshrimali @mauvilsa @otaj @pre-commit-ci @rohitgr7 @semaphore-egg @tkonopka @wayi1,,0

Index,Commit Message,Score,Correspond Changelog Sentence,Commit Description,Label
1,ci/docs: clean toods (#16982),0.55190146,"docs for all Metrics (#2184, #2209)",Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
2,Add profiling for on_load_checkpoint/on_save_checkpoint callback and LM hooks (#12149),0.7368926,The signature and behavior of the on_load_checkpoint and on_save_checkpoint callback hooks have changed (#14835):,,1
3,Avoid deprecated usage in accelerator connector tests (#10184),0.76730484,"accelerator connector methods x/n (#3469, #3470, #3474)",,1
4,fixe docs,0.75031143,"    },",Co-authored-by: bas bas.krahmer@talentflyxpert.com,1
5,Docker: building XLA base image (#2494),0.5212455,xla_device_utils >> xla_device,Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
6,Set MLFlowLogger status to FAILED when training raises an error (#12292),0.68425727,"- The `MLFlowLogger.finalize()` now sets the status to `FAILED` when an exception occurred in `Trainer`, and sets the status to `FINISHED` on successful completion ([#12292](https://github.com/Lightning-AI/lightning/pull/12292))",Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
7,Update deepspeed requirement from <0.6.0 to <0.7.0 in /requirements (#13048),0.74189746,Support for manual optimization with DeepSpeed (#7970),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
8,Add troubleshooting section for tpus (#8277),0.64511263,Fixing critical bugs in newly added hooks and hparams assignment.,Co-authored-by: awaelchli aedu.waelchli@gmail.com,0
9,Learning Rate finder (#1347),0.9375645,Learning Rate Finder (#13802),"  fix: get project   test   fix tests   fix tests   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",1
10,cleaned up imports for examples,0.7244854,import argparse,Co-authored-by: awaelchli aedu.waelchli@gmail.com,1
11,Log epoch metrics before firing the on_evaluation_end hook (#7272),0.9823679,Log epoch metrics before the on_evaluation_end hook (#7272),,1
12,Add back deterministic support in accelerator_connector (#11999),0.68636245,"accelerator connector methods x/n (#3469, #3470, #3474)",,0
13,Add missing test for testing custom registered training plugin (#10225),0.7006118,Trainer is now raising a MisconfigurationException instead of a warning if Trainer.{validate/test} is missing required methods (#10016),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
14,Remove unnecessary dependency available checks (#10050),0.62573004,Early stopping checks on_validation_end (#1458),Co-authored-by: thomas thomas@thomass-MacBook-Pro.local,0
15,Update Fabric.init_module for FSDP (#17510),0.60534775,Implemented ready for components (#16129),"  Fix docs levels and broken links   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Update advanced_level_17.rst   Delete oryx-build-commands.txt   Update BECOMING_A_CORE_CONTRIBUTOR.md   Update expert_level_24.rst    Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
16,feat(wandb): log models as artifacts (#6231),0.7752265,Changed WandbLogger(log_model={True/'all'}) to log models as artifacts (#6231),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,1
17,only proc 0 can submit a continuation slurm job,0.58292484,# use slurm job id for the port number,Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
18,EvalResult support for val loop (PR 3/5) (#2651),0.8442094,"EvalResult support for train and val. loop (#2615, #2651)",,1
19,fix incomplete progress bar when refresh_rate > num batches (#4577),0.68154,Changed progress_bar_refresh_rate trainer flag to disable progress bar when setting to 0. (#1108),,0
20,release v0.3.6.9,0.7447399,"    version=""0.0.1"",",Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,1
21,ci(Mergify): configuration update (#12599),0.5744111,Configuration Validator (#9779),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
22,added multi-node locked ip search,0.5060841,# guarantees unique ports across jobs from same grid search,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: awaelchli aedu.waelchli@gmail.com,0
23,Clean-up dtype management (#14823),0.6720388,clean up data reset (#3161),,0
24,Adding test for legacy checkpiont created with 1.8.0 (#15450),0.6449723,Configuration Validator (#9779),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
25,Add EarlyStop documentation (#245),0.5943785,EarlyStopping now runs at the end of the training epoch by default (#8286),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
26,No auto load weights (#985),0.6598215,Loading Model Weights,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
27,test deprecation warnings (#1470),0.7725581,Deprecation warning (#3844),,1
28,Add a check for optimizer attatched to lr_scheduler (#5338),0.79775006,Disabled lr_scheduler.step() in manual optimization  (#6825),,1
29,CI code cleaning (#7615),0.7185409,"Cleaning (#5948, #5949, #5950)",,1
30,fix reinit_schedulers with correct optimizer (#5519),0.7456099,Refactored optimizer (#4658),,1
31,fix typos in validation_step and test_step docs (#5438),0.6032285,Removed no return warning from val/test step (#6139),,0
32,Fix data fetcher selection (#11294),0.64597434,"Accessing dataloaders (#16726, #16800)",Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
33,fix mypy typing errors in pytorch_lightning/tuner/lr_finder.py (#13513),0.6920953,+ # pytorch_lightning==1.7.0,,0
34,Add lightning app examples (#13456),0.85177207,Lightning App,,1
35,Remove deprecated use_amp attributes (#14832),0.72506213,Removed attributes and methods:,,1
36,unify model test acc (#696),0.6367623,- Full tests that run multiple models in different configs,docs,0
37,Fix mypy in utilities.parsing (#8132),0.5355778,LightningCLI.init_parser now returns the parser instance (#8721),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
38,Support no pre-fetching (#11606),0.6432041,- Added support for no pre-fetching to `DataFetcher` ([#11606](https://github.com/PyTorchLightning/pytorch-lightning/pull/11606)),,0
39,Update version for rc0 release (#12423),0.61229074,This release has breaking API changes. See #124 for all details. ,,0
40,copyright (#2710),0.6274991,@property,,0
41,fix missing return statement. Do not normalize remote paths (#2894),0.58577824,Fix for load_from_checkpoint() not working with absolute path on Windows (#2294),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
42,"Remove GradInformation module, including from LightningModule hierarchy (#8831)",0.82464993,Removed deprecated GradInformation module in favor of pytorch_lightning.utilities.grads (#8831),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
43,Update hooks pseudocode (#7713),0.5927259,Removed deprecated model hooks (#3980),,0
44,Add warnings to on_before/after_batch_transfer hooks (#6059),0.64987075,"- Removed patching of `on_before_batch_transfer`, `transfer_batch_to_device` and `on_after_batch_transfer` hooks in `LightningModule` ([#10603](https://github.com/PyTorchLightning/pytorch-lightning/pull/10603))",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
45,store: mock/fixture home (#16536),0.52642834,Refactored EpochResultStore (#5522),,0
46,remove deprecated data_loader (#1077),0.7525723,Removed deprecated: (#2760),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
47,Pin sphinx version (fix for current docs build errors) (#1382),0.66673833,Pinned sphinx-autodoc-typehints with <v1.15 (#11400),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
48,Fix evaluation logging on epoch end with multiple dataloaders (#11132),0.7030544,bug fix with logging val epoch end + monitor (#3812),,1
49,added gradient clipping,0.8465673,Gradient Clipping Customization,,1
50,Fix typing annotations for the ipu strategy (#13786),0.6112723,Raised a MisconfigurationException if batch transfer hooks are overriden with IPUAccelerator (13961),,0
51,cleaning up stale logger tests + flake8 (#3490),0.9099189,Cleaning up stale logger tests (#3490),"  Adding tests for legacy checkpoints   2.0   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   fix simple   utils   import   pl   num_features=24   num_features=24   length=6000   other   rm   dru run   rm   prune   import    Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",1
52,Mark forward_module as required (#16386),0.6532521,- Mark the `forward_module` argument as required ([#16386](https://github.com/Lightning-AI/lightning/pull/16386)),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: thomas thomas@thomass-MacBook-Pro.local Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
53,Avoid the deprecated onnx.export(example_outputs=...) in torch 1.10 (#11116),1.0,Avoid the deprecated onnx.export(example_outputs=...) in torch 1.10 (#11116),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
54,CI: hotfix last version (#14627),0.5338832,"Updated DeepSpeed ZeRO (#6546, #6752, #6142, #6321)",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
55,Add tpuvm section in TPU docs (#7714),0.6091655,Renamed TPUHalfPrecisionPlugin to TPUBf16PrecisionPlugin (#10026),,0
56,Add warning to trainstep output (#7779),0.6342595,- Added a warning that shows when `max_epochs` in the `Trainer` is not set ([#10700](https://github.com/PyTorchLightning/pytorch-lightning/pull/10700)),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
57,Merge branch 'master' into feature/817-fairscale-5n,0.48420718,Changed resolve_training_type_plugins to allow setting num_nodes and sync_batchnorm from Trainer setting (#7026),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
58,4/n Move Accelerator into strategy - remove X_step() from accelerator (#10890),0.7631762,move specific accelerator code (#3457),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,1
59,Update API references (#10040),0.69066614,Refactor cloud dispatch and update to new API (#16456),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
60,update changelog (#16348),0.7359802,Complete changelog,Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,1
61,fixed callback metrics ddp bug,0.6685232,Changed warnings and recommendations for dataloaders in ddp_spawn (#6762),,0
62,[Typo] update introduction.rst (#13791),0.6046562,Renamed TPUHalfPrecisionPlugin to TPUBf16PrecisionPlugin (#10026),,0
63,add @akihironitta among CI/CD owners (#13139),0.5971296,@ajtritt @akihironitta @carmocca @rohitgr7,,0
64,Update data docs (#16839),0.6541909,Refactor cloud dispatch and update to new API (#16456),,0
65,add doctests for example 1/n (#5079),0.6467813,"docs for all Metrics (#2184, #2209)",,0
66,Threading support for legacy loading of checkpoints (#12814),0.72740066,DataModule hooks for loading and saving checkpoints,,1
67,Resolved wrong mv usage for extracted directory (#9678),0.5943496,Prevent to cd into non-existent folders (#16645),Co-authored-by: thomas thomas@thomass-MacBook-Pro.local,0
68,Merge pull request #11228 from PyTorchLightning/docs/strategies-code-owners,0.6373416,from lightning.pytorch.utilities import CombinedLoader,,0
69,CHANGELOG update after v1.3.6 release (#7988),0.7129472,Here is a selection of important changes that are not backward compatible with versions < 1.5. The full list of changes and removals are listed in the changelog at the bottom.,,1
70,Refactor some loops code and hook tests (#7682),0.74384403,refactor eval loop to use hooks - use test_mode for if so we can split later (#3129),  update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update    Co-authored-by: thomas thomas@thomass-MacBook-Pro.local,1
71,Collapse 2 DeepSpeed tests (#6108),0.7626293,DeepSpeed Stage 1,,1
72,refactored horovod backend (#3122),0.984256,"refactored Horovod backend (#3121, #3122)",Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: awaelchli aedu.waelchli@gmail.com,1
73,Fix trainer.logger deprecation message (#12671),0.6778388,Removed the deprecated TrainerLoggingMixin class (#8609),,0
74,fix argparse conflicting options error (#5569),0.71051526,argparse_utils >> argparse,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
75,Update Readme with tunning overhead time (#2082),0.5684119,Reset epoch progress with batch size scaler (#13846),,0
76,Do not shuffle in LightningDataModule.from_datasets for IterableDataset (#7053),0.74189436,LightningModule.from_datasets() now accepts IterableDataset instances as training datasets. (#7503),,1
77,[App] Increased DeepDiff's verbose level to properly handle dict changes (#13960),0.94710845,Increased DeepDiff's verbose level to properly handle dict changes (#13960),Co-authored-by: Dingu Sagar dingu.sagar@engati.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka jirka.borovec@seznam.cz,1
78,CI: enable testing for PT 1.11 (#11792),0.6425996,"trainer.test(model, ckpt_path=""/path/to/checkpoint.ckpt"")",Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
79,Replace codecov pip package with codecov uploader (#17349),0.5989596,Enabled cp (upload) at project level (#16631),,0
80,Move DP warning suppression to the DataParallel Plugin (#7421),0.82526857,Moved ignore_scalar_return_in_dp warning suppression to the DataParallelPlugin class (#7421),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
81,Fix NeptuneLogger unusable after pytorch-lightning 1.7.0 (#13988),0.72947955,Deprecated pytorch_lightning.logging (#767),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,1
82,Combine the pip install commands in conda workflow (#14744),0.5703119,pip install rich,  Add missing MANIFESTs   move   one more   Ignore version.info properly   move   manifest    Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
83,Ensure first entry of connect.txt is the app name (#15443),0.5466531,all logging related calls in a connector (#3395),,0
84,[tests/core] Updated with BoringModel and added BoringDataModule (#5432),0.5665313,Deprecated the TestTubeLogger (#9065),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz,0
85,Update CODEOWNERS,0.5554216,Refactor cloud dispatch and update to new API (#16456),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
86,remove capture on on_train_batch_end,0.6485405,Removed the deprecated outputs argument in both the LightningModule.on_train_epoch_end and Callback.on_train_epoch_end hooks (#8587),,0
87,Merge load functions (#995),0.61656463,"combined_loader = CombinedLoader(iterables, mode=""min_size"")",Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
88,[App] Add support for plugins to return actions (#16832),0.6782049,Enabled plugins (#4041)," [pre-commit.ci] pre-commit suggestions  updates: - github.com/PyCQA/docformatter: v1.4 → v1.6.0 - github.com/psf/black: 22.12.0 → 23.3.0 - github.com/charliermarsh/ruff-pre-commit: v0.0.237 → v0.0.260  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci   update   apply   fixing   docs/lines    Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz",0
89,Update mypy (#11096),0.58431673,"Removed PyTorch 1.6 support (#10367, #10738)",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
90,Update debugging doc (#11445),0.6474402,DDP Debugging Improvements,,0
91,Switch theme for Fabric (#16961),0.5977524,fabric.launch(),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
92,Allow easy CLI trainer re-instantiation (#9241),0.8622438,Allow easy trainer re-instantiation (#7508),,1
93,Removes need to unsqueeze from dp (#1319),0.781635,On DP and DDP2 unsqueeze is automated now (#1319),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka jirka.borovec@seznam.cz,1
94,Fixes around Strategy.set_world_ranks (#16966),0.6130803,- Fixed environment variable priority for global rank determination ([#11406](https://github.com/PyTorchLightning/pytorch-lightning/pull/11406)),,0
95,docs: Clarify versioning and API stability (#14549),0.6219153,Refactor cloud dispatch and update to new API (#16456),,0
96,Checking ipywidgets is installed for ensure tqdm working (#2417),0.63407695,Updated logic for checking TPUs availability (#6767),,0
97,Merge pull request #7872 from PyTorchLightning/refactor/logger-poc-changes,0.67485654,- Fixed logging to loggers with multiple eval dataloaders ([#12454](https://github.com/PyTorchLightning/pytorch-lightning/pull/12454)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
98,fixes typing in pytorch_lightning/callbacks/stochastic_weight_avg.py (#13685),0.7639954,Renamed pytorch_lightning.callbacks.swa to pytorch_lightning.callbacks.stochastic_weight_avg (#6259),Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,1
99,removed print lines,0.6042622,Removed,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
100,Remove Sourcerer (#5172),0.6357593,Remove deprecated distributed_backend from Trainer (#10017),,0
101,updated docs (#3268),0.7074683,Docs improvements,,1
102,Fix when _stable_1d_sort to work when n >= N (#6177),0.5307799,"Changed the order of backward, step, zero_grad to zero_grad, backward, step (#6147)",Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
103,"tweak trainer call, minor spelling tweaks in notebooks (#4315)",0.5514931,Syntax changes are: ,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
104,Fix GAN training. (#603),0.66872597,We have fixed GAN training - supporting multiple optimizers.,Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
105,remove lightning module datamodule property (#9233),0.8334972,Removed deprecated property LightningModule.datamodule in favor of Trainer.datamodule (#9233),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
106,NGC container PoC (#6187),0.51430786,Pinning starsessions to 1.x (#14333),Updates the requirements on uvicorn to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: uvicorn   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
107,Update link to new forum (#16449),0.52436686,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",,0
108,Set path filters in favor of the required-job action (#14294),0.5307517,Disabled lr_scheduler.step() in manual optimization  (#6825),Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz,0
109,Mark accelerator connector as protected (#10032),0.7285679,Deprecated access to the AcceleratorConnector.configure_slurm_ddp method and marked it as protected (#10101),,1
110,Fix type hints of callbacks/finetuning.py (#13516),0.6515842,Avoid redundant callback restore warning while tuning (#13026),,0
111,Ignore README in pre-commit rules (#17151),0.557251,Do not override PYTHONWARNINGS (#4700),,0
112,callback method for on_save_checkpoint (#2501),0.7283809,The signature and behavior of the on_load_checkpoint and on_save_checkpoint callback hooks have changed (#14835):,,1
113,docs: Add empty lines in docstring [ci skip] (#4232),0.6180464,Remove unnecessary intermediate layers in Dockerfiles (#5697),Co-authored-by: Luca Antiga luca.antiga@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
114,fix jit for PT1.7 (#4172),0.60147893,Obligatory post 1.0 minor release. Main fix is to make Lightning module fully compatible with Jit (had some edge-cases we had not covered).,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: Yurij Mikhalevich yurij@grid.ai,0
115,Metric aggregation (#3321),0.70377445,Regression metrics (#2221),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
116,proc 0 only for save hpc. all procs for hpc load,0.50879776,Used checkpoint_connector.hpc_save in SLURM (#4217),,0
117,Fix logic to check for spawn in worker_check (#9902),0.581711,Resolved a bug about a race condition when sending the work state through the caller_queue (#14074),,0
118,protect ByteCounter (#10300),0.5414013,Marked FitLoop.should_accumulate as protected (#9515),,0
119,ref: ddp backend refactor (3) (#3208),0.8866341,refactored DDP backend forward (#3119),  update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update    Co-authored-by: thomas thomas@thomass-MacBook-Pro.local,1
120,forward key metrics (#3467),0.6241069,Regression metrics (#2221),,0
121,"Update rich requirement from !=10.15.*,<=12.0.0,>=10.2.2 to >=10.2.2,!=10.15.0.a,<13.0.0 in /requirements (#13047)",0.6485354,- Changed minimum supported version of `rich` from `10.14.0` to `12.13.0` ([#16798](https://github.com/Lightning-AI/lightning/pull/16798)),,0
122,Improving instructions in finetuning docstring (#10484),0.65477484,Resolve bug with Finetuning (#5744),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
123,Tighten up regression testing,0.57842237,test_percent_check in favour of limit_test_batches,,0
124,Remove AcceleratorConnector.num_gpus and deprecate Trainer.num_gpus (#12384),0.77872515,"- Removed the deprecated device attributes `Trainer.{devices,gpus,num_gpus,ipus,tpu_cores}` in favor of the accelerator-agnostic `Trainer.num_devices` ([#14829](https://github.com/Lightning-AI/lightning/pull/14829))",,1
125,Prioritize Previously-Used GPUs During Auto-Selection (#10485),0.8195936,GPU selection,"[TPU] Do not delete jobs with ""keepalive"" in the name",1
126,Fixes in sphinx docs links (#16255),0.59101486,Pinned sphinx-autodoc-typehints with <v1.15 (#11400),  drop failing e2e quick app   codeowners   Apply suggestions from code review ,0
127,lightning delete cluster CLI command help text update (#15760),0.6751589,CLI Commands for Lightning Apps,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
128,Fix DPP + SyncBN (#6838),0.647398,Disabled batch transfer in DP mode (#6098),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
129,extend docs notes (#1004),0.6228812,Docs improvements,  [TPU] Fix workflow   Whitespace ,0
130,modified single gpu init,0.6158973,refactored GPU backend __step (#3120),,0
131,Typo in trainer/supporters.py (#13455),0.62483895,Deprecated TrainerModelHooksMixin in favor of pytorch_lightning.utilities.signature_utils (#7422),Bump default E2E image version Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
132,Update Lightning App docs (#13537),0.9965466,Update the Lightning App docs (#13537),,1
133,Fix srun availability check (#15211),0.5878474,Fix an issue with the SLURM srun detection causing permission errors (#15485),,0
134,Fix typo in Quick Start/Step-by-step walk-through (#3007),0.46782476,move run_pretrain_routine -> setup_training (#3294),,0
135,Make test_tube optional (#274),0.67716694,Deprecated the TestTubeLogger (#9065),,0
136,Add check for uninitialized _sync_dir in DDP Plugin to avoid errors during error handling (#9267),0.77412987,Ensure the existence of DDPPlugin._sync_dir in reconciliate_processes (#8939),,1
137,Terminate process when main process raises error in ServableModuleValidator (#14217),0.5947088,ServableModule and its Servable Module Validator Callback,,0
138,cleaning tests (#2201),0.79612327,"Cleaning (#5948, #5949, #5950)",,1
139,prune deprecated profiler as bool (#6164),0.6324444,Deprecated bool values in Trainer's profiler parameter (#3656),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
140,Run GPU tests with PyTorch 1.12 (#13716),0.7149434,GPU training (#2704),,1
141,Delete COPYING,0.625414,rm: Delete files from your Cloud Platform Filesystem,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
142,update links to Discord (#17087),0.608668,Update the Lightning App docs (#13537),,0
143,Improved docs for callbacks (#1370),0.6695694,Read more about callback entry points in our docs.,,0
144,ref: training flag tests (val_check_interval) (#3825),0.7561358,"Refactored training_batch + tests to verify correctness (#2327, #2328)",,1
145,fix loading model with kwargs (#2387),0.5759221,Removed deprecated model hooks (#3980),,0
146,CI: apps (#15242),0.7269269,Introducing CLI commands for apps (#13602)!,Co-authored-by: Luca Antiga luca.antiga@gmail.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
147,Introduce PrecisionPlugin.forward_context() (#9988),0.6719356,Precision Plugins (#5718),,0
148,LearningRateLogger in multi-scheduler setting (#1944),0.76935685,Changed LearningRateLogger to LearningRateMonitor (#3251),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
149,Fix bug in lightning_cli_advanced_3.rst (#16792),0.79245543,Improved the error message when the LightningWork is missing the run method (#14759),Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,1
150,ci: wheels with continue-on-error (#16675),0.51600087,Porting fixes to autoscaler component (#16249),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
151,CI: Enable dependabot for GitHub Actions (#13589),0.5668127,"GitHubComponent(api_token=os.environ[""API_TOKEN""])",Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
152,Fig logging with log_gpu_memory='min_max' (#9013),0.63211304,Do not fail if batch size could not be inferred for logging when using DeepSpeed (#10438),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
153,ref: group connectors (#3472),0.97030246,group connectors (#3472),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,1
154,ci: freeze pypi releasing (#16724),0.6287116,Drop PyTorch 1.9 support (#15347),,0
155,"Set the state before saving ""last"" or ""none"" checkpoints (#11481)",0.7881565,"Versioning of ""last"" checkpoints","  skip app dcotest   Proposed change   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  delete   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",1
156,Skip code formatters on _notebooks submodule (#13867),0.583768,Allowing decorate model init with saving hparams inside (#4662),,0
157,changes examples to pl_examples for name connflict,0.50960565,Simplify the PL examples structure (shallower and more readable) (#1247),  Replace GKE in CI with manual gcloud usage   Fix XRT test   Reduce timeout to 35 minutes   [TPU] Run tests with PJRT   runtime as part of the job name   CHANGELOG   Update for app too ,0
158,[App] Scale out/in interval for autoscaler (#16093),0.70394135,Enabled users to have more control over scaling out/in intervals (#16093),,1
159,Improve annotations for _defaults_from_env_vars decorator (#11888),0.6001811,- Moved `trainer.connectors.env_vars_connector._defaults_from_env_vars` to `utilities.argsparse._defaults_from_env_vars` ([#10501](https://github.com/PyTorchLightning/pytorch-lightning/pull/10501)),  don't call set_world_ranks in xla strategy   update   fabric and other strategies   CHANGELOG   Typos   Reuse test    Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
160,Fix typo (#1118),0.62783283,Changed overwrite to True (#16009),,0
161,testing single process ddp,0.69368273,DDP Debugging Improvements,,0
162,add note,0.7925538,NOTE,,1
163,Add missing deprecation notes for deprecated Trainer flags (#10296),0.7217827,Deprecated flags: (#2213),"  Update fastapi dependency pins   Apply suggestions from code review   Update test.txt   Update requirements/app/base.txt   Revert ""Update requirements/app/base.txt""   This reverts commit 59918ffc6c2cfd8b5509df48ca84b67ad264bc30.   cloud update   Bad merge   fastapi 0.69.0 which pins starlette 0.15.0   https://github.com/pydantic/pydantic/issues/1985   Avoid CVE: https://github.com/tiangolo/fastapi/pull/3213   Strict trio   Skip windows test    Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com",1
164,FCCV Docs (#15598),0.5373566,"docs for all Metrics (#2184, #2209)",,0
165,Fabric: drop FairScale's sharded implementation (#16329),0.64154017,"For reference, FairScale's implementation can be used with",,0
166,Detach hiddens and add test (#8249),0.6891989,    # 6. Remove the return of `hiddens`,,0
167,Fix backwards compatibility for optional logging dependencies (#900),0.603925,Un-balanced logging properly supported (#5119),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
168,Bump tj-actions/changed-files from 23 to 24 (#13956),0.51731944,moved TPU xxx_step to backend (#3118),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
169,verified tfx support,0.60195863,  validation_if_necessary(),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
170,Fix ReduceLROnPlateau update issue while resuming from a checkpoint (#14702),0.7215959,Refactor load in checkpoint connector (#4593),,1
171,Pin protobuf (#14512),0.6022361,Renamed TPUHalfPrecisionPlugin to TPUBf16PrecisionPlugin (#10026),Bumps peter-evans/create-pull-request from 4 to 5. - Release notes - Commits  updated-dependencies: - dependency-name: peter-evans/create-pull-request   dependency-type: direct:production   update-type: version-update:semver-major ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
172,Fix Horovod distributed backend to set the root_gpu property (#1669),0.75646055,"refactored Horovod backend (#3121, #3122)","  App: Fix frontends when using multiprocessing in the cloud   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  Update CHANGELOG.md   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",1
173,[App] Fix import errors for the packages installed by shebang (#15996),0.6433556,Module imports are now catching ModuleNotFoundError instead of ImportError (#9867),  Trigger TPU tests if [TPU] is in the PR title   Remove TODO   checkgroup   DEBUG   Update   1h timeout   Update   Update   Update   Update   Remove DEBUG ,0
174,Bugfix for predict progressbar (#6884),0.71551466,- Fixed bug where progress bar was not being disabled when not in rank zero during predict ([#11377](https://github.com/PyTorchLightning/pytorch-lightning/pull/11377)),,1
175,fallback to hparams str (#2259),0.6120205,Refactor Model backward (#2276),,0
176,Update README .gif (#5777),0.56022066,Pkg: fix parsing versions by @Borda in https://github.com/Lightning-AI/lightning/pull/15401,,0
177,coverage increase (#1167),0.50184774,[1.2.7] - 2021-04-06,  Update LightningDataModule docs   Avoid torchvision ,0
178,enable any logged metric to be accessible in callbacks (#3598),0.67690885,Do not override the existing epoch value in logged_metrics when already logged by the user (#7982),,0
179,Use pytest tmpdir fixture  (#482),0.6849831,Enable PyTorch 1.7 compatibility (#3541),,0
180,Add link to TorchIO tutorial in PyTorch Ecosystem examples (#7612),0.7190809,PyTorch 2.0 and torch.compile,,1
181,fix mypy typing errors in lightning/trainer/optimizers.py (#13470),0.72361684,- Fixed wrong typehint for `Trainer.lightning_optimizers` ([#11155](https://github.com/PyTorchLightning/pytorch-lightning/pull/11155)),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
182,[bugfix] Add mechanism to prevent deadlock for DDP on Exception Trigger (#8167),0.6027069,- Removed deadlock detection / process reconciliation (`PL_RECONCILE_PROCESS=1`) ([#16204](https://github.com/Lightning-AI/lightning/pull/16204)),,0
183,Bump IPU version (#8290),0.6674792,Graphcore IPU devices,  update links   one more ,0
184,fix imports of collections.abc for py3.10 (#14345),0.6290749,from pytorch_lightning.plugins import CheckpointIO,"  update docs   update docs index   Delete version.info   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update glossary   typo   restructure   rm newline   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
185,Consume the prediction batch indices iteratively (#16826),0.6881399,- Predict's custom BatchSampler that tracks the batch indices no longer consumes the entire batch sampler at the beginning ([#16826](https://github.com/Lightning-AI/lightning/pull/16826)),,0
186,resolve failing test (#10191),0.64821124,Removed no return warning from val/test step (#6139),Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
187,Loop flattening: reduce base interface (#16429),0.6035621,Simplify optimization Logic (#4984),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
188,Add documentation for ways to access all batch outputs for on_train_epoch_end hook (#7389),0.7307934,Pass batch outputs to on_train_batch_end instead of epoch_end outputs (#4369),,1
189,Add useful errors when model is not configured correctly (#1199),0.7070153,Replace MisconfigurationException with warning in ModelCheckpoint Callback (#4560),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
190,[2/n] Fix rich model summary for tuples (#9756),0.55404884,model_utils >> model_helpers,"  fixing master   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
191,fix(wandb): allow custom init args (#6989),0.6020496,WandbLogger does not force wandb reinit arg to True anymore and creates a run only when needed (#4648),Updates the requirements on setuptools to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: setuptools   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
192,"Update comet-ml requirement from <3.31.8,>=3.1.12 to >=3.1.12,<3.31.16 in /requirements (#15081)",0.647702,Using .comet.config file for CometLogger (#1913),Bumps pytest from 7.2.0 to 7.2.2. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: pytest   dependency-type: direct:production   update-type: version-update:semver-patch ... Signed-off-by: dependabot[bot] support@github.com,0
193,ci: hotfix precommit/poetry/isort (#16549),0.4956427,[1.1.8] - 2021-02-08,  Update benchmarks.rst   small fix ,0
194,Deprecate on_configure_sharded_model callback hook for v1.6 (#11627),0.72908115,Updated hooks arguments - breaking for setup and teardown (#2850),,1
195,[App] Fix local app run with relative import (#16835),0.61016726,Module imports are now catching ModuleNotFoundError instead of ImportError (#9867),,0
196,update tutorials (#13268),0.6054021,"Base classes (#1326, #1877)",,0
197,fixed dp + amp bug,0.7288615,enable_pl_optimizer=False by default to temporarily fix AMP issues (#5163),,1
198,Fix import for OrderedDict in Python 3.7.0 (#15359),0.559089,Module imports are now catching ModuleNotFoundError instead of ImportError (#9867),,0
199,Add Loop.replace (#10324),0.6856898,Refactored Loops,,0
200,Add Yield loop example (#9983),0.6236619,expand eval loop out (#3165),,0
201,don't override PYTHONWARNINGS (#4700),0.9907551,Do not override PYTHONWARNINGS (#4700),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,1
202,Add Dali MNIST example (#3721),0.51744604,[1.7.4] - 2022-08-31,,0
203,Update explained variance metric (#4024),0.9850936,Updated explained variance metric (#4024),,1
204,Simplify progress bar args (#1108),0.73887694,Better progress bar (#16695),Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
205,Rename HorovodPlugin to HorovodStrategy (#11195),0.8150546,    * Renamed the `HorovodPlugin` to `HorovodStrategy` ([#11195](https://github.com/PyTorchLightning/pytorch-lightning/pull/11195)),,1
206,Smart handling of EarlyStopping.check_on_train_epoch_end (#8888),0.7670343,Changed EarlyStopping callback from by default running EarlyStopping.on_validation_end if only training is run. Set check_on_train_epoch_end to run the callback at the end of the train epoch instead of at the end of the validation epoch (#7069),  ci: wheel from cache   ci: wheel from cache   rev ,1
207,fix setup and on fit calls (#2252),0.61241806,  * `Callback.on_configure_sharded_model` in favor of `Callback.setup`,,0
208,Merge pull request #48 from williamFalcon/readme-patch,0.5073199,refactored dataloader process hook (#3139),,0
209,Trigger TPU tests if [TPU] is in the PR title,0.63547087,Updated logic for checking TPUs availability (#6767),,0
210,update bug template (#3902),0.6183763,Use correct python version in lightning component template (#13790),  Add Fabric diff and resulting code to the README   Reduce horizontal space   sub   Contributor count   Feedback   Other snippets ,0
211,Fix #3417 (#3419),0.6333616,Resolve bug with Finetuning (#5744),,0
212,docs: migration guide to the latest [2/n] (#17103),0.6914892,You can find a migration guide for this change in this PR's description.,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
213,Fix fork skip condition in GitHub workflows (#14955),0.52341545,- Added a more descriptive error message when attempting to fork processes with pre-initialized CUDA context ([#14709](https://github.com/Lightning-AI/lightning/pull/14709)),,0
214,test memory printing,0.6431156,Resolve memory leak for evaluation (#6326),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: awaelchli aedu.waelchli@gmail.com Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
215,Delete on_after_backward unused argument (#7925),0.60653687,  * Removed `opt_idx` argument from `Callback.on_before_optimizer_step` callback method,Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
216,Update TPU docs for installation (#6794),0.7247119,Updated logic for checking TPUs availability (#6767),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
217,ref: inner train loop (intermediate step) 3/n (#3362),0.7801604,"inner train loop (#3359, #3361, #3362, #3363, #3365, #3366, #3367, #3368, #3369, #3370, #3371, #3372, #3373, #3374, #3375, #3376, #3385, #3388, #3397)",,1
218,fixed str crash err,0.4766914,Disabled val and test shuffling (#1600),,0
219,"Update jsonargparse[signatures] requirement from <4.19.0,>=4.18.0 to >=4.18.0,<4.22.0 in /requirements (#17633)",0.56133837,Improved error messages in replace_sampler when the DataLoader attributes are not included in the signature or the signature is missing optional arguments (#8519),,0
220,[CLI] Add config -c argument (#12039),0.63077563,Introducing CLI commands for apps (#13602)!,,0
221,Fix current epoch value override on restart (#12429),0.67553675,Reset current progress counters when restarting an epoch loop that had already finished (#9371),  integrations   Rename   Apply suggestions from code review   cut out   skip _   fix   setup   1e-2   uninstall   revert   note ,0
222,Ensure SIGTERM handlers other than ours can be added (#16534),0.69594973,Another feature is automatic SIGTERM handling:,,0
223,Merge pull request #13123 from Lightning-AI/mps_accelerator,0.67552805,- Added `teardown()` method to `Accelerator` ([#11935](https://github.com/Lightning-AI/lightning/pull/11935)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Cole Hawkins  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: awaelchli aedu.waelchli@gmail.com,0
224,Add warning_cache.deprecation and set warning stacklevel [1/2] (#8005),0.6108191,Changed warnings and recommendations for dataloaders in ddp_spawn (#6762),"  Support BaguaStrategy with external implementation   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   add an introduction of BaguaStrategy   update the introduction of BaguaStrategy   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Apply suggestions from code review   link   Fix document formatting issues   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update lightning-bagua version   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update lightning-bagua version   Apply suggestions from code review    Co-authored-by: Yafen Fang fangyaf@spaceml1.ethz.ch Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz",0
225,validate manual optimization and supported features before running training (#7788),0.7088245,"Refactor RunningStage and TrainerState usage (#4945, #7173)","  ci unfreeze for master   accept str   crone   is in   str   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",1
226,Put back initialization of properties in trainer (#9594),0.7605407,Removed TrainerProperties mixin and moved property definitions directly into Trainer (#9495),Co-authored-by: Sabine sabine.nyholm@neptune.ai Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
227,Docs- update lightning 2 steps guide (#3602),0.78694737,Update the Lightning App docs (#13537),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
228,[1 / 3] improvements to saving and loading callback state (#6886),0.6929538,Callback hooks for loading and saving checkpoints,,0
229,Allow dataloaders without sampler field present (#1907),0.99999994,Allow dataloaders without sampler field present (#1907),,1
230,[warning] Add warning when values are not being reduced (#6417),0.64286906,Updated Multinode Warning (#16091),,0
231,ref: organize args 2/n (#3448),0.8523711,"organize args (##3435, #3442, #3447, #3448, #3449, #3456)",Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
232,Doc Terminology updates (#13972),0.70796025,Docs,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,1
233,[feat] Support custom filesystems in LightningModule.to_torchscript (#7617),0.69534683,Optionally customize logging in LightningModule,Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
234,extend documentation (#569),0.53350556,expand eval loop out (#3165),Co-authored-by: awaelchli aedu.waelchli@gmail.com,0
235,Enable all ddp params for hpu parallel strategy (#13067),0.6617011,moves configure ddp to each backend (#3924),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
236,Fault Tolerant: move signal to SIGTERM (#10605),0.75894606,- Fault Tolerant relies on `signal.SIGTERM` to gracefully exit instead of `signal.SIGUSR1` ([#10605](https://github.com/PyTorchLightning/pytorch-lightning/pull/10605)),,1
237,[bugfix] TPU + all_gather + SingleTPU shouldn't call xm.all_gather (#6296),0.5662919,Accelerator all_gather supports collection (#5221),,0
238,[docs] add ray tune tutorial link to sidebar and readme (#2648),0.57354105,You can find more documentation about the tuner here.,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
239,isolate PL debugger in tests (#4643),0.62753826,Deprecated the TestTubeLogger (#9065),,0
240,3/n Move Accelerator into strategy - remove model_sharded_context() (#10886),0.67685217,The argument distributed_backend has been removed from the Trainer in favor of the new accelerator and strategy arguments (#10017).,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
241,[accelerator][FeatBugFix] Improve manual optimization API (#5771),0.71581227,"Refactored TrainingBatchLoop and extracted OptimizerLoop, splitting off automatic optimization into its own loop (#9191)",,1
242,Switch multi-optimizer tests to manual optimization (#16559),0.8328855,Working with multiple optimizers (#16539),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
243,Bump pytest from 7.1.3 to 7.2.0 in /requirements (#15677),0.7374092,Dropped official support/testing for PyTorch <1.6 (#8288),"  Update README.md   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Apply suggestions from code review   Apply suggestions from code review    Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",1
244,[App] Enable properties for the Lightning flow (#15750),0.8167373,Lightning App,,1
245,"[docs] Update deepspeed docs, add some more information and link to streamlit (#8691)",0.6645628,Updated precision attributes in DeepSpeedPlugin (#10164),,0
246,fix bug_report template (#2052),0.5986665,Fixing critical bugs in newly added hooks and hparams assignment.,,0
247,Optimizer closure (#4190),0.88310647,Check if optimizer supports closure (#4981),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
248,release v0.4.9,0.77351236,0.4.0,,1
249,Update pytorch_lightning imports in examples (#16615),0.8333024,from pytorch_lightning import LightningModule,,1
250,update docs on logging (#3916),0.7345358,Logging,,1
251,CI: Update colossalai version (#16747),0.54209435,"- Removed the `ColossalAIStrategy` and `ColossalAIPrecisionPlugin` in favor of the new [lightning-colossalai](https://github.com/Lightning-AI/lightning-colossalai) package ([#16757](https://github.com/Lightning-AI/lightning/pull/16757), [#16778](https://github.com/Lightning-AI/lightning/pull/16778))",Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
252,consolidate callbacks and hooks (#950),0.78467166,Callback hooks,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
253,Fix mypy in utilities.device_parser (#8136),0.525513,"device parser (#3400, #3405)",Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
254,limit (#2756),0.61303174,Deprecated max_nb_epochs and min_nb_epochs (#567),,0
255,Attempt to add broken test to mimic transformers use case (#2272),0.6331276,"- The `Trainer.{fit,validate,test,predict,tune}` methods now raise a useful error message if the input is not a `LightningModule` ([#13892](https://github.com/Lightning-AI/lightning/pull/13892))",,0
256,Improvements related to save of config file by LightningCLI (#7963),0.79171276,Saved the LightningCLI config on setup and only on the main process (#8017),,1
257,add make for docs (#5685),0.5881649,Docs,,0
258,Tune cc-bot settings (#10544),0.6155346,Changed fsspec to tuner (#4458),,0
259,skip some description from pypi (#5234),0.64196634,py,,0
260,Remove unneeded check,0.66214955,"Removed the on_epoch guard from the ""should stop"" validation check (#7701)",Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
261,Docs: Update BYOC IAM Policy Permissions (#16474),0.55387914,Updated governance docs,,0
262,[Metrics] Unification of regression (#4166),0.7345622,Regression metrics (#2221),  docs: update links to PL latest   also stable   last   Apply suggestions from code review   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   fixing   .   fabric   fixing   .    Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
263,update (#8674),0.6013223,Key updates,typo,0
264,update XLA nightly version check (#5989),0.51783615,- Removed the deprecated `XLAStatsMonitor` callback ([#12688](https://github.com/Lightning-AI/lightning/pull/12688)),,0
265,Add support for devices='auto' (#10264),0.65007305,Support auto_select_gpus with the accelerator and devices API (#12608),Co-authored-by: Jirka jirka.borovec@seznam.cz,0
266,Bump JamesIves/github-pages-deploy-action from 4.4.0 to 4.4.1 (#15152),0.54512566,  * Removed the default `Loop.run()` implementation ([#16384](https://github.com/Lightning-AI/lightning/pull/16384)),  update links to Discord   link   Apply suggestions from code review   Co-authored-by: Luca Antiga luca.antiga@gmail.com   slack   Update docs/source-app/levels/expert/index.rst   Apply suggestions from code review    Co-authored-by: Luca Antiga luca.antiga@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
267,Add LightningDataModule.load_from_checkpoint to load datamodules directly from checkpoint (#12550),0.8971439,- Added `LightningDataModule.load_from_checkpoint` to support loading datamodules directly from checkpoint ([#12550](https://github.com/Lightning-AI/lightning/pull/12550)),"  docs: migration guide - structure   update   try   ...   1.9   1.8   1.7   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   cleaning 1.9   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   crosslink   ...   placeholder   1.6   placeholders   1.5   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   1.4   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com",1
268,split restore_training_state into logical parts [1 / 2] (#7901),0.627416,Refactored training loop (#2336),,0
269,Standardize model attribute access in training type plugins (#11072),0.70418024,Automatically set sync_batchnorm for training_type_plugin (#6536),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: awaelchli aedu.waelchli@gmail.com Co-authored-by: Jirka jirka.borovec@seznam.cz,1
270,Refactor access to trainer attributes in LightningModule (#5730),0.78728956,reference to the Trainer on the LightningDataModule (#3684),,1
271,Make the FaultToleranceCheckpoint callback opt-in (#16512),0.7117911,Replace MisconfigurationException with warning in ModelCheckpoint Callback (#4560),  move   init files   reset change   missed   more   relative path   e2e tests   update checkgroup   reference   update app_* ,1
272,Use fsspec in load to resolve more paths/URLs from storage backends (#3692),0.65676725,Swaped torch.load for fsspec load in DDP spawn backend (#3787),  rename PL installation   update   Apply suggestions from code review   Update docs/source-pytorch/starter/installation.rst   Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com  Update docs/source-pytorch/starter/installation.rst   Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com,0
273,Prune metrics: AUC & AUROC (#6572),0.5712731,Deprecated reorder parameter of the auc metric (#4237),Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,0
274,Use full torch.distributed import (#8200),0.79045683,import torch,,1
275,Update strategy import statements (#11238),0.6469324,import argparse,,0
276,[CI] Trick Bagua into installing appropriate wheel in GPU tests (#14380),0.6738659,GPU training (#2704),,0
277,Fix pre-commit isort failure on tests/base/*.py (#5429),0.6810161,Dropped official support/testing for PyTorch <1.6 (#8288),Co-authored-by: awaelchli aedu.waelchli@gmail.com,0
278,Remove unused mixin attributes (#6487),0.9864671,Removed unused mixin attributes (#6487),,1
279,added override for hparams in load_from_ckpt (#1797),0.6211772,Deprecated tags_csv in favor of hparams_file (#1271),,0
280,updated example doc links,0.65243983,Docs improvements,,0
281,Update docs for GradientAccumulationScheduler (#9891),0.6458844,Separate the Gradient Accumulation Scheduler from Trainer (#16729),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: awaelchli aedu.waelchli@gmail.com,0
282,CI: adjust GPU timeout (#13628),0.6876254,Changed min-max GPU memory to be on their own plots (#1358),,0
283,[metrics] Renaming of precision recall metric (#3308),0.9815365,Renaming of precision recall metric (#3308),,1
284,ShardedGradScaler should only be set for FP16 (#12915),0.68466175,Changed fsspec to tuner (#4458),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
285,Add a flavor of training_step that takes dataloader_iter as an argument (#8807),0.7635904,Training Step With DataLoader Iterator,,1
286,Add the on_before_backward hook (#7865),0.68018806,The on_after_backward hook is now called on accumulating iterations. Use the on_before_optimizer_step hook to mimic the old behaviour (#8328),,0
287,Attach data refactor and tuner bugs [4/n] (#7258),0.6945057,Porting fixes to autoscaler component (#16249),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
288,CI: Reset cache weekly (#7686),0.5549206,Periodically sync database to the drive (#15441),add past versions link,0
289,"Fix self.log(sync_dist=True, reduce_fx={mean,max}) (#9142)",0.7860902,Deprecated self.log(sync_dist_op) in favor of self.log(reduce_fx). (#7891),,1
290,Deprecate on_epoch_start/on_epoch_end hook (#11578),0.78725743,# 3. Rename the hook to `on_*_epoch_end`,,1
291,Prune deprecated classif. metrics (#7499),0.8079672,Pruned deprecated classif. metrics from pytorch_lightning.metrics.functional.classification (#7499),,1
292,fix mock pkgs in docs (#4591),0.578081,"docs for all Metrics (#2184, #2209)",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
293,Update docs iterable datasets (#1281),0.63433284,return iterabledataset,,0
294,Bump playwright from 1.30.0 to 1.32.1 in /requirements (#17537),0.47658962,[1.4.0] - 2021-07-27,,0
295,Update LightningDataModule docs (#10678),0.9189495,Update the Lightning App docs (#13537),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,1
296,Improve some tests (#5049),0.6977874,test_percent_check in favour of limit_test_batches,Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
297,"Reverse width, height to height, width in docs (#8612)",0.5583558,"docs for all Metrics (#2184, #2209)",,0
298,fix setting batch_size attribute in batch_size finder (finishing PR #2523) (#3043),0.7839793,Batch Size Finder (#11089),  Data connection mounts for jobs running from CloudSpaces   Tests   Tests fix   Bump Lightning Cloud ,1
299,fix default arg (#1927),0.5916004,Remove beta arg from F1 class and functional (#5076),,0
300,test/hotfix: DDPSpawnStrategy (#16889),0.6401216,"Implemented {DDPShardedPlugin,DDPShardedSpawnPlugin}._setup_model_and_optimizers (#10028, #10064)",,0
301,Update metrics to use Enum (#5689),0.62842476,Updated metrics to use LightningEnum (#5689),,0
302,Handle set_to_none when using DeepSpeed optimizer in Lite (#16275),0.7645346,Support for manual optimization with DeepSpeed (#7970),,1
303,added warning to crash (#1625),0.6282792,warning_utils >> warnings,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
304,give a more complete GAN example (#6294),0.5351028,Have a look at the full example here.,,0
305,Add CI for python lightning app Python unit tests (#13491),0.6529001,python script.py test,,0
306,[CLI] fix cluster creation CLI requiring instance-type selection (#14056),0.6905279,Enabled custom clusters (#4048),,0
307,Allow CUDA and IPU tests without the CI environment var (#13676),0.53943634,"- Trainer queries the CUDA devices through NVML if available to avoid initializing CUDA before forking, which eliminates the need for the `PL_DISABLE_FORK` environment variable introduced in v1.7.4 ([#14631](https://github.com/Lightning-AI/lightning/pull/14631))",,0
308,Increase multiple optimizers parity for drone CI (#4884),0.7385488,Working with multiple optimizers (#16539),,1
309,fix hparams issue,0.7045765,Fixing critical bugs in newly added hooks and hparams assignment.,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
310,general doc updates 2/n (#12517),0.63329303,Docs improvements,,0
311,"Update myst-parser requirement from <0.17,>=0.15 to ==0.18.1 in /requirements (#14417)",0.5275718,Deprecated PrecisionPlugin.master_params() in favor of PrecisionPlugin.main_params() (#10105),,0
312,set next App dev (#14609),0.6086501,App,,0
313,v1.2.0 (#6065),0.7516234,"    version=""0.0.1"",","  System customization syncing for jobs run   Constants moved   Moving files   Update src/lightning/app/runners/cloud.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   Update test_cloud.py   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  Update test_cloud.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",1
314,Add CI for app examples (#13495),0.70756406,Introducing CLI commands for apps (#13602)!,,1
315,[HOTFIX] Logging for evaluation (#4684),0.6797364,Logging,,0
316,Fix checkpoint values when saving and resetting the tuner state (#11518),0.7017152,Save a checkpoint to restore the state on exception (opt-in) (#8362),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
317,Add cute teaser animations to Fabric docs (#17021),0.5005324,Learn more about Fabric and what it can do in the new docs!,Co-authored-by: Luca Antiga luca.antiga@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
318,Adjust mergify's number of reviewer rules (#14293),0.55103743,Deprecation warning (#3844),,0
319,Fix master import conflict (#11203),0.65678906,Module imports are now catching ModuleNotFoundError instead of ImportError (#9867),,0
320,add s-rog to core (#4017),0.5498531,Enabled custom clusters (#4048),,0
321,Docs/template (#2152),0.69748056,"docs for all Metrics (#2184, #2209)",,0
322,test: adjust is_timing_close (#17178),0.53512406,Early stopping checks on_validation_end (#1458),,0
323,Check early stopping metric in the beginning of the training (#542),0.6744777,"trainer = pl.Trainer(callbacks=EarlyStopping(..., log_rank_zero_only=True))",,0
324,Lite: Fix DataLoader shuffling when using DistributedSampler (#15931),0.610601,"- The LightningModule no longer gets wrapped with data-parallel modules when not fitting in DDPPlugin, DDPSpawnPlugin, DDPShardedPlugin, DDPSpawnShardedPlugin.",,0
325,Adding Intersphinx documentation links (#1369),0.53656983,Pinned sphinx-autodoc-typehints with <v1.15 (#11400),,0
326,[FSDP] Move on save checkpoint outside of zero check (#7134),0.7040556,"-def on_save_checkpoint(self, checkpoint):",,1
327,Write progress bar to stdout (#531),0.85484767,Changed the default progress bar to print to stdout instead of stderr (#531),,1
328,Update lightning_module_template.py,0.76570964,Update the Lightning App docs (#13537),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
329,Use is to compare type of objects (#8404),0.479436,Changed type checker with explicit cast of ref_model object (#4457),,0
330,Drop pre-commit from /requirements (#17049),0.5538453,Removed support for the deprecated on_load_checkpoint signature. The hook now takes a pl_module positional parameter (#8697),"What does this PR do? Passes environment variables to the CloudRuntime when loading an app from a file. Allows apps / works / flows to be properly configured when passing variables at runtime. Limitations  Only implemented for the CloudRuntime   Before submitting  - Was this **discussed/agreed** via a GitHub issue? (not for typos and docs) - [ ] Did you read the [contributor guideline](https://github.com/Lightning-AI/lightning/blob/master/.github/CONTRIBUTING.md), **Pull Request** section? - [ ] Did you make sure your **PR does only one thing**, instead of bundling different changes together? - Did you make sure to **update the documentation** with your changes? (if necessary) - Did you write any **new necessary tests**? (not for typos and docs) - [ ] Did you verify new and **existing tests pass** locally with your changes? - Did you list all the **breaking changes** introduced by this pull request? - Did you **update the CHANGELOG**? (not for typos, docs, test updates, or minor internal changes/refactors)   In the CHANGELOG, separate each item in the unreleased section by a blank line to reduce collisions   PR review Anyone in the community is welcome to review the PR. Before you start reviewing, make sure you have read the review guidelines. In short, see the following bullet-list:  Reviewer checklist  - [ ] Is this pull request ready for review? (if not, please submit in draft mode) - [ ] Check that all items from **Before submitting** are resolved - [ ] Make sure the title is self-explanatory and the description concisely explains the PR - [ ] Add labels and milestones (and optionally projects) to the PR so it can be classified     Did you have fun?  Make sure you had fun coding 🙃   cc @borda",0
331,Fix erroneous warning for unset max_epochs (#13262),0.7137114,Deprecated max_nb_epochs and min_nb_epochs (#567),,1
332,Deprecate auto mode from ModelCheckpoint and EarlyStopping (#4695),0.96166897,Deprecated mode='auto' from ModelCheckpoint and EarlyStopping (#4695),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
333,Bump default Lightning e2e image version for custom dependencies integration tests (#16975),0.6226244,Updated compatibility for LightningLite to run with the latest DeepSpeed 0.7.0 (13967),Co-authored-by: awaelchli aedu.waelchli@gmail.com,0
334,Rename TPUHalfPrecisionPlugin to TPUBf16PrecisionPlugin (#10026),0.98568374,Renamed TPUHalfPrecisionPlugin to TPUBf16PrecisionPlugin (#10026),,1
335,Fix Grid run commands (#8021),0.5629767,Improved error messages for invalid configure_optimizers returns (#3587),,0
336,CI: enable CI run for PT 1.13 (#15128),0.5397197,Introducing CLI commands for apps (#13602)!,,0
337,Use base version when comparing torch versions (#16657),0.74463594,- Fixed an issue with comparing torch versions when using a version of torch built from source ([#17030](https://github.com/Lightning-AI/lightning/pull/17030)),,1
338,Remove redundant strategy property (#16811),0.6331586,Renamed Strategy.reduce to Strategy.all_reduce in all strategies (#16370),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
339,Update CODEOWNERS (#17322),0.57190955,Deprecated @data_loader decorator  (#926),,0
340,Address code review for deepspeed (#6042),0.7014215,Ensure we check deepspeed/sharded in multinode DDP (#6297),,1
341,update tests with new auto_opt api (#5466),0.5943177,Updated app testing (#16000),,0
342,Mock GPU accelerator connector tests (#10554),0.72641635,GPU training (#2704),Co-authored-by: Nikhil Shenoy nikhilshenoy@dhcp-128-189-229-146.ubcsecure.wireless.ubc.ca Co-authored-by: Nikhil Shenoy nikhilshenoy@dhcp-128-189-231-235.ubcsecure.wireless.ubc.ca Co-authored-by: Nikhil Shenoy nikhilshenoy@dhcp-128-189-229-169.ubcsecure.wireless.ubc.ca Co-authored-by: Nikhil Shenoy nikhilshenoy@dhcp-128-189-230-40.ubcsecure.wireless.ubc.ca,1
343,updated optimizer_step docs,0.84300226,        optimizer.step(),"  add pkg info files   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Update src/lightning_app/shell-folder_code-lives-lightning.info   Update src/lightning_app/shell-folder_code-lives-lightning.info   Update src/lightning_fabric/shell-folder_code-lives-lightning.info   Update src/lightning_fabric/shell-folder_code-lives-lightning.info   Update src/pytorch_lightning/shell-folder_code-lives-lightning.info   Update src/pytorch_lightning/shell-folder_code-lives-lightning.info    Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Luca Antiga luca.antiga@gmail.com",1
344,fix(cloud): detect and ignore venv (#16056),0.6493498,Only check versions / env when not in the cloud (#15504),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
345,Enable Probot CheckGroup V2 (#15612),0.6008856,Configuration Validator (#9779),  docs: building releases   codeowners   todo   rev   on *   push tag   name ,0
346,Make dataloader_idx optional for batch start/end hooks (#16753),0.76193,"- The `dataloader_idx` argument is now optional for the `on_{validation,test,predict}_batch_{start,end}` hooks. Remove it or default it to 0 if you don't use multiple dataloaders ([#16753](https://github.com/Lightning-AI/lightning/pull/16753))",,1
347,enhanced optimizer return options (#120),0.8137535,"return model, optimizers",Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,1
348,Fix current_epoch value on training end (#8578),0.806475,Made training_epoch_end behave like validation_epoch_end (#1357),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,1
349,Remove the deprecated loop output format (#14373),0.6207745,- Removed support for loop customization,,0
350,Add missing arg to docker build. (#2905),0.5737423,"add .log to lightning module (#3686, #3699, #3701, #3704, #3715)", [Do not merge] debugging custom dependencies integration tests,0
351,release v0.5.3,0.7709256,0.4.0,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka jirka.borovec@seznam.cz,1
352,Check broken links (#16168),0.5914285,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
353,metrics: add BLEU (#2535),0.63512564,Renamed class metric Fbeta >> FBeta (#4656),Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,0
354,clear skipping tests (#1285),0.586845,test_percent_check in favour of limit_test_batches,,0
355,[fix] [easy] Update Model Checkpoint callback overrides to use base Callback signature (#6908),0.69382167,Removed support for the deprecated on_save_checkpoint signature. The hook now takes a checkpoint positional parameter (#8697),,0
356,Add a more direct test of multi-gpu training working (#2084),0.76466674,GPU training (#2704), E2E image version using from environment variable instead of hardcoded one,1
357,Rename AcceleratorConnector.training_type_plugin to AcceleratorConnector.strategy (#11212),0.77251303,Removed deprecated connect_precision_plugin and connect_training_type_plugin from Accelerator (#9019),,1
358,"Update numpy requirement from <=1.22.3,>=1.17.2 to >=1.17.2,<1.22.5 in /requirements (#13361)",0.6152854,"- num_devices = max(1, trainer.num_gpus, trainer.num_processes)",,0
359,implement fix and test (#3459),0.65716636,Enabling val/test loop disabling (#2692),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
360,Fix val_loop run on restart (#11552),0.6698487,Set Loop.restarting=False at the end of the first iteration (#8362),,0
361,fix import removed DDPShardedStrategy (#16341),0.6628107,Merged the implementation of DDPSpawnStrategy into DDPStrategy and removed DDPSpawnStrategy (#14952),,0
362,Extend the deprecation of Trainer(resume_from_checkpoint) (#11334),0.788749,"trainer = Trainer(callbacks=[FineTuneBatchSizeFinder(milestones=(5, 10))])",,1
363,type accelerators (#6148),0.7587523,Selects the accelerator,,1
364,"Fixed configure_ddp, removed lr scheduler modification, added unit tests",0.63284075,Disabled lr_scheduler.step() in manual optimization  (#6825),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Luca Antiga luca.antiga@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
365,conda speedup (#2546),0.6205298,"many speed improvements (how we move data, adjusted some flags & PL now adds 300ms overhead per epoch only!)",,0
366,Fixed uploading best model checkpoint in NeptuneLogger (#10369),0.6542201,Skipped best_model_path if checkpoint_callback is None (#2962),,0
367,[App] Temporarily disable ready (#15958),0.6501415,"Apps without UIs no longer activate the ""Open App"" button when running in the cloud (#15875)",,0
368,Fixes #361 (#391),0.5896955,Resolve bug with Finetuning (#5744),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
369,Fix failing docs (#1821),0.5529788,Fixing critical bugs in newly added hooks and hparams assignment.,"  message fix for maverick   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  cleanup   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
370,add missing overfit_pct docs (#1204),0.6687559,overfit_pct in favour of overfit_batches,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
371,Fix #4375: Always use trainer.global_step for step (#4376),0.6726362,Access to trainer.global_step during an intra-training validation hook will now correctly return the number of optimizer steps taken already. In pseudocode:,,0
372,Test Callback.on_load_checkpoint order (#8588),0.7502612,"Deprecated Callback.on_load_checkpoint(checkpoint) in favor of Callback.on_load_checkpoint(trainer, pl_module, checkpoint) (#7253)",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
373,Error messages for removed Logger APIs (#15067),0.81349736,Removed LoggerStages (#5673),,1
374,Update CI torch-xla version to 1.8 (#7019),0.6371851,Removed dependency on torchvision (#797),,0
375,fixes typing errors in rich_progress.py (#14963),0.6097096,- Changed `MisconfigurationException` to `ModuleNotFoundError` when `rich` isn't available ([#11360](https://github.com/PyTorchLightning/pytorch-lightning/pull/11360)),Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
376,"Update tensorboard requirement from <2.11.0,>=2.9.1 to >=2.9.1,<2.12.0 in /requirements (#15746)",0.7100811,Improved the error message for installing tensorboard or tensorboardx (#17053),,1
377,Update 2,0.6631158,0.4.0,,0
378,Fix support for dataloader with None batches (#7342),0.7133266,      for batch in dataloader:,,1
379,[doc] Add Fault Tolerant Documentation Page (#9256),0.75342757,- Fault Tolerant Manual,Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz,1
380,Make Trainer readable and debuggable (3/n) (#14871),0.7217444,Allow easy trainer re-instantiation (#7508),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,1
381,guard against None in pytorch get_xla_supported_devices (#9572),0.5905188,"Removed PyTorch 1.6 support (#10367, #10738)", Update torchmetrics requirement in /requirements  Updates the requirements on torchmetrics to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: torchmetrics   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com  task  Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz,0
382,try update horovod (#4004),0.7541945,"refactored Horovod backend (#3121, #3122)",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
383,Fixing order of operations bug in Qnet (#9621),0.49914983,Resolved a bug about a race condition when sending the work state through the caller_queue (#14074),,0
384,Add path filters for some non-required jobs (#14539),0.51542604,System customization syncing for jobs run (#16932),,0
385,ci: pypi / fabtic (#16272),0.64058447,PyTorch,,0
386,Prepare v1.5.0rc1 (#10068),0.5974865,Implemented ready for components (#16129),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
387,Fix(ci) ONC-114: reduce the ci load by only installing lmdb in tests (#14581),0.5783105,remove weight loading hack for ddp_cpu (#3808),,0
388,Clarify val_check_interval description (#11951),0.6888722,Trainer(val_check_interval=100),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
389,[App][CI] Delete apps older than 1 hours CI account (#15238),0.4748772,Do not override the existing epoch value in logged_metrics when already logged by the user (#7982),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
390,Lazy import check for wandb dependency (#13474),0.64853966,Wrapped imports for traceability (#13924),Maverick registration (#16913),0
391,Fix typing in hparams methods (#9116),0.6917484,Fixing critical bugs in newly added hooks and hparams assignment.,"  docs: move fabric to Lai   update imports   links   drop link to Trainer   own docs   ci   trigger   prune cross-links   cleaning   cleaning   template   imports   template   path   links   tensorboardX   plugins   label   drop fixme   drop copy nb + examples   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Apply suggestions from code review   try again   rev    Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
392,Further clean up aggregation logic (#12053),0.60167944,clean up hooks in run_evaluation (#3156),,0
393,[2/n] add Stateful functionality support for Callbacks (#12232),0.70261776,"    # Now, the state for this callback gets passed to this new method",Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka jirka.borovec@seznam.cz,1
394,Fix info message when EarlyStopping 'mode' not provided [ci skip] (#4282),0.61436176,Removed mode='auto' from EarlyStopping (#6167),,0
395,Don't convert namedtuple to tuple (#1589),0.7524451,Don't convert namedtuple to tuple when transferring the batch to target device (#1589),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
396,Move global_step incrementing (#412),0.7375077,+ global_step += 1,"  docs: rename source-app   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   ci   group check   trigger   param   fix   cleaning    Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",1
397,[bugfix] Minor improvements to apply_to_collection and type signature of log_dict (#7851),0.93738663,Minor improvements to apply_to_collection and type signature of log_dict (#7851),,1
398,feat: Add Rich Progress Bar (#8929),0.8753215,New Rich Progress Bar,,1
399,docs (#4081),0.7464155,Docs,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
400,Add Metric <-> Lightning Module integration tests (#4008),0.74373204,Updated metrics to use LightningEnum (#5689),,1
401,Fix rich progress bar render only on main pbar (#11690),0.7719085,- Fixed an issue in `RichProgressbar` to display the metrics logged only on main progress bar ([#11690](https://github.com/PyTorchLightning/pytorch-lightning/pull/11690)),,1
402,adding docs,0.74254435,Docs,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,1
403,fixing test (#451),0.6736528,- fixed all the .test() calls,,0
404,Docs (#1068),0.77114254,Docs,  update docs imports   ci   fabric   trigger   links   .   docstring   chlog   cleaning ,1
405,[docs] Docs for ColossalaiStrategy (#15093),0.58398134,Docs,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
406,handle fsspec inconsistency in PyArrowHDFS (#3805),0.6273658,Used fsspec instead of gfile for all IO (#3320),,0
407,remove legacy accelerators (#5949),0.69589436,Removed on_train_epoch_end from Accelerator (#9035),,0
408,Move Colab setup to ProgressBar (#10542),0.69749737,Better progress bar (#16695),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
409,fix tpu transfer bug,0.62358713,Updated logic for checking TPUs availability (#6767),,0
410,update copyright in PL & Fabric (#16481),0.50618017,Refactored EpochResultStore (#5522),,0
411,update documentation with upcoming supported regions (#16331),0.5661258,Parsed local package versions (#13933),,0
412,Remove deprecated GPUStatsMonitor callback (#12554),0.78130996,Deprecated GPUStatsMonitor and XLAStatsMonitor in favor of DeviceStatsMonitor callback (#9924),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,1
413,Update API references for Callbacks (#11350),0.70167017,Refactor cloud dispatch and update to new API (#16456),,1
414,[bugfix] Logging only on not should_accumulate() during training (#5417),0.86356866,Logging only on not should_accumulate() during training (#5417),,1
415,Fix zero division error for empty dataloaders (#12885),0.60795987,"Raised MisconfigurationException when total length of dataloader across ranks is zero, and give warning when total length is non-zero, but only local rank length is zero. (#9827)",,0
416,Fix Metric.state_dict (#5614),0.809932,Metric states are no longer as default added to state_dict (#4685),Co-authored-by: Luca Antiga luca.antiga@gmail.com,1
417,Include images with the mirror package (#15659),0.50613177,"        image, _ = self.trainer.train_dataloader.loaders.dataset[0]",,0
418,Remove the QuantizationAwareTraining callback (#16750),0.7740338,- Removed the `QuantizationAwareTraining` callback ([#16750](https://github.com/Lightning-AI/lightning/pull/16750)),,1
419,Move NaN/Inf detection to a separate utilities file (#6834),0.599247,Deprecated TrainerTrainingTricksMixin in favor of a separate utilities module for NaN/Inf detection for gradients and parameters (#6834),,0
420,Fix XLAEnvironment detection on TPU pod (#16806),0.61428154,Enabled manual optimization for TPUs (#8458),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
421,continue (#2450),0.60807323,[1.5.5] - 2021-12-07,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
422,Added CHANGELOG section (#5065),0.7101374,Full Changelog,,1
423,refactor default model (#1652),0.7396732,Refactor Model backward (#2276),,1
424,fix mypy typing errors in pytorch_lightning/strategies/single_device.py (#13532),0.69172025,+ # pytorch_lightning==1.7.0,,0
425,Fix inspection of unspecified args for container hparams (#9125),0.6283173,Fixing critical bugs in newly added hooks and hparams assignment.,Co-authored-by: Ethan Harris ethanwharris@gmail.com,0
426,bugfix: Resolve interpolation bug with Hydra (#5406),0.9423739,Resolve interpolation bug with Hydra (#5406) ,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
427,restore functional metrics (#3943),0.7926645,"Add deprecated metric utility functions back to functional (#5067, #5068)",,1
428,Fallback to module available check for mlflow (#17467),0.6000079,Updated mlflow with using resolve_tags (#6746),,0
429,Fixed docs for WandbLogger (#5128),0.6766229,"The default project name in WandbLogger is now ""lightning_logs"" (#14145)",Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
430,docs order contributors alphabetically (#17102),0.5213993,"docs for all Metrics (#2184, #2209)",,0
431,Update Github issues template (#13857),0.5904228,- Deprecated `LightningLoggerBase.update_agg_funcs` ([#11871](https://github.com/PyTorchLightning/pytorch-lightning/pull/11871)),,0
432,Raise an exception when using DeepSpeed with an invalid accelerator (#12699),0.7335098,- Added a friendly error message when attempting to use `DeepSpeedStrategy` on unsupported accelerators ([#12699](https://github.com/Lightning-AI/lightning/pull/12699)),,1
433,Update fault_tolerant_training_basic.rst (#16012),0.7445287,Fault-tolerant Training,,1
434,Dont try to update an instance that isnt running yet (#15998),0.58649015,Resolved a bug where the install command was not installing the latest version of an app/component by default (#14181),,0
435,Resolve App e2es (#15302),0.5477482,"Resolved LightningApp(..., debug=True) (#14464)",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
436,Simplified gpu api. No NVIDIA flag managing by lightning for cluster (#213),0.7000432,- Removed the deprecated automatic GPU selection ([#16184](https://github.com/Lightning-AI/lightning/pull/16184)),,1
437,Gpu idx (#2796),0.68942654,GPU training (#2704),,0
438,Organize accelerator tests (#13986),0.75731283,Selects the accelerator,,1
439,Remove precision_plugin pre_dispatch() method (#10887),0.7628081,- Removed method `pre_dispatch` from the `PrecisionPlugin` ([#10887](https://github.com/PyTorchLightning/pytorch-lightning/pull/10887)),,1
440,docs: fix order of on_fit_start() hook (#16180),0.6777998,| on_pretrain_routine_start  | on_fit_start                 | ,,0
441,Mark the loop classes as protected (#16445),0.7761885,  * The loop classes are now marked as protected ([#16445](https://github.com/Lightning-AI/lightning/pull/16445)),Co-authored-by: awaelchli aedu.waelchli@gmail.com,1
442,Fix double precision casting complex buffers (#8208),0.6021399,Mixed precision overhaul (#16783),,0
443,Organize model summary utilities (#13893),0.6160854,Direct support for compiled models (#15922),,0
444,Add docs for datamodule hparams (#9428),0.70377415,Allow metrics logged together with hparams (#1630),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,1
445,[FIX] Average Pbar Metrics (#4534),0.6232436,Updated explained variance metric (#4024),"  fix(app): URLs, create run on app run   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   fix tests   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
446,unify logger testing (#9081),0.749792,Cleaning up stale logger tests (#3490),,1
447,Refactor signature for launcher (#11967),0.6639231,Configuration Validator (#9779),,0
448,Use class name in SWA info message (#8602),0.47996366,# use slurm job id for the port number,changelog Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
449,Don't collapse Lightning API section (#10545),0.7048318,Gave warnings for unimplemented required lightning methods (#1317),,1
450,Fuse_modules in a qat-respecting way (#12891),0.52832747,Split profilers module (#6261),,0
451,CI/PT 1.9 (#7380),0.5532088,[0.6.0] - 2022-09-08,Co-authored-by: thomas chaton thomas@grid.ai,0
452,added templates (#4077),0.60308224,Allowing decorate model init with saving hparams inside (#4662),,0
453,add accelerator.is_available() check (#12104),0.73617923,Ensure accelerator is valid if running interactively (#5970),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
454,Log LR using LearningRateMonitor even when LR Scheduler is not defined.  (#9786),0.70245045,Consistently use step=trainer.global_step in LearningRateMonitor independently of logging_interval (#4376),,1
455,Update changelog after v1.7.3 release (#14398),0.71293414,Full Changelog,,1
456,add missing flag (#1805),0.6855478,Deprecated flags: (#2213),,0
457,"Removed comments, skip test",0.5665426,- fixed all the .test() calls,,0
458,[Feat-BugFix] Resolve custom DataLoader (#5745),0.67946017,Resolve bug with Finetuning (#5744),,0
459,"Rename ""master"" methods to ""main"" in ClusterEnvironment plugins (#10103)",0.67722833,"- Deprecated `ClusterEnvironment.master_{address,port}` in favor of `ClusterEnvironment.main_{address,port}` ([#10103](https://github.com/PyTorchLightning/pytorch-lightning/pull/10103))",,0
460,fix tqdm standalone test (#12493),0.6136757,Updated logic for checking TPUs availability (#6767),,0
461,Missing steps in run on your own machine docs (#15033),0.53807473,The default start_method for creating Work processes locally on macOS is now 'spawn' (previously 'fork') (#16089),,0
462,Support save_hyperparameters() in LightningModule dataclass (#7992),0.745862,- Fixed an `AttributeError` when calling `save_hyperparameters` and no parameters need saving ([#11827](https://github.com/PyTorchLightning/pytorch-lightning/pull/11827)),,1
463,fixes 3549 (#3564),0.6410048,Tons of bug fixes,,0
464,Enable gradients at train start (#2200),0.5801248,Moved the gradient unscaling in NativeMixedPrecisionPlugin from pre_optimizer_step to post_backward (#9606),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
465,Make device and dtype required (#9168),0.61575735,"            device_ids=device_ids,",,0
466,docs 5/n (#15669),0.6959683,"docs for all Metrics (#2184, #2209)",,0
467,CI: update prune_pkgs (#12382),0.60049427,"Removed PyTorch 1.6 support (#10367, #10738)",update docker,0
468,Add XLA Profiler section to docs (#11436),0.5550474,Moved profilers to their own file (#7822),,0
469,Fix root node resolution in slurm environment,0.7156837,    root_node = os.environ['SLURM_NODELIST'].split(' ')[0],,1
470,Bump lightning cloud for memory leak fix (#14697),0.86460763,Resolved the memory leak issue with the Lightning Cloud package and bumped the requirements to use the latest version (#14697),,1
471,added log saving when early epoch stop,0.69701576,Do not override the existing epoch value in logged_metrics when already logged by the user (#7982),,0
472,Fault Tolerant Manual: Enable the feature (#10707),0.8644719,- Fault Tolerant Manual,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
473,release v0.121,0.7914745,"    version=""0.0.1"",",,1
474,Datamodule (#2668),0.70792454,Replaced _DataModuleWrapper with __new__ (#7289),,1
475,Custom argparser extension with Trainer arguments (argument types added)  (#1147),0.8621616,Trainer.add_argparse_args classmethod fixed. Now it adds a type for the arguments (#1147),Co-authored-by: Ethan Harris ethanwharris@gmail.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: thomas thomas@thomass-MacBook-Pro.local,1
476,Sharing Datasets Across Process Boundaries (#10951),0.6484524,Native Fully Sharded Data Parallel Strategy,  remove bagua   remove   remove docker file entry ,0
477,debug,0.7157997,DDP Debugging Improvements,,1
478,[App] Only check versions / env when not in the cloud (#15504),0.9396883,Only check versions / env when not in the cloud (#15504),Co-authored-by: thomas thomas@thomass-MacBook-Pro.local,1
479,Update HorovodStrategy to use optimizers property from within class (#11728),0.6917729,Deprecated the HorovodStrategy class,,0
480,[doc] Fix closure in manual optimization (#6374),0.7388215,Executing the optimizer_closure is now required when overriding the optimizer_step hook (#9360),,1
481,Add typing for _FxValidator [1/3] (#9269),0.49863163,Enabling val/test loop disabling (#2692),,0
482,fixed alternating loss,0.6223681,Backward Incompatible Changes,,0
483,feat(wandb): support media logging (#9545),0.5864886,Logging,,0
484,Remove legacy teardown check in train loop (#7917),0.6963867,Removed deprecated TrainResult (#5323),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
485,Remove deprecated stochastic_weight_avg argument from Trainer (#12535),0.81565326,Deprecated passing stochastic_weight_avg to the Trainer constructor in favor of adding the StochasticWeightAveraging callback directly to the list of callbacks (#8989),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
486,Update ddp_plugin.py (#4363),0.72652173,- The `DDPSpawnPlugin` no longer overrides the `post_dispatch` plugin hook ([#10034](https://github.com/PyTorchLightning/pytorch-lightning/pull/10034)),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
487,[pre-commit.ci] pre-commit suggestions (#11301),0.5009621,The preemption/termination signal is now configurable (#14626):,Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
488,Avoid deprecation warning after #9901 (#9951),0.82808256,Deprecation warning (#3844),  update CI   config / import   lightning_app imports   source/ dir   html   ci: dirs   pr   req dir   on push   rename   drop   cleaning ,1
489,[bug] Replace_sampler attach previous multiprocessing_context (#4742),0.63938797,Fixing critical bugs in newly added hooks and hparams assignment.,,0
490,depre (#4088),0.7863023,Deprecation warning (#3844),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
491,Minor warning message fix (#2173),0.68993765,warning_utils >> warnings,Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
492,Fixed encoding issues on terminals that do not support unicode characters (#12828),0.5197128,"    img_str = base64.b64encode(buffered.getvalue()).decode(""UTF-8"")",,0
493,mark OptimizerLoop.backward method protected (#9514),0.92459655,Marked OptimizerLoop.backward as protected (#9514),,1
494,move profiler.step from training_step_and_backward to optimizer_step_… (#8224),0.7169674,        optimizer.step(),,1
495,Fix TPU CI for non-forks (#14688),0.6363857,Enabled manual optimization for TPUs (#8458),,0
496,update docs on log_save_interval (#3345),0.6731409,moved eval loop logging to loggers (#3408),,0
497,Update optimizers.py (#1383),0.78735983,Refactored optimizer (#4658),,1
498,"Fixes #3668, #3887 as a bonus (#3888)",0.6406578,        return 7  # lucky number 7,Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
499,update bug report template (#15905),0.61436266,Fixing critical bugs in newly added hooks and hparams assignment.,,0
500,Rename DDPPlugin to DDPStrategy (#11142),0.81085974,Merged the implementation of DDPSpawnStrategy into DDPStrategy and removed DDPSpawnStrategy (#14952),Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,1
501,fixed model checkpoint frequency (#3852),0.65490365,Enable None model checkpoint default (#3669),,0
502,Update examples - use DataModule (#4740),0.71327084,Replaced _DataModuleWrapper with __new__ (#7289),,1
503,Fix segmentation example (#876),0.62340325,Resolve bug with Finetuning (#5744),,0
504,add missing punctuation in lightning_cli.rst (#7554),0.62845194,LightningCLI additions:,,0
505,[App] Resolve inconsistency where the flow.flows property isn't recursive leading to flow overrides (#15466),0.7614845,Changed the flow.flows to be recursive wont to align the behavior with the flow.works (#15466),,1
506,Prevent crash if sync_dist=True on CPU (#4626),0.9999998,Prevent crash if sync_dist=True on CPU (#4626),,1
507,test PL examples (#4551),0.62187177,test_percent_check in favour of limit_test_batches,Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com,0
508,Restore signals on teardown (#10611),0.61519176,"refactor DDP backend (#3204, #3207, #3208, #3209, #3210)",introducing maverick,0
509,remove parsing comments (#7958),0.57832384,remove _evaluate fx (#3197),Connect and Disconnect node,0
510,Update TPUSpawnPlugin spawn methods (#10022),0.7328033,Renamed TPUHalfPrecisionPlugin to TPUBf16PrecisionPlugin (#10026),,1
511,fix typo in multi-gpu docs (#5402),0.5538541,Fixing critical bugs in newly added hooks and hparams assignment.,,0
512,Fix restarting attribute for lr finder (#15620),0.6714834,Update lr_finder to check for attribute if not running fast_dev_run (#5990),Co-authored-by: thomas thomas@thomass-MacBook-Pro.local,0
513,[App] fix panel requirements (#14450),0.55987656,Updated error message for interactive incompatible plugins (#9896),Co-authored-by: thomas thomas@thomass-MacBook-Pro.local,0
514,Change the return type of tune() in trainer.py to TypedDict (#13631),0.6531035,trainer.tune() now returns the tuning result (#7258),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
515,release v0.2.2,0.76799583,0.4.0,,1
516,[pre-commit.ci] pre-commit suggestions (#13540),0.5179641,The preemption/termination signal is now configurable (#14626):,,0
517,[bugfix] Resolve example after LightningCLI update (#9520),0.7695194,"Resolved LightningApp(..., debug=True) (#14464)",,1
518,Fix min/max logging default value (#11310),0.65886796,bug fix with logging val epoch end + monitor (#3812),,0
519,Fix default logging levels for train step specific hooks (#10756),0.65018606,Removed restrictions in the Trainer that loggers can only log from rank 0; the existing logger behavior has not changed (#8608),,0
520,Log LearningRateMonitor values to Trainer.callback_metrics for EarlyStopping (#17626),0.80678535,    --trainer.callbacks.LearningRateMonitor \,,1
521,Avoid relpath bug on Windows (#16164),1.0000001,Avoid relpath bug on Windows (#16164),,1
522,prune codecov exceptions (#1107),0.545434,Sanitize None params during pruning (#6836),,0
523,Option to skip cuda rng in isolate_rng utility function (#16423),0.62172294,- Added an argument `include_cuda` in `pytorch_lightning.utilities.seed.isolate_rng` to disable managing `torch.cuda`'s rng ([#16423](https://github.com/Lightning-AI/lightning/pull/16423)),,0
524,Feature/log computational graph (#3003),0.7865962,Auto log the computational graph for loggers that support this (#3003),,1
525,allow loss to be used for early stopping (#127),0.6211789,Deprecate early_stop_callback Trainer argument (#3845),,0
526,Update data docs (#11042),0.6431495,Updated governance docs,,0
527,Move logic to error out on deprecation warnings into conftest (#14902),0.67491233,Deprecation warning (#3844),,0
528,update PR template (#4210),0.59005684,Use correct python version in lightning component template (#13790),,0
529,Update docs/source/new-project.rst (#3272),0.6002016,Update the Lightning App docs (#13537),,0
530,Fixes a potential bug on pip install (#7256),0.63287747,pip install rich,,0
531,Fix logs overwriting issue for remote fs (#7889),0.61911005,Removed LoggerStages (#5673),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
532,[App] Resolve Research Studio Bugs (#15313),0.61352026,"Resolved LightningApp(..., debug=True) (#14464)",,0
533,[App] Introduce Commands (#13602),0.7707325,Introducing CLI commands for apps (#13602)!,  update   update   update   update    Co-authored-by: thomas thomas@thomass-MacBook-Pro.local,1
534,removed pdb,0.707952,Removed,,1
535,CI: run CLI after install (#14659),0.63281846,Introducing CLI commands for apps (#13602)!,,0
536,update an outdated error message in DDPPlugin (#9005),0.6246723,Silenced some warnings. verified ddp refactors (#3483),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
537,Allow any input in to_onnx and to_torchscript (#4378),0.9999999,Allow any input in to_onnx and to_torchscript (#4378),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
538,Do not warn about the scheduler's interval key during manual optim (#16308),0.6423876,Do not warn when the name key is used in the lr_scheduler dict (#5057),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
539,fix back-compatibility for Accel (#6655),0.5586097,Porting fixes to autoscaler component (#16249),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
540,Update README (#17082),0.53910255,Updated Multinode Warning (#16091),,0
541,Update arg_parse.py,0.67080224,args = parser.parse_args(),,0
542,Remove DataLoader serialization (under fault tolerance) (#16533),0.64498436,Support **DictConfig for hparam serialization (#2519),Co-authored-by: Nikhil Shenoy nikhilshenoy@dhcp-128-189-225-225.ubcsecure.wireless.ubc.ca Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
543,ref: decouple apex second attemp part 3/n (#4055),0.627738,remove _evaluate fx (#3197),  copyright Lightning AI team   more... ,0
544,fix syntax issue (#1900),0.59298605,Fixing critical bugs in newly added hooks and hparams assignment.,Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
545,Fix typo in PR titles generated by github-actions bot (#16003),0.47426134,- Fixed to avoid common hook warning if no hook is overridden ([#12131](https://github.com/PyTorchLightning/pytorch-lightning/pull/12131)),,0
546,Update content for S3 persistent storage (#14060),0.5369053,Periodically sync database to the drive (#15441),  update   update   update   update   update   update   update   update   update   update   update   update   update   update    Co-authored-by: thomas thomas@thomass-MacBook-Pro.local,0
547,GANs in pl-examples updated for lightning-0.9 (#3152),0.70554215,Deprecated LightningModule.grad_norm in favor of pytorch_lightning.utilities.grads.grad_norm (#7292),,1
548,(app) Documentation fix for Work resources (#14182),0.57664704,Resolved a bug where the work statuses will grow quickly and be duplicated (#13970),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
549,Update setup logic in training type plugins (sharded) [4 / 4] (#10028),0.76963377,The accelerator and training type plugin setup hooks no longer have a model argument (#8536),,1
550,Merge pull request #24 from williamFalcon/keys,0.48885792,"merge backends (#3476, #3477, #3478, #3480, #3482)",,0
551,fix mypy typing errors in pytorch_lightning/strategies/horovod.py (#13570),0.6863266,- Fixed an issue where `HorovodStrategy.teardown()` did not complete gracefully if an exception was thrown during callback setup [#11752](https://github.com/PyTorchLightning/pytorch-lightning/pull/11752),Co-authored-by: Ethan Harris ethanwharris@gmail.com Co-authored-by: thomas thomas@thomass-MacBook-Pro.local Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
552,Fix typo in _validate_scheduler_optimizer() (#9886),0.7126074,Improved error messages for invalid configure_optimizers returns (#3587),Co-authored-by: thomas thomas@thomass-MacBook-Pro.local,1
553,Prune DeprecatedTrainerAttributes (#9598),0.6003783,Pruned deprecated classif. metrics from pytorch_lightning.metrics.functional.classification (#7499),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
554,Drop PyTorch 1.7 support (#12432),0.9661699,Drop PyTorch 1.9 support (#15347),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
555,Bump pytest from 7.2.2 to 7.3.1 in /requirements (#17588),0.72231746,Skipped testing with PyTorch 1.7 and Python 3.9 on Ubuntu (#11217),,1
556,Move max_batches definition to the Loops (#16820),0.6487731,expand eval loop out (#3165),"  update   update   update   update   update   update   update   update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   update   update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   update    Co-authored-by: thomas thomas@thomass-MacBook-Pro.local Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
557,Bump actions/setup-node from 2 to 3 (#14286),0.56049114,The MultiNode components now warn the user when running with num_nodes > 1 locally (#15806),,0
558,updated gitignore,0.5494852,"GitHubComponent(api_token=os.environ[""API_TOKEN""])",,0
559,Fix module dict in base finetuning (#8170),0.6742961,Resolve bug with Finetuning (#5744),,0
560,docs: update links to 1.6 1.5 1.4 (#17181),0.58222884,Docs improvements,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
561,Remove return value from the backward closure (#9770),0.6353757,Moved result teardown to the loops (#8245),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
562,Update incorrect entries in changelog (#11501),0.6949789,Complete changelog,Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
563,Reuse _TORCHVISION_AVAILABLE (#6976),0.7185857,Removed dependency on torchvision (#797),,1
564,Added missing docs on[ TrainResult and EvalResult source docs. (#3157),0.6762199,Removed support for EvalResult and TrainResult (#3968),,0
565,Allow user to disable the automatic formatting of checkpoint file names. (#6277),0.7034987,The tuner now usees a unique filename to save a temporary checkpoint (#9682),,1
566,Cast on host instead of IPU when using precision=16  (#13880),0.7364293,"- When training with `precision=16` on IPU, the cast has been moved off the IPU onto the host, making the copies from host to IPU cheaper ([#13880](https://github.com/Lightning-AI/lightning/pull/13880))",,1
567,Do not add return dict items to callback_metrics (#6682),0.6702434,Metric compute() method will no longer automatically call reset() (#5409),,0
568,"Update numpy requirement from <1.22.5,>=1.17.2 to >=1.17.2,<1.23.1 in /requirements (#13413)",0.6099649,"- num_devices = max(1, trainer.num_gpus, trainer.num_processes)",Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
569,Rename some doc files (#12163),0.57810163,Docs improvements,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz,0
570,add about log_dict info in docs (#12552),0.6350716,Minor improvements to apply_to_collection and type signature of log_dict (#7851),Co-authored-by: Luca Antiga luca.antiga@gmail.com Co-authored-by: thomas thomas@thomass-MacBook-Pro.local Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
571,Add Peter Yu (yukw777) to core maintainers (#2690),0.49175468,Renamed utils modules (#5199),,0
572,Avoid entry_points deprecation warning (#14052),0.7602711,Avoid metadata.entry_points deprecation warning on Python 3.10 (#14052),,1
573,another try to filter master from CircleCI jobs (#2734),0.5051159,Removed ProfilerConnector (#7654),,0
574,Remove deprecated precision plugin checkpoint hooks (#14833),0.82570565,- Removed the deprecated precision plugin checkpoint hooks `PrecisionPlugin.on_load_checkpoint` and `PrecisionPlugin.on_save_checkpoint` ([#14833](https://github.com/Lightning-AI/lightning/pull/14833)),,1
575,ref: enable custom clusters (1/n) (#4048),0.92790926,Enabled custom clusters (#4048),,1
576,Merge branch 'better_simple_profiler' of https://github.com/PyTorchLightning/pytorch-lightning into better_simple_profiler,0.7943661,- Deprecated `pytorch_lightning.profiler.BaseProfiler` in favor of `pytorch_lightning.profiler.Profiler` ([#12150](https://github.com/PyTorchLightning/pytorch-lightning/pull/12150)),,1
577,Removed deprecated trainer_lightning_optimizers (#14889),0.8453794,- Removed the deprecated `Trainer.lightning_optimizers` ([#14889](https://github.com/Lightning-AI/lightning/pull/14889)),,1
578,Fix inconsistent outputs in on_*_end and *_end (#6969),0.62764907,"Removed output argument from *_batch_end hooks (#3965, #3966)","  Drop support for PyTorch 1.10   CHANGELOG   READMEs   mypy   ls   New poplar version   Fixed tests   links   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   skip azure badges   Table   Matching dockerfiles   Drop unnecessary channels and packages   Push nightly   Undo unrelated changes   Revert ""Push nightly""   This reverts commit 9618f737c4dc65331fef4bb11fe46a61513d220a.  Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
579,Document exceptions in loggers (#6171),0.6904988,- pickling errors with loggers (txs @awaelchli),,0
580,Update Contributing Guide (#6118),0.6553796,Contributors,,0
581,[bug-fix] DDP and automatic_optimization=False (#4485),0.7344327,DDP Debugging Improvements,,1
582,Bugfix: accumulation and suggestion for learning rate finder (#1801),0.80227846,Learning Rate Finder (#13802),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
583,[hotfix] Extend Optimizer + update doc (#5095),0.6935003,Extend LightningOptimizer to exposure underlying Optimizer attributes + update doc (#5095),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
584,Fix DP Logging Aggregation (#4138),0.7391025,DDP + loggers should be fixed,,1
585,release v0.2.6,0.76371825,0.4.0,,1
586,"Revert ""Revert ""join coverage (#2460)"" (#2499)"" (#2500)",0.63558984,[1.3.0] - 2021-05-06,,0
587,bump version 1.3 to 1.4 in README (#8582),0.5439999,"    version=""0.0.1"",",,0
588,Profile LightningDataModule hooks (#12971),0.8437472,- Added profiling of `LightningDataModule` hooks ([#12971](https://github.com/Lightning-AI/lightning/pull/12971)),,1
589,ref: all logging related calls in a connector 1/n (#3395),0.9537669,all logging related calls in a connector (#3395),,1
590,ignore tests file,0.655033,- fixed all the .test() calls,,0
591,CI: trigger PL for lite (#15402),0.5467607,- Fixed issue where the CLI fails with certain torch objects ([#13153](https://github.com/Lightning-AI/lightning/pull/13153)),Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
592,fix tb hparams logging (#2974),0.6970747,Removed LoggerStages (#5673),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
593,ref: decouple apex second attemp part 6/n (#4060),0.62006783,"Decoupled Appex (#4052, #4054, #4055, #4056, #4058, #4060, #4061, #4062, #4063, #4064, #4065)",,0
594,[doc] Update Dict Train Loader doc.  (#6579),0.6365473,Removed deprecated TrainResult (#5323),Co-authored-by: Jirka jirka.borovec@seznam.cz,0
595,cleaning (#2030),0.8588074,"Cleaning (#5948, #5949, #5950)",,1
596,Update DeepSpeed precision handling after moving PrecisionPlugin (#10657),0.8715331,Updated precision attributes in DeepSpeedPlugin (#10164),ci: replace SD with Flashy,1
597,Docs on sanity_checking property (#17579),0.6941143, - Sanity checking: `Trainer(num_sanity_val_steps>0)`,,0
598,1/n Move Accelerator into strategy - move batch_to_device to strategy (#10649),0.71504277,"The Accelerator and PrecisionPlugin have moved into Strategy. All strategies now take an optional parameter accelerator and precision_plugin (#11022, #10570).",,1
599,changed to absolute imports and added docs (#881),0.61737317,import argparse,,0
600,Fix retrieval of batch indices when dataloader num_workers > 0 (#10870),0.6388812,"    num_workers=10,",,0
601,Remove deprecated master_params attributes in PrecisionPlugin (#10372),0.83054113,Deprecated PrecisionPlugin.master_params() in favor of PrecisionPlugin.main_params() (#10105),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,1
602,Disable CPU Offload as default for DeepSpeed (#6262),0.90054613,"Changed default for DeepSpeed CPU Offload to False, due to prohibitively slow speeds at smaller scale (#6262)",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
603,Some minor CI cleanup (#10088),0.6941521,some minor cleaning,,0
604,Replaces ddp .spawn with subprocess (#2029),0.7053009,Merged the implementation of DDPSpawnStrategy into DDPStrategy and removed DDPSpawnStrategy (#14952),,1
605,Lightning Lite core and tests (#10175),0.7749456,LightningLite:,,1
606,Add ddp_*_find_unused_parameters_false to Plugins Registry. (#8483),0.7916453,Changed the default of find_unused_parameters to False in DDP (#5185),,1
607,ci: fix wheels' cashing (#16869),0.51876473,Resuming from checkpoints (#16167),,0
608,Update has_len_all_ranks to use Strategy.root_device (#12144),0.60760117,Renamed Strategy.reduce to Strategy.all_reduce in all strategies (#16370),Co-authored-by: awaelchli aedu.waelchli@gmail.com,0
609,move accelerator legacy tests (#5948),0.7756692,move specific accelerator code (#3457),,1
610,Compose RunIf utilities (#17520),0.6255959,Expose RunWorkExecutor to the work and provides default ones for the MultiNode Component (#15561),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
611,Cleaned up val/tng/test nb batches (#163),0.63588834,"nb_test_batches to num_test_batches,",,0
612,Remove tbptt self.log flags and other dead code [5/n] (#7644),0.6810398,Removed logger_connector legacy code (#6733),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
613,Pid port + duplicate rank_zero logging (#2231),0.65851873,"Defines shared proc. rank, remove rank from instances (e.g. loggers) (#1408)",,0
614,TPUSpawn + IterableDataset error message (#6875),0.6397867,Disabled sampler replacement when using IterableDataset (#11507),,0
615,Prune metrics base classes 2/n (#6530),0.68156457,"Base classes (#1326, #1877)",,0
616,Update environment variable for cache reset for consistency (#12455),0.53222626,Refactor cloud dispatch and update to new API (#16456),,0
617,Remove the deprecated code in pl.utilities.cloud_io (#16438),0.6982143,Removed deprecated API (#2073),,0
618,Update changelog for v1.2.2 (#6325),0.7422054,Complete changelog,,1
619,Merge pull request #12766 from PyTorchLightning/docs/slack,0.6576087,- Removed `configure_sync_batchnorm` from `ParallelStrategy` and all other strategies that inherit from it ([#11754](https://github.com/PyTorchLightning/pytorch-lightning/pull/11754)),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
620,document exceptions for metrics/regression (#6202),0.68061393,Regression metrics (#2221),,0
621,Use debug instead of detail logging for per-iteration hooks (#12281),0.612691,"Our logging mechanism previously supported log(""key"", {""something"": 123}) (not using log_dict). However, this added significant complexity to the implementation with little benefit, as these keys could not be monitored by our Callbacks and most logger implementations do not support this notation. If you were using this feature with a compatible logger, you can still publish data directly to the Logger using self.logger.log_metrics().",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
622,revert,0.6675349,Refactoring,,0
623,release v0.112,0.76504946,"    version=""0.0.1"",",,1
624,Added check for apex AMP and unit tests for Horovod + AMP (#3404),0.6717625,training AMP scaling refactor (#3135),,0
625,ci: simplify cashing (#16599),0.5007875,Simplify optimization Logic (#4984),Co-authored-by: awaelchli aedu.waelchli@gmail.com Co-authored-by: Luca Antiga luca@lightning.ai,0
626,Update Colossal AI docs and integration (#16778),0.7290802,Colossal-AI,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,1
627,Optimization docs (#6907),0.8092886,Working with multiple optimizers (#16539),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Fixes https://github.com/Lightning-AI/lightning/issues/16486,1
628,Mark Trainer.terminate_on_nan protected and deprecate public property (#9849),0.7654592,Deprecated Trainer.terminate_on_nan public attribute access (#9849),,1
629,Allow sys.argv and args in LightningCLI (#16808),1.0,Allow sys.argv and args in LightningCLI (#16808),,1
630,Set progressbar refresh rate in Google Colab (#5516),0.77422494,Changed the default value for the progress_bar_refresh_rate Trainer argument in Google COLAB notebooks to 20 (#5516),Remove docs about the experimental automatic fault tolerance,1
631,refactor imports of logger dependencies (#4860),0.8207232,Re-Enable Logger's ImportErrors (#1938),,1
632,release v0.1.dev16,0.6764744,"    version=""0.0.1"",",,0
633,add how to contribute (#8129),0.6220613,Contributors,,0
634,[FEAT]  logging refactors 1/n (#4439),0.8584218,Refactored logging,,1
635,Remove the deprecated LightningDeepSpeedModule (#16041),0.9900499,Removed the deprecated LightningDeepSpeedModule (#16041),,1
636,Re-Enable Import Errors (#1938),0.79479086,Re-Enable Logger's ImportErrors (#1938),,1
637,refactor reading env defaults (#6510),0.5636045,  * `env_prefix`,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
638,fix python syntax in code blocks to be consistent (#166),0.5392765,$ PL_FAULT_TOLERANT_TRAINING=MANUAL python script.py,Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
639,resolve conflits,0.42001718,This conversation was marked as resolved by carmocca,Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz,0
640,"Update correct var name, from step to stage (#9055)",0.59644675,Renames model steps (#1051),Added codebuild permissions to IAM policy JSON,0
641,Remove Vulture (#8381),0.60168993,Removed deprecated: (#2760),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
642,Replace obsolete _FakeQueue in multiprocessing launcher (#16873),0.5884611,"Redesigned multi-dataloader support (#16743, #16784, #16939)",  Remove old platform docs   More   More ,0
643,fix appveyor (#69),0.5930993,Resolved a bug where the install command was not installing the latest version of an app/component by default (#14181),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
644,update min req (#1934),0.58051103,Refactor Model backward (#2276),,0
645,Move grad_norm to a dedicated utilities file  (#7292),0.62477374,from lightning.pytorch.utilities import grad_norm,,0
646,[CLI] add support for cluster management (#13835),0.79328144,Enabled custom clusters (#4048),Co-authored-by: thomas thomas@thomass-MacBook-Pro.local Co-authored-by: Your Name you@example.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
647,docs (#4003),0.7077583,"docs for all Metrics (#2184, #2209)",update copy write in PL & Fabric,1
648,fixes typing in stochastic_weight_avg.py (follow-up of #13685) (#13860),0.6398455,Deprecated passing stochastic_weight_avg to the Trainer constructor in favor of adding the StochasticWeightAveraging callback directly to the list of callbacks (#8989),,0
649,Delete deprecated save function (#8680),0.7719915,Removed the deprecated save_function property in ModelCheckpoint (#8680),,1
650,Separate the concept of a Drive from that of a Mount (#15120),0.44100586,Decoupled device parsing logic from Accelerator connector to Trainer (#8180),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
651,Fix problems in trainer docs (#17561),0.6745176,trainer = pl.Trainer(plugins=SLURMEnvironment(auto_requeue=False)),,0
652,Add LSFEnvironment to API reference (#8423),0.5393728,Updated references to self.forward() to instead use the __call__ interface. (#1211),,0
653,Remove unused utility function _parse_devices (#14386),0.63631296,  * Deprecated the `pytorch_lightning.utilities.device_parser.parse_gpu_ids` in favor of `lightning_lite.utilities.device_parser.parse_gpu_ids`,,0
654,Update remaining tests in test_accelerator_connector in preparation for #11040 (#12466),0.71786743,"accelerator connector methods x/n (#3469, #3470, #3474)",,1
655,Minor typo (#1987),0.5023743,Lite,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
656,Remove legacy examples from logging docs (#14686),0.7433648,Removed LoggerStages (#5673),,1
657,update xla version (#6464),0.49555454,0.4.0,Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: thomas thomas@thomass-MacBook-Pro.local,0
658,Fix val_progress_bar total with num_sanity_val_steps (#3751),0.6304226,Better progress bar (#16695),,0
659,Fix examples that uses type_as (#1129),0.55054194,Trainer.add_argparse_args classmethod fixed. Now it adds a type for the arguments (#1147),Co-authored-by: thomas thomas@thomass-MacBook-Pro.local,0
660,Fix DataParallel typo (#2154),0.6549958,Replaced _DataModuleWrapper with __new__ (#7289),Co-authored-by: thomas thomas@thomass-MacBook-Pro.local,0
661,Custom Plugin is_distributed (#6537),0.68821704,Enabled plugins (#4041),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
662,Validate that state-key is unique when using multiple callbacks of the same type (#15634),0.6445044,The Trainer now raises an error if it is given multiple stateful callbacks of the same time with colliding state keys (#15634),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
663,ignore tests in DeepSource analyses (#8151),0.5616566,Removed callback metrics from test results obj (#2994),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
664,Update debugging.py,0.69192934,DDP Debugging Improvements,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
665,Fix PyTorch MPS test failure in master (#17405),0.7018379,Skipped testing with PyTorch 1.7 and Python 3.9 on Ubuntu (#11217),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
666,Add typing for utilities/enums.py (#11298),0.63688266,Deprecates the pytorch_lightning.utilities.enums.AMPType enum,,0
667,Add LSF support (#5102),0.6558267,BFloat16 Support,Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
668,add QA to docs (#1374),0.5112821,Docs improvements,Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
669,Update logic to make sure logged_metrics always contain tensors (#11270),0.6492238,"- `Trainer.logged_metrics` now always contains scalar tensors, even when a Python scalar was logged ([#11270](https://github.com/PyTorchLightning/pytorch-lightning/pull/11270))",Co-authored-by: awaelchli aedu.waelchli@gmail.com,0
670,CI: Azure CPU pool (#14367),0.59965575,CPU stats monitoring,,0
671,Prepare CI to run on 3090s (#14910),0.6354636,Implemented ready for components (#16129),,0
672,Reset current_fx properties on lightning module in teardown (#7247),1.0,Reset current_fx properties on lightning module in teardown (#7247),Resolves https://github.com/Lightning-AI/lightning/issues/16411,1
673,Update Changelog for v1.2.4 (#6581),0.75487536,Complete changelog,,1
674,removed exception crashing from val,0.6174051,Removed auto val reduce (#2462),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
675,Set fixed seed for pytest execution order (#17614),0.58774173,"Resolved schedule step bug for PyTorch Profiler (#6674, #6681)",  Loop flattening: remove the default .run() implementation   None return   mypy   Loop flattening: reduce base interface   Fix   DOcs   Bad merge   Fix   Fix ,0
676,Solved minor bug with MLFlow logger (#16418),0.6996393,"    loggers=[WandbLogger(...), MLFlowLogger(...)]",,0
677,remove unused folder (#4211),0.6834023,Prevent to cd into non-existent folders (#16645),,0
678,Merge pull request #8497 from PyTorchLightning/v1.4.0rc1,0.68131065,- Removed `configure_sync_batchnorm` from `ParallelStrategy` and all other strategies that inherit from it ([#11754](https://github.com/PyTorchLightning/pytorch-lightning/pull/11754)),,0
679,refactored training_batch + tests to verify correctness (#2328),0.99372953,"Refactored training_batch + tests to verify correctness (#2327, #2328)",,1
680,auto add reviewrs via Mergify (#1257),0.57235336,Add support for listing Lightning AI apps (#13987),,0
681,Fix mypy errors attributed to pytorch_lightning.trainer.connectors.data_connector.py (#13806),0.7729721,Removed pytorch_lightning.trainer.connectors.OptimizerConnector (#10120),,1
682,Remove redundant fit call from accelerator connector test (#10626),0.73092926,Removed deprecated connect_precision_plugin and connect_training_type_plugin from Accelerator (#9019),,1
683,Do not require omegaconf to run tests (#10832),0.5147238,"validation_step, val_dataloader are now optional.   ",Remove the collaborative strategy,0
684,[tiny] Fix training_dataloader usage to be train_dataloader instead. (#2521),0.8319956,train_dataset = trainer.train_dataloader.loaders[0].dataset,,1
685,Better error mesage and type checking for gpus arg and devices arg in Trainer (#12530),0.7276541,Improved Argparse usability with Trainer,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
686,[refactor results 1] - refactor backward (#2276),0.8793888,Refactor Model backward (#2276),,1
687,add tests for Trainer attributes (#5261),0.8303633,trainer.test(),,1
688,Update LR schedulers only when their corresponding Optimizer is being… (#4868),0.76839054,Disabled lr_scheduler.step() in manual optimization  (#6825),,1
689,[feat] pp 2/n (#5026),0.6822165,[1.2.1] - 2021-02-23,,0
690,increase Parity threshold (#4795),0.57950604,0.5.1,,0
691,update fsspec to 2021.06.0 (#7869),0.7065358,Changed fsspec to tuner (#4458),,1
692,Remove the deprecated code in pl.utilities.apply_func (#16413),0.7510145,- Removed the deprecated code in:,,1
693,Deprecate TrainerProperties Mixin and move property definitions directly into trainer.py (#9495),0.8991891,Removed TrainerProperties mixin and moved property definitions directly into Trainer (#9495),,1
694,Brew update to fix mac tests (#6970),0.58863187,Updated app testing (#16000),,0
695,Fixed reference,0.5903282,model reference not provided,,0
696,added instructions to test,0.62854403,"nb_test_batches to num_test_batches,",,0
697,Remove useless pass and abc (#11522),0.54256636,Remove beta arg from F1 class and functional (#5076),,0
698,Update beautifulsoup4 requirement from <=4.8.2 to <4.11.2 in /requirements (#15745),0.5497395,0.4.0,,0
699,Prune metric: helpers and inputs 3/n (#6547),0.56756246,Allow logging of metrics together with hparams (#1630),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
700,Logger support in Lite (#16121),0.7508024,for logger in loggers:,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
701,Ignore leaked XLA environment variables (#16582),0.51686627,Renamed xxx_AVAILABLE as protected (#5082),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
702,running new CE then DDT,0.6347929,DDP Debugging Improvements,,0
703,Remove _NATIVE_AMP_AVAILABLE checks (#9747),0.6123941,enable_pl_optimizer=False by default to temporarily fix AMP issues (#5163),,0
704,Bugfix/4156 filter hparams for yaml - fsspec (#4158),0.5909289,Changed fsspec to tuner (#4458),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
705,Add 1.1.4 section to CHANGELOG (#5378),0.71714175,Full Changelog,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
706,Simplify root node resolution for SLURM environment (#14912),0.70213914,    root_node = os.environ['SLURM_NODELIST'].split(' ')[0],Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
707,clarify forward (#3609),0.6544666,[1.3.5] - 2021-06-08,,0
708,split trainer mixins (#209),0.64255923,Trainer.fit hook clean up (#3198),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Fixes https://github.com/Lightning-AI/lightning/pull/16149#discussion_r1054271661,0
709,Support gradient accumulation using Horovod's backward_passes_per_step (#11911),0.8046839,- Enable gradient accumulation using Horovod's `backward_passes_per_step` ([#11911](https://github.com/PyTorchLightning/pytorch-lightning/pull/11911)),,1
710,Disable batch transfer in DP mode (#6098),0.9539759,Disabled batch transfer in DP mode (#6098),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
711,Load checkpoint from Bytes (#4314),0.65752625,Refactor load in checkpoint connector (#4593),"  [WIP] Fix protobuf incompatibility blocking CI   1.0.0 bad, try 1.9.0   1.9.0 bad, try 1.16.0   1.16.0 good, try 1.13.0 ",0
712,Remove on epoch guard from the should stop validation check (#7701),0.87242866,"Removed the on_epoch guard from the ""should stop"" validation check (#7701)",Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
713,Fix typo in checkgroup.yml (#14959),0.5331112,Replace MisconfigurationException with warning in ModelCheckpoint Callback (#4560),,0
714,Move sync code from step result to lightning module [6/n] (#7651),0.72885156,LightningModule.lr_scheduler_step,,1
715,fix display bug (#7395),0.6517134,bug fix with logging val epoch end + monitor (#3812),Co-authored-by: awaelchli aedu.waelchli@gmail.com,0
716,"Revert ""Add support for len(datamodule) (#9895)"" (#10072)",0.6601299,"Redesigned multi-dataloader support (#16743, #16784, #16939)",Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz,0
717,Remove the deprecated LightningModule.summarize (#12559),0.87328666,Removed the deprecated LightningDeepSpeedModule (#16041),Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,1
718,Add load_from_checkpoint function to Loading docs (#4196),0.6689595,Called on_load_checkpoint before loading state_dict (#4057),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
719,fix collecting training_step outputs (#8613),0.7199638,Move training_output validation to after train_step_end (#7868),,1
720,Update CHANGELOG after the 1.6.5 release (#13641),0.6893331,Here is a selection of important changes that are not backward compatible with versions < 1.5. The full list of changes and removals are listed in the changelog at the bottom.,,0
721,update tags in bug issue (#17551),0.6772672,Updated mlflow with using resolve_tags (#6746),Co-authored-by: Jirka jirka.borovec@seznam.cz,0
722,Use the pull_request_target workflow event (#14603),0.65226746,- Added a try / catch mechanism around request processing to avoid killing the flow ([#15187](https://github.com/Lightning-AI/lightning/pull/15187),,0
723,Update link to image in Fabric PPO example (#17360),0.58631384,    pil_image = T.ToPILImage()(image),,0
724,Add .actions CODEOWNERS (#15602),0.5697171,"Accessing dataloaders (#16726, #16800)",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
725,Remove the deprecated pl.callbacks.ProgressBar (#12658),0.8086165,Removed deprecated callbacks (#3979),,1
726,Pin Docker image for testing on GPUs (#12368),0.6457508,GPU training (#2704),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
727,wandb logger 'global_step' affects other logger (#1492),0.681288,Removed wandb logger's finalize method (#1193),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
728,"extract training teardown into method, catch KeyboardInterrupt (#856)",0.6962892,"Support graceful training cleanup after Keyboard Interrupt (#856, #1019)",,0
729,Add ManualOptimization loop (#9266),0.72263336,Loop customization improvements,Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,1
730,[bugfix] Always return batch indices to prevent duplicated logic for the users (#9432),0.59081304,set validation to a fix number of batches,,0
731,Replace .get_model() with explicit .lightning_module (#6035),0.9195528,Deprecated .get_model() with explicit .lightning_module property (#6035),  bump version 1.9.0   17th   drop lite ,1
732,Clarify lr scheduler frequency (#9843),0.6677996,    --lr_scheduler=OneCycleLR \,,0
733,Switch v1.10 deprecation references (#16321),0.6100314,Deprecation warning (#3844),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
734,add promxial policy optimization template to pl_examples  (#5394),0.6315297,        :param optimizers:,,0
735,ref: inner train loop (intermediate step) 10/n (#3369),0.8000965,"inner train loop (#3359, #3361, #3362, #3363, #3365, #3366, #3367, #3368, #3369, #3370, #3371, #3372, #3373, #3374, #3375, #3376, #3385, #3388, #3397)", Bump playwright from 1.28.0 to 1.29.1 in /requirements  Bumps playwright from 1.28.0 to 1.29.1. - Release notes - Commits  updated-dependencies: - dependency-name: playwright   dependency-type: direct:production   update-type: version-update:semver-minor ... Signed-off-by: dependabot[bot] support@github.com   docker   note   Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz,1
736,Avoid fallback on CPU if no devices are provided (#12410),0.85787785,No longer fallback to CPU with no devices,,1
737,[pre-commit.ci] pre-commit suggestions (#17271),0.5271052,The preemption/termination signal is now configurable (#14626):,Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
738,added warning for None dataloader (#1745),0.7113602,Changed warnings and recommendations for dataloaders in ddp_spawn (#6762),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,1
739,Remove numpy from src/lightning/pytorch and use torch only (#17278),0.7117318,Disable torch.inference_mode with torch.compile in PyTorch 2.0 (#17215),Bumps pytest-rerunfailures from 10.2 to 10.3. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: pytest-rerunfailures   dependency-type: direct:production   update-type: version-update:semver-minor ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,1
740,CI: drop code formatters (#5971),0.5519146,Add code_dir argument to tracer run (#15771),,0
741,extend release testing (#4506),0.57448065,Updated app testing (#16000),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
742,"Reordered sections for intuitive browsing. (e.g. limit_train_batches was at the end of the page, far from limit_test/val_batches) (#5283)",0.53271633,Truncated long version numbers in progress bar (#2594),  tensorboardX > extra   default   chlog   doctest_skip   mypy   Update docs   plus   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Luca Antiga luca.antiga@gmail.com   fix   mypy   docs   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   .   ll   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Luca Antiga luca.antiga@gmail.com,0
743,"Expand the use cases, move them up for discoverability (#8692)",0.6250555,The main focus of this release was on adding flexibility and generalization to support broad research cases.,,0
744,docs fixes (#4080),0.63946533,Docs improvements,Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
745,Bump actions/cache from 2 to 3 (#13955),0.53950715,Update the logic to check for accumulation steps with deepspeed (#9826),,0
746,Fix Wrapping optimizers upon assignment (#6006),0.7605481,Refactored optimizer (#4658),,1
747,flake8 + yapf,0.46495944,"| ""bf16-mixed""                      | ""bf16""     |",,0
748,Don't import torch_xla.debug for torch-xla<1.8 (#10836),0.7163812,import torch,Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,1
749,Add typing for LoggerConnector [2/3] (#9270),0.782799,Dramatically simplify the LoggerConnector (#7882),,1
750,Allow setting the SLURMEnvironment.main_address via an env variable (#17596),0.5584826,  * `env_prefix`,,0
751,Rename leftover definitions in Lite tests (#16309),0.5134077,Deprecated the TestTubeLogger (#9065),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
752,Future 4/n: test & legacy in test/ folder (#13295),0.5066645,"nb_test_batches to num_test_batches,",,0
753,CI: Define reusable workflow - check schema (#13562),0.6286129,refactored dataloader process hook (#3139),Co-authored-by: Akihiro Nitta akihiro@lightning.ai,0
754,deprecate passing ModelCheckpoint instance to Trainer(checkpoint_callback=...) (#4336),0.93780184,Deprecated passing ModelCheckpoint instance to checkpoint_callback Trainer argument (#4336),,1
755,changelog (#1616),0.82622063,Complete changelog,,1
756,CI: fix upload-artifact (#11962),0.63267124,Enabled cp (upload) at project level (#16631),,0
757,Update Plugins doc (#12440),0.6824203,Enabled plugins (#4041),,0
758,Move logger utilities with the loggers (#16680),0.8108774,moved eval loop logging to loggers (#3408),Co-authored-by: awaelchli aedu.waelchli@gmail.com,1
759,"Update fsspec[http] requirement from <2023.2.0,>2021.06.0 to >2021.06.0,<2023.5.0 in /requirements (#17536)",0.5592505,Updated model_checkpoint's to_yaml to use fsspec open (#3801),,0
760,Add XLAEnvironment plugin (#11330),0.6776606,- Added `XLAEnvironment` cluster environment plugin ([#11330](https://github.com/Lightning-AI/lightning/pull/11330)),,0
761,update logger init (#727),0.6526796,Dropped logging config in package init (#1015),Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
762,Fix mypy typing errors in optimizer loop (#9317),0.6499549,Improved error messages for invalid configure_optimizers returns (#3587),,0
763,Remove horovod (#16150),0.7778597,"refactored Horovod backend (#3121, #3122)",,1
764,release (#2414),0.7994976,This release includes:,,1
765,Faster Accuracy metric (#2775),0.6862345,Renaming of precision recall metric (#3308),Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Nikhil Shenoy nikhilshenoy@dhcp-128-189-225-81.ubcsecure.wireless.ubc.ca Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: awaelchli aedu.waelchli@gmail.com Fixes https://github.com/Lightning-AI/lightning/issues/16148,0
766,Deprecate and remove calls to agg_and_log_metrics (#11832),0.7093396,- Removed the deprecated `Logger.agg_and_log_metrics` hook in favour of `Logger.log_metrics` and the `agg_key_funcs` and `agg_default_func` arguments. ([#14840](https://github.com/Lightning-AI/lightning/pull/14840)),,1
767,Avoid deprecated progress_bar_refresh_rate usage (#10520),0.7993802,"Deprecated passing progress_bar_refresh_rate to the Trainer constructor in favor of adding the ProgressBar callback with refresh_rate directly to the list of callbacks, or passing enable_progress_bar=False to disable the progress bar (#9616)",,1
768,[App] Add support for private data (#16738),0.562152,"Accessing dataloaders (#16726, #16800)",,0
769,[docs][App] Include components in the API reference (#16414),0.617688,Implemented ready for components (#16129),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
770,Tests/docker (#1573),0.6394072,Enabling val/test loop disabling (#2692),,0
771,verified epoch logging (#3830),0.7553084,epoch can now log independently (#3843),,1
772,Allow changing the logged step value in validation_step (#4130),1.0000002,Allow changing the logged step value in validation_step (#4130),,1
773,Resolve schedule step bug for PyTorch Profiler (#6674),0.98720545,"Resolved schedule step bug for PyTorch Profiler (#6674, #6681)",,1
774,Update Custom Callback Docs (#17161),0.576165,  callbacks:,"  organize   organize   organize   organize   Fix   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   accelerator   distributed launch   notebooks   code structure   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   lightning_module   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   x   update   conflicts   fix duplicates   links.rst   api folder   add todo for build errors   resolve duplicate reference warnings   address review by eden   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
775,Metric aggregation testing (#3517),0.7058724,Regression metrics (#2221),,1
776,Removes timeout from streamlit e2e test (#14667),0.5806067,Deprecated the TestTubeLogger (#9065),Co-authored-by: awaelchli aedu.waelchli@gmail.com,0
777,Add forgotten colon (#8076),0.4784035,Replaced _DataModuleWrapper with __new__ (#7289),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
778,move torchtext as optional (#2395),0.68589616,Disable torch.inference_mode with torch.compile in PyTorch 2.0 (#17215),,0
779,Use monkeypatch.chdir instead of os.chdir in tests (#15579),0.5341363,"trainer.test(ckpt_path=""my_path"") # load path (NEW BEHAVIOR!)",,0
780,[Fix] Ensure we set the default device before initializing deepspeed (#6460),0.7193699,"Changed default for DeepSpeed CPU Offload to False, due to prohibitively slow speeds at smaller scale (#6262)",,1
781,remove support for optimizer_idx in the training_step for manual optimization (#8576),0.94424194,Removed the deprecated optimizer_idx from training_step as an accepted argument in manual optimization (#8576),,1
782,Callback collection through entry points (#12739),0.798482,Callback registration through entry points,,1
783,Update single_cpu_template.py,0.5901816,The psutil package is now required for CPU monitoring (#17010),,0
784,add Drone CI (#1115),0.50046945,adding Trainer.tune() (#3293),"  rename integrations   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   name   Artifact   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
785,Remove call to deprecated fit_loop (#8873),0.6616578,Removed deprecated early_stop_callback (#3982),Co-authored-by: Nikhil Shenoy nikhilshenoy@dhcp-128-189-229-137.ubcsecure.wireless.ubc.ca Co-authored-by: Nikhil Shenoy nikhilshenoy@dhcp-128-189-227-69.ubcsecure.wireless.ubc.ca Co-authored-by: Nikhil Shenoy nikhilshenoy@dhcp-128-189-228-93.ubcsecure.wireless.ubc.ca Fixes https://github.com/Lightning-AI/lightning/issues/8107,0
786,Remove the deprecated AllGatherGrad class (#16360),0.6940051,Removed the deprecated TrainerLoggingMixin class (#8609),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
787,Remove write_predictions from LightningModule (#8850),0.89623195,Removed LightningModule.write_predictions and LightningModule.write_predictions_dict (#8850),,1
788,"Revert ""Remove unused param tpu_core_idx (#1948)"" (#1963)",0.6427976,Removed deprecated: (#2760),Co-authored-by: thinkin-machine you@example.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
789,align lit-utils version to post0 (#16593),0.562519,Set version as today (#13906),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
790,Fix callback default (horror bug!) (#1534),0.65692806,Avoid redundant callback restore warning while tuning (#13026),,0
791,[App] Add healthz endpoint to plugin server (#16882),0.6106282,Initial plugin server (#16523),,0
792,Fix wandb test writing artifacts to cwd (#15551),0.6036551,Deprecated the TestTubeLogger (#9065),  adding silent dependencies   versions   strict   freeze   client ,0
793,"Update args, kwargs doc for load_from_checkpoint() (#1839)",0.5537348,"def on_fit_start(self, *args, **kwargs):",,0
794,Remove the obsolete Strategy.dispatch method (#16618),0.6827136,- Removed `Strategy.dispatch` ([#16618](https://github.com/Lightning-AI/lightning/pull/16618)),rece condition fix when setting server to be free for next request,0
795,Deprecate DistributedType in favor of StrategyType (#10505),0.61931527,- Deprecated `DistributedType` in favor of `_StrategyType` ([#10505](https://github.com/PyTorchLightning/pytorch-lightning/pull/10505)),"  Fixes app CLI tests by checking for the dynamically assigned port.   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: Noha Alon nohaalon@Nohas-MacBook-Air.local Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
796,Clear dataloader references before attaching new dataloaders to Trainer (#8442),0.7678647,Removed trainer.reset_*_dataloader() methods (#16726),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
797,pep 8 (#2967),0.59198433,[1.1.8] - 2021-02-08,Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com,0
798,hotfix build docs + docker (#16351),0.561584,Fix frontend hosts when running with multi-process in the cloud (#17324),Fixes https://github.com/Lightning-AI/lightning/issues/16265,0
799,Set precision=16 when use_amp is passed as True (#1145),0.6621145,    precision=16,,0
800,reduce nb CI builds (#1078),0.5882469,Remove nan loss in manual optimization (#5121),,0
801,Replace 'step' with 'global_step' (#7244),0.75536793,- global_step += 1,  autoscaler update   cold-start-proxy-updates   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Ethan Harris ethanwharris@gmail.com,1
802,fix issue labeler (#17501),0.6231606,Resolve bug with Finetuning (#5744),,0
803,Update docs regarding deprecation window (#15089),0.64798844,Deprecation warning (#3844),,0
804,Move some tests to correct subfolder/file (#1312),0.52758646,move backends back to individual files (#3712),Co-authored-by: Nikhil Shenoy nikhilshenoy@dhcp-128-189-225-81.ubcsecure.wireless.ubc.ca Co-authored-by: Nikhil Shenoy nikhilshenoy@dhcp-128-189-227-53.ubcsecure.wireless.ubc.ca Co-authored-by: Nikhil Shenoy nikhilshenoy@dhcp-128-189-226-199.ubcsecure.wireless.ubc.ca Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: Nikhil Shenoy nikhilshenoy@dhcp-128-189-227-171.ubcsecure.wireless.ubc.ca Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Nikhil Shenoy nikhilshenoy@dhcp-128-189-230-32.ubcsecure.wireless.ubc.ca Co-authored-by: Nikhil Shenoy nikhilshenoy@dhcp-128-189-224-226.ubcsecure.wireless.ubc.ca Co-authored-by: Nikhil Shenoy nikhilshenoy@dhcp-128-189-229-137.ubcsecure.wireless.ubc.ca Fixes https://github.com/Lightning-AI/lightning/issues/8107,0
805,Fix type hint for filepath (#9434),0.66728246,Changed Checkpoint path parameter from filepath to dirpath (#1016),"  Selects random port for lightning app when running multiple apps locally   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Uses already created utility function instead of creating a new one   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   changelog update   reformatting   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   fix cli tests   fixing silly imports   remove port assignement from constants file and into the CLI dispatch process   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   lint errors   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   resolves lint error: unused import   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update tests   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   tests   tests   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   tests   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  update changelog  Co-authored-by: Noha Alon nohaalon@Nohas-MacBook-Air.local Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Jirka jirka.borovec@seznam.cz",0
806,2/n Move Accelerator into strategy - remove dispatch functions from Accelerator (#10885),0.6864097,"The Accelerator and PrecisionPlugin have moved into Strategy. All strategies now take an optional parameter accelerator and precision_plugin (#11022, #10570).",  release   Apply suggestions from code review   cleaning   Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz,0
807,Fix device parser logic to avoid creating CUDA context (#14319),0.5898725,"- Trainer queries the CUDA devices through NVML if available to avoid initializing CUDA before forking, which eliminates the need for the `PL_DISABLE_FORK` environment variable introduced in v1.7.4 ([#14631](https://github.com/Lightning-AI/lightning/pull/14631))",Co-authored-by: Nikhil Shenoy nikhilshenoy@dhcp-128-189-229-137.ubcsecure.wireless.ubc.ca,0
808,chlogs for 1.0 [skip ci] (#3978),0.5392455,Pinning starsessions to 1.x (#14333),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
809,removed reduce on non-loss outputs from dp (#78),0.6458626,fix result obj DP auto reduce (#3013),,0
810,temp freeze TM v0.2 (#7147),0.5677188,Setup: added requirement freeze for the next major version (#14480),Update streamlit requirement in /requirements Updates the requirements on streamlit to permit the latest version. - Release notes - Commits  updated-dependencies: - dependency-name: streamlit   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
811,update mergify (#2784),0.6593896,[1.3.0] - 2021-05-06," [pre-commit.ci] pre-commit suggestions  updates: - github.com/pre-commit/pre-commit-hooks: v4.3.0 → v4.4.0 - github.com/asottile/pyupgrade: v2.34.0 → v3.3.1 - https://github.com/myint/docformatter → https://github.com/PyCQA/docformatter - github.com/PyCQA/docformatter: v1.4 → v1.5.1 - github.com/asottile/yesqa: v1.3.0 → v1.4.0 - github.com/PyCQA/isort: 5.10.1 → 5.11.4 - github.com/psf/black: 22.6.0 → 22.12.0 - github.com/executablebooks/mdformat: 0.7.14 → 0.7.16  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
812,Docs for LAI (#13312),0.7190781,Release LAI docs as stable (#14250),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
813,Simplify optimization Logic (#4984),0.9999999,Simplify optimization Logic (#4984),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
814,fixes #154 (#155),0.6196022,"At last, lots of bug fixes (see below).",,0
815,updated docs (#2995),0.71335804,Docs improvements,Fixes https://github.com/Lightning-AI/lightning/issues/8107,1
816,CI: fix pypi flow (#15944),0.67596304,"Resolved schedule step bug for PyTorch Profiler (#6674, #6681)",Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com,0
817,Mark the lite DeviceDtypeModuleMixin as protected (#14548),0.58629954,Moved DeviceDtypeModuleMixin and HyperparametersMixin mixin to core (#8396),,0
818,Set running_torchscript recursively (#14657),0.584801,Add code_dir argument to tracer run (#15771),  fabric docs   fix reference   fabric ,0
819,fix ONNX model save on GPU (#3145),0.6973481,refactored GPU backend __step (#3120),  Rename LightningLite to Fabric   Fix introspection test   Fix deprecated Lite tests   Undo accidental Horovod removal   Fixes ,0
820,Remove every_n_val_epochs from ModelCheckpoint (#10366),0.84219146,Removed deprecated property ModelCheckpoint.period in favor of ModelCheckpoint.every_n_epochs (#9213),,1
821,release v0.113,0.7662811,"    version=""0.0.1"",",Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,1
822,Fix typing in pl.core.mixins.hparams_mixin (#10800),0.7469759,| Import pl.core.mixins.DeviceDtypeModuleMixin                                                           | 1.10             | No longer supported |,Updates the requirements on ipython[all] to permit the latest version. - Release notes - Commits  updated-dependencies: - dependency-name: ipython[all]   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,1
823,"Fix ""finished"" status code in MLFlowLogger (#16340)",0.77920485,"- The `MLFlowLogger.finalize()` now sets the status to `FAILED` when an exception occurred in `Trainer`, and sets the status to `FINISHED` on successful completion ([#12292](https://github.com/Lightning-AI/lightning/pull/12292))",,1
824,Pass args to ShardedDataParallel (#9483),0.9154709,Pass init args to ShardedDataParallel (#9483),Co-authored-by: awaelchli aedu.waelchli@gmail.com,1
825,Add typing for LightningOptimizer (#9990),0.7018812,Introduce lightning connect (#14452),Co-authored-by: awaelchli aedu.waelchli@gmail.com,1
826,Update req. for pyDeprecate version flexibility (#11629),0.58672917,Enable PyTorch 1.7 compatibility (#3541),,0
827,Convert progress bar metrics to float (#5692),0.8508979,Progress bar metrics tensors are now converted to float (#5692),,1
828,update alumni (#7545),0.47553033,Veteran,,0
829,Drop torch 1.6 support (#10367),0.71259046,Removed dependency on torchvision (#797),Update s3fs requirement in /requirements Updates the requirements on s3fs to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: s3fs   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,1
830,Move fsdp_native to fine-tuning recommendation (#16630),0.77134234,Native FSDP replaces Fairscale FSDP (#16400),Update numpy requirement in /requirements Updates the requirements on numpy to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: numpy   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,1
831,Add trainer flag step [ci skip] (#4147),0.698499,adding Trainer.tune() (#3293),fix link to gpu/advanced section,0
832,refactor optimizer loop logic for manual and automatic optimization (#7526),0.9499231,Refactored the logic around manual and automatic optimization inside the optimizer loop (#7526),,1
833,[bug-fix] Call transfer_batch_to_device in DDPlugin   (#5195),0.67323697,Disabled batch transfer in DP mode (#6098),,0
834,Fix Inconsistencies after introducing _step_end and _epoch_end (#1072),0.59920716,Made training_epoch_end behave like validation_epoch_end (#1357),Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
835,Add submodule update to contributing (#9578),0.54246986,Meta Module,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
836,Fixes CPU DDP breaking change and DDP change (#1635),0.73767513,"refactor DDP backend (#3204, #3207, #3208, #3209, #3210)",,1
837,Fix _module_available to detect horovod.torch properly (#12377),0.7858065,- Fixed check for horovod module ([#12377](https://github.com/PyTorchLightning/pytorch-lightning/pull/12377)),Co-authored-by: Shashwat  Fixes https://github.com/Lightning-AI/lightning/issues/16186,1
838,feat(semseg): allow model customization (#1371),0.5326336,model_utils >> model_helpers,Fixes https://github.com/Lightning-AI/lightning/issues/16170,0
839,fix(app): remove full story from the app template (#15166),0.5411836,Included app templates to the lightning and app packages (#13731),  Introduce basic auth to Lightning CLI for app creation   Parsing creds added   Adding auth field to app instance body   Adding tests   Adding changelog entry   Adding more tests   Update runtime.py   Setting auth on update   Fix test   Update lightning-cloud dep   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   Update runtime.py   Fix for release   Update base.txt   Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
840,Remove BasePlugin (#9066),0.6889156,- The base Plugin class has been removed. ,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
841,update README,0.5184378,Key updates,,0
842,Improve SSIM (#2833),0.770172,Updated SSIM metric (#4566)(#4656),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
843,Add enable_model_summary flag and deprecate weights_summary (#9699),0.65534264,Deprecated passing weights_summary to the Trainer constructor in favor of adding the ModelSummary callback with max_depth directly to the list of callbacks (#9699),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz Fixes https://github.com/Lightning-AI/lightning/issues/15855,0
844,release v0.6.4.dev1,0.74361396,0.4.0,,1
845,Use torch.autocast (#10053),0.62650573,Changed PyTorchProfiler to use torch.autograd.profiler.record_function to record functions (#6349),,0
846,Fix name order in CITATION.cff (#9423),0.5492909,Re-enabled naming metrics in ckpt name (#3060),,0
847,[refactor] Move save_function to accelerator 1/n [DeepSpeed] (#6689),0.8576555,Moved save_function to accelerator (#6689),,1
848,adjusted imports,0.78040826,import argparse,,1
849,ruff: fixing flake8-comprehensions (#17385),0.6063502,Resolve bug with Finetuning (#5744),,0
850,MANIFEST.in and setup.py clean-up (#7614),0.649005,"Removed PyTorch 1.6 support (#10367, #10738)",,0
851,amp now supports multiple optimizers,0.7317643,- Extended optimizer support with particular frequency,,1
852,Only load global step when fitting (#15532),0.5738567,Enabled prepare_data from correct processes - clarify local vs global rank (#2166),,0
853,Deprecate callback hooks on_init_start and on_init_end (#10940),0.7184816,Callback hooks,,1
854,fix missing arg,0.6842965,argparse_utils >> argparse,fix build model link,0
855,Update CI config after #17122 (#17134),0.52348995,This release fixes that core issue, debug install hotfix local refactor lit swap pruning,0
856,Changed LearningRateLogger to LearningRateMonitor (#3251),1.0,Changed LearningRateLogger to LearningRateMonitor (#3251),,1
857,Lightning cloud client call with key word arguments (#14685),0.7898031,LightningCloud client calls to use keyword arguments instead of positional arguments (#14685),,1
858,[3/n] add additional rich version check (#9757),0.6225896,- Changed minimum supported version of `rich` from `10.14.0` to `12.13.0` ([#16798](https://github.com/Lightning-AI/lightning/pull/16798)),,0
859,working on dp state fix,0.65197694,"Enables DP, but with many limitations",,0
860,Rename distributed_backend to accelerator in examples (#4657),0.7365225,Renamed all backends to Accelerator (#4066),,1
861,Bump docker/build-push-action from 3 to 4 (#16641),0.5324641,refactored GPU backend __step (#3120),,0
862,relax hparams (#919),0.593299,[1.4.9] - 2021-09-30,,0
863,Fix broken link to CLI docs (#15723),0.61790013,- Fixed an issue when using the CLI without arguments ([#14877](https://github.com/Lightning-AI/lightning/pull/14877)),"  .   why   Revert ""why""   This reverts commit 375d3e85f442226c6990ecf0e812fff94bed2de9.   tried api access with fixed values   Revert ""tried api access with fixed values""   This reverts commit f1720f6b1ad64b0734b5a3ea7b010124cf39b5b7.   Fix typo :tada:   update chglog   revert removing lines in chlog   update chglog   Co-authored-by: Akihiro Nitta akihiro@lightning.ai",0
864,update (#4343),0.6104584,"Updated DeepSpeed ZeRO (#6546, #6752, #6142, #6321)",,0
865,Mocking Loggers Part 5/5 (final) (#3926),0.80081767,"Mocking loggers (#3596, #3617, #3851, #3859, #3884, #3853, #3910, #3889, #3926)",  cold start proxy   Update src/lightning_app/components/serve/auto_scaler.py   changelog   better-doc   Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,1
866,Extract dataloader utilities from TrainerDataLoadingMixin (#10145),0.74720716,train_dataset = trainer.train_dataloader.dataset,,1
867,add skipif warpper (#6258),0.54977816,Changed fsspec to tuner (#4458),,0
868,LR scheduler docs update (#5678),0.69905376,lr_scheduler now activated after epoch    ,,0
869,fix nb tests for auto-merge (#2686),0.6192883,"nb_test_batches to num_test_batches,", change msg update chgl show the user's class name,0
870,avoid suppressing exception in FitLoop (#9206),0.6821114,Marked FitLoop.should_accumulate as protected (#9515),Co-authored-by: thomas thomas@thomass-MacBook-Pro.local,0
871,Added check to verify xla device is TPU (#3274),0.6503445,Updated logic for checking TPUs availability (#6767),Co-authored-by: thomas thomas@thomass-MacBook-Pro.local,0
872,fix when torchtext not installed (#2402),0.6735581,- Removed deprecated support for old torchtext versions ([#14375](https://github.com/Lightning-AI/lightning/pull/14375)),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
873,Make verify_loop_configurations a utility function (#9976),0.6599189,"Simplified ""should run validation"" logic (#7682)",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
874,Add none check for func,0.4701632,Did not interfere with a default sampler (#1318),Co-authored-by: thomas thomas@thomass-MacBook-Pro.local,0
875,Fix typo in TensorBoardLogger.log_metrics error message (#11595),0.70235914,Changed the default logger to TensorBoardLogger (#609),,1
876,Remove Strategy.init_optimizers (#11236),0.7583587,- Removed `Strategy.init_optimizers` in favor of `Strategy.setup_optimizers` ([#11236](https://github.com/PyTorchLightning/pytorch-lightning/pull/11236)),,1
877,Rename callbacks/base.py to callbacks/callback.py (#13031),0.70338523,Removed the deprecated pytorch_lightning.callbacks.base module in favor of pytorch_lightning.callbacks.callback (#16319),,1
878,Fix deadlocks for distributed training for RichProgressBar (#10428),0.621119,- Fixed `RichProgressBar` progress validation bar total when using multiple validation runs within a single training epoch ([#11668](https://github.com/PyTorchLightning/pytorch-lightning/pull/11668)),"  CI: settle file names   rename   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
879,Update sync_dist warning for multiple processes (#6790),0.69032747,Prevent crash if sync_dist=True on CPU (#4626),Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz,0
880,added single gpu train,0.7137635,Train on 2 GPUs in a Jupyter notebook,,1
881,Remove deprecated get_memory_profile (#12659),0.6593034,Removed deprecated: (#2760),Co-authored-by: thomas chaton thomas@grid.ai,0
882,[docs] Add docs for non-SLURM cluster setup (#5754),0.7150633,Enabled custom clusters (#4048),,1
883,fix docs rendering in datamodule (#7064),0.59854895,Replaced _DataModuleWrapper with __new__ (#7289),  Adding arguments for scale out/in interval   Tests ,0
884,fixes #3798 (#3849),0.6483944,Tons of bug fixes,  fixing the bug where num_replica=0 would fail   changelog ,0
885,update changelog 1.8.3 (#15845),0.7577927,Complete changelog,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,1
886,drop install FairScale for TPU (#5113),0.6341542,Updated logic for checking TPUs availability (#6767),[App] Improve the autoscaler UI (#16063),0
887,ci: fix & unify pkg name (#15470),0.5926136,Renamed TPUHalfPrecisionPlugin to TPUBf16PrecisionPlugin (#10026),Co-authored-by: thomas thomas@thomass-MacBook-Pro.local,0
888,Add checkpoint saving on graceful shutdown (ctr+c) (#3067),0.70828056,Disable saving checkpoints if not trained (#4372),,1
889,Fix type check for non-standard schedulers in horovod (#14215),0.6126315,"refactored Horovod backend (#3121, #3122)",Co-authored-by: thomas thomas@thomass-MacBook-Pro.local,0
890,[Hot Fix] Resolve Skipper CI (#13670),0.56298906,Porting fixes to autoscaler component (#16249),,0
891,Sample datatype for Serve Component (#15623),0.63034034,"for data: val_dataloader, test_dataloader, train_dataloader",,0
892,Drop PyTorch 1.7 testing from the CI (#12191),0.81534016,Drop PyTorch 1.9 support (#15347),  docs: add PT version   stable   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
893,"pytorch_lightning.loops file structure: group by dataloader, epoch, and batch loop  (#8077)",0.6453494,-   - class_path: pytorch_lightning.callbacks.EarlyStopping,,0
894,Enforce that the optimizer closure is executed when optimizer_step is overridden (#9360),0.9048701,Executing the optimizer_closure is now required when overriding the optimizer_step hook (#9360),,1
895,Fix spelling :P (#1035),0.60479367,py,,0
896,update contributors (#895),0.61919045,Contributors,,0
897,[test] lr_find with bs_scale (#6422),0.5888158,Run batch size finder for validate/test/predict.,,0
898,Fix caplog with logger.propagate=False (#10577),0.68334675,Re-Enable Logger's ImportErrors (#1938),Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
899,Fix NeptuneLogger to work in ddp mode (#1753),0.7841572,Enable NeptuneLogger to work with distributed_backend=ddp (#1753),,1
900,Fix import statement in tutorial  (#13130),0.682815,Module imports are now catching ModuleNotFoundError instead of ImportError (#9867),,0
901,Structured results (train loop only. val loop separate PR) (PR 2/5) (#2615),0.72424406,"EvalResult support for train and val. loop (#2615, #2651)",,1
902,fix typo in docs (#129),0.51520705,Changed overwrite to True (#16009),,0
903,fix num_workers for Windows example (#5375),0.66700566,"    num_workers=10,",,0
904,Remove training_epoch_end outputs check (#9719),0.83668756,Made training_epoch_end behave like validation_epoch_end (#1357),Co-authored-by: Ethan Harris ethanwharris@gmail.com,1
905,Update Trainer property docstrings (#16989),0.750517,Removed TrainerProperties mixin and moved property definitions directly into Trainer (#9495),,1
906,[refactor] Add should_raise_exception for gpus / tpus utilities (#8194),0.5809964,Updated logic for checking TPUs availability (#6767),Co-authored-by: thomas thomas@thomass-MacBook-Pro.local Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
907,Remove dead code in the loops (#16754),0.6524457,Moved result teardown to the loops (#8245),"Revert ""Load app before setting LIGHTNING_DISPATCHED (#16057)"" This reverts commit 8d3339a0e99ed91871e0c4f7214aca1146e95a89.",0
908,Added new email to enforce the Code of Conduct (#13833),0.4820337,A message informs about the changed settings:,,0
909,Avoid deprecated Lite import (#16058),0.73165566,There were two different ways of importing Lite in <= 1.9.0,,1
910,update Docs/changelog (#1398),0.694765,Full Changelog,,0
911,Allow ModelCheckpoint monitor to be None (#3633),1.0,Allow ModelCheckpoint monitor to be None (#3633),,1
912,Remove the deprecated tuning property and enums (#16379),0.6668328,Removed deprecated EvalResult (#5633),Adds guards to cluster deletion. - If cluster has running apps -> throw an error - If cluster has stopped apps -> confirm w/ user that apps and logs will be deleted,0
913,lightning entry point (#13490),0.7738724,Introduce lightning connect (#14452),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
914,The psutil package is now required for CPU monitoring (#17010),0.9999999,The psutil package is now required for CPU monitoring (#17010),,1
915,"Update traitlets requirement from <5.9.0,>=5.3.0 to >=5.3.0,<5.10.0 in /requirements (#17398)",0.59861517,| Attribute Trainer.tuning                                    | 1.10             | No longer supported         |,,0
916,Update test_gpu_stats_monitor.py to use devices instead of gpus or ipus (#11340),0.6546949,Device Stats Monitoring support for HPUs,,0
917,Horovod: fixed early stopping and added metrics aggregation (#3775),0.66747946,"refactored Horovod backend (#3121, #3122)",,0
918,Fix csv extension check (#6436),0.5850211,Deprecated tags_csv in favor of hparams_file (#1271),Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Fixes https://github.com/Lightning-AI/lightning/issues/15143,0
919,Refactor unnecessary else / elif when if block has a return statement (#8156),0.5556716,refactored inner eval loop (#3141),,0
920,Enforce pre-commit to use a recent and fixed version of isort. (#5408),0.5420539,Validation is now always run inside the training epoch scope (#7357),Co-authored-by: Nikhil Shenoy nikhilshenoy@dhcp-128-189-224-163.ubcsecure.wireless.ubc.ca Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
921,Improve Comet Logger pickled behavior (#2553),0.6557641,Made TensorBoardLogger and CometLogger pickleable (#2518),,0
922,Use Optional for arguments set to None by default (#4164),0.5356831,Enable None model checkpoint default (#3669),,0
923,docs: add PT version (#16010),0.57924396,Docs improvements, Merge setup_tools and assistant Project root Fix PROJECT_ROOT  Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
924,Create  hpu-ci-runner Dockerfile (#13239),0.6210995,Support **DictConfig for hparam serialization (#2519),,0
925,Add back support for logging in the gradient clipping hooks (#14298),0.65078247,Un-balanced logging properly supported (#5119),Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
926,quick start docs changes (#3028),0.62734616,Docs improvements,,0
927,Conda: PT 1.8 (#3833),0.63807833,[1.1.8] - 2021-02-08,Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
928,rename _call_ttp_hook to _call_strategy_hook (#11150),0.61162895,Callback hooks, abstract pkg build share ci syntax Checkgroup folders whl 1st doctest  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
929,Use TorchVision's Multi-weight Support and Model Registration API on Lightning (#14567),0.6767235,Extend LightningOptimizer to exposure underlying Optimizer attributes + update doc (#5095),Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
930,Fix property setter override by default setter (#14259),0.5457822,Raised a MisconfigurationException if batch transfer hooks are overriden with IPUAccelerator (13961),Co-authored-by: Jake Schmidt jake.schmidt@utexas.edu Co-authored-by: awaelchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
931,Add deprecation path for the old LightningModule module (#12740),0.7779778,Deprecated LightningDataParallel in favor of new wrapper module LightningParallelModule (#5670),Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
932,Set warnings : Unify epoch numbers to be zero-based : #675 (#786),0.68683934,Changed epoch indexing from 0 instead of 1 (#2289),simple Co-authored-by: hhsecond sherin@grid.ai,0
933,Fix setup callback hook to pass LightningModule through (#4608),0.7438166,- Fixed a missing call to `LightningDataModule.load_state_dict` hook while restoring checkpoint using `LightningDataModule.load_from_checkpoint` ([#14883](https://github.com/Lightning-AI/lightning/pull/14883)),Update deepdiff requirement in /requirements Updates the requirements on deepdiff to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: deepdiff   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,1
934,unify tests (#1940),0.55955243,"nb_test_batches to num_test_batches,",Update docker requirement in /requirements Updates the requirements on docker to permit the latest version. - Release notes - Commits  updated-dependencies: - dependency-name: docker   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
935,Deprecate pl/utilities/cloud_io.py (#14515),0.7101113,- Deprecated all functions in `pytorch_lightning.utilities.cloud_io` in favor of `lightning_lite.utilities.cloud_io` ([#14515](https://github.com/Lightning-AI/lightning/pull/14515)),,1
936,Merge pull request #31 from williamFalcon/examples,0.55638945,"merge backends (#3476, #3477, #3478, #3480, #3482)",Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com,0
937,Deprecate dataloader_idx from on_train_batch_start/end (#9816),0.6629325,def train_dataloader(...):,,0
938,Wraps sharded model for proper access to it state_dict in FSDP strategy (#16558),0.65736103,Native FSDP implementation,  dont try to replicate new works in the existing machine   update chglog   Update comment   Update src/lightning_app/components/auto_scaler.py   add test ,0
939,Update CONTRIBUTING.md,0.5908007,Contributors,,0
940,Fix DDP on XLA (#16020),0.6256774,"refactor DDP backend (#3204, #3207, #3208, #3209, #3210)",Co-authored-by: thomas thomas@thomass-MacBook-Pro.local,0
941,Use DistributedSampler when running with custom accelerator (#7814),0.6309549,Enabled passing in custom accelerators (#4050),,0
942,CI: Update Windows version from 2019 to 2022 (#14129),0.5761472,[0.7.0] - 2022-10-20,,0
943,"Update wandb requirement from <0.13.2,>=0.10.22 to >=0.10.22,<0.13.4 in /requirements (#14771)",0.5618549,Removed wandb logger's finalize method (#1193),"  import fix   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   import fix   move req   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Update requirements/app/base.txt   revert   skip doctest   import fix   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz",0
944,Fix detection of whether app is running in cloud (#16045),0.67107105,The utility lightning.app.utilities.cloud.is_running_in_cloud now returns True during the loading of the app locally when running with --cloud (#16045),signal if,0
945,Move test_hooks.py code (#7689),0.6330719,Skipped testing with PyTorch 1.7 and Python 3.9 on Ubuntu (#11217), [App] Install exact version whn upgrading and not when testing Update CHANGELOG.md  Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
946,update lr_finder to check for attribute if not running fast_dev_run (#5990),0.99882877,Update lr_finder to check for attribute if not running fast_dev_run (#5990),  document running dev lightning on the cloud   document running dev lightning on the cloud   Update .github/CONTRIBUTING.md   Co-authored-by: Noha Alon nohalon@gmail.com   document running dev lightning on the cloud   git clone & pip install -e   Update .github/CONTRIBUTING.md   Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: Noha Alon nohalon@gmail.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,1
947,fix selecting GPUs using CUDA_VISIBLE_DEVICES (#2739),0.74162304,GPU selection, fix cloudcomputes updates cloudcompute registration changelog,1
948,ref: refactored gpu backend __step (#3120),0.93200636,refactored GPU backend __step (#3120), ci: update signaling config,1
949,Correct behavior for argument gpus in Trainer (#561),0.7113887,  * Removed the `Trainer(gpus=...)` argument, fix multinode cloud component add tests,1
950,Document exception for metrics/classification (#6190),0.71260345,Classification metrics overhaul (#4837), Fix action_name usage in XLAProfiler add changelog Update src/pytorch_ligh Update xla.py  Co-authored-by: awaelchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,1
951,Add Accelerators section to Lightning docs (#10755),0.7143893,LightningCLI additions:, dont hardcode port in python server add another chglog,1
952,Increase typing_extensions minimal version (#13657),0.64767486,Refactored setup for typing friendly (#6590), Make autoscaler dependency optional update chglog dont directly import aiohttp,0
953,Fix exception message for FSDP running on CPU (#11325),0.6422695,Removed DeepSpeed FP16 Exception as FP32 is now supported (#8462),,0
954,Skip flaky ddp-spawn test on windows (#16942),0.7680642,Run ddp_spawn dataloader checks on Windows (#6930),,1
955,fix modify _DDPSinkBackward view inplace error for pytorch nightly 1.10 (#9649),0.69213295,"Resolved schedule step bug for PyTorch Profiler (#6674, #6681)","  Apply dynamo to training_step, validation_step, test_step, predict_step   Add entry to CHANGELOG.md ",0
956,Adds the option of saving the last model on checkpoint (#1908),0.733201,"Versioning of ""last"" checkpoints",Co-authored-by: thomas thomas@thomass-MacBook-Pro.local Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Ethan Harris ethanwharris@gmail.com Co-authored-by: Jirka jirka.borovec@seznam.cz,1
957,add version for CPU users (#4794),0.60361433,The psutil package is now required for CPU monitoring (#17010),,0
958,Merge pull request #1623 from PyTorchLightning/hparams,0.6381371,- Removed `configure_sync_batchnorm` from `ParallelStrategy` and all other strategies that inherit from it ([#11754](https://github.com/PyTorchLightning/pytorch-lightning/pull/11754)),,0
959,hotfix: mock examples (#6632),0.50187474,- Fixed issue where the CLI fails with certain torch objects ([#13153](https://github.com/Lightning-AI/lightning/pull/13153)),,0
960,Allow kwargs in Wandb & Neptune + kwargs docstring (#3475),0.55130756,Allow passing model hyperparameters as complete kwarg list (#1896) , Make LightningModule torch.jit.script-able again remove skip  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
961,Remove beta arg from F1 class and functional (#5076),1.0,Remove beta arg from F1 class and functional (#5076),,1
962,Workarounds for log support with Torch 2.0 (#16986),0.66696835,Disable torch.inference_mode with torch.compile in PyTorch 2.0 (#17215),,0
963,Update reinforce_learn_Qnet.py (#4814),0.5470619,- Fixed an issue with resuming from a checkpoint trained with QAT ([#11346](https://github.com/PyTorchLightning/pytorch-lightning/pull/11346)),Co-authored-by: thomas thomas@thomass-MacBook-Pro.local Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
964,Fix horovod installation base-cuda Dockerfile (#11811),0.72297025,"refactored Horovod backend (#3121, #3122)",,1
965,update chlog after 1.6.1 release (#12802),0.6543435,0.4.0,,0
966,0.10.0 (#3965),0.7444926,[0.6.0] - 2022-09-08,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
967,fix: Enable manual optimization for TPUs (#8458),0.9716029,Enabled manual optimization for TPUs (#8458),Co-authored-by: thomas thomas@thomass-MacBook-Pro.local Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
968,Add on_exception callback hook (#9183),0.8386128,An on_exception Callback hook has been added which allows the user to perform custom exception handling., Fix restarting attribute for lr finder update lite executor update trainer executor update spawn executor add multinode component tests add testing helpers add lite tests add trainer tests update changelog update trainer update workflow update tests debug add reason for skipif Apply suggestions from code review switch skipif  Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,1
969,Handle case where wrapper has not been initialized within the plugin,0.59123874,"    * The hooks/callbacks `prepare_data`, `setup`, `configure_sharded_model` and `teardown` now run under initialized process group for spawn-based plugins just like their non-spawn counterparts", Upgrade to HPU release 1.7.1 Update torch version check for hpu  Signed-off-by: Jerome janand@habana.ai Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
970,Avoid instantiating CombinedDataset unnecessarily (#16805),0.6457448,"DataModules now avoid duplicate {setup,teardown,prepare_data} calls for the same stage (#7238)",,0
971,Fix mypy errors attributed to pytorch_lightning.loggers.csv_logs.py (#13538),0.7464768,Deprecated pytorch_lightning.logging (#767),Exlucde pycache in setuptools,1
972,Fix GpuUsageLogger to work on different platforms (#3008),0.5726056,"Deprecated log_gpu_memory, gpu_metrics, and util funcs in favor of DeviceStatsMonitor callback (#9921)", Make gradients available for all_gather on TPU Modify switch and tests Apply suggestions from code review Modify tests Fix test Drop test  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
973,"fix(docs/app/lit_tabs): remove unused app_id, enable run instead (#15702)",0.620193,Improved support for running apps when dependencies aren't installed (#15711), Add configure_layout method for works Check for api access availability Updates from review Update CHANGELOG.md Apply suggestions from code review  Co-authored-by: Sherin Thomas sherin@lightning.ai,0
974,"Update hydra-core requirement from <1.3.0,>=1.0.5 to >=1.0.5,<1.4.0 in /requirements (#16736)",0.5714849,Temporarily removed support for Hydra multi-run (#15737), Bump playwright from 1.27.1 to 1.28.0 in /requirements  Bumps playwright from 1.27.1 to 1.28.0. - Release notes - Commits  updated-dependencies: - dependency-name: playwright   dependency-type: direct:production   update-type: version-update:semver-minor ... Signed-off-by: dependabot[bot] support@github.com  1.28  Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz,0
975,Update profiler doc (2/n) (#11430),0.6770864,Moved profilers to their own file (#7822), Update docs  Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
976,New logger connector code (#7882),0.79745126,Removed logger_connector legacy code (#6733), Wait for full file to be transferred in Path / Payload Fixes,1
977,removed save model logging,0.7247391,Removed LoggerStages (#5673), Fix bug when using structures with works Add test Update CHANGELOG.md,1
978,[App] Add cloud platform exception (#13928),0.66461873,Resolved a bug where the wrong client was passed to collect cloud logs (#14684), Remove SingleProcessRuntime Remove unused queues Docs,0
979,Refactor codebase to use trainer.loggers over trainer.logger when needed (#11920),0.7123307,trainer = pl.Trainer(plugins=SLURMEnvironment(auto_requeue=False)), CI: fixing pypi syntax (#15943) connect input,1
980,Remove unnecessary comprehension (#8405),0.5757578,remove _evaluate fx (#3197), Fix LRScheduler import for PyTorch 2.0 Add comment for posterity,0
981,Rename SingleTPUPlugin to SingleTPUStrategy (#11182),0.80491984,    * Renamed the `SingleTPUPlugin` to `SingleTPUStrategy` ([#11182](https://github.com/PyTorchLightning/pytorch-lightning/pull/11182)),fixed conflicts,1
982,"Fix self.log(on_epoch=True, reduce_fx=sum) on_batch_start (#9791)",0.6924031,"self.log(""loss"", loss, prog_bar=True, on_step=True)","  Exlucde pycache in setuptools   Add load balancer example   wip   Update example   rename   remove prints   _LoadBalancer -> LoadBalancer   AutoScaler(work)   change var name   remove locust   Update docs   include autoscaler in api ref   docs typo   docs typo   docs typo   docs typo   remove unused loadtest   remove unused device_type   clean up   clean up   clean up   Add docstring   type   env vars to args   expose an API for users to override to customise autoscaling logic   update example   comment   udpate var name   fix scale mechanism and clean up   Update exampl   ignore mypy   Add test file   .   update impl and update tests   Update changlog   .   revert docs   update test   update state to keep calling 'flow.run()'   Co-authored-by: Aniket Maurya theaniketmaurya@gmail.com   Add aiohttp to base requirements   Update docs   Co-authored-by: Luca Antiga luca.antiga@gmail.com   Use deserializer utility   fake trigger   wip: protect /system/* with basic auth   read password at runtime   Change env var name   import torch as optional   Don't overcreate works   simplify imports   Update example   aiohttp   Add work_args work_kwargs   More docs   remove FIXME   Apply Jirka's suggestions   Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci   clean example device   add comment on init threshold value   bad merge   nit: logging format   {in,out}put_schema -> {in,out}put_type   lowercase   docs on seconds   process_time -> processing_time   Dont modify work state from flow   Update tests   worker_url -> endpoint   fix exampl   Fix default scale logic   Fix default scale logic   Fix num_pending_works   Update num_pending_works   Fix bug creating too many works   Remove up/downscale_threshold args   Update example   Add typing   Fix example in docstring   Fix default scale logic   Update src/lightning_app/components/auto_scaler.py   Co-authored-by: Noha Alon nohalon@gmail.com  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci   rename method   rename locvar   Add todo   docs ci   docs ci   asdfafsdasdf pls docs   Apply suggestions from code review   Co-authored-by: Ethan Harris ethanwharris@gmail.com  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci   .   doc   Update src/lightning_app/components/auto_scaler.py   Co-authored-by: Noha Alon nohalon@gmail.com  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci  Revert ""[pre-commit.ci] auto fixes from pre-commit.com hooks""  This reverts commit 24983a0a5ab915fad3456690499a8ea4157e58f0.  Revert ""Update src/lightning_app/components/auto_scaler.py""  This reverts commit 56ea78b45f3a2d5e28b622cfc2240d95a906ac6d.   Remove redefinition   Remove load balancer run blocker   raise RuntimeError   remove has_sent   lower the default timeout_batching from 10 to 1   remove debug   update the default timeout_batching   .   tighten condition   fix endpoint   typo in runtimeerror cond   async lock update severs   add a test   {in,out}put_type typing   Update examples/app_server_with_auto_scaler/app.py   Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com  Update .actions/setup_tools.py  Co-authored-by: Aniket Maurya theaniketmaurya@gmail.com Co-authored-by: Luca Antiga luca.antiga@gmail.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Noha Alon nohalon@gmail.com Co-authored-by: Ethan Harris ethanwharris@gmail.com Co-authored-by: Akihiro Nitta aki@pop-os.localdomain Co-authored-by: thomas chaton thomas@grid.ai",0
983,Remove rank_zero_only on DataModule prepare_data (#7945),0.6659653,Rank-zero only EarlyStopping messages, Enable back inference mode support with hpu Remove unused Update document link and address comment  Signed-off-by: Jerome janand@habana.ai Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
984,Raise MisconfigurationException when the accelerator is available but… (#12708),0.78861487,"Raise MisconfigurationException when the accelerator is available but the user passes invalid ([]/0/""0"") values to the devices flag (#12708)",,1
985,ruff: PT some more fixes (#17569),0.6449082,Mixed precision overhaul (#16783),,0
986,Add DETAIL logs for batch use cases (#11008),0.6435435,Auto log the computational graph for loggers that support this (#3003),Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
987,Missing TorchScript trace's update (#4586),0.6749295,- Removed deprecated support for old torchtext versions ([#14375](https://github.com/Lightning-AI/lightning/pull/14375)),,0
988,Update README.md (#4036),0.528138,Updated mlflow with using resolve_tags (#6746), update param Apply suggestions from code review,0
989,Fix some tiny typos in docs (#13939),0.53479797,Fixing critical bugs in newly added hooks and hparams assignment., initial input type checkpointing fsdp in pl all_close  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
990,[bugfix] Correct call to torch.no_grad (#5124),0.8931667,Corrected call to torch.no_grad (#5124),  Direct support for compiled models   Update test   Update src/pytorch_lightning/core/module.py   Co-authored-by: Ethan Harris ethanwharris@gmail.com Co-authored-by: Ethan Harris ethanwharris@gmail.com,1
991,fix gpu example (#2466),0.6409334,GPU training (#2704),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
992,protect progress bar callback (#1855),0.71999866,progress bar,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
993,[TPU] Add testing matrix with PJRT (#17368),0.6492606,TPU training (#2708),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
994,Merge pull request #62 from williamFalcon/imports,0.5990648,import argparse,Updates the requirements on setuptools to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: setuptools   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
995,fix CI for PT 1.10 (#8526),0.5534629,Porting fixes to autoscaler component (#16249),,0
996,Update optimizer configuration info message in DeepSpeedStrategy (#11327),0.76302546,"Implemented DeepSpeedPlugin._setup_model_and_optimizers (#10009, #10064)",,1
997,Fix docstring (#4585),0.6701989,Docs,"  Add is_headless when dispatching in the cloud   Bump cloud version   Add tests   Dont open app page for headless apps locally   Refactor   Update CHANGELOG.md   Support dynamic UIs at runtime   Comments   Fix   Updates   Fixes and cleanup   Fix tests   Dont open view page for headless apps   Fix test, resolve URL the right way   Remove launch   Clean   Cleanup tests   Fixes   Updates   Add test   Increase app cloud tests timeout   Increase timeout   Wait for running   Revert timeouts   Clean   Dont update if it hasnt changed   Increase timeout ",0
998,updated sync bn (#2838),0.7762488,moves sync bn to each backend (#3925),"  initial work on deleting apps   after PR review   delete CLI working   restructred to make tests easier   revert manifest changes   added changelog, fix mypy issue   updates   Update src/lightning_app/cli/cmd_apps.py   Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com  Update src/lightning_app/cli/lightning_cli_delete.py  Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com  Update src/lightning_app/cli/lightning_cli_delete.py  Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com  Update src/lightning_app/cli/lightning_cli_delete.py  Co-authored-by: Sherin Thomas sherin@lightning.ai  Update src/lightning_app/cli/lightning_cli_delete.py  Co-authored-by: Sherin Thomas sherin@lightning.ai   import typing   adding tests   finished adding tests   addressed code review comments   fix mypy error   make mypy happy   make mypy happy   make mypy happy   make mypy happy   fix windows cli   Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: Sherin Thomas sherin@lightning.ai",1
999,"Fix logic and add test for apex check, rename file, add DDP launcher tests",0.64681005,DDP Debugging Improvements,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
1000,Print test results only if prog_bar_metrics is not empty (#1411),0.5677296,test_percent_check in favour of limit_test_batches, waiting builds,0
1001,set_epoch for validation and prediction data loaders (#12197),0.6506916,"The ModelCheckpoint.save_on_train_epoch_end attribute is now computed dynamically every epoch, accounting for changes to the validation dataloaders (#15300)",  drop name column from cluster list   change create cluster to accept id as well   rename validator   remove cluster name from logs   fix merge with master   more merge with master issues ,0
1002,Remove mps config for test (#14379),0.65191823,Refactored setup_training and remove test_mode (#5388),,0
1003,move device-specific teardown logic from training loop to accelerator (#5973),0.9865189,Moved device-specific teardown logic from training loop to accelerator (#5973), fixing legacy checkpoints Apply suggestions from code review  Co-authored-by: Akihiro Nitta nitta@akihironitta.com,1
1004,Remove AcceleratorConnector.parallel_devices (#12075),0.8076489,- Removed `AcceleratorConnector.parallel_devices` property ([#12075](https://github.com/PyTorchLightning/pytorch-lightning/pull/12075)), prune dependency for benchmarks drop,1
1005,ci: fix torch +cpu in wheels (#16674),0.62699294,- Fixed issue where the CLI fails with certain torch objects ([#13153](https://github.com/Lightning-AI/lightning/pull/13153)),,0
1006,Fix pickling of KFoldLoop (#12441),0.7078868,Tested pickling (#1636),,1
1007,Make training_epoch_end behave like validation_epoch_end (#1357),0.98995125,Made training_epoch_end behave like validation_epoch_end (#1357),  terminating only once   changelog ,1
1008,Drop PyTorch 1.9 support (#15347),0.99999994,Drop PyTorch 1.9 support (#15347),"  moving the requirements to components extras   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   component requirements to devel   importing torch in local scope   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  skipping doctest  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com",1
1009,Support setting the trainer reference recursively for ensembles (#13638),0.63659626,Refactored trainer _run_* functions and separate evaluation loops (#8065),"  Error when running on multiple clusters   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Revert this in separate PR: keep this focused   Improve testing   fixup! Improve testing   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   pass flake8   Update changelog   Address PR feedback   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   remove unused import   Reword error message   Error if running on cluster that doesn't exist   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   fixup! Error if running on cluster that doesn't exist   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  Remove unsued import  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com",0
1010,Remove redundant GPU test (#13623),0.71838284,- Removed the deprecated automatic GPU selection ([#16184](https://github.com/Lightning-AI/lightning/pull/16184)),freeze ipy,1
1011,Feature/4244 iou input expectations (#4261),0.5680382,Changed iou [func] to allow float input (#4704),"Cluster creation and deletion can take a long time. Instead of having these long running operations happen in the background, they should happen in the foreground. The advantage is that failures are brought to the users attention immediately, instead of the next time they decide to run lightning list clusters. While the CLI waits for the cluster to run / delete, it will display cluster status changes to the user. This PR also hides the --enable-performance and --edit-before-creation creation flags, as well as the --force deletion flag. They are either not frequently used (performance mode is expensive), or prone to misuse. Co-authored-by: Neven Miculinic neven.miculinic@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Raphael Randschau nicolai86@users.noreply.github.com",0
1012,pipeline release CI (#5494),0.49690735,Pipeline Parallelism,changelog,0
1013,missing chlogs (#2672),0.5171478,- Fixed check for horovod module ([#12377](https://github.com/PyTorchLightning/pytorch-lightning/pull/12377)),"  fix import torch   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   plugin   fix   skip   patch require   seed   warn   .   ..   skip True   0.0.3   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
1014,enable recursive parsing for single gpu inputs (#121),0.6391458,Parsing of GPU Argument,Update cloudpickle requirement in /requirements Updates the requirements on cloudpickle to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: cloudpickle   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
1015,extend pip install info (#194),0.7208218,pip install rich,Bumps hivemind from 1.0.1 to 1.1.2. - Release notes - Commits  updated-dependencies: - dependency-name: hivemind   dependency-type: direct:production   update-type: version-update:semver-minor ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,1
1016,Add Fabric.launch to Fabric methods section (#17437),0.7807975,fabric.launch(),Bumps google-github-actions/get-gke-credentials from 0 to 1. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: google-github-actions/get-gke-credentials   dependency-type: direct:production   update-type: version-update:semver-major ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,1
1017,CI: fix example imports for App as standalone (#15260),0.6303365,from setuptools import setup,Update fairscale requirement in /requirements Updates the requirements on fairscale to permit the latest version.  updated-dependencies: - dependency-name: fairscale   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
1018,Introduce ClusterEnvironment.detect() (#10564),0.7466557,Enabled custom clusters (#4048), [CLI] fix ssh listing stopped components update CHANGELOG,1
1019,Add run_name argument to the MLFlowLogger constructor (#7622),0.93321234,MLFlowLogger now accepts run_name as an constructor argument (#7622), fix debug test simplify  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
1020,Skip reconciliate_processes if used within a cluster environment that creates processes externally (#9389),0.6839102,Made cluster creation/deletion async by default (#16185),Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: awaelchli aedu.waelchli@gmail.com,0
1021,ci: update recurent events (#5480),0.6159419,Reset current progress counters when restarting an epoch loop that had already finished (#9371),,0
1022,drop unused Tox (#1242),0.53239703,remove _evaluate fx (#3197),,0
1023,added dp and ddp flag,0.6442749,decoupled DDP2 (#3816),Co-authored-by: awaelchli aedu.waelchli@gmail.com,0
1024,Remove deprecated sync_batchnorm and num_nodes attributes in DDP plugins (#10357),0.8416061,Deprecated num_nodes and sync_batchnorm arguments in DDPPlugin and DDPSpawnPlugin (#7026),,1
1025,Some more fixes,0.8357076,"At last, lots of bug fixes (see below).",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
1026,Remove extinct parameters from lightning_module.rst (#6801),0.7161083,Removed the deprecated pytorch_lightning.core.lightning module in favor of pytorch_lightning.core.module (#16318),,1
1027,[feat] Add stronger validation for checkpoint_callback argument (#7539),0.6850507,Changed the Trainer's checkpoint_callback argument to allow only boolean values (#7539),Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
1028,Avoid instantiating every accelerator in the registry (#14591),0.6895806,Automatic accelerator selection (#16847),,0
1029,Remove to-device functionality from fetchers (#16731),0.59262675,Remove MetricsHolder (#7909),,0
1030,Fix AttributeError: 'NoneType' object has no attribute 'finalize'  on TPU (#6221),0.5216171,Improved error messages in replace_sampler when the DataLoader attributes are not included in the signature or the signature is missing optional arguments (#8519),,0
1031,Exclude some examples from docs navigation (#17081),0.5865948,"docs for all Metrics (#2184, #2209)",,0
1032,Borisdayma: fix(wandb) - fix watch method (#1361),0.53394914,WandbLogger does not force wandb reinit arg to True anymore and creates a run only when needed (#4648),,0
1033,"Update scikit-learn requirement from <1.1.3,>0.22.1 to >0.22.1,<1.2.1 in /requirements (#16107)",0.7406039,Removed dependency on scikit-learn (#801),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
1034,deepcopy model state_dict in tests (#2887),0.6017425,Increased DeepDiff's verbose level to properly handle dict changes (#13960),,0
1035,"Fixed imports, swap to relying on function for entire batch",0.5803088,from setuptools import setup,Update lightning-utilities requirement in /requirements Updates the requirements on lightning-utilities to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: lightning-utilities   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
1036,readme: logo 800px (#17108),0.49122077,(#16002),,0
1037,Fix LightningLite.run signature for arbitrary arguments (#12629),0.77840513,- Fixed ``LightningCLI`` signature parameter resolving for some lightning classes ([#13283](https://github.com/Lightning-AI/lightning/pull/13283)),  update   update   update   update   update   update   update   update   update   update   update   update   update   update   updte   update   update   update   update   update   update   update   update   update   update   update   Update src/lightning_app/CHANGELOG.md   Co-authored-by: Ethan Harris ethanwharris@gmail.com  Update src/lightning_app/utilities/port.py  Co-authored-by: Ethan Harris ethanwharris@gmail.com  Update src/lightning_app/utilities/port.py  Co-authored-by: Ethan Harris ethanwharris@gmail.com  Update src/lightning_app/utilities/port.py  Co-authored-by: Ethan Harris ethanwharris@gmail.com  Update src/lightning_app/utilities/port.py  Co-authored-by: Ethan Harris ethanwharris@gmail.com  Update src/lightning_app/utilities/port.py  Co-authored-by: Ethan Harris ethanwharris@gmail.com  Update src/lightning_app/utilities/port.py  Co-authored-by: Ethan Harris ethanwharris@gmail.com Co-authored-by: Ethan Harris ethanwharris@gmail.com,1
1038,find_unused_parameters=True,0.99999964,            find_unused_parameters=True,,1
1039,fix best score on wrong device in EarlyStopping callback (#8295),0.6561893,Removed deprecated early_stop_callback (#3982),,0
1040,Gen ddp support (#1961),0.6600411,Removed support for the DDP2 strategy,,0
1041,Add missing callbacks to callbacks.rst (#9223),0.68711424,  callbacks:,"  remove deprecated base profilers   Update changelog   remove import statement   rip   correct deprecation version   update changelog   Mark buried classes as private   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci  Add typing  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
1042,Cleanup cluster waiting (#16054),1.0000002,Cleanup cluster waiting (#16054),,1
1043,add Azure tags trigger (#6066),0.55810404,Updated mlflow with using resolve_tags (#6746),split examples and pytests,0
1044,Update docs for alternative dataset projects (#17096),0.6103478,Data-Loading improvements,,0
1045,[App] Automate missing requirements installation for CLI (#15198),0.66939384,Introducing CLI commands for apps (#13602)!,fixed ssh-keys docs,0
1046,Add TPU example (#5109),0.7133368,TPU training (#2708),  rename _examples dir   refactor   clean   path   add inits   skip   e2e   azure   e2e   rev   unify single depth for ignore docs req.   group ,1
1047,Fix isort failures in trainer (#5529),0.74159175,trainer = pl.Trainer(plugins=SLURMEnvironment(auto_requeue=False)), unify remove and delete command groups & the add and delete command groups added changelog fix tests Apply suggestions from code review  Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,1
1048,Fix which groups require docs builds (#15581),0.5770451,Fixing critical bugs in newly added hooks and hparams assignment., updated the lighting delete cluster CLI command help text output updated changelog typo fix Apply suggestions from code review  Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
1049,Add usage of Jupyter magic command for loggers (#12333),0.6833919,Changed automatic casting for LoggerConnector metrics (#5218),,0
1050,Restructure Fabric docs (#17111),0.64293915,Learn more about Fabric and what it can do in the new docs!, Fix app dag example Add test Update doc Update tests/tests_app_examples/test_app_dag.py  Co-authored-by: Sherin Thomas sherin@grid.ai,0
1051,2/n Simplify spawn plugins: Spawn immediately (#10896),0.65570563,"    * All spawn-based plugins now spawn processes immediately upon calling `Trainer.{fit,validate,test,predict}`",Bumps pytest from 7.1.3 to 7.2.0. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: pytest   dependency-type: direct:production   update-type: version-update:semver-minor ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
1052,update tutorials (#16263),0.5743629,Mixed precision overhaul (#16783),,0
1053,Update Lightning Lite docs (3/n) (#16245),0.8672954,Update the Lightning App docs (#13537), Enable CORS in StreamlitFrontend to support upload Only disable XSRF when running on localhost Update test Use utility fn to detect if localhost  Co-authored-by: Luca Antiga luca@lightning.ai,1
1054,Clean-up PL's Lite imports (#14769),0.69370735,There were two different ways of importing Lite in <= 1.9.0,Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
1055,training forward refactor (#3134),1.0000001,training forward refactor (#3134), update chlog after 1.8.2 Apply suggestions from code review,1
1056,Update lr_logger.py (#2847),0.7127284,Changed pytorch_lightning.logging to pytorch_lightning.loggers (#767), Switch from tensorboard to tensorboardx in logger Warn if log_graph is set to True but tensorboard is not installed Fix warning message formatting Apply suggestions from code review simplify for TBX as required pkg docs example chlog tbx 2.2  Co-authored-by: Luca Antiga luca@lightning.ai Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz,1
1057,fix _reset_eval_dataloader() for IterableDataset (#1560),0.75124705,return iterabledataset,  add custom data iter docs   add custom data iter docs   Update docs/source-pytorch/data/custom_data_iterables.rst   remove ToDevice   nit   Update docs/source-pytorch/data/custom_data_iterables.rst   Co-authored-by: Luca Antiga luca.antiga@gmail.com   clarification for @lantiga   typo   Update docs/source-pytorch/data/custom_data_iterables.rst   Update docs/source-pytorch/data/custom_data_iterables.rst   Update docs/source-pytorch/data/custom_data_iterables.rst   Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Luca Antiga luca.antiga@gmail.com,1
1058,CI: fix running PT 1.11 (#12304),0.53999746,"PyTorch 1.10 and Python 3.7 no longer supported (#16492, #16579)", revert new hydra cwd behavior remove debug statements changelog  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
1059,Sanitize None params during pruning (#6836),1.0000001,Sanitize None params during pruning (#6836),Co-authored-by: Luca Antiga luca@lightning.ai,1
1060,Unified API upstream with suggestion to ben,0.6358855,A new stateful API,Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Ethan Harris ethanwharris@gmail.com,0
1061,mergify: drop ready if conflicts (#12396),0.5413263,"- The `dataloader_idx` argument is now optional for the `on_{validation,test,predict}_batch_{start,end}` hooks. Remove it or default it to 0 if you don't use multiple dataloaders ([#16753](https://github.com/Lightning-AI/lightning/pull/16753))",,0
1062,revert #9125 / fix back-compatibility with saving hparams as a whole container (#9642),0.57049596,refactored DDP backend forward (#3119),,0
1063,enabled no returns from eval (#2446),0.9886591,Enabled no returns from eval (#2446),,1
1064,Replace GPU device idx with current process index (#1541),0.6702876,refactored GPU backend __step (#3120),,0
1065,Enable the auto-cc bot (#10531),0.5893949,Porting fixes to autoscaler component (#16249), Update beautifulsoup4 requirement in /requirements  Updates the requirements on beautifulsoup4 to permit the latest version.  updated-dependencies: - dependency-name: beautifulsoup4   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com  Apply suggestions from code review  Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
1066,[Typo] update some out-dated links from pytorch clip_grad_value_ (#8533),0.64239264,Removed pytorch_lightning/trainer/evaluation_loop.py (#8056),Update tensorboard requirement in /requirements Updates the requirements on tensorboard to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: tensorboard   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
1067,test cloudpickle (#2105),0.60939294,  * `pl.utilities.cloud_io` ([#16438](https://github.com/Lightning-AI/lightning/pull/16438)),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
1068,Group metrics generated by DeviceStatsMonitor for better visualization (#11254),0.71212757,device_stats = DeviceStatsMonitor(),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
1069,Notify the user of ignored requirements (#15799),0.5802406,Show a message when BuildConfig(requirements=[...]) is passed but a requirements.txt file is already present in the Work (#15799),,0
1070,GPU CI - run torch 1.8 (LTS) (#8116),0.70083416,GPU training (#2704),,1
1071,add secondary label to templates (#16577),0.5743259,Allowing decorate model init with saving hparams inside (#4662),,0
1072,Improve val step logging (#7351),0.7092813,Metric reduction with Logging (#5150),Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
1073,Fix lr_finder suggesting too high learning rates (#7076),0.7273898,Learning Rate Finder (#13802),torch inference mode for prediction,1
1074,Deprecate sheety API (#14004),1.0000002,Deprecate sheety API (#14004),,1
1075,prune deprecated Trainer arg enable_pl_optimizer (#6163),0.78332686,Removed deprecated Trainer argument enable_pl_optimizer and automatic_optimization (#6163),,1
1076,Fix button selector for Lightning app e2e tests (#13984),0.66360754,Lightning App,,0
1077,Change the classifier input from 2048 to 1000. (#5232),0.58030003,Changed LearningRateLogger to LearningRateMonitor (#3251),,0
1078,(app) Introduce LightningTrainingComponent (#13830),0.8526722,Lightning App,Bumps coverage from 6.4.2 to 6.5.0. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: coverage   dependency-type: direct:production   update-type: version-update:semver-minor ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,1
1079,Fix azure path excludes (#15756),0.5992735,Removed deprecated checkpoint argument filepath (#5321),fixed command parsing so that all lines in the file are parsed,0
1080,rtfd: try to collapse docs (#17020),0.5405258,Disabled val and test shuffling (#1600),,0
1081,Doc fixes (#1362),0.6670569,Docs improvements,Co-authored-by: manskx ahmed.mansy156@gmail.com,0
1082,Deprecate LoggerCollection in favor of trainer.loggers (#12147),0.7993255,Removed restrictions in the Trainer that loggers can only log from rank 0; the existing logger behavior has not changed (#8608),,1
1083,Document CI/CD (#12980),0.52340895,(#16002),"  examples   fix few examples   Update pl_multinode.py   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
1084,added barrier (#2245),0.54243094,and nb_val_batches to num_val_batches (#567),,0
1085,flake8 ++,0.4665427,Mixed precision overhaul (#16783),,0
1086,Unblock GPU CI (#11934),0.7754545,Enable non-blocking for device transfers to GPU (#1843),Bumps google-github-actions/auth from 0 to 1. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: google-github-actions/auth   dependency-type: direct:production   update-type: version-update:semver-major ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,1
1087,Remove _call_accelerator_hook Trainer method (#10999),0.8337561,Removed call_configure_sharded_model_hook property from Accelerator and TrainingTypePlugin (#9612),Updates the requirements on onnxruntime to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: onnxruntime   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,1
1088,bug fix: restore_optimizers correctly handles non-mapping values in optimizer.state.values() (#11757),0.69004184,Improved error messages for invalid configure_optimizers returns (#3587),Bumps google-github-actions/setup-gcloud from 0 to 1. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: google-github-actions/setup-gcloud   dependency-type: direct:production   update-type: version-update:semver-major ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
1089,docker: drop pt 1.9 (#15345),0.6459441,Drop PyTorch 1.9 support (#15347),,0
1090,Add openmpi to our base cuda container for MPI support (#6026),0.5664484,Enabled cp (upload) at project level (#16631),,0
1091,"Revert ""Fixes EarlyStopping With Precision=16 (#1996)"" (#2032)",0.6717166,Mixed precision overhaul (#16783),,0
1092,Update tensorboard.py,0.7077514,Switch from tensorboard to tensorboardx in TensorBoardLogger (#15728),Co-authored-by: otaj 6065855+otaj@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz,1
1093,Group torch.compile utilities together (#16711),0.67822546,Let TorchCollective works on the torch.distributed WORLD process group by default (#16995),,0
1094,add a hook for on_tng_metrics so that users get access to the grad_norm and mem_map dicts.,0.5626365,"This enables users to customize how the gradient norm is computed and logged, without needing to wrangle with the Trainer or override the log_grad_norm hook.",Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
1095,tests/hotfix: import pl (#16935),0.6382065,import argparse, add title and description update test apply suggestions  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
1096,[App/Improvement] Cleaning up Queue abstraction (#14977),0.59810174,refactored dataloader process hook (#3139),Co-authored-by: Marc Skov Madsen masma@orsted.dk Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Felonious-Spellfire felonious.spellfire@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Mansy ahmed.mansy156@gmail.com,0
1097,Add XLAStatsMonitor Callback (#8235),0.69463146,Deprecated GPUStatsMonitor and XLAStatsMonitor in favor of DeviceStatsMonitor callback (#9924),Co-authored-by: Raphael Randschau nicolai86@users.noreply.github.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
1098,set xxx_AVAILABLE as protected (#5082),0.9432012,Renamed xxx_AVAILABLE as protected (#5082),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
1099,Documentation Fixes [skip ci] (#3955),0.5636794,Resolve bug with Finetuning (#5744),Co-authored-by: awaelchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
1100,ref: decouple apex second attemp part 7/n (#4061),0.6211893,remove _evaluate fx (#3197),Co-authored-by: awaelchli aedu.waelchli@gmail.com,0
1101,Clear predict_progress_bar in ProgressBar.getstate (#7608),0.80485106,progress bar, global distrib ver codeowners Apply suggestions from code review  Co-authored-by: otaj 6065855+otaj@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
1102,Add highlighting to the BibTeX entry in README (#1356),0.4225464,print(trainer.logger),Prevent warning when shutil.executable returns a symlink Co-authored-by: Luca Antiga luca@lightning.ai,0
1103,[bugfix] Clean Validation Sanity Checking metrics (#8171),0.6851429,"Validation DataLoader 0:  38%|███      | 12/32 [00:12<00:20,  1.01s/it]","  update chlogs   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  Apply suggestions from code review  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Fix  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com",0
1104,[App] Add support for running with multiprocessing in the cloud (#16624),0.6257036,Support running on multiple clusters (#16016),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka jirka.borovec@seznam.cz,0
1105,Dependency pinning (#14463),0.59063977,group connectors (#3472),Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
1106,Restrict setup methods to accept a single model (#10064),0.6152884,- Added model configuration checking before it runs,  Change app root / config path to be the app.py parent directory   Update CHANGELOG.md   mypy   Fix   Mypy ,0
1107,Fix handling of script arguments in tracer (#15518),0.71485615,Add code_dir argument to tracer run (#15771),,1
1108,ci: disable flagship apps as required (#16895),0.5223044,"Apps without UIs no longer activate the ""Open App"" button when running in the cloud (#15875)",,0
1109,cleaned up some if statements,0.5902505,  validation_if_necessary(),Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
1110,Remove the trainer.data_parallel property (#16703),0.80985403,Removed trainer.reset_*_dataloader() methods (#16726),,1
1111,fix ModelCheckpoint docs (#4426),0.7459391,Replace MisconfigurationException with warning in ModelCheckpoint Callback (#4560),Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,1
1112,ci: cleaning caches (#16752),0.6470077,Cleanup cluster waiting (#16054),,0
1113,"Update docutils requirement from <0.19,>=0.16 to >=0.16,<0.20 in /requirements (#14664)",0.5323732,Setup: added requirement freeze for the next major version (#14480),,0
1114,Fix typo in cluster_advanced.rst (#13173),0.64172953,Dropped name column from cluster list (#15721),This reverts commit 4ea44dd276c9ffa350553a05d33d2050cffec242.,0
1115,Add deprecation path for the old Loop module (#13043),0.6478751,Refactored Loops,Co-authored-by: rohitgr7 rohitgr1998@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
1116,Remove deprecated LightningDistributed (#13549),0.863423,- Removed deprecated `LightningDistributed` ([#13549](https://github.com/Lightning-AI/lightning/pull/13549)),,1
1117,Update/merge multi-gpu docs (#2021),0.56383294,Updated governance docs,,0
1118,Update setup.py (#174),0.7137823,"PyTorch 1.10 and Python 3.7 no longer supported (#16492, #16579)",,1
1119,Fix mypy 0.800 plus when prepending $PYTHONPATH to sys.path (#5698),0.5425817,"PyTorch 1.10 and Python 3.7 no longer supported (#16492, #16579)",,0
1120,ci: replace flake8 by ruff (#16433),0.6098624,Mixed precision overhaul (#16783),,0
1121,Make Plugins Proxies after transfering ownership (#8117),0.5727599,Enabled plugins (#4041),Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
1122,update usage of deprecated automatic_optimization (#5011),0.76019967,Refactored the logic around manual and automatic optimization inside the optimizer loop (#7526),,1
1123,docs (#4106),0.76978534,"docs for all Metrics (#2184, #2209)","  Remove function deprecated in v1.5   Update changelog   rip   Update graveyard   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Add death test  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   Update test case name   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  Address pre-commit failures  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",1
1124,Use completed over processed in reset_on_restart (#9656),1.0,Use completed over processed in reset_on_restart (#9656),,1
1125,rc2 (#7057),0.54225874,(#16002),,0
1126,Merge branch 'warning' of https://github.com/williamFalcon/pytorch-lightning into warning,0.6570724,- Set the `prog_bar` flag to False in `LightningModule.log_grad_norm` ([#11472](https://github.com/PyTorchLightning/pytorch-lightning/pull/11472)),"  remove source-lit   docs   docs   docs   docs   ic   deploy   deploy   deploy   deploy   deploy   deploy   Apply suggestions from code review   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  make build run  Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Rick Izzo rick@grid.ai",0
1127,Hyperparameters docs refresh (#10280),0.6661054,Flattening Wandb Hyperparameters (#2459),"  introducing serve component   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   clean up tests   clean up tests   doctest   mypy   structure-fix   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   cleanup   cleanup   test fix   addition   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   test fix   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   requirements   getting future url   url for local   sample data typeg   changes   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   prediction   updates   updates   manifest   fix type error   fixed test   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Rick Izzo rick@grid.ai Co-authored-by: Jirka jirka.borovec@seznam.cz",0
1128,[TPU] Improve TPU workflow (#17237),0.7778031,Enabled manual optimization for TPUs (#8458),update,1
1129,Fix LightningModule step methods bypassing DDP wrapper in Fabric (#17424),0.7794885,Deprecated LightningDistributedDataParallel in favor of new wrapper module LightningDistributedModule (#5185),  update   update ,1
1130,release v0.5.2.1,0.80280155,0.4.0,  Drop 1.9   Everything else   READMEs   Missed some   IPU skips   Remove exception type   Add back ,1
1131,[App] Fix support for streamlit > 1.14 (#16139),0.636555,- Fixed a bug when launching an app on multiple clusters ([#15226](https://github.com/Lightning-AI/lightning/pull/15226)),  fixed condition in which app command comments would not execute before the flow is dispatched   remove debug print statment   updated example code   fix test   updates to test   Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
1132,simplify _copy_trainer_model_properties (#12788),0.611612,Removed TrainerProperties mixin and moved property definitions directly into Trainer (#9495),update,0
1133,fix logging config and add profiler test (#1267),0.6833378,Cleaning up stale logger tests (#3490),,0
1134,fixed nccl init,0.5048657,moves init apex from LM to apex connector (#3923),  update   update   update   update   update   udpate   update   update   update   update   update   updatre   update   update   update   updar   update   update   update   remove print   update   update   update   update   update   update   update   update ,0
1135,Fix missing deepspeed distributed call (#9540),0.6193626,Removed DeepSpeed FP16 Exception as FP32 is now supported (#8462),,0
1136,Set the optimization output result class as a class attribute (#9977),0.686798,Changed automatic_optimization to be a model attribute (#4602),,0
1137,Remove should_rank_save_checkpoint property from TTP (#11070),0.7899767,Removed should_rank_save_checkpoint property from Trainer (#9433),Serve component (#15609),1
1138,Fix save_weights_only flag in ModelCheckpoint (#1780),0.67174476,Removed the deprecated model argument from ModelCheckpoint.save_checkpoint (#8688),Signed-off-by: Jerome janand@habana.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
1139,Avoid in-place ops during logging result updates (#11401),0.6726743,Logging,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
1140,Update LiteOptimizer signature after optimizer changes in TrainingTypePlugin (#10708),0.6661241,Allowed training type plugin to delay optimizer creation (#6331),"  Enable v2   Run on draft   DEBUG   maintainers and owner   Revert ""DEBUG""   This reverts commit 1414e8590285d62962a44590ae70cf9635c0d4e7.",0
1141,review changes #44,0.65106815,Noteworthy changes:, add click wrapped argparse lite -> model notebook update  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
1142,Remove AcceleratorConnector.num_nodes  (#12107),0.81033826,- Removed `AcceleratorConnector.num_nodes` ([#12107](https://github.com/PyTorchLightning/pytorch-lightning/pull/12107)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
1143,Add separators to performance docs (#6882),0.58203113,"Working with multiple dataloaders (#16800, #16753)"," Change retry mechansim default in LightningClient add changelog fix weird patching mechanism remove unused import hack the system try again, maybe something has changed in the package  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com",0
1144,Improve error message on TypeError during DataLoader reconstruction (#10719),0.6784993,Replace MisconfigurationException with warning in ModelCheckpoint Callback (#4560),,0
1145,Create GH action for automated docker builds on releases (#1559),0.5387628,- Added support for cascading a SIGTERM signal to launched processes after the launching process (rank 0) receives it ([#16525](https://github.com/Lightning-AI/lightning/pull/16525)),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
1146,added cpu + amp error,0.665001,enable_pl_optimizer=False by default to temporarily fix AMP issues (#5163),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
1147,Attempt try catch to prevent errors,0.49245295,Fixing critical bugs in newly added hooks and hparams assignment.,  update   update   update   update ,0
1148,setup py 3.8 (#2135),0.7600795,Enable PyTorch 1.7 compatibility (#3541),,1
1149,"Add AMP for validation, prediction and testing (#6565)",0.6951056,training AMP scaling refactor (#3135),   Fix diverse issues introduced when the documentation was restructured.    Change the docs to be focused on configure hyperparameters instead of reduction of bolierplate.   Update docs/source-pytorch/cli/lightning_cli.rst   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com    Fix diverse issues introduced when the documentation was restructured.    Change the docs to be focused on configure hyperparameters instead of reduction of bolierplate.   Fixes based on review.   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   Fix wrap width.   Move save_hyperparameters and load_from_checkpoint to lightning_module page.   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec 6035284+Borda@users.noreply.github.com,0
1150,Add Plugins Registry to docs (#10181),0.6713449,Enabled plugins (#4041), improve deadlock detection test check if failure gets caught remove,0
1151,Merge pull request #9 from Derek-Wds/master,0.5551834,"merge backends (#3476, #3477, #3478, #3480, #3482)",  initial work   this seems to work well   added example test   updated docs & logging   fixed errors   fix typing error   now using the --setup flag to decide if we should execute app comment commands or not   updated tests   added tests   added test to ci   fixed failing tests   code review   updates ,0
1152,Fixed WandbLogger save_dir is not set after creation (#12748) (#14326),0.7050439,WandbLogger does not force wandb reinit arg to True anymore and creates a run only when needed (#4648),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
1153,Some fixes in the CLI docs (#17575),0.60948,Fixing critical bugs in newly added hooks and hparams assignment.,Use new syntax for setting github output Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
1154,Disallow invalid seed string values (#8787),0.7515925,seed_everything now fails when an invalid seed value is passed instead of selecting a random seed (#8787),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
1155,"Revert ""Remove skipping logic in favor of path filtering (#14170)"" (#14244)",0.5514583,- Removed the deprecated `flush_logs_every_n_steps` argument from the `Trainer` constructor ([#13074](https://github.com/Lightning-AI/lightning/pull/13074)),"  Initial commit   Update cloud runner   Add start_with_flow flag   Update CHANGELOG.md   Update src/lightning_app/core/work.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   Update cloud runner   Revert, not needed   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com",0
1156,Fix lr_find to generate same results on multiple calls (#9704),0.59513867,tuner.lr_find(...),Updates the requirements on fastapi to permit the latest version. - Release notes - Commits  updated-dependencies: - dependency-name: fastapi   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
1157,fix normalize mode at confusion matrix (replace nans with zeros) (#3465),0.63175166,Remove nan loss in manual optimization (#5121),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
1158,ref: inner train loop (intermediate step) 5/n (#3365),0.77811766,"inner train loop (#3359, #3361, #3362, #3363, #3365, #3366, #3367, #3368, #3369, #3370, #3371, #3372, #3373, #3374, #3375, #3376, #3385, #3388, #3397)",,1
1159,fix changelog (#1864),0.7562712,Complete changelog,,1
1160,Bump tj-actions/changed-files from 29.0.1 to 29.0.3 (#14541),0.5398957,"- Removed deprecated `LightningModule.log(tbptt_reduce_fx, tbptt_reduce_token, sync_dist_op)` ([#10423](https://github.com/PyTorchLightning/pytorch-lightning/pull/10423))",Fixed typo Havana -> Habana HPUs are accelerators built by Habana Labs.,0
1161,Deprecate lr_sch_names from LearningRateMonitor (#10066),0.98184514,Deprecated lr_sch_names from LearningRateMonitor (#10066), Support DDP for LRFinder Apply suggestions from code review rank 0 is the decision maker  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
1162,ref: unify slurm and TE under backendPlugin 1/n (#4578),0.73150074,"Unify sLURM/TorchElastic under backend plugin (#4578, #4580, #4581, #4582, #4583)", fix: fix bagua manual backward Update bagua module Simplify test case Fix type annotations  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Akihiro Nitta akihiro@pytorchlightning.ai,1
1163,Fix sync_dist for tpus (#6950),0.66769844,Remove .item which causes sync issues (#1254), Enable quick start e2e test to run without installing dependencies yaml formatting clone the repo,0
1164,Fix fast_dev_run running validation twice (#1365),0.6636029,Tuner algorithms will be skipped if fast_dev_run=True (#3903),,0
1165,pkg: parse local versions (#13933),0.775,Parsed local package versions (#13933),  extend matrix   drop   group-check   groups   cat   typo   cat2type   cat2type   env vars   ''   Rename to slow. Fix timeout   Examples are GPU only   str   Extra step   ''   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
1166,Fix trainer.fit_loop.split_idx reference (#8601),0.7426554,trainer.fit_loop.replace(epoch_loop=MyCustomLoop), prune installation artifact  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
1167,Bugfix: Lr finder and hparams compatibility (#2821),0.61895275,Enable PyTorch 1.7 compatibility (#3541),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
1168,Update logging docs (#10734),0.6544604,Update the Lightning App docs (#13537),  Simplify PL CODEOWNERS   Add William   Update apps too ,0
1169,simplify training phase as Enum (#5419),0.6219232,| Enum TrainerFn.TUNING                                    | 1.10             | No longer supported         |,,0
1170,Add dynamo RunIf skip condition (#17404),0.5366604,Changed the Trainer's checkpoint_callback argument to allow only boolean values (#7539),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
1171,Fix val_check_interval with fast_dev_run (#5540),0.71527445,Tuner algorithms will be skipped if fast_dev_run=True (#3903),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
1172,Update sphinx version to 4.0 or later (#7716),0.5353618,0.4.0,,0
1173,install lite for mypy & docs (#15437),0.5298655,There were two different ways of importing Lite in <= 1.9.0,,0
1174,Update child modules docs (#11198),0.53874075,- Remove deprecated method `ClusterEnvironment.creates_children` ([#10339](https://github.com/PyTorchLightning/pytorch-lightning/pull/10339)),  Fix result transfer in multiprocessing launcher on multi-node   add simple test   add comment   update test   changelog   use tempfile   fix   assert None   unused import   add comment ,0
1175,Existence check for hparams now uses underlying filesystem (#5250),0.54917943,Support **DictConfig for hparam serialization (#2519),Co-authored-by: awaelchli aedu.waelchli@gmail.com,0
1176,"Revert ""Update deepspeed requirement from <0.6.0 to <0.7.0 in /requirements (#13048)"" (#13177)",0.7104212,"Changed default for DeepSpeed CPU Offload to False, due to prohibitively slow speeds at smaller scale (#6262)",Co-authored-by: Bryn Lloyd lloyd@itis.swiss,1
1177,feat(val_sanity): enable skipping validation sanity  (#176),0.74238694, - Sanity checking: `Trainer(num_sanity_val_steps>0)`, Reuse connection if it matches a connection from an active terminal Remove unused import Include both name and id in the check Fix messages and tests Add test Handle monkeypatching more cleanly Remove unused imports  Co-authored-by: Luca Antiga luca@lightning.ai Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
1178,Fix typing in pl.callbacks.xla_stats_monitor (#11219),0.59033406,- Removed the deprecated `XLAStatsMonitor` callback ([#12688](https://github.com/Lightning-AI/lightning/pull/12688)), build config commands Apply suggestions from code review,0
1179,Allowed setting attributes on DataLoader and BatchSampler when instantiated inside *_dataloader hooks (#14212),0.7384385,- `DataLoader` instantiated inside a `*_dataloader` hook will not set the passed arguments as attributes anymore ([#12981](https://github.com/Lightning-AI/lightning/pull/12981)), Fix: Revert  to preserve backward compatibility  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
1180,docs: explain how Lightning uses closures for automatic optimization (#8551),0.7063427,Removed automatic_optimization as a property from the training loop in favor of LightningModule.automatic_optimization (#7130),,1
1181,renamed options,0.5498675,A message informs about the changed settings:,Move myself to Alumni,0
1182,save initial arguments (#4163),0.58981526,"Removed output argument from *_batch_end hooks (#3965, #3966)",This reverts commit e818e823e315625d06db9316ec62e7f58fc047fd.,0
1183,Barrier (#2257),0.59468657,[1.2.1] - 2021-02-23,Fix tests_pytorch import error,0
1184,Clean docs (#1604),0.6438135,Docs improvements,,0
1185,fix hparams issue (#1623),0.6875936,Fixing critical bugs in newly added hooks and hparams assignment.,,0
1186,drop GH e2e cloud & add cron for Azure (#15306),0.58647066,Refactor cloud dispatch and update to new API (#16456),Bumps pytest-cov from 3.0.0 to 4.0.0. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: pytest-cov   dependency-type: direct:production   update-type: version-update:semver-major ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
1187,Rename loggers/base.py to loggers/logger.py and changed LightningLoggerBase to Logger (#12014),0.82138205,Changed pytorch_lightning.logging to pytorch_lightning.loggers (#767),Updates the requirements on uvicorn to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: uvicorn   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,1
1188,Use a dataclass as the scheduler config (#11443),0.6113155,Implemented DataParallelPlugin._setup_model (#10010),  add basic ssh documentation   rename workflow ssh debugging   Update docs/source-app/workflows/ssh/index.rst   Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com  Update docs/source-app/workflows/ssh/index.rst  Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com  Update docs/source-app/workflows/ssh/index.rst  Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com   add more details about ssh command   Update docs/source-app/workflows/ssh/index.rst   Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com   add more motivation to the audience section   fix sphinx errors   Update docs/source-app/workflows/index.rst   Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com   add details how to get app id   add docs about component name   add more context to the audience section   Update docs/source-app/workflows/ssh/index.rst   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update docs/source-app/workflows/ssh/index.rst  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   address adrians comment about order   add one-time notice   fix headers   wording   update to match ssh params   Update docs/source-app/workflows/ssh/index.rst   Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com  Update docs/source-app/workflows/ssh/index.rst  Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com  Update docs/source-app/workflows/ssh/index.rst  Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com   drop verification   fix merge conflict error   remove symlink   fix doctree   Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
1189,Add possibility for custom naming when using multiple dataloaders (#6274),0.6268153,"Working with multiple dataloaders (#16800, #16753)","  remove source-lit   remove source-lit   remove source-lit   remove source-lit   remove source-lit   remove source-lit   remove source-lit   remove source-lit   remove source-lit   remove source-lit   remove source-lit   remove source-lit   remove source-lit   remove source-lit   remove source-lit   remove source-lit   remove source-lit   remove source-lit   remove source-lit   remove source-lit   remove source-lit   remove source-lit   remove source-lit   remove source-lit   remove source-lit   remove source-lit   remove source-lit   remove source-lit   remove source-lit   remove source-lit   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
1190,An instance of SaveConfigCallback should only save the config once (#14927),0.67040664,"- `SaveConfigCallback` instances should only save the config once to allow having the `overwrite=False` safeguard when using `LightningCLI(..., run=False)` ([#14927](https://github.com/Lightning-AI/lightning/pull/14927))",Co-authored-by: awaelchli aedu.waelchli@gmail.com,0
1191,Delete legacy multinode tests (#11175),0.6044051,remove _evaluate fx (#3197),Co-authored-by: awaelchli aedu.waelchli@gmail.com,0
1192,"Simplify variables: step, epoch, max_epochs, min_epochs (#589)",0.7467693,"Renamed step_idx to step, epoch_idx to epoch, max_num_epochs to max_epochs and min_num_epochs to min_epochs (#589)",,1
1193,"Move epoch_{start,end} hooks from TrainingEpochLoop to FitLoop (#11201)",0.75352323,"Refactored internal loop interface; added new classes FitLoop, TrainingEpochLoop, TrainingBatchLoop (#7871, #8077)",update Co-authored-by: Luca Antiga luca.antiga@gmail.com,1
1194,"Update pandas requirement from <1.5.2,>1.0 to >1.0,<1.5.4 in /requirements (#16472)",0.7199752,Removed dependency on pandas (#736),,1
1195,ci: drop secondary pkg for LAI (#17565),0.56983167,Release LAI docs as stable (#14250),Co-authored-by: Bryn Lloyd lloyd@itis.swiss,0
1196,Add missing deprecation rerouting for add_to_queue in TPUSpawnPlugin #10854,0.6352996,"Deprecated add_to_queue, get_from_queue from LightningModule in favor of corresponding methods in the DDPSpawnPlugin (#9118)",Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: otaj 6065855+otaj@users.noreply.github.com,0
1197,Split callbacks (#849),0.8074451,Split callbacks in multiple files (#849),,1
1198,Updated quantization imports in PyTorch 1.10 (#9878),0.61870724,| Import pytorch_lightning.profiler                                                                          | 1.9             | pytorch_lightning.profilers                   |,,0
1199,fix pip install (#7170),0.6911605,pip install rich,,0
1200,Update new-project.rst (#11593),0.592546,Refactored EpochResultStore (#5522),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
1201,Mark all result classes as protected (#11130),0.6447783,"Marked several methods in PredictionLoop as protected: on_predict_start, on_predict_epoch_end, on_predict_end, on_predict_model_eval (#9516)","  Do not modify PACKAGE_NAME on install   Fix ci pkg action   Required   Typos   Apply suggestions from code review   Undo defaults   Cleanup   Implement idea   Fuck   Apps mock fix   Fix app-pytest with PKG_NAME=app   Justus suggestion   Debug Windows   Update setup.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Revert ""Debug Windows""  This reverts commit 9fe3ba366545ba9e308b0757876279b1fff8a553.   SSH action   Crazy bug   Revert ""SSH action""   This reverts commit 5061e8e7d6fdf47b38c676623b02387cd451d403.   Package import step   Avoid env conflict   Debug   Whitespace   Try removing existing lite build   This should be redundant now   Add back env now that source-lit is gone   Remove download artifact   checkgroup   TODOs suggested by Jirka   _   Revert ""_"". These are local variables, do not need protected   This reverts commit 8340b85991bb6927d851d86861b7efe2c845377b. Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com",0
1202,Pass the scaler as an input to NativeMixedPrecisionPlugin (#10055),0.61669993,NativeMixedPrecisionPlugin and its subclasses now take an optional GradScaler instance (#10055),,0
1203,[docs] Add link to Ray Tune in hyperparameters.rst (#2647),0.6347819,Flattening Wandb Hyperparameters (#2459), skip jobs in draft types,0
1204,Fix docs filterwarnings snippet (#10671),0.6013715,Resolve bug with Finetuning (#5744),,0
1205,fixed hooks,0.72165096,New Hooks,"Revert ""Replace pull_request_target event from docs workflow (#15526)"" This reverts commit 4750e221d1e7622bbf2d0e5b09e7136032e9c410. Co-authored-by: Luca Antiga luca.antiga@gmail.com",1
1206,Update Rich github links (#12718),0.6310679,- Changed minimum supported version of `rich` from `10.14.0` to `12.13.0` ([#16798](https://github.com/Lightning-AI/lightning/pull/16798)),Update governance.rst,0
1207,Remove unnecessary TrainingEpochLoop return (#9298),0.7616433,Removed the deprecated TrainerLoggingMixin class (#8609),  Don't assume script args start with double dash   add changelog   Co-authored-by: Luca Antiga luca@lightning.ai Co-authored-by: awaelchli aedu.waelchli@gmail.com,1
1208,Rename variables (#124),0.5952421,Rename failed -> error in tables (#15608),,0
1209,ref: moved eval loop logging to loggers 1/n (#3408),0.96396124,moved eval loop logging to loggers (#3408),"  merge master   merge master   merge master   merge master   install colors   install colors   install colors   install colors   install colors   install colors   install colors   install colors   docs   docs   docs   docs   docs   docs   docs   docs   Revert ""docs""   This reverts commit c83d9854fb4bb4248b4e478430907cb4aa808b66.  Revert ""docs""  This reverts commit a2bb66d2f06d4e995cefdd079bcdf25f927e92ef.   docs   docs   remove source-lit   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   precommit   files   folder   Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",1
1210,Add dataloader_idx to batch transfer hooks (#6241),0.737385,Raised a MisconfigurationException if batch transfer hooks are overriden with IPUAccelerator (13961),,1
1211,expand eval loop out (#3165),0.9999999,expand eval loop out (#3165),,1
1212,Prevent flickering progress bar (#6009),0.7500148,Changed progress_bar_refresh_rate trainer flag to disable progress bar when setting to 0. (#1108),  remove source-lit   remove source-lit   remove source-lit ,1
1213,v1.2.0rc2 (#6063),0.63069534,0.4.0,  update   update   update   update   update   resolve attachment   update   update   update   update   update   update   update   update   update   update   update   update   update ,0
1214,Copy README.md in preparation for the new one (#13305),0.5064234,use prepare_data to download and process the dataset.,,0
1215,Covv1 (#4072),0.57793367,(#16002),,0
1216,Deprecate pl/utilities/apply_func (#14516),0.64825386,  * `pl.utilities.apply_func` ([#16413](https://github.com/Lightning-AI/lightning/pull/16413)),,0
1217,Refactor GPU examples tests (#8294),0.7696208,GPU training (#2704),Set env var,1
1218,Correct SaveConfigCallback error message (#17119),0.6691456,Replace MisconfigurationException with warning in ModelCheckpoint Callback (#4560),"  placeholder   pytorch   fix CI   fix package name   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   use os theme for pytorch docs   switch source-app to lai_sphinx_theme   pull_request   doc error fix   another build error fix   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   removed unused glossary.rst   lit   doc fixes   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   fix last warning   try   lit   flake8   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: Yurij Mikhalevich yurij@grid.ai Co-authored-by: rohitgr7 rohitgr1998@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
1219,Integrate total_batch_idx with progress tracking (#8598),0.73652947,Progress tracking,,1
1220,Deprecate call_hook (#10979),0.7376444,Removed deprecated callbacks (#3979),  implement ssh command   add tests that ssh command is available   most SSH command tests   update changelog   Update src/lightning_app/cli/lightning_cli.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   adrian feedback pt1   Update src/lightning_app/cli/lightning_cli.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   more feedback   rework to support app name   update tests based on different interface   Update src/lightning_app/cli/lightning_cli.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update src/lightning_app/cli/lightning_cli.py  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update src/lightning_app/cli/lightning_cli.py  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update src/lightning_app/cli/lightning_cli.py  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   update tests with changed expectation   fix tests that broke with introduction of shutils   fix too long line   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
1221,Tensorboard path generalisation (#804),0.70022124,Switch from tensorboard to tensorboardx in TensorBoardLogger (#15728),"  docs   docs updates   docs updates   docs updates   docs updates   d   d   d   d   d   d   ??   d1   d1   d1   d1   d1   d1   d1   d1   d1   d1   d1   d1   d1   d1   d1   d1   d1   d1   d1   d1   d1   d1   d1   d1   d1   d1   d1   d1   d   d   d   d   d   d   d   d   d   d   new title   new title   new title   new title   new title   new title   new title   new title   new title   new title   new title   new title   new title   new title   new title   new title   new title   new title   new title   new title   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   only select from parent   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   use OSS template   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   only select from parent   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  Update docs/README.md  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: William Falcon williamfalcon@Williams-MacBook-Pro-2.local Co-authored-by: William Falcon williamfalcon@Williams-MBP-2.lan Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",1
1222,[CLI] Cluster logs CLI improvements: new log labels + test coverage increasing (#14459),0.5910952,Cleaning up stale logger tests (#3490),,0
1223,Fix mypy typing for utilities.cloud_io.py (#8671),0.58406246,Resolved a bug where the wrong client was passed to collect cloud logs (#14684),,0
1224,Remove metric reset after checkpoint load (#16661),0.721401,Remove MetricsHolder (#7909),,1
1225,Update ddp.py (#11929),0.7075592,    * Renamed the `DDPSpawnPlugin` to `DDPSpawnStrategy` ([#11145](https://github.com/PyTorchLightning/pytorch-lightning/pull/11145)),Only check versions / env when not in the cloud,1
1226,Call LightningDataModule.load_state_dict hook while restoring checkpoint using LightningDataModule.load_from_checkpoint (#14883),0.8922019,- Fixed a missing call to `LightningDataModule.load_state_dict` hook while restoring checkpoint using `LightningDataModule.load_from_checkpoint` ([#14883](https://github.com/Lightning-AI/lightning/pull/14883)),Co-authored-by: Luca Antiga luca@lightning.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: awaelchli aedu.waelchli@gmail.com,1
1227,"Remove unnecessary endpoint logic, rename collaborative to hivemind (#13392)",0.74718755,"    * Removed unnecessary endpoint logic, renamed `collaborative` to `hivemind` ([#13392](https://github.com/Lightning-AI/lightning/pull/13392))",Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
1228,Update changelog after v1.7.4 release (#14479),0.7104575,Full Changelog,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
1229,Typo fix in metrics docs (#2237),0.6835176,"docs for all Metrics (#2184, #2209)",  using lai docs tempalte   sync   prune   lai_sphinx_theme   ls   aws   ext   list   cleaning   pr ,0
1230,add pkg info files (#16610),0.5281356,PyTorch 1.5  support,  CI: filter mypy triggers   Fixes to checkgroup   Remove setup.cfg   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Luca Antiga luca.antiga@gmail.com,0
1231,Update onnxruntime requirement from <=1.12.0 to <1.13.0 in /requirements (#14083),0.5581558,- Do not update on-plateau schedulers when reloading from an end-of-epoch checkpoint ([#14702](https://github.com/Lightning-AI/lightning/pull/14702)), improve srun detection changelog try catch is obsolete  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
1232,Update RichProgressBarTheme after detecting light theme on colab (#10993),0.8719152,- Update `RichProgressBarTheme` styles after detecting light theme on colab ([#10993](https://github.com/PyTorchLightning/pytorch-lightning/pull/10993)),,1
1233,Use .comet.config file for CometLogger (#1913),0.9979584,Using .comet.config file for CometLogger (#1913),  Add auto-upgrade from the CLI and check for current env   No longer require python -m in docs   Tabs -> spaces   Ignore pre-releases   Test + docs ,1
1234,add Any annotation (#16861),0.5144331,"adding compute environments (#3837, [#3842)",,0
1235,Deprecate early_stop_callback Trainer argument (part 2) (#3845),0.9894637,Deprecate early_stop_callback Trainer argument (#3845),  releasing 1.8.1   post1 ,1
1236,Fix #997 (#1018),0.62304175,This release fixes that core issue,"  Remove pytest as a requirment to run app   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
1237,Include link to bug report template in GitHub bug issue (#15270),0.61012244,- Fixed a bug where the upload files endpoint would raise an error when running locally ([#14924](https://github.com/Lightning-AI/lightning/pull/14924)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
1238,Remove reinit_schedulers_properties (#10271),0.68398595,Removed deprecated property ModelCheckpoint.period in favor of ModelCheckpoint.every_n_epochs (#9213),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
1239,[Docs] Note on running metric in dp (#4494),0.75591147,"docs for all Metrics (#2184, #2209)",,1
1240,Deprecate LightningModule.get_progress_bar_dict (#8985),0.86577594,- Removed deprecated `get_progress_bar_dict` property from `LightningModule` ([#12839](https://github.com/Lightning-AI/lightning/pull/12839)), fix typo in workflow pr trigger,1
1241,app: hotfix import pytest (#16510),0.68856144,from pytorch_lightning.plugins import CheckpointIO,,0
1242,"Filter ""unsqueeze"" warning when using DP (#5622)",0.66528624,On DP and DDP2 unsqueeze is automated now (#1319), update docs deploy gcp - bucket,0
1243,Introduce Stateful PrecisionPlugin (#11638),0.7503771,Precision Plugins (#5718), testing fix regex,1
1244,RTFD: building docs for with redirect (#16993),0.5442651,refactored dataloader process hook (#3139),Updates the requirements on psutil to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: psutil   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
1245,"[CI] Move DeepSpeed into CUDA image, remove DeepSpeed install from azure (#6043)",0.64049506,"Changed default for DeepSpeed CPU Offload to False, due to prohibitively slow speeds at smaller scale (#6262)", adjust examples Apply suggestions from code review  Co-authored-by: Akihiro Nitta nitta@akihironitta.com,0
1246,Checkgroup config fixes (#15787),0.6380947,Configuration Validator (#9779),,0
1247,bug fix for #138 (#143),0.639225,Resolve bug with Finetuning (#5744),  [create-pull-request] automated change   Add legacy to checkgroup   Co-authored-by: akihironitta akihironitta@users.noreply.github.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com,0
1248,Update tests to avoid the deprecated weights_summary (#10446),0.62656724,- Removed the deprecated `weights_summary` argument from the `Trainer` constructor ([#13070](https://github.com/Lightning-AI/lightning/pull/13070)),Updates the requirements on setuptools to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: setuptools   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
1249,Small documentation fix (#3589),0.65524924,Resolve bug with Finetuning (#5744),Co-authored-by: awaelchli aedu.waelchli@gmail.com,0
1250,Pure package & base tests (#2418),0.6237124,Implemented ready for components (#16129),Updates the requirements on ipython[all] to permit the latest version. - Release notes - Commits  updated-dependencies: - dependency-name: ipython[all]   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
1251,create meta package [RFC] (#13327),0.74849445,Meta Module,,1
1252,Always run standalone tests (#16705),0.6014305,Disabled val and test shuffling (#1600),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
1253,nb steps in early stop (#3909),0.6281084,EarlyStopping now runs at the end of the training epoch by default (#8286), Use sklearn in runif test by removing sklearn dep remove repeated code seed,0
1254,Minor changes in preparation for saving the loops state (#10783),0.73119843,The Loop's state is now included as part of the checkpoints saved by the library. This enables finer restoration of custom loops., CI: try upload e2e Artifacts Apply suggestions from code review  Co-authored-by: Akihiro Nitta nitta@akihironitta.com,1
1255,ci: fix install lite on GPUs (#15375),0.6848825,GPU training (#2704),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz,0
1256,updated example image,0.5371797,Refactored EpochResultStore (#5522),,0
1257,Update issue template ads (#10310),0.53021514,Included app templates to the lightning and app packages (#13731),rc2,0
1258,Fix docstring typo (#13447),0.5751598,Docs,"  ci: update install lite   try without lite in req file   ci: install   app   init   Revert ""app""   This reverts commit f3f09e7888163db9730012c9efd35d8f2617a0cf.   ci: cpu   ci: gpu   pkg   env   bench   trigger   notes   prune   set version   fix version   git reset   hpu, ipu   adjust   --hard   git checkout   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com   rc2   L   docs   hpu   Co-authored-by: awaelchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Luca Antiga luca.antiga@gmail.com",0
1259,[TPU] v4 support (#17227),0.69761276,Support 8-core TPU on Kaggle,Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
1260,updated demo name,0.51443565,New features,,0
1261,Add warning when not saving hparams,0.7723571,Don't raise a warning when nn.Module is not saved under hparams (#12669),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
1262,Added support for HPU device stats monitor (#13819),0.88054526,Device Stats Monitoring support for HPUs,"What does this PR do? Removes the ability to specify --instance-types when creating clusters. Instead, all clusters will be able to use every instance type supported by the platform.",1
1263,parse strategies as own extras (#12975),0.54166764,parser = argparse.ArgumentParser(),,0
1264,BUG - Wandb: Sanitize callable. (#4320),0.6364182,Allow use of sweeps with WandbLogger (#1512),,0
1265,Fix typo in 1_bug_report.yaml (#15764),0.51746666,Fixing critical bugs in newly added hooks and hparams assignment.,Co-authored-by: awaelchli aedu.waelchli@gmail.com,0
1266,Fix: hparams.yaml saved twice when using TensorBoardLogger (#5953),0.6304395,Parsing of enums type hyperparameters to be saved in the haprams.yaml file by TensorBoard and CSV loggers has been fixed and made in line with how OmegaConf parses it (#9170),,0
1267,Deprecate LightningModule.model_size (#8495),0.9488037,Deprecated LightningModule.model_size (#8343),,1
1268,update PR template (#16620),0.5694981,Refactored EpochResultStore (#5522),,0
1269,added docs for cluster grid search,0.6623682,Enabled custom clusters (#4048),,0
1270,switch azure pool (#10266),0.5186008,Changed to the NeptuneLogger (#16761):,Co-authored-by: awaelchli aedu.waelchli@gmail.com,0
1271,Fix PL docs build on readthedocs.org (#15511),0.54227614,Docs improvements,Update matplotlib requirement in /requirements Updates the requirements on matplotlib to permit the latest version. - Release notes - Commits  updated-dependencies: - dependency-name: matplotlib   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
1272,Fix a typo (#9169),0.59756786,Changed overwrite to True (#16009),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: awaelchli aedu.waelchli@gmail.com,0
1273,ref: inner train loop (intermediate step) 12/n (#3372),0.7833852,"inner train loop (#3359, #3361, #3362, #3363, #3365, #3366, #3367, #3368, #3369, #3370, #3371, #3372, #3373, #3374, #3375, #3376, #3385, #3388, #3397)",fix stale bot language,1
1274,update governance docs (#894),0.9136901,Updated governance docs,increase timeout in pytorch jobs due to long installation,1
1275,Support Mean in DDP Sync (#2568),0.68694204,Ensure the existence of DDPPlugin._sync_dir in reconciliate_processes (#8939), Fix default CloudCompute for flows Unit test added,0
1276,Improve LightningCLI documentation and tests (#7156),0.83889055,LightningCLI Improvements,Avoid using fixed container name,1
1277,Close profiler when StopIteration is raised (#14945),0.624145,Early stopping checks on_validation_end (#1458),,0
1278,Fix initialization of optimizers in DDP Strategy (#11952),0.7677815,"Implemented {DDPShardedPlugin,DDPShardedSpawnPlugin}._setup_model_and_optimizers (#10028, #10064)",  Init   Updates   Add test for model building   Imports   Fix   Typing   Ignore serve streamlit in mypy ,1
1279,Update utilities API references (#11450),0.6646999,Refactor cloud dispatch and update to new API (#16456),,0
1280,updated examples,0.64650965,Noteworthy changes:," add cli commands for adding/ removing resources  as discussed with Adrian, we want to adopt ""lightning add"" and ""lightning remove"" for  ssh-keys, as the resource already exists.  implement ssh-key management one parameter for public key, optional name handle the case where a private key file was provided make ssh key-mgmt support classes protected re-order add ssh-key args change types signatures of add_key rename test cases update changelog  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
1281,ci: fix dependabot (#16749),0.61724645,Porting fixes to autoscaler component (#16249),"as we patch our base images, this e2e image needs to be updated all the time as well. Instead of changing this with a PR all the time this PR makes the e2e container image version configurable through the ENV. Co-authored-by: Jirka jirka.borovec@seznam.cz",0
1282,Nuke RPC (#8101),0.56332946,- Removed `AcceleratorConnector.num_processes` property ([#12388](https://github.com/PyTorchLightning/pytorch-lightning/pull/12388)),,0
1283,"LightningFabric: Error handling for accelerator=""mps"" and ddp strategy pairing (#16455)",0.73309726,Deprecated LightningModule.loaded_optimizer_states_dict (#8229),,1
1284,[docs] Fix truncated_bptt_steps docs (#7846),0.6608377,Deprecated Trainer.truncated_bptt_steps in favor of LightningModule.truncated_bptt_steps (#7323),Co-authored-by: thomas chaton thomas@grid.ai,0
1285,ref: inner train loop (intermediate step) 11/n (#3370),0.7933911,"inner train loop (#3359, #3361, #3362, #3363, #3365, #3366, #3367, #3368, #3369, #3370, #3371, #3372, #3373, #3374, #3375, #3376, #3385, #3388, #3397)",,1
1286,ref: checkpoint connector methods 4/n (#3474),0.82539934,Refactor load in checkpoint connector (#4593),,1
1287,Update training tricks docs (#11169),0.6894554,Refactored training loop (#2336),,0
1288,CI: Fix false negatives by the labeler (#13722),0.5583995,Resolve bug with Finetuning (#5744),Hide region CLI flag,0
1289,Deprecate ModelCheckpoint.save_checkpoint (#12456),0.88747644,Removed the deprecated model argument from ModelCheckpoint.save_checkpoint (#8688),App tests hang on Windows with Python 3.8,1
1290,Update README.md (#3106),0.554614,refactored dataloader process hook (#3139),,0
1291,Add support for (accelerator='cpu'|'gpu'|'tpu'|'ipu'|'auto') (#7808),0.6592576,Support auto_select_gpus with the accelerator and devices API (#12608),,0
1292,tests: drop CircleCI (#2412),0.5667313,tests for val loop flow (#2605),,0
1293,[BugFix] Resolve bugs in computer_vision_fine_tuning.py example (#5985),0.75009,Resolve bug with Finetuning (#5744),  docker: drop pt 1.9   Missed some   Last one   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Luca Antiga luca.antiga@gmail.com,1
1294,Remove deprecated task_idx (#10441),0.73670155,Removed deprecation warnings being called for on_{task}_dataloader (#9279),  update   update   update   update   changelog   update   update   update   update   update   update   update   update   uipdate   update   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
1295,handle keyboard interrupt for ddp .test() (#1019),0.6205691,Run ddp_spawn dataloader checks on Windows (#6930),"What does this PR do? The lightning CLI uses the lightning APIs to create, read, update, and delete resources such as apps and clusters. If an API call fails due to client error (e.g. invalid input, unauthorized, unauthenticated) the API will return an HTTP status code in the 400s, along with a custom message in the response body. The CLI (usually) does not directly handle these exceptions. As a result the exception bubbles up through the click framework (which does not recognize the exception type) and to the python runtime, which produces a long traceback and dumps the raw exception to the user. This PR fixes the user experience. When our API fails on a client error, the CLI will display the message from the server without a traceback.",0
1296,Docs/fixes (#5914),0.6641545,Tons of bug fixes, weird,0
1297,Fix missing secrets in legacy checkpoint workflow (#15358),0.61813974,Saved checkpoints will no longer use the type of a Callback as the key to avoid issues with unpickling (#6886),,0
1298,Remove unnecessary use of comprehension (#8147),0.51614785,"Simplified ""should run validation"" logic (#7682)",,0
1299,Register torch's unresolvable import paths in cli module (#13153),0.69572943,import torch,,0
1300,Fix ddp tests + .test() (#2512),0.65052015,- fixed all the .test() calls,"  placeholder   mirror + prune   makedir   setup   ci   ci   name   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   ci clean   empty   py   parallel   doctest   flake8   ci   typo   replace   clean   Apply suggestions from code review   re.sub   fix UI path   full replace   ui path?   replace   updates   regex   ci   fix   ci   path   ci   replace   Update .actions/setup_tools.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   also convert lightning_lite tests for PL tests to adapt mocking paths   fix app example test   update logger propagation for PL tests   update logger propagation for PL tests   Apply suggestions from code review   Revert ""update logger propagation for PL tests""   This reverts commit c1a5e119c740b5468daac63028de8aa799a177ac.   playwright   py   update import in tests   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   try edit import in overwrite   debug code   rev playwright   Revert ""try edit import in overwrite""   This reverts commit c02f766521c91454be36b19784c6a3ed2f715109.   ci: adjust examples   adjust examples cloud   mock lightning_app   Install assistant dependencies   lightning   setup   Apply suggestions from code review   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   Apply suggestions from code review   disable cache   move doctest to install   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   )   echo ./   ci   lru   revert disabling cache, prints   ci   prune ci jobs   prune ci jobs   training loop standalone tests   add sys modules cleanup fixture   make use of fixture   revert standalone   ci e2e   fix imports in lightning   fix imports of lightning in tests   Revert ""make use of fixture""   This reverts commit c15efdd205da2353187275a8d3da141d0ec0ec0a.   Revert other commits for fixtures   revert use of fixture   py3.9   fix mocking   fix paths   hack mocking   docs   Apply suggestions from code review   rev suggestion   Minor changes to the parametrizations   Update checkgroup with the new and changed jobs   include frontend dir   cli   fix imports and entry point   Revert standalone   rc1   e2e on staging   Revert ""Revert standalone""   This reverts commit 9df96685b866b1719fcdeb0b2e832255e3a5f8c0.   groups   to   ci: pt ver   docker   Apply suggestions from code review   Copy over changes from previous commit to other groups   Add back changes from bad merge   Uppercase step name everywhere   update   ci   ci: lai oldest   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Justus Schock justus.schock@posteo.de Co-authored-by: manskx ahmed.mansy156@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Luca Antiga luca.antiga@gmail.com",0
1301,refactor slurm_job_id (#10622),0.70009845,- Deprecated the property `Trainer.slurm_job_id` in favor of the new `SLURMEnvironment.job_id()` method ([#10622](https://github.com/PyTorchLightning/pytorch-lightning/pull/10622)),,1
1302,Fix _should_reload_dl_epoch causing inconsistent validation dataloader reloading (#11036),0.6617317,refactored dataloader process hook (#3139),,0
1303,Remove docs about automatic fault tolerance (#16500),0.7754611,"Removed experimental fault-tolerance support (#16516, #16533)",,1
1304,Add toma comments to auto_scale_batch_size (#1994),0.64372516,tuner.scale_batch_size(...),Update pandas requirement in /requirements Updates the requirements on pandas to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: pandas   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
1305,[bug/feat] Support parameters_to_ignore in DDP (#7239),0.6497857,Removed support for the DDP2 strategy,  replace oldest in lite   Fix PyTorch versions in Lite CI   This will be moved to install pkg workflow in the mirror PR   1.13 fixes   Windows fix   sorting   Co-authored-by: otaj ota@lightning.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
1306,removed self.model refs,0.6530816,Removed the deprecated model argument from ModelCheckpoint.save_checkpoint (#8688),,0
1307,Relax on_train_batch_* hook check with dataloader_iter to a warning (#16062),0.78783166,"Overriding the on_train_batch_{start,end} hooks in conjunction with taking a dataloader_iter in the training_step no longer errors out and instead shows a warning (#16062)",,1
1308,Fix typo in comet.py (#16326),0.6176516,Using .comet.config file for CometLogger (#1913), New running conditions for tests found one more mistake,0
1309,Organize trainer properties (#7758),0.722781,Removed TrainerProperties mixin and moved property definitions directly into Trainer (#9495),,1
1310,Fix AttributeError when using CombinedLoader in prediction (#11111),0.596756,"Refactored prediction loop interface; added new classes PredictionLoop, PredictionEpochLoop (#7700, #8077)", extend interactive mode detection update test names changelog test,0
1311,Fix torchtext data to gpu (#4785),0.65116525,"Unify sLURM/TorchElastic under backend plugin (#4578, #4580, #4581, #4582, #4583)",Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
1312,Fix docs typo (#2747),0.51639843,Rename failed -> error in tables (#15608),,0
1313,Corrected typo python -m pip pre-commit install (#2447),0.6240041,"PyTorch 1.10 and Python 3.7 no longer supported (#16492, #16579)",,0
1314,Fix outdated docs (#1227),0.5639214,Docs improvements, mark internal lite apis as protected formatting docs update  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
1315,Construct the hook kwargs inside each loop (#11511),0.6919193,moved hooks around in eval loop (#3195),,0
1316,Remove the deprecated pytorch_lightning.core.lightning module (#16318),0.96607697,Removed the deprecated pytorch_lightning.core.lightning module in favor of pytorch_lightning.core.module (#16318),,1
1317,Fixed a crash bug in MLFlow logger  (#4716),0.66903853,bug fix with logging val epoch end + monitor (#3812),,0
1318,Mark callback_connector as protected (#10121),0.6453701,"- Marked the `{Accelerator,Signal,Callback,Checkpoint,Data,Logger}Connector` classes as protected ([#17008](https://github.com/Lightning-AI/lightning/pull/17008))",Narrow CI timeouts,0
1319,Add DDPSpawnPlugin.spawn() (#10018),0.6633537,"Implemented {DDPShardedPlugin,DDPShardedSpawnPlugin}._setup_model_and_optimizers (#10028, #10064)",,0
1320,Validate the precision input earlier (#9763),0.65574574,Renaming of precision recall metric (#3308),"Stops a bug when cross-launching an app between clusters. Currently the platform does not allow running multiple app instances. If you have app-1 running on cluster-1 and try to run it on cluster-2, the CLI will succeed but the app will never start. This PR prevents this disconnect. The app should not be uploaded / released if it won't run. An error is presented to the user explaining what happened and how to proceed (specify a different --name: e.g. app-2). Once the platform supports multiple app instances / running individual apps on multiple clusters, this PR can be reverted.",0
1321,"Update torchmetrics requirement from <0.10.1,>=0.7.0 to >=0.7.0,<0.11.1 in /requirements (#15904)",0.7599963,"Set PyTorch 1.4 as min requirements, also for testing and examples torchvision>=0.5 and torchtext>=0.5 (#5418)","  Update pl CPU testing matrix   Remove standalone comment, could be confused   Heurisitc for PyTorch latest   partition rest   ckpgoup   These do not exist   This ALSO does not exist ",1
1322,Loop Refactor 4/N - Remove Old Evaluation Loop (#8056),0.7081081,Refactored trainer _run_* functions and separate evaluation loops (#8065), Fix GH e2e cloud drop gha cron,1
1323,add doctests for example 2/n segmentation (#5083),0.5919411,Updated semantic segmentation example with custom u-net and logging (#1371),"  use more recent lightning cloud launcher   allow LightningApp to use custom cloud compute for flows   feedback from adrian   adjust other cloud tests   update   update   update commens   Update src/lightning_app/core/app.py   Co-authored-by: Sherin Thomas sherin@grid.ai   Close profiler when StopIteration is raised (#14945)   Find last checkpoints on restart (#14907)   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   Remove unused gcsfs dependency (#14962)   Update hpu mixed precision link (#14974)   Signed-off-by: Jerome janand@habana.ai  Bump version of fsspec (#14975)  fsspec verbump   Fix TPU test CI (#14926)   Fix TPU test CI   +x first   Lite first to uncovert errors faster   Fixes   One more   Simplify XLALauncher wrapping to avoid pickle error   debug   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Debug commit successful. Trying local definitions   Require tpu for mock test   ValueError: The number of devices must be either 1 or 8, got 4 instead   Fix mock test   Simplify call, rely on defaults   Skip OSError for now. Maybe upgrading will help   Simplify launch tests, move some to lite   Stricter typing   RuntimeError: Accessing the XLA device before processes have spawned is not allowed.   Revert ""RuntimeError: Accessing the XLA device before processes have spawned is not allowed.""   This reverts commit f65107ebf3e062d497f1033bfbbd59774f2d253f.   Alternative boring solution to the reverted commit   Fix failing test on CUDA machine   Workarounds   Try latest mkl   Revert ""Try latest mkl""   This reverts commit d06813aa67cc161879775e24be24b735e2925555.   Wrong exception   xfail   Mypy   Comment change   Spawn launch refactor   Accept that we cannot lazy init now   Fix mypy and launch test failures   The base dockerfile already includes mkl-2022.1.0 - what if we use it?   try a different mkl version   Revert mkl version changes   Co-authored-by: awaelchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com   Trainer: fix support for non-distributed PyTorch (#14971)   Trainer: fix non-distributed use   Update CHANGELOG   fixes typing errors in rich_progress.py (#14963)   revert default cloud compute rename   allow LightningApp to use custom cloud compute for flows   feedback from adrian   update   resolve merge with master conflict   remove preemptible   update CHANGELOG   add basic flow cloud compute documentation   fix docs build   add missing symlink   try to fix sphinx   another attempt for docs   fix new test   Signed-off-by: Jerome janand@habana.ai Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Sherin Thomas sherin@grid.ai Co-authored-by: Ziyad Sheebaelhamd 47150407+ziyadsheeba@users.noreply.github.com Co-authored-by: otaj 6065855+otaj@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jerome Anand 88475913+jerome-habana@users.noreply.github.com Co-authored-by: awaelchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Adam J. Stewart ajstewart426@gmail.com Co-authored-by: DP 10988155+donlapark@users.noreply.github.com",0
1324,"Makefile: Refer to CONTRIBUTING doc, reword test to avoid ""example"" (#5910)",0.48740405,Show a message when BuildConfig(requirements=[...]) is passed but a requirements.txt file is already present in the Work (#15799),Pick queue type only if specified (#15295),0
1325,Remove deprecated 'terminate_on_nan' argument from Trainer (#12553),0.81381154,Deprecated Trainer argument terminate_on_nan in favor of detect_anomaly(#9175),  example full tests on master   Modify checkgroup   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
1326,Fix pre-commit isort failure on tests/trainer/*.py (#5421),0.6745021,Dropped official support/testing for PyTorch <1.6 (#8288),"  update   prune   examples   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
1327,[checkpoint logic] Fix bug which doesn't account for NoneType for model.hparams (#1817),0.63882095,Replace MisconfigurationException with warning in ModelCheckpoint Callback (#4560),  hot fix   update   update   update   cmd   pkg   update   Apply suggestions from code review   update   update   Apply suggestions from code review   update   update   update   update   update   update   update   update   update   update   update   update   update   update   cleaning   Apply suggestions from code review   update   update   Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
1328,Loaders (#422),0.6229649,loading,,0
1329,Add support for async method and remove context PythonServer (#16453),0.7672686,Add support for async predict method in PythonServer and remove torch context (#16453),  Remove examples and loggers from develop dependencies   remove more references   Fix mypy   Keep logger file for docs mocking   Simpler fix   Fix docs build   Global testsetup   Matching files   Undo change   loggers as info   Clarify   Update requirements/pytorch/loggers.info   Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: otaj 6065855+otaj@users.noreply.github.com,1
1330,Reset epoch progress with batch size scaler (#13846),1.0000001,Reset epoch progress with batch size scaler (#13846),  Fix reference error   Skip flaky hanging test   .   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
1331,Fix entry point test for Python 3.10 (#14154),0.6806458,python script.py test,  fix requirements   https   verbose   Co-authored-by: Luca Antiga luca.antiga@gmail.com,0
1332,Override optimizer_zero_grad when using the IPUStrategy (#12913),0.8000212,Stopped optimizer_zero_grad from being called after IPU execution (#12913),"  config fixes   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",1
1333,remove deprecated model_size from LightningModule (#12641),0.89523727,Deprecated LightningModule.model_size (#8343), Bump google-github-actions/get-gke-credentials from 0.2.1 to 0.8.2  Bumps google-github-actions/get-gke-credentials from 0.2.1 to 0.8.2. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: google-github-actions/get-gke-credentials   dependency-type: direct:production   update-type: version-update:semver-minor ... Signed-off-by: dependabot[bot] support@github.com   Update .github/workflows/tpu-tests.yml   Don't use deprecated credentials input   Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com,1
1334,[docs]: pass parser to Trainer.add_argparse_args() (#7029),0.88756233,parser = Trainer.add_argparse_args(parser),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
1335,made imports absolute,0.72325754,import argparse,,1
1336,Deprecate TrainerDataLoadingMixin and move logic to DataConnector (#11282),0.70258427,Removed trainer.reset_*_dataloader() methods (#16726),,1
1337,fix tests req.,0.6226584,- fixed all the .test() calls,  update   update   update   update   update   update   update   update   update   update   update   Update .azure/app-cloud-e2e.yml   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  update  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
1338,Merge pull request #8174 from PyTorchLightning/bugfix/8159_log_gpu_memory_on_step,0.66112983,- Fixed logging to loggers with multiple eval dataloaders ([#12454](https://github.com/PyTorchLightning/pytorch-lightning/pull/12454)),  Disable package parametrizations until they are fixed   Apply suggestions from code review   Reword   What does this mean?   Missed some   Update .github/workflows/ci-app-examples.yml   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   Update .github/workflows/ci-app-examples.yml   trigger ci   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: thomas chaton thomas@grid.ai,0
1339,docs (#4107),0.75992376,"docs for all Metrics (#2184, #2209)",Co-authored-by: Mauricio Villegas mauricio_ville@yahoo.com,1
1340,Drop PyTorch 1.8 support (#13155),0.9696913,Drop PyTorch 1.9 support (#15347),"  cleaning   adjust examples   fix some imports   fix imports   ci   fixing   cmd_install._install_app   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   bloody queues   queue   mock   queue   scope   win   Co-authored-by: otaj ota@lightning.ai Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",1
1341,fixed ddp crash,0.6676099,"refactor DDP backend (#3204, #3207, #3208, #3209, #3210)",Co-authored-by: Akihiro Nitta nitta@akihironitta.com,0
1342,store: update API in messages (#16535),0.6728307,Refactor cloud dispatch and update to new API (#16456), Update to the latest playwright container v1.27.1  Co-authored-by: Jirka jirka.borovec@seznam.cz,0
1343,Add troubleshooting section to MPS docs (#14642),0.67719245,Fixing critical bugs in newly added hooks and hparams assignment.,  Ci: fix install pkg name   param ,0
1344,Docs5 (#1033),0.7089499,Docs,Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
1345,ci/docs: wheels from cache (#17201),0.48890388,"docs for all Metrics (#2184, #2209)",testing monolithic package,0
1346,nightly releases (#3552),0.65470266,This release includes:, update mypy version type-ignore-comments more mypy-fix import-fix Update Lite too simpler implementation for flatten dict Fix rich progress Simplify rich test True None  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
1347,reduce to 0.22,0.64131933,reduced all simplified forward (#3126), CI: apps fix imports req doctest SQLModel fsspec azure docs falek8  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
1348,Fault Tolerant Manual: Add _rotate_worker_indices utility (#10647),0.73107517,- Fault Tolerant Manual,,1
1349,Enable quick-start-app-e2e (#14542),0.5079145,Improved support for running apps when dependencies aren't installed (#15711),,0
1350,Add support for IterableDatasets everywhere (#1104),0.76648724,Make the _LiteDataLoader an iterator and add supports for custom dataloader (#10279),,1
1351,fix warnings on windows (#3555),0.5847888,warning_utils >> warnings,,0
1352,Docs: upgrade packages (#5600),0.65596294,Docs improvements,,0
1353,Simplify list extension (#13435),0.66852236,Simplify optimization Logic (#4984),,0
1354,Update experiment_logging.rst (#1066),0.62600476,Cleaning up stale logger tests (#3490), publishing workflow Apply suggestions from code review  Co-authored-by: otaj 6065855+otaj@users.noreply.github.com,0
1355,[feat] 3/n pp (#5036),0.6681441,"Epoch 3:  50%|█████     | 16/32 [00:36<01:32, 23.12it/s]","  Migrate TPU tests to GitHub actions   No working dir   Keep _target   Dont skip draft   CHECK_SLEEP   Not yet   Remove recurrent cleanup script   Set secrets   a step cannot have both the uses and run keys   Version $PYTHON_VER was not found in the local cache   can't load package ... ($GOPATH not set)   The set-env command is disabled   Try updating go   Match timeout   simplify path   More cleanup   Install coverage. Unmark draft   Update .github/workflows/ci-pytorch-test-tpu.yml   DEBUG echo   Revert ""DEBUG echo""   This reverts commit 4011856e6ea076e45fe40b942c20ee63ed7433f3.   More debug   SSH   Im stupid   Remove always()   Forgot some   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Luca Antiga luca.antiga@gmail.com",0
1356,skip some docker builds (temporally pass) (#3913),0.54135954,Cleanup cluster waiting (#16054),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
1357,Fix deterministic behavior in ddp_spawn (#3573),0.6335115,Changed warnings and recommendations for dataloaders in ddp_spawn (#6762),,0
1358,Replace pull_request_target event from docs workflow (#15526),0.5927805,refactored dataloader process hook (#3139),,0
1359,pin sphinx-autodoc-typehints version to v1.15 (#11400),0.9213443,Pinned sphinx-autodoc-typehints with <v1.15 (#11400),,1
1360,fix PL release docker (#13439),0.6273296,This release fixes that core issue,,0
1361,Mark swa_lrs argument in StochasticWeightAveraging callback as required (#12556),0.7774205,- Marked `swa_lrs` argument in `StochasticWeightAveraging` callback as required ([#12556](https://github.com/Lightning-AI/lightning/pull/12556)),Co-authored-by: otaj ota@lightning.ai,1
1362,release 0.7.2rc4 (#1402),0.73211336,0.4.0,  renamed Mount argument   fix tests   Apply suggestions from code review   Co-authored-by: Luca Antiga luca.antiga@gmail.com  updated examples as well  Co-authored-by: Luca Antiga luca.antiga@gmail.com,1
1363,Multi-node documentation for Fabric (#16495),0.66565216,Fabric,  update   update   update   update   update   update   update   wording   Co-authored-by: Mansy ahmed.mansy156@gmail.com   update   update   update   update   update   update   update   update   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Mansy ahmed.mansy156@gmail.com,0
1364,add CI for building dockers (#3383),0.5552068,"adding compute environments (#3837, [#3842)",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
1365,Mock packages for RTD docs build (follow up to doctests) (#1739),0.55265224,"For a full tutorial and running example, visit our docs. TODO: add to docs",,0
1366,fixed error with shorter batch cycles,0.6334101,set validation to a fix number of batches,  update   update   update   Apply suggestions from code review   Apply suggestions from code review   Apply suggestions from code review   ll   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Luca Antiga luca.antiga@gmail.com,0
1367,ref: clean up ddp before final fix (#3817),0.6952108,"refactor DDP backend (#3204, #3207, #3208, #3209, #3210)", Update assistant and workflow files Update .actions/assistant.py  Co-authored-by: otaj 6065855+otaj@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: otaj ota@lightning.ai,0
1368,Remove memory-retaining epoch-end hooks (#16520),0.6682543,The *_epoch_end hooks were removed (#16520), Collective's PREMUL_SUM support with PyTorch 1.13 Fix test Skip under 1.13,0
1369,Bugfix/lr finder (#1676),0.6254643,tuner.lr_find(...),,0
1370,Remove partitioning of model in ZeRO 3 (#10655),0.5120107,Removed deprecated model hooks (#3980),command -v Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,0
1371,[bugfix] Revert inference mode support from #8813 (#9443),0.6340668,"Removed experimental fault-tolerance support (#16516, #16533)",,0
1372,elaborate on the correlation between overfit_pct and xxx_percent_check (#132),0.7109759,overfit_pct in favour of overfit_batches,[App] Authentication for HTTP queue (#15202),1
1373,[App] Fix AutoScaler example (#16557),0.7826859,Porting fixes to autoscaler component (#16249), rename instances of lightning_app and pytorch_lightning solve failing azure tests wrong runif import lit_app  Co-authored-by: Jirka jirka.borovec@seznam.cz,1
1374,Add property to skip restoring optimizers and schedulers via plugin (#8644),0.67681915,Allowed training type plugin to delay optimizer creation (#6331),Make lite tests safe for combined package Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,0
1375,Drop greetings comment (#5563),0.5271553,@pritamsoni-hsr,,0
1376,removed support for EvalResult and TrainResult (#3968),0.9797447,Removed support for EvalResult and TrainResult (#3968), Apply suggestions from code review enable CI to run for PT 1.13  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
1377,Add Promoted CLI to API Reference Section (#14072),0.6736525,Add --app_args support from the CLI (#13625), bump minimal requirement include app requirements in oldest,0
1378,Drop Python 3.6 support (#11117),0.93704987,Drop Python 3.6 support,  added mount class and configured it into compute config   added mount to the cloud runtime dispatcher   raise error if s3 bucket is passed to a drive telling the user to utilize mounts   added example for app   udpated tests   updated tests   addressed code review comments   fix bug   bugfix   updates'   code review comments   updates   fixed tests after rename   fix tests ,1
1379,Un-skip some Horovod tests (#8676),0.6966773,horovod deprecation (#16141),,0
1380,Fix some pyright member access errors in training module (#2121),0.6361862,Removed pytorch_lightning/trainer/training_loop.py (#7985), switch LAI deployment branch update links,0
1381,Rename and move Result (#7736),0.62591237,Renames model steps (#1051),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
1382,check if checkpoint_callback exists (#2832),0.6637379,Resuming from checkpoints (#16167),,0
1383,prepare v1.1.8 (#5839),0.6611992,Implemented ready for components (#16129), extras test.txt doctest Apply suggestions from code review Fix imports Oops  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
1384,Removed weights_summary argument from Trainer (#13070),0.77856994,- Removed the deprecated `weights_summary` argument from the `Trainer` constructor ([#13070](https://github.com/Lightning-AI/lightning/pull/13070)),,1
1385,Do not print empty evaluation result tables (#12427),0.674369,- Do not print an empty table at the end of the `EvaluationLoop` ([#12427](https://github.com/PyTorchLightning/pytorch-lightning/pull/12427)),,0
1386,refactor: move RunIf to PL pkg (#16923),0.62066656,Renamed TPUHalfPrecisionPlugin to TPUBf16PrecisionPlugin (#10026),,0
1387,update chlog after 1.0.7 release (#4735),0.6661671,0.4.0,,0
1388,Update standalone tests (#12472),0.6437024,Deprecated the TestTubeLogger (#9065), use native oldest for package print adjust,0
1389,(app) Resolve a bug where the state changes isn't detected properly (#14465),0.6090522,Resolved a bug where the work statuses will grow quickly and be duplicated (#13970),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
1390,Enforce an epoch scheduler interval when using SWA (#6588),1.0000002,Enforce an epoch scheduler interval when using SWA (#6588),,1
1391,Use viewcode extension instead of linkcode (#3503),0.5674128,Changed computer_vision_fine_tunning example to use BackboneLambdaFinetuningCallback (#5377),,0
1392,Docs2 (#1069),0.714499,Docs,,1
1393,Drop support for Python 3.7 (#16579),0.9024943,Drop Python 3.6 support,,1
1394,Update fsspec dependency and remove un-needed code (#7210),0.67033035,Changed fsspec to tuner (#4458),,0
1395,[bugfix] Accumulated_gradient and TensoBoard (#4738),0.6203309,Resolve bug with Finetuning (#5744),Update gym[classic_control] requirement in /requirements Updates the requirements on gym[classic_control] to permit the latest version. - Release notes - Commits  updated-dependencies: - dependency-name: gym[classic_control]   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
1396,Update README page in pl_examples folder (#10114),0.54758984,Simplify the PL examples structure (shallower and more readable) (#1247),Update torchmetrics requirement in /requirements Updates the requirements on torchmetrics to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: torchmetrics   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
1397,Update multi_node_cluster_auto_slurm.py,0.5751828,"- Removed deprecated arguments `num_nodes` and `sync_batchnorm` from `DDPPlugin`, `DDPSpawnPlugin`, `DeepSpeedPlugin` ([#10357](https://github.com/PyTorchLightning/pytorch-lightning/pull/10357))",Bumps JamesIves/github-pages-deploy-action from 4.4.0 to 4.4.1. - Release notes - Commits  updated-dependencies: - dependency-name: JamesIves/github-pages-deploy-action   dependency-type: direct:production   update-type: version-update:semver-patch ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
1398,fix merge issue (#12420),0.6642908,"merge backends (#3476, #3477, #3478, #3480, #3482)",,0
1399,update chlog after 0.5.5 (#14133),0.6640048,0.4.0,Update comet-ml requirement in /requirements Updates the requirements on comet-ml to permit the latest version.  updated-dependencies: - dependency-name: comet-ml   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
1400,Split GPUStatsMonitor function (#3644),0.72177213,refactored GPU backend __step (#3120),Update neptune-client requirement in /requirements Updates the requirements on neptune-client to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: neptune-client   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,1
1401,release v0.3.1,0.8275001,"    version=""0.0.1"",",,1
1402,CI/CD: Add CUDA version to docker image tags (#13831),0.5612072,A header with the version that generated the config is now included.,Co-authored-by: awaelchli aedu.waelchli@gmail.com,0
1403,Tests for fixed TypeError (#14821),0.6008864,Enabling val/test loop disabling (#2692),,0
1404,Remove extraneous f character from f-string. (#679),0.580153,remove _evaluate fx (#3197),,0
1405,Fix merge _module_available,0.59702057,Module imports are now catching ModuleNotFoundError instead of ImportError (#9867),,0
1406,Remove TrainerModelHooksMixin (#10322),0.8753567,Removed the deprecated TrainerLoggingMixin class (#8609),,1
1407,Replace PostLocalSGDOptimizer with a dedicated model averaging component (#12378),0.7458152,- Replaced PostLocalSGDOptimizer with a dedicated model averaging component ([#12378](https://github.com/PyTorchLightning/pytorch-lightning/pull/12378)),"  fix import to be true stand-alone   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",1
1408,Move device parser utility function (#10230),0.6748928,"device parser (#3400, #3405)",,0
1409,Restore log step during restart (#13467),0.7097036,Removed LoggerStages (#5673),  Prepare changelog for 1.8   update   update   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
1410,Metric ddp bugfix (#4482),0.6712566,Silenced some warnings. verified ddp refactors (#3483),,0
1411,Renamed lr_dict to lr_scheduler_config (#9313),0.6676035,lr_scheduler now activated after epoch    ,Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
1412,Temporary Fix for docs build failure (#1413),0.55634326,Fixing critical bugs in newly added hooks and hparams assignment.,,0
1413,Fix Wrong exception message (#5492),0.6332086,Replace MisconfigurationException with warning in ModelCheckpoint Callback (#4560),,0
1414,Remove warning (#1634),0.71932155,Removed Warning from trainer loop (#1634),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
1415,Fix old_loader attribute in name _PatchDataLoader.__init__ (#9087),0.67031723,Restore original loaders if replaced by entrypoint (#8885),,0
1416,Broadcast dirpath for tighter consistency in model checkpoint callback (#6978),0.6168591,Refactor load in checkpoint connector (#4593),,0
1417,CI/CD: Update base image nvidia/cuda from 11.1 to 11.1.1 (#14019),0.5805286,nvidia/apex deprecation (#16039),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
1418,moves sync bn to each backend (#3925),1.0,moves sync bn to each backend (#3925),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
1419,Add model summary when using DeepSpeed Stage 3 (#13427),0.80279875,- Fixed Model Summary when using DeepSpeed Stage 3 ([#13427](https://github.com/Lightning-AI/lightning/pull/13427)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
1420,Run CI (#6402),0.5640801,Introducing CLI commands for apps (#13602)!,,0
1421,Fix profiler test on Windows minimal (#8556),0.5724046,Disabled optimizers setup during testing (#3059),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,0
1422,[pre-commit.ci] pre-commit suggestions (#12613),0.51407814,The preemption/termination signal is now configurable (#14626):,,0
1423,Example and documentation for LightningCLI linking model and data arguments (#7299),0.7144949,Example using the LightningCLI:,  secrets docs   Update docs/source-app/glossary/secrets.rst   Co-authored-by: Yurij Mikhalevich yurij@grid.ai  Apply suggestions from code review  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   Update secrets.rst   links   Co-authored-by: Yurij Mikhalevich yurij@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka jirka.borovec@seznam.cz,1
1424,Refactor 1: moved tpu xxx_step to backend (#3118),0.9247776,moved TPU xxx_step to backend (#3118),,1
1425,update logging docs and decorators (#4431),0.6556214,Refactored logging,,0
1426,[bug-fix] Trainer.test points to latest best_model_path (#5161),0.7869668,trainer.test(ckpt_path=None) # load best model (NEW BEHAVIOR!),Co-authored-by: HELSON c2h214748@gmail.com Co-authored-by: rohitgr7 rohitgr1998@gmail.com Co-authored-by: otaj ota@lightning.ai Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
1427,Lightning 1.5 release (#10306),0.79816175,Lightning 2.0 is the official release for Lightning Fabric :tada:,,1
1428,feat(wandb): save models on wandb (#1339),0.6825477,Allow uploading models on W&B (#1339), Remove unused Lite code Remove duplicate import Group variable Fix monkeypatch,0
1429,added coverage badge,0.52553564,New features,Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
1430,[CLI] Support shorthand for loggers (#11533),0.7348757,"Finally, loggers are also now configurable with shorthand:",,1
1431,replace pyright by mypy (#5021),0.68051064,"Removed PyTorch 1.6 support (#10367, #10738)",,0
1432,model_checkpoint to save all models (#1359),0.7951455,Removed the deprecated model argument from ModelCheckpoint.save_checkpoint (#8688),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: otaj ota@lightning.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
1433,merge,0.72803664,),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
1434,testing multiple calles,0.5908265,- Full tests that run multiple models in different configs,,0
1435,[docs] Add ananthsub to core (#5476),0.54102296,Allowing decorate model init with saving hparams inside (#4662),Update gym[classic_control] requirement in /requirements Updates the requirements on gym[classic_control] to permit the latest version. - Release notes - Commits  updated-dependencies: - dependency-name: gym[classic_control]   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
1436,Update README.md (#3289),0.5425439,refactored dataloader process hook (#3139),,0
1437,ref: moved ___step_end hooks (#3130),0.95995975,moved ___step_end hooks (#3130),Signed-off-by: Max Ehrlich max.ehr@gmail.com Co-authored-by: awaelchli aedu.waelchli@gmail.com,1
1438,"Update sharded install to latest fairscale release, add reasoning why fork required for sequential parallelism (#5380)",0.63963544,"In this release, we've added support for Distributed Data Parallel in Jupyter notebooks using the fork mechanism to address these shortcomings. This is only available for MacOS and Linux (sorry Windows!).",,0
1439,Refactor supporters (#16662),0.55447876,Refactoring,Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
1440,Merge pull request #1624 from PyTorchLightning/tests/cache,0.6566629,"- Fixed logging on `{test,validation}_epoch_end` with multiple dataloaders ([#11132](https://github.com/PyTorchLightning/pytorch-lightning/pull/11132))",,0
1441,Update CHANGELOG after the 1.6.2 release (#12904),0.68896693,Here is a selection of important changes that are not backward compatible with versions < 1.5. The full list of changes and removals are listed in the changelog at the bottom.,,0
1442,Update fairscale version (#11567),0.6215247,"For reference, FairScale's implementation can be used with",Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
1443,[App] HTTP Removing Queue health check from Individual App (#15023),0.56590825,Removed implicit ui testing with testing.run_app_in_cloud in favor of headless login and app selection (#16741),,0
1444,Fix LightningCLI signature parameter resolving for some lightning classes (#13283),0.91449344,- Fixed ``LightningCLI`` signature parameter resolving for some lightning classes ([#13283](https://github.com/Lightning-AI/lightning/pull/13283)),,1
1445,"Revert ""Support serialized checkpoint loading (#9605)"" (#10057)",0.73268086,Resuming from checkpoints (#16167),Co-authored-by: otaj ota@lightning.ai,1
1446,Fix collective tests with PyTorch 1.13 (#15167),0.67508245,Skipped testing with PyTorch 1.7 and Python 3.9 on Ubuntu (#11217),remove unnecessary version check,0
1447,Remove deprecated accelerator pass through functions in Accelerator (#10403),0.7304567,- Removed deprecated passthrough methods and properties from `Accelerator` base class:, feat: add base path uvicorn fix arg Add prefix update with base_path fix replace base path with root path Apply suggestions from code review  Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Kaushik B kaushikbokka@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
1448,[Docs] fix on_after_backward example (#5278),0.572492,The PrecisionPlugin.backward hooks no longer returns a value (#8328),  removing expensive health check from Queue abstraction   removing expensive health check from Queue abstraction ,0
1449,fix MANIFEST,0.69790494,Tons of bug fixes,[App/Feature] HTTP Queues (#14978),0
1450,Add Trainer(gradient_clip_algorithm='value'|'norm') (#6123),0.72222,adding Trainer.tune() (#3293),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
1451,CI: use testing with PL released (#13647),0.5772099,Extensively tested code.   , add BatchSizeFinderCallback callback enable fast_dev_run test keep tune and remove early_exit move exception to setup Apply suggestions from code review  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Laverne Henderson laverne.henderson@coupa.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
1452,Comet.ml logger - add usage tracking (#14906),0.74493283,More logger diversity including Comet.ml., move config Apply suggestions from code review  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: otaj 6065855+otaj@users.noreply.github.com,1
1453,Move the lightning_optimizers ownership to the Strategy (#11444),0.86859393,- Moved ownership of the lightning optimizers from the `Trainer` to the `Strategy` ([#11444](https://github.com/PyTorchLightning/pytorch-lightning/pull/11444)),Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: otaj 6065855+otaj@users.noreply.github.com,1
1454,[App] Add missing python-multipart dependency (#17244),0.6159651,Improved support for running apps when dependencies aren't installed (#15711),"  update optimizer typing   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   forgot one file   update types   hopefully_last   zero grad not required as can also be done on model   consistency with other typing annotations   revert for deepspeed   Update deepspeed.py   Update deepspeed.py   revert for base plugin   Update types.py   add protocol inheritance   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update typing for precision plugin   Update module.py   typo   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
1455,Refactor PyTorch profiler 4/5 (#6349),0.7391591,PyTorch 1.5  support, Remove existing weekly reset logic clear cache every week Use main tag,1
1456,Update Fabric README (#17112),0.64010125,Fabric,,0
1457,remove extra kwargs from Trainer init (#1820),0.7193016,Removed the deprecated TrainerTrainingTricksMixin class (#8679),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
1458,ref: unify slurm and TE under backendPlugin 3/n (#4581),0.73402286,"Unify sLURM/TorchElastic under backend plugin (#4578, #4580, #4581, #4582, #4583)",,1
1459,[CLI] Fix SaveConfigCallback with DDP spawn (#12011),0.60024476,Ensure the existence of DDPPlugin._sync_dir in reconciliate_processes (#8939),,0
1460,Resolve s3 drive issue where the root folder doesn't need to exist locally (#15127),0.58428377,Prevent to cd into non-existent folders (#16645),,0
1461,Disable lr_scheduler.step() in manual optimization (#6825),0.973197,Disabled lr_scheduler.step() in manual optimization  (#6825),[App/Improvement] Cleaning up Queue abstraction (#14977),1
1462,Document gradient clipping in Fabric (#16943),0.7000474,Gradient Clipping Customization,,1
1463,Fix root logger propagation (#15206),0.7267803,Return the default_root_dir as the log_dir when the logger is a LoggerCollection (#8187),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
1464,ReduceLROnPlateau doc fixed (#4459),0.6755566,Refactored EpochResultStore (#5522),"  Remove the deprecated device_stats_monitor_prefix_keys   Added pr no to changelog.md   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: otaj 6065855+otaj@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com",0
1465,docs: typo in habana docs (#16910),0.54154646,Docs,,0
1466,fix duplicate console logging bug v2 (#6275),0.6541525,Cleaning up stale logger tests (#3490), Trainer: fix non-distributed use Update CHANGELOG,0
1467,Add fsspec to tuner  (#4458),0.9342813,Changed fsspec to tuner (#4458),"  Fix TPU test CI   +x first   Lite first to uncovert errors faster   Fixes   One more   Simplify XLALauncher wrapping to avoid pickle error   debug   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Debug commit successful. Trying local definitions   Require tpu for mock test   ValueError: The number of devices must be either 1 or 8, got 4 instead   Fix mock test   Simplify call, rely on defaults   Skip OSError for now. Maybe upgrading will help   Simplify launch tests, move some to lite   Stricter typing   RuntimeError: Accessing the XLA device before processes have spawned is not allowed.   Revert ""RuntimeError: Accessing the XLA device before processes have spawned is not allowed.""   This reverts commit f65107ebf3e062d497f1033bfbbd59774f2d253f.   Alternative boring solution to the reverted commit   Fix failing test on CUDA machine   Workarounds   Try latest mkl   Revert ""Try latest mkl""   This reverts commit d06813aa67cc161879775e24be24b735e2925555.   Wrong exception   xfail   Mypy   Comment change   Spawn launch refactor   Accept that we cannot lazy init now   Fix mypy and launch test failures   The base dockerfile already includes mkl-2022.1.0 - what if we use it?   try a different mkl version   Revert mkl version changes   Co-authored-by: awaelchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com",1
1468,"Update tensorboard requirement from <=2.8.0,>=2.2.0 to >=2.2.0,<2.10.0 in /requirements (#13049)",0.6955472,Improved the error message for installing tensorboard or tensorboardx (#17053),fsspec verbump,0
1469,Support DDPPlugin to be used on CPU (#6208),0.6617897,Made DDP the default if no backend specified with multiple GPUs (#1789),Signed-off-by: Jerome janand@habana.ai,0
1470,quick-fix for --gpus flag bug (#2674),0.6036495,- Fixed bug that forced overriding `configure_optimizers` with the CLI ([#11672](https://github.com/PyTorchLightning/pytorch-lightning/pull/11672)),,0
1471,Add kubeflow cluster environment (#7300),0.69261575,Enabled custom clusters (#4048),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
1472,Add back external colossalai test (#16817),0.49021477,Enabling val/test loop disabling (#2692),,0
1473,Update Python testing (#10269),0.7541145,python script.py test,Co-authored-by: Seppo Enarvi seppo.git@marjaniemi.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
1474,"Update numpy requirement from <1.23.1,>=1.17.2 to >=1.17.2,<1.24.1 in /requirements (#16199)",0.60995734,"- num_devices = max(1, trainer.num_gpus, trainer.num_processes)",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: otaj 6065855+otaj@users.noreply.github.com,0
1475,Update gpu warning (#6181),0.58223116,- Removed the deprecated automatic GPU selection ([#16184](https://github.com/Lightning-AI/lightning/pull/16184)),Co-authored-by: awaelchli aedu.waelchli@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: otaj 6065855+otaj@users.noreply.github.com,0
1476,Update precision input type annotations (#14857),0.6670006,Mixed precision overhaul (#16783),,0
1477,fix logger type hint (#8359),0.71991456,Removed LoggerStages (#5673),Co-authored-by: otaj 6065855+otaj@users.noreply.github.com,1
1478,fix mypy errors for loggers/wandb.py (#13483),0.6748613,Deprecated pytorch_lightning.logging (#767),,0
1479,Move init_ddp_connection to distributed utilities (#9044),0.6553107,Deprecated LightningDistributed and moved the broadcast logic to DDPPlugin and DDPSpawnPlugin directly (#9691),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
1480,Bump version of fsspec (#14975),0.6749023,Changed fsspec to tuner (#4458),Co-authored-by: otaj ota@lightning.ai Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
1481,unfreeze torchtext version (#6302),0.7105843,- Removed deprecated support for old torchtext versions ([#14375](https://github.com/Lightning-AI/lightning/pull/14375)),Fix typo,1
1482,GPU CI: Increase timeout from 65 to 100min (#13104),0.67328435,Changed min-max GPU memory to be on their own plots (#1358),,0
1483,Clean utilities/argparse and add missing tests (#6607),0.61391675,Deprecated the TestTubeLogger (#9065),"  Remove conda job   Remove conda job from readme   Remove conda jobs from checkgroup   Remove conda from docker builds   Remove base-conda dockerfile   Rewrite the strategy matrix while keeping equivalent   Run the workflow on this branch   Revert ""Rewrite the strategy matrix while keeping equivalent""   This reverts commit e54298d60e57cffbf8107890987be3fe4a006c77.   Add PyTorch versions   Run on draft and disable unrelated costly CI   Revert ""Run the workflow on this branch""   This reverts commit 51ed8b905d8926b630dce4817124bd486135d3ec.   tmp: Lightweight relevant CI   Fix CI pathfilter   Update matrix   Drop skipping logic   pip list   reorder pip list   tmp: lightweight ci   Install specified pytorch   Fix torch installation   Uncomment steps   Increase timeout   bad merge   Revert ""Run on draft and disable unrelated costly CI""   This reverts commit eb5dc5e6bd07ba801eea34111052e7d31701fddc.   Update checkgroup   Update docs and remove Python/PyTorch versions   Remove pip-list   Fail if wrong pytorch version installed   Add Python 3.8, PyTorch 1.9 job   tmp: remove azure jobs   tmp: remove dockers   tmp: remove others   Run all combinations   Include oldest   Exclude no Python 3.10 distributions   tmp: no concurrency   tmp: double timeout   Add pytest log reporter   Add pytest-reportlog   Fewer jobs   Revert ""tmp: no concurrency""   This reverts commit 4a7978dcb3499ce754306580412110b7a42920cd.   fix artifact name   Revert test reports   Revert unrelated changes   Revert unrelated changes   Add the combination of ex-conda jobs   Update checkgroup   revert timeout   remove conda job   revert docker build workflow file   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",0
1484,Resultd (#2947),0.62762684,(#16002),Co-authored-by: awaelchli aedu.waelchli@gmail.com,0
1485,GH action - auto-update PRs (#5451),0.5590145,- Removed deprecated `GPUStatsMonitor` callback ([#12554](https://github.com/Lightning-AI/lightning/pull/12554)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com,0
1486,Added accumulation of loggers' metrics for the same steps (#1278),0.7300713,Metric reduction with Logging (#5150),Co-authored-by: awaelchli aedu.waelchli@gmail.com Co-authored-by: otaj 6065855+otaj@users.noreply.github.com,1
1487,Use tensorboard 2.3.0 (#3011),0.8084647,Switch from tensorboard to tensorboardx in TensorBoardLogger (#15728),Co-authored-by: Aliaksandr.Kuzmik AliaksandrK@comet.ml,1
1488,[App] Introduce auto scaler (#15769),0.57191825,tuner.scale_batch_size(...),Co-authored-by: thomas chaton thomas@grid.ai,0
1489,Update TBLogger docs (#6315),0.62119526,Deprecated the TestTubeLogger (#9065),,0
1490,Bump pytest-cov from 3.0.0 to 4.0.0 in /requirements (#15563),0.71774054,Dropped official support/testing for PyTorch <1.6 (#8288),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
1491,re-enable skipped tests (#2762),0.6570986,Enabling val/test loop disabling (#2692),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: otaj 6065855+otaj@users.noreply.github.com,0
1492,Remove setup_optimizers_in_pre_dispatch logic (#10906),0.7913994,- Removed method `setup_optimizers_in_pre_dispatch` from the `strategies` and achieve the same logic in `setup` and `pre_dispatch` methods ([#10906](https://github.com/PyTorchLightning/pytorch-lightning/pull/10906)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
1493,Fix for load_from_checkpoint (#2776),0.7164365,Refactor load in checkpoint connector (#4593),Co-authored-by: Kushashwa Ravi Shrimali kushashwaravishrimali@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
1494,[App] Enable running an app from the Gallery (#15941),0.5857056,App,,0
1495,Refactor Horovod NCCL check (#11948),0.7204757,"refactored Horovod backend (#3121, #3122)",Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com,1
1496,fixed root node addr,0.8645216,# figure out the root node addr, fairscale imports refactor to avoid meta package build issue  Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: thomas chaton thomas@grid.ai,1
1497,Use coverage>=6.4 (#13132),0.6104066,- Code coverage (99%),,0
1498,docs: add repo_name in the upright corner (#171),0.5347892,# 3. Rename the hook to `on_*_epoch_end`,,0
1499,Update Checkpointing.md (#83),0.6734948,Resuming from checkpoints (#16167),  Clean up CODEOWNERS for PL and Lite   Update ,0
1500,added module properties docs,0.65474176,New properties,,0
1501,Remove redundant progress bar refresh (#13462),0.77210724,Changed progress_bar_refresh_rate trainer flag to disable progress bar when setting to 0. (#1108),,1
1502,Add logo_light.svg (#8327),0.56014055,- Update `RichProgressBarTheme` styles after detecting light theme on colab ([#10993](https://github.com/PyTorchLightning/pytorch-lightning/pull/10993)),,0
1503,"ref: inner train loop (intermediate step) 8/n"" (#3367)",0.78450316,"inner train loop (#3359, #3361, #3362, #3363, #3365, #3366, #3367, #3368, #3369, #3370, #3371, #3372, #3373, #3374, #3375, #3376, #3385, #3388, #3397)",,1
1504,Update setuptools requirement from <=59.5.0 to <65.6.0 in /requirements (#15421),0.60534203,Setup: added requirement freeze for the next major version (#14480),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
1505,[1/N] Define dataclasses for progress tracking (#6603),0.7920766,"Connect the progress tracking dataclasses to the loops (#8244, #8362)",Co-authored-by: Laverne Henderson laverne.henderson@coupa.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
1506,Only call load_spawn_weights if COLAB_GPU or KAGGLE_URL_BASE,0.5923985,"Disabled strict loading in multiprocessing launcher (""ddp_spawn"", etc.) when loading weights back into the main process (#16365)", Remove deprecated LightningIPUModule chlog fix import Fix 1.10 depr test  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
1507,Add support for empty gpus list to run on CPU (#10246),0.62307775,Enable non-blocking for device transfers to GPU (#1843),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: otaj ota@lightning.ai Co-authored-by: Jirka jirka.borovec@seznam.cz,0
1508,[Fix] Move init dist connection into the setup function (#6506),0.68696046,moves init apex from LM to apex connector (#3923),,0
1509,Use callable object for patching dataloaders (#971),0.7071172,refactored dataloader process hook (#3139),,1
1510,Apply suggestions from code review,0.56647193,- Code coverage (99%),,0
1511,Remove self._log_dir from BaseProfiler (#11740),0.7117212,Deprecated self.log(sync_dist_op) in favor of self.log(reduce_fx). (#7891)," Revert ""Add BatchSizeFinder callback (#11089)""  This reverts commit d1a3a3ebf543629fc38783d9f5e39f2bfb6b37d7.  Revert ""Revert ""Add BatchSizeFinder callback (#11089)""""  This reverts commit 9cc4695925d0ec934009b31b96d3fdc3c1ffd579.   remove pl   add torch   add numpy   rm packages   add packages   add packages   import from PL   import from PL   always install PL for doctests   remove unnecessary requirements   always install PL in editable mode   once more   another attempt   maybe fix app test?   Redundant checkgroup path   Revert ""maybe fix app test?""   This reverts commit 8210a43ef499b29a00a7cd10d1d4c55d1fa6829d.   speed up install deps   damn this   damn trio   Co-authored-by: otaj ota@lightning.ai Co-authored-by: otaj 6065855+otaj@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com",1
1512,Update gpus flag with accelerator and devices flag (#12156),0.6411847,refactored GPU backend __step (#3120),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
1513,try random TPU config (#5992),0.65503734,Updated logic for checking TPUs availability (#6767),,0
1514,Update deepspeed and fairscale versions (#12860),0.72706574,Updated precision attributes in DeepSpeedPlugin (#10164),"  add BatchSizeFinderCallback callback   temp rm from init   skip with lr_finder tests   restore loops and intergrate early exit   enable fast_dev_run test   add docs and tests   keep tune and remove early_exit   add more tests   patch lr finder   disable skip   force_save and fix test   mypy and circular import fix   fix mypy   fix   updates   rebase   address reviews   add more exceptions for unsupported functionalities   move exception to setup   chlog   unit test   address reviews   Apply suggestions from code review   update   update   mypy   fix   use it as a util func   license   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   mypy   mypy   review   fix   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   fix   updates   updates   fix import   Protect callback attrs   don't reset val dataloader   update test   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: otaj 6065855+otaj@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com",1
1515,Integrate Lite Precision into PL (#14798),0.7366833,- Integrated the Lite Precision plugins into the PL Precision plugins - the base class in PL now extends the `lightning_lite.precision.Precision` base class ([#14798](https://github.com/Lightning-AI/lightning/pull/14798)),,1
1516,Update Trainer config tests to use acelerator and devices (#12152),0.7242297,Trainer Device Attributes,Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
1517,Update logic for checking TPUs availability (#6767),0.9913459,Updated logic for checking TPUs availability (#6767),,1
1518,Do not autodetach extras (#10424),0.6784237,Removed auto val reduce (#2462),,0
1519,Add callback to manage fault-tolerance checkpoints (#11862),0.7281768,- Added a private callback to manage the creation and deletion of fault-tolerance checkpoints ([#11862](https://github.com/PyTorchLightning/pytorch-lightning/pull/11862)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,1
1520,Sized iterable typing improvements (#16585),0.71177447,Refactored setup for typing friendly (#6590),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
1521,Rename special to standalone (#10779),0.566674,Re-enabled naming metrics in ckpt name (#3060),,0
1522,Add onnx export (#2596),0.70268214,Avoid the deprecated onnx.export(example_outputs=...) in torch 1.10 (#11116),,1
1523,MIT -> apache 2 license,0.4122967,Support **DictConfig for hparam serialization (#2519),,0
1524,Fix ReduceOp type hint in ColossalAI strategy (#15535),0.58601284,Renamed Strategy.reduce to Strategy.all_reduce in all strategies (#16370),,0
1525,STY Minor flake8 fix (#197),0.6463028,Changed fsspec to tuner (#4458),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
1526,fixed default sampler (#1425),0.8455589,Did not interfere with a default sampler (#1318),,1
1527,Rerun flaky profiler tests on failure (#10035),0.6513001,"Deprecated model steps training_end, validation_end and test_end (#1051, #1056)",Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
1528,Change WarningCache to subclass set (#7995),0.57814014,Replace MisconfigurationException with warning in ModelCheckpoint Callback (#4560), remove profile_iterable remove imports remove depricated api update changelog  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
1529,feat(wandb): log in sync with Trainer step (#4405),0.8561552,W&B log in sync with Trainer step (#4405),,1
1530,release v1 (#4516),0.7040912,This release includes:,fixed comet -> mlflow typo Co-authored-by: Devin Conathan devin.conathan@libertymutual.com,1
1531,Fixes resuming checkpoints rerunning last epoch (#866),0.84222424,Resuming from checkpoints (#16167), drop duplicate docs requirements skip empty dir mypy for #14861,1
1532,add docs title (#1063),0.5558944,Docs,"  clean trainer 3/n   clean trainer 3/n   clean trainer 3/n   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  clean trainer 3/n  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
1533,[bugfix] Add reloading support using BaseFinetuning (#7253),0.64230764,- Removed `BaseFinetuning.on_save_checkpoint` and `BaseFinetuning.on_load_checkpoint` in favor of `BaseFinetuning.state_dict` and `BaseFinetuning.load_state_dict` ([#11887](https://github.com/PyTorchLightning/pytorch-lightning/pull/11887)),"  clean trainer 2/n   clean trainer 2/n   clean trainer 2/n   clean trainer 2/n   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
1534,fix logging test,0.78388584,Cleaning up stale logger tests (#3490),"  clean trainer 1/n   clean trainer 1/n   clean trainer 1/n   clean trainer 1/n   clean trainer 1/n   clean trainer 1/n   clean trainer 1/n   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   clean trainer 1/n   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",1
1535,Break hpu graphs into two for better performance (#14656),0.93975854,Break HPU Graphs into two parts (forward + backward as one and optimizer as another) for better performance (#14656),,1
1536,CI: hotfix signal if (#15995),0.52423537,The preemption/termination signal is now configurable (#14626):,,0
1537,Improvements and changes to progress tracking dataclasses (#8140),0.79707056,"Connect the progress tracking dataclasses to the loops (#8244, #8362)",,1
1538,WandbLogger's log_image can use step argument (#11716),0.6954324,Changed WandbLogger(log_model={True/'all'}) to log models as artifacts (#6231),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: rohitgr7 rohitgr1998@gmail.com Co-authored-by: Jirka jirka.borovec@seznam.cz,0
1539,Prepare changelog for 1.7 release (#13979),0.7400276,Complete changelog,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
1540,[App] Resolve PythonServer on M1 (#15949),0.59740347,$ PL_FAULT_TOLERANT_TRAINING=MANUAL python script.py,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
1541,change default logger to dedicated one (#1064),0.9928596,Change default logger to a dedicated one (#1064), Remove deprecated precision plugin checkpoint hooks chlog,1
1542,Temporarily pin sphinx version (#8377),0.6735919,Pinned sphinx-autodoc-typehints with <v1.15 (#11400), Remove deprecated device attributes from Trainer changelog  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
1543,App: drop flaky doctest example (#17366),0.57068384,Avoid using the deprecated LooseVersion (#16162),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
1544,ddp pickle,0.6800596,Tested pickling (#1636),,0
1545,Fix saving native AMP scaler state (#1777),0.6992645,training AMP scaling refactor (#3135), Fix attribute error in SWA when running with Tuner changelog add better test  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
1546,Replace deprecated distributed_backend by acc in examples (#7795),0.6151093,Remove deprecated distributed_backend from Trainer (#10017), Remove deprecated use_amp attributes chlog,0
1547,Fix typo ddp_spawn_sharded -> ddp_sharded_spawn (#13062),0.5563167,Changed the default of find_unused_parameters back to True in DDP and DDP Spawn (#6438), tests for 14809 Apply suggestions from code review  Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
1548,Update Slack link (#12421),0.5939324,Updated hooks arguments - breaking for setup and teardown (#2850),Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,0
1549,fix deprecated default_save_path (#1449),0.64414793,Removed deprecated checkpoint argument filepath (#5321),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
1550,Fix TQDMProgressBar usage in logging.rst (#14768),0.6799828,Changed automatic casting for LoggerConnector metrics (#5218),,0
1551,mergify: drop ready for draft (#15766),0.5242144,[0.5.6] - 2022-08-16,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
1552,added training router,0.5894587,moved accelerator router (#3309),Co-authored-by: awaelchli aedu.waelchli@gmail.com,0
1553,[FIX] Enable mixed precision in the Fully Sharded Strategy when precision=16 (#12965),0.80269736,Enable mixed precision in DDPFullyShardedStrategy when precision=16 (#12965),Co-authored-by: Jirka jirka.borovec@seznam.cz,1
1554,make it clear the example is under the hood (#2607),0.5807295,Have a look at the full example here.,Co-authored-by: Laverne Henderson laverne.henderson@coupa.com,0
1555,Fix mypy errors in strategies/ddp_spawn.py (#13865),0.5985476,Improved error messages for invalid configure_optimizers returns (#3587),,0
1556,Accelerator Refactor/RPC + Sharded (#5732),0.722818,Refactored accelerator backends:,Co-authored-by: awaelchli aedu.waelchli@gmail.com,1
1557,[App] Add start method to the LightningWork  (#15523),0.7440028,Lightning App, Fixed TypeError on 1.7.6 when distributed unavailable changelog,1
1558,[Fix] Move log value to cpu. (#4592),0.62391484,Dramatically simplify the LoggerConnector (#7882), fix mypy lightning_cli errors,0
1559,Update lightning_cli_advanced_2.rst (#15257),0.79542553,Update the Lightning App docs (#13537),Signed-off-by: dependabot[bot] support@github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
1560,doctest for .rst files (#1511),0.5747834,"docs for all Metrics (#2184, #2209)",Co-authored-by: Ritsuki Yamada ritsuki.yamada@uzabase.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
1561,Update configuration_validator.py (#12123),0.6709479,Renamed and moved core/step_result.py to trainer/connectors/logger_connector/result.py (#7736),,0
1562,Lightning 1.9.0 Release Candidate 0 (#16264),0.77652806,Lightning 2.0 is the official release for Lightning Fabric :tada:,,1
1563,Docs 3/n (#15554),0.6914164,Docs, Update hpu-tests.yml to support v1.6.0 release Update Dockerfile,0
1564,lit extras (#15793),0.6099352,(#16002),,0
1565,adding tests,0.6814885,"nb_test_batches to num_test_batches,",,0
1566,"Update streamlit requirement from <=1.15.2,>=1.0.0 to >=1.0.0,<1.16.1 in /requirements (#16221)",0.56413805,- Changed minimum supported version of `rich` from `10.14.0` to `12.13.0` ([#16798](https://github.com/Lightning-AI/lightning/pull/16798)),Update wandb requirement in /requirements Updates the requirements on wandb to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: wandb   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
1567,Document Gradient Clipping during Manual Optimization (#16023),0.8013025,Gradient Clipping Customization,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka jirka.borovec@seznam.cz,1
1568,Use google-github-actions/get-gke-credentials@v0 (#15264),0.5260026,- Added support for `Callback` registration through entry points ([#12739](https://github.com/Lightning-AI/lightning/pull/12739)),Update arrow requirement in /requirements Updates the requirements on arrow to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: arrow   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
1569,Do not use the base version by default in _compare_version (#10051),0.6161299,Avoid using the deprecated LooseVersion (#16162),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
1570,LightningCLI add --config option after parser init (#15048),0.7244221,LightningCLI.init_parser now returns the parser instance (#8721),  update   update   update   update   update   update   update   update   update   update ,1
1571,add 1.13.1 to adjust versions (#16099),0.67160195,Set version as today (#13906),  Removes the old HPO content   Remove source-lit symlinks for HPO   drop ref   Co-authored-by: Jirka jirka.borovec@seznam.cz,0
1572,Minor formatting & grammar fixes in docs [CI SKIP] (#3952),0.5948293,Remove unnecessary intermediate layers in Dockerfiles (#5697),,0
1573,remove redundant accumulation normalization in manual optimization (#9769),0.7024059,     # 1. Switch to manual optimization,Co-authored-by: awaelchli aedu.waelchli@gmail.com,1
1574,Add auto_device_count method to Accelerators (#10222),0.74711376,"Custom Accelerator implementations must now implement two new abstract methods: is_available() (#11797) and auto_device_count() (#10222). The latter determines how many devices get used by default when specifying Trainer(accelerator=..., devices=""auto"").",Co-authored-by: Kushashwa Ravi Shrimali kushashwaravishrimali@gmail.com,1
1575,docstring changes in tuner (#6264),0.6821236,Changed fsspec to tuner (#4458),,0
1576,move tensorboardX to extra (#16349),0.82031476,Switch from tensorboard to tensorboardx in TensorBoardLogger (#15728),,1
1577,clean up unused attributes in LightningModule (#8259),0.7191502,- Removed the `LightningModule.precision` attribute ([#16203](https://github.com/Lightning-AI/lightning/pull/16203)),,1
1578,test: save hparams to yaml (#2198),0.6481333,    hparams_file='/path/to/hparams_file.yaml',,0
1579,added mkdocs config,0.5477433,  * `save_config_multifile`,,0
1580,update docs example with sharded eval step (#7748),0.57288325,Docs improvements, Bump Lightning Cloud to 0.5.7 :tada: Fix link in changelog  Co-authored-by: Sherin Thomas sherin@grid.ai,0
1581,fix docs (#982),0.6297139,This release fixes that core issue,,0
1582,Use PrecisionType enum instead of checking raw values  (#10704),0.56049144,Parsing of enums type hyperparameters to be saved in the haprams.yaml file by TensorBoard and CSV loggers has been fixed and made in line with how OmegaConf parses it (#9170),,0
1583,Update ClusterEnvironment docs (#7120),0.7173903,Enabled custom clusters (#4048),,1
1584,Update dev branch to continue with 1.6 (#10332),0.5806933,Set version as today (#13906),debug=True for boring_app (dynamic app also has debug=True) Co-authored-by: thomas chaton thomas@grid.ai,0
1585,Remove the deprecated code in pl.utilities.xla_device (#16404),0.7197616,- Removed the deprecated code in:,,1
1586,fix(wandb): use same logger on multiple training loops (#2055),0.8258706,Allow use of same WandbLogger instance for multiple training loops (#2055), Remove deprecated torch_distributed_backend logic changelog mention deprecated imports  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz,1
1587,Do not modify MANIFEST.in on install (#15646),0.6222044,Avoid using the deprecated LooseVersion (#16162),Co-authored-by: Kushashwa Ravi Shrimali kushashwaravishrimali@gmail.com,0
1588,Replace a MisconfigurationException with warning in ModelCheckpoint callback (#4560),0.9971589,Replace MisconfigurationException with warning in ModelCheckpoint Callback (#4560),,1
1589,Fixed PEP8 definition,0.44654796,Removed environment variable PL_EXP_VERSION from DDP subprocesses (#7403),,0
1590,Add the probot check-group action (#14621),0.6065358,group prepare data hook (#3212), simplify logic remove hpc update add changelog more tests update test  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
1591,Fixes #2792 (#3857),0.6286695,Resolve bug with Finetuning (#5744),,0
1592,Remove some incorrect comments in ddp.py (#9319),0.61341065,Silenced some warnings. verified ddp refactors (#3483),,0
1593,release v0.21,0.77892005,"    version=""0.0.1"",", Update installation  Updates to use python -m pip install -U lightning and adds troubleshooting note  Apply suggestions from code review  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
1594,fix building nightly (#3642),0.51455885,"Cleaning (#5948, #5949, #5950)", updates the Lightning App docs theme to the one without Pytorch Lightning docs Google Tag Manager hardcoded sets the GTM id in the conf.py for Lightning App docs,0
1595,proper checkpoint implementation (#1043),0.71679926,Checkpoint and early stopping now work without val. step (#1041),Created a YAML form to replace the MD form for issues.  Minor update to form Removed MD issue form More updates Renamed the file  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
1596,docs (#4105),0.7688416,"docs for all Metrics (#2184, #2209)", lower,1
1597,[App] Remove --instance-types from cluster creation (#15314),0.74433196,Cluster creation and deletion now waits by default [#15458,Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
1598,Update new project code sample (#2287),0.5757799,"Refactored evaluation loop interface; added new classes DataLoaderLoop, EvaluationLoop, EvaluationEpochLoop (#7990, #8077)",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,0
1599,Remove dev_debugger.call_count (#8317),0.72027546,Removed deprecated callbacks (#3979),,1
1600,Enable tpu device ids test (#12428),0.70486563,Updated logic for checking TPUs availability (#6767),  mv releases to a standalone page   Include release_policy in index   Update policy   mv releases to a standalone page   Include release_policy in index   Update policy   Update title   remove release_policy.rst   Update versioning   syntax   simplify wording   Include examples that don't follow X+2 rule   syntax   update   consistency   rm noninformative statement   .   Reduce redundancy in the deprecation process   grammar?   consistency   Update docs/source-pytorch/versioning.rst   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
1601,Fix typo (#2907),0.60725844,Changed overwrite to True (#16009),"Adds a new '--secret' flag to 'lightning run app': lightning run app --cloud --secret MY_SECRET=my-secret-name app.py When the Lightning App runs in the cloud, the 'MY_SECRET' environment variable will be populated with the value of the referenced Secret. The value of the Secret is encrypted in the database, and will only be decrypted and accessible to the Flow/Work processes in the cloud. Co-authored-by: Sherin Thomas sherin@grid.ai Co-authored-by: Noha Alon nohalon@gmail.com Co-authored-by: thomas chaton thomas@grid.ai",0
1602,always calls the lr scheduler  with epoch nb. Fixes #98 (#252),0.70043635,lr_scheduler now activated after epoch    ,  Remove a todo from trainer regarding exception handling   Remove mentions of TODO(@awaelchli) from code   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
1603,Standalone Lite CI setup (#14451),0.62596977,        # Let Lite setup your dataloader(s),  Add Lite codeowners   remove Borda on request ,0
1604,Update CONTRIBUTING.md (#2927),0.5676031,[1.3.7post0] - 2021-06-23,Bump lightning cloud for memory leak fix (#14697),0
1605,[bug] Update broadcast + reduce decision ModelCheckpoint] (#6410),0.6444033,Removed a redundant warning with ModelCheckpoint(monitor=None) callback (#9875), Tune the checkgroup config Lite does not support HPU and IPU atm Skip HPU as the server is down,0
1606,Fix double precision + ddp_spawn (#6924),0.74771744,Enable mixed precision in DDPFullyShardedStrategy when precision=16 (#12965),,1
1607,Remove deperecated code in pl.utilities.meta (#16038),0.7817765,- Removed the deprecated code in:,,1
1608,Fix save_hyperparameters when no parameters need saving (#11827),0.81297064,Moved save_hyperparameters to its own function (#7119),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
1609,Move progress bar disabling out of the Trainer (#11377),0.82528186,Changed progress_bar_refresh_rate trainer flag to disable progress bar when setting to 0. (#1108),,1
1610,Inline functions called by _run_stage (#17015),0.5484518,Moved result teardown to the loops (#8245),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
1611,Update optimizer_step methods in accelerator and plugins (#10023),0.7736312,"Moved the optimizer_step and clip_gradients hook from the Accelerator and TrainingTypePlugin into the PrecisionPlugin (#10143, #10029)",Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
1612,Always run validation inside the training loop epoch (#7357),0.9002327,Validation is now always run inside the training epoch scope (#7357),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
1613,fix set_epoch on TPUs (#2740),0.6096851,Renamed TPUHalfPrecisionPlugin to TPUBf16PrecisionPlugin (#10026),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
1614,Run ddp_spawn dataloader checks on windows (#6930),0.99884444,Run ddp_spawn dataloader checks on Windows (#6930), Update trainer.rst Update datamodule.rst,1
1615,ref: merge backends x/n (#3482),0.82924753,"merge backends (#3476, #3477, #3478, #3480, #3482)",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
1616,Guard against AttributeError in dataloaders. (#161),0.6282866,"- The `dataloader_idx` argument is now optional for the `on_{validation,test,predict}_batch_{start,end}` hooks. Remove it or default it to 0 if you don't use multiple dataloaders ([#16753](https://github.com/Lightning-AI/lightning/pull/16753))", Added functions to the WandbLogger to download and use artifacts without having to access the experiment object Updated CHANGLELOG.md Added suggested changes Delete test_script  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: awaelchli aedu.waelchli@gmail.com,0
1617,Fix ModelCheckpoint misformats filename (#17610),0.8199873,Fix saved filename in ModelCheckpoint if it already exists (#4861),"  add accelerator implementations to lite   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   fix imports   rename registry argument   fix test   fix tests   remove duplicated test   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   fix tests   deprecation   deprecations   flake8   fixes   add mps to runif   fix tests   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  Apply suggestions from code review  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci   remove more   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   local import   undo device stats :(   fix import   stupid typehints   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   more refactors :(   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   fix   rename init_device to setup_device   remove unused import   make uppercase to differentiate from class   trick test after moving import locally   add base classes and registry   reg   registry   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   tests   update to other branches   resolve todo(lite)   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   add very basic unit tests   fix name assignment   Update src/lightning_lite/strategies/parallel.py   Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com   remove deprecated property   remove pre- and post backward for now   protecting the registry utility function   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  remove unused import  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com",1
1618,[App] Optimise Queues Calls (#15235),0.57583904,Working with multiple optimizers (#16539),Co-authored-by: otaj ota@lightning.ai,0
1619,Update Bagua section example (#11899),0.59200186,The Bagua Strategy,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
1620,Update notebooks submodule (#10827),0.5526539,Moved DeviceDtypeModuleMixin and HyperparametersMixin mixin to core (#8396),,0
1621,document exceptions for metrics/functional (#6273),0.6266886,Removed deprecated metrics (#6161),,0
1622,Changes in preparation to #8578 (#11562),0.6742777,[0.6.0] - 2022-09-08,,0
1623,Removed flush_logs_every_n_steps argument from Trainer (#13074),0.8183255,Rename Trainer arguments row_log_interval >> log_every_n_steps and log_save_interval >> flush_logs_every_n_steps (#3748),Updates the requirements on ipython[all] to permit the latest version. - Release notes - Commits  updated-dependencies: - dependency-name: ipython[all]   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,1
1624,Add mocked function assert to test_error_handling_all_stages (#9182),0.64303607,- fixed all the .test() calls,Signed-off-by: Jerome janand@habana.ai Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,0
1625,fix mypy typing errors in pytorch_lightning/strategies/parallel.py (#13556),0.7221379,+ # pytorch_lightning==1.7.0,,1
1626,move to Pages dir (#4869),0.5277431,moved TPU xxx_step to backend (#3118),,0
1627,Removed lines,0.66500485,Removed, remove toml ref fix conflicts small fix move assertion  Co-authored-by: rohitgr7 rohitgr1998@gmail.com,0
1628,resolving documentation warnings (#833),0.67612433,warning_utils >> warnings,  update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update ,0
1629,Add support to Tensorboard logger for OmegaConf hparams (#2846),0.72146875,Switch from tensorboard to tensorboardx in TensorBoardLogger (#15728),"  Content for Lightning with iOS and Android   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   make link clickable   Update docs/source-app/glossary/ios_and_android.rst   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: awaelchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",1
1630,Fault Tolerant Manual: Add support for collecting states across processes (#10639),0.7372935,- Fault Tolerant Manual,Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Akihiro Nitta nitta@akihironitta.com,1
1631,"Change the seq of on_train_batch_end, on_batch_end & on_train_epoch_end, on_epoch_end hooks (#5688)",0.9867977,"Changed the seq of on_train_batch_end, on_batch_end & on_train_epoch_end, on_epoch_end hooks (#5688)",  resolve_boring_app   update   update   update   update   update   update   update   update   update   update   update   update   update ,1
1632,fix missing tutorials (#17311),0.586059,Removed the deprecated TrainerLoggingMixin class (#8609),,0
1633,Fix Trainer.plugins type declaration (#7288),0.7473806,trainer = pl.Trainer(plugins=SLURMEnvironment(auto_requeue=False)),Update CODEOWNER,1
1634,Combine utilities,0.5781449,"Alternatively, use the utility function (with care!)",,0
1635,"Fix ModelCheckpoint(monitor=None, save_last=True) not saving checkpoints (#6136)",0.7641606,"Allow ModelCheckpoint monitor to be None, meaning it will always save ([3630)",Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
1636,Remove limitation of batch scaler (#4006),0.73611593,Reset epoch progress with batch size scaler (#13846), Traitlets >= 5.3.0 Apply suggestions from code review  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
1637,Raise TypeError instead of using asserting with condition of types. (#5536),0.5909269,Changed type checker with explicit cast of ref_model object (#4457),,0
1638,fixex imports,0.7505258,import argparse,  Move storage from app prefix to project/app prefix: checking and legacy support   Changelog message   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
1639,minor fix: indent spaces in comment-out (#16076),0.53002286,Resolve bug with Finetuning (#5744),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
1640,Add support for colossalai 0.1.11 (#15888),0.57899857,Add --app_args support from the CLI (#13625), Update traitlets requirement from <5.2.0 as strict in /requirements  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
1641,remove deprecated model hooks (#3980),0.98345876,Removed deprecated model hooks (#3980), Add parameter to change the preemption signal Make the signal connector use the custom signal from SLURMEnvironment  Signed-off-by: Max Ehrlich max.ehr@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
1642,Fix GPU tests that fail to raise expected configuration error when run in a CUDA environment (#14983),0.6195735,GPU training (#2704),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
1643,refactored model tests,0.70712876,- Full tests that run multiple models in different configs,Bumps tj-actions/changed-files from 29.0.3 to 29.0.4. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: tj-actions/changed-files   dependency-type: direct:production   update-type: version-update:semver-patch ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,1
1644,add congratulations at the end of our notebooks (#4555),0.556677,"    },",Co-authored-by: manskx ahmed.mansy156@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz,0
1645,[BUG-FIX] WandbLogger _sanitize_callable (#4422),0.69717765,Allow use of sweeps with WandbLogger (#1512),,0
1646,Provide backward compatibility for #124 (#400),0.606103,"merge backends (#3476, #3477, #3478, #3480, #3482)",Updates the requirements on psutil to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: psutil   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
1647,Update model_checkpoint.py (#7625),0.67654955,- Deprecated `ModelCheckpoint.save_checkpoint` in favor of `Trainer.save_checkpoint` ([#12456](https://github.com/PyTorchLightning/pytorch-lightning/pull/12456)),Update docutils requirement in /requirements Updates the requirements on docutils to permit the latest version.  updated-dependencies: - dependency-name: docutils   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
1648,Resolve e2e CI (#17480),0.5465722,except Exception as e:,Drop skipping logic,0
1649,Use update_wrapper in test_hooks.py (#10578),0.6570885,"- Changed `training_step`, `validation_step`, `test_step` and `predict_step` method signatures in `Accelerator` and updated input from caller side ([#10908](https://github.com/PyTorchLightning/pytorch-lightning/pull/10908))", Removes timeout from streamlit e2e test We have a timeout on the app view which waits for the button but it causes a refresh on the page which causes playwright to miss the button on each refresh. we can remove the timeout altogether since we have a time limit on the test itself in the CI setup  Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
1650,trainer updates,0.79967785,  trainer:,Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,1
1651,Fix reference issues during epoch end result collection (#8621),0.580462,Changed epoch indexing from 0 instead of 1 (#2289)," fixes mypy errors in trainer/supporters.py Fxes mypy error when accessing ""init"" directly add an assertion in lr_finder.py Make init calls reset in TensorRunningAccum Fixes formatting Add self.window_length to __init__  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
1652,fixed demo,0.56015134,setup(,,0
1653,add Codecov info (#144),0.5894991,Add code_dir argument to tracer run (#15771),Co-authored-by: Mansy ahmed.mansy156@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
1654,Disable non blocking to device with MPS (#14368),0.6580199,Enable non-blocking for device transfers to GPU (#1843),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
1655,fix notebooks/README.md colab links (#3599),0.577656,Fixing critical bugs in newly added hooks and hparams assignment.,,0
1656,1/n Move precision plugin into strategy - update reference (#10570),0.7592686,precision plugins (#3504)," Run CircleCI with the HEAD sha, not the base Different solution",1
1657,fix SVG images (#1409),0.49180564,Resolve bug with Finetuning (#5744),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,0
1658,update info for GPU (#1021),0.78041315,GPU training (#2704),,1
1659,extras,0.65506405,        else:,  Set running_torchscript recursively   CHANGELOG ,0
1660,Fix finetuning complex models correctly unfreezes. (#6880),0.62291455,Resolve bug with Finetuning (#5744),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
1661,Remove TrainingTypePlugin.post_dispatch in favor of teardown (#10939),0.7496306,- Deprecated `TrainingTypePlugin.post_dispatch` in favor of `TrainingTypePlugin.teardown` ([#10939](https://github.com/PyTorchLightning/pytorch-lightning/pull/10939))," Bump docker/build-push-action from 1.1.0 to 3.1.1  Bumps docker/build-push-action from 1.1.0 to 3.1.1. - Release notes - Commits  updated-dependencies: - dependency-name: docker/build-push-action   dependency-type: direct:production   update-type: version-update:semver-major ... Signed-off-by: dependabot[bot] support@github.com  Revert ""Bump docker/build-push-action from 1.1.0 to 3.1.1""  This reverts commit 05f9bfb084fd00657d4396214938f448a3f9b143.  use v3  Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com",1
1662,Fix false num_classes warning in metrics (#2781),0.6469331,Avoid raising the sampler warning if num_replicas=1 (#14097),,0
1663,CI buils with minimal and latest requirements (#500),0.50968,Graphcore IPU devices,Delete base.txt Co-authored-by: thomas chaton thomas@grid.ai,0
1664,Update Fabric docs with installation instructions (#16996),0.5774244,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",,0
1665,Standalone Lite: DDP Strategy Family  (#14670),0.6853324,"ddp = pl.strategies.DDPStrategy(process_group_backend=""fairring"")",,0
1666,Add missing methods to logger collection (#2723),0.6746501,Removed LoggerStages (#5673),,0
1667,Update resume_from_checkpoint docs (#9952),0.8148681,Changed resume_from_checkpoint warning to an error when the checkpoint file does not exist (#7075),"  docs   t1   simple import   simple import   simple import   simple import   Update version.py   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",1
1668,Rename the TrainingTypePlugin base to Strategy (#11120),0.8362504,"All TrainingTypePlugins have been renamed to Strategy (#11120). Strategy is a more appropriate name because it encompasses more than simply training communcation. This change is now aligned with the changes we implemented in 1.5, which introduced the new strategy and devices flags to the Trainer.",Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
1669,Fix tbptt docs (#484),0.6057168,Renamed TPUHalfPrecisionPlugin to TPUBf16PrecisionPlugin (#10026),,0
1670,Make subprocess launcher the default in Lite (#16388),0.5731665,        # Let Lite setup your dataloader(s),  Add support for bagua-cuda116   Remove bagua-cuda115 from installation   Co-authored-by: Akihiro Nitta nitta@akihironitta.com,0
1671,Add JustPy Frontend (#15002),0.70164937,- Add a `JustPyFrontend` to ease UI creation with `https://github.com/justpy-org/justpy` ([#15002](https://github.com/Lightning-AI/lightning/pull/15002)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
1672,Update accelerator connector messages after the addition of strategy (#9937),0.7023188,Removed deprecated connect_precision_plugin and connect_training_type_plugin from Accelerator (#9019),Update s3fs requirement in /requirements Updates the requirements on s3fs to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: s3fs   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,1
1673,replace MD template with Yaml - docs (#15909),0.6432109,Replace mata_tags.csv with hparams.yaml (#1271), Avoid instantiating every accelerator in the registry when listing available ones,0
1674,[App] Resolve a condition bug with spawning (#15812),0.6023527,Resolved a bug where the work statuses will grow quickly and be duplicated (#13970), Update content for S3 persistent storage Updates based on feedback Fix unstructured validation issue Updates based on feedback  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com Co-authored-by: Luca Antiga luca.antiga@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
1675,"Fix typo: ""optimizeres"" -> ""optimizers"" (#13030)",0.6590966,Changed default behaviour of configure_optimizers to use no optimizer rather than Adam. (#1279),LooseVersion was not correctly evaluation RC and set it as last even though the full release is out...,0
1676,Disable validation when val_percent_check=0 (#1251),0.7478415,val_percent_check in favour of limit_val_batches,,1
1677,Update run_ptl_script.py,0.6521597,python script.py \, Use the pull_request_target workflow event Minor  cleanup ready_for_review,0
1678,logger docs and api docs (#3950),0.6541276,Return the default_root_dir as the log_dir when the logger is a LoggerCollection (#8187),,0
1679,[pre-commit.ci] pre-commit suggestions (#9819),0.51408863,The preemption/termination signal is now configurable (#14626):,,0
1680,Fix load disparity between normal and hpc (#4526),0.5803477,remove weight loading hack for ddp_cpu (#3808),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
1681,Update README.md (#7717),0.5319348,Updated mlflow with using resolve_tags (#6746),Updates the requirements on fastapi to permit the latest version. - Release notes - Commits  updated-dependencies: - dependency-name: fastapi   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
1682,Avoid false positive warning about using sync_dist when using torchmetrics (#14143),0.9971055,Avoided false positive warning about using sync_dist when using torchmetrics (#14143),Update neptune-client requirement in /requirements Updates the requirements on neptune-client to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: neptune-client   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,1
1683,added load on CPU first (#221),0.6034999,Multiple CPU processes,reduce the ci load by only installing lmdb in tests,0
1684,release v0.3.6.1,0.77350616,"    version=""0.0.1"",",Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Laverne Henderson laverne.henderson@coupa.com Co-authored-by: Felonious-Spellfire felonious.spellfire@gmail.com,1
1685,Fix mypy errors attributed to pytorch_lightning.callbacks.quantization (#13782),0.7160907,"        ""pytorch_lightning.callbacks_factory"": [",Co-authored-by: Piero Coronica piero.coronica@mpcdf.mpg.de Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
1686,Added save_config_filename init argument to LightningCLI (#7741),0.7756711,| Argument LightningCLI(save_config_filename=...)                                    | 1.10             | LightningCLI(save_config_kwargs=dict(config_filename=...))         |,,1
1687,Simplify the Trainer core logic (#17017),0.71770597,"Refactor RunningStage and TrainerState usage (#4945, #7173)",,1
1688,2/n inter batch parallelism (#9047),0.8930007,Inter Batch Parallelism,,1
1689,Lite: Remove legacy code (#15953),0.6777123,Removed logger_connector legacy code (#6733),,0
1690,Auto-set DataLoader.worker_init_fn with seed_everything (#6960),1.0,Auto-set DataLoader.worker_init_fn with seed_everything (#6960),  Fix doc examples: use bytes instead of strings while writing   Add a note (comment)   nit   Update any_server.rst   Update docs/source-app/workflows/add_server/any_server.rst   Update docs/source-app/workflows/add_server/any_server.rst   Update docs/source-app/workflows/add_server/any_server.rst   Apply suggestions from code review   Co-authored-by: Laverne Henderson laverne.henderson@coupa.com Co-authored-by: Ethan Harris ethanwharris@gmail.com Co-authored-by: thomas chaton thomas@grid.ai,1
1691,Update error message for interactive incompatible plugins (#9896),0.9934477,Updated error message for interactive incompatible plugins (#9896),Update fsspec[http] requirement in /requirements Updates the requirements on fsspec[http] to permit the latest version. - Release notes - Commits  updated-dependencies: - dependency-name: fsspec[http]   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,1
1692,Simpledocs (#3265),0.54310787,"adding compute environments (#3837, [#3842)",,0
1693,Remove AcceleratorConnector.use_dp (#12112),0.8163216,- Removed `AcceleratorConnector.use_dp` property ([#12112](https://github.com/PyTorchLightning/pytorch-lightning/pull/12112))," Typo in major heading seen by newcomers  Correct typo in one of the first major headings newcomers to Lightning see when they are considering migrating their code to use Lightning. I know this is a trivial change in terms of the text change itself, but I really think it's valuable for one of the most important landing pages that users first investigating Lightning see - to have rock-solid, professional text without obvious typos. Here was a typo in the main heading itself. I suggest fixing it straightaway via this PR. Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com",1
1694,Grammar corrections for Fabric docs (#16494),0.51337713,Syntax changes are: ,,0
1695,Standalone Lite: Precision Plugins (#14547),0.8457615,Precision Plugins (#5718),,1
1696,drop colossalai from testing as no stable release yet (#16122),0.5553291,"- Removed the `ColossalAIStrategy` and `ColossalAIPrecisionPlugin` in favor of the new [lightning-colossalai](https://github.com/Lightning-AI/lightning-colossalai) package ([#16757](https://github.com/Lightning-AI/lightning/pull/16757), [#16778](https://github.com/Lightning-AI/lightning/pull/16778))",Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
1697,Prepare for torch==1.11 (#11679),0.7433506,"Native torch metrics (#1488, #2062)",,1
1698,Add missing val/test hooks in LightningModule (#5467),0.7482002,Improved the error message when the LightningWork is missing the run method (#14759),,1
1699,Add LightningModule.lr_scheduler_step (#10249),0.9305067,LightningModule.lr_scheduler_step,,1
1700,Remove deprecated disable_validation property from Trainer (#10450),0.9005638,Deprecated the Trainer.disable_validation property in favor of not Trainer.enable_validation (#8291),,1
1701,[CLI] Respect existing seed by default (#13110),0.60146004,Removed support for LightningCLI(seed_everything_default=None) (#16131),,0
1702,Update governance and codeowners,0.71576715,Updated governance docs,Co-authored-by: otaj 6065855+otaj@users.noreply.github.com Co-authored-by: rohitgr7 rohitgr1998@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
1703,fixes mypy errors in trainer/supporters.py (#14633),0.6136844,Deprecated TrainerModelHooksMixin in favor of pytorch_lightning.utilities.signature_utils (#7422),Bumps actions/checkout from 2 to 3. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: actions/checkout   dependency-type: direct:production   update-type: version-update:semver-major ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
1704,CI: update badges for release (#5002),0.51261556,Key updates,Bumps tj-actions/changed-files from 29.0.1 to 29.0.3. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: tj-actions/changed-files   dependency-type: direct:production   update-type: version-update:semver-patch ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
1705,Handle collision of user argument when using ShardedDDP (#9512),0.5915115,Pass init args to ShardedDataParallel (#9483),,0
1706,early stopping check_val_every_n_epoch fix (#743),0.79263675,Early stopping checks on_validation_end (#1458),,1
1707,docs (#4096),0.7550955,"docs for all Metrics (#2184, #2209)",,1
1708,Fix dtype inference during gradient norm computation (#14051),0.6540997,Gradient norm tracking (#16745),,0
1709,running tests,0.6689322,Extensively tested code.   ,,0
1710,Prefix log_metrics keys with class name in callbacks (#12228),0.6296951,- Added class name prefix to metrics logged by `DeviceStatsMonitor` ([#12228](https://github.com/Lightning-AI/lightning/pull/12228)),,0
1711,Update changelog after 1.7.1 release (#14127),0.7111658,Full Changelog,,1
1712,Update index.rst (#12519),0.55612826,Updated SSIM metric (#4566)(#4656),,0
1713,Remove the deprecated device_stats_monitor_prefix_keys (#14890),0.79831666,- Removed the deprecated `device_stats_monitor_prefix_metric_keys` ([#14890](https://github.com/Lightning-AI/lightning/pull/14890)),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
1714,eval step scaling factor (#3136),0.9999999,eval step scaling factor (#3136), pkg: include lite in PL Apply suggestions from code review ci: nb dirs  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
1715,Create pytorch_lightning/utilities/types.py (#7048),0.7874336,from pytorch_lightning import LightningModule,,1
1716,[CI] Bump CUDA in Docker images to 11.6.1 (#14348),0.50992703,- Added a more descriptive error message when attempting to fork processes with pre-initialized CUDA context ([#14709](https://github.com/Lightning-AI/lightning/pull/14709)),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
1717,Launch options for Lightning Lite (#14992),0.717286,from lightning.lite import LightningLite,Co-authored-by: Roberto Estevão robertode@microsoft.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
1718,Add ModelCheckpoint(save_on_train_epoch_end) (#8389),0.7983886,"The ModelCheckpoint.save_on_train_epoch_end attribute is now computed dynamically every epoch, accounting for changes to the validation dataloaders (#15300)",,1
1719,Write predictions in LightningModule instead of EvalResult (#3882),0.9977701,Write predictions in LightningModule instead of EvalResult [#3882, Fix message on BYOC cluster creation  Co-authored-by: thomas chaton thomas@grid.ai,1
1720,split profilers (#6261),0.9149544,Split profilers module (#6261),,1
1721,Maverick message fix (#16940),0.55643487,Updated Multinode Warning (#16091),,0
1722,Better error message if no loss was returned from model.training_step() (#294),0.68068373,"Deprecated model steps training_end, validation_end and test_end (#1051, #1056)",Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
1723,Update fit with val hook test (#8060),0.6419698,Disabled val and test shuffling (#1600),Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
1724,update org paths & convert logos (#685),0.5557065,Changed Checkpoint path parameter from filepath to dirpath (#1016),,0
1725,[App] Improve Storage Commands (#16645),0.58751744,Application storage prefix moved from app_id to project_id/app_id (#14583),,0
1726,drop block contribution do app (#14010),0.5196071,Prevent WandbLogger from dropping values (#5931),Co-authored-by: rohitgr7 rohitgr1998@gmail.com,0
1727,"Update neptune-client requirement from <0.16.3,>=0.10.0 to >=0.10.0,<0.16.4 in /requirements (#13416)",0.6459574,"It now supports neptune-client 0.16.16 and neptune >=1.0, and we have replaced the log() method with append() and extend().","bump of fsspec and s3fs to version supporting retry on ""SlowDown"" response",0
1728,Make optimizers skippable when using amp (#7975),0.7358948,- Extended optimizer support with particular frequency,,1
1729,added lbfgs support (#310),0.7336997,BFloat16 Support, [App][CLI] Fix lightning cli --version,1
1730,[3 / 3] improvements to saving and loading callback state (#7161),0.6862545,Callback hooks for loading and saving checkpoints, deps pinned Apply suggestions from code review  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
1731,fixed gpu map location,0.50231564,"Case 3: Model fits into GPU memory. No action required, use any strategy you want. ",,0
1732,yapf tests trainer (#5844),0.7326287,trainer.test(),"  update   update   update   update   Review of content   Formatting updates   Fomatting updates   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Updates based on new commits   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   update   update   Introduce lightning connect (#14183)   Co-authored-by: Luca Antiga luca.antiga@gmail.com Co-authored-by: Felonious-Spellfire felonious.spellfire@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Luca Antiga luca.antiga@gmail.com",1
1733,Update gitignore (#3595),0.58533394,Pkg: fix parsing versions by @Borda in https://github.com/Lightning-AI/lightning/pull/15401,,0
1734,Deprecate LightningLite (#16314),0.8385266,LightningLite:, Remove the unused pyDeprecate dependency CHANGELOG,1
1735,Do not sanity check on reload (#10785),0.6496852,Deprecated the on_sanity_check_start hook in ModelHooks (#598),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
1736,Added a check to validate that wrapped FSDP models are used while initializing optimizers (#15301),0.82004356,Generalized Optimizer validation to accommodate both FSDP 1.x and 2.x (#16733),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
1737,Update DataLoader.persistent_workers warnings in ddp_spawn (#6762),0.7645425,Changed warnings and recommendations for dataloaders in ddp_spawn (#6762), rm _check-shema.yml Reuse devtools' check schema,1
1738,Fairscale integration tests for Lite (#14921),0.59880865,FairScale deprecation (in favor of PyTorch's FSDP implementation) (#16353),,0
1739,fix dtype conversion of example_input_array in model summary (#2510),0.5774513,Remove model.trainer call inside of dataloading mixin (#7317),  new custom base image   image tag ,0
1740,Update CODEOWNERS (remove myself from defaults + some specifics) (#14084),0.56432134,Deprecated @data_loader decorator  (#926),,0
1741,[App] Rename to new convention (#15621),0.59646916,Rename failed -> error in tables (#15608),  Cluster logs improvements   Unit tests added   Labels for processing deletion errors ,0
1742,Added missing term 'Data' in 'LigthningModuleAPI' (#3284),0.69331795,Replaced _DataModuleWrapper with __new__ (#7289),,0
1743,Fix shuffle for distributed sampler (#2789),0.7383631,Enforced eval shuffle warning only for default samplers in DataLoader (#12653),"  update base requirements   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   try main   Apply suggestions from code review   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   extract into separate function   drop   up   up   optional   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  .  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",1
1744,added cpu model test,0.6862098,- Full tests that run multiple models in different configs,,0
1745,Replace instances of self.lightning_module.trainer with trainer directly in ddp_spawn and tpu_spawn (#8942),0.6989752,"Trainer(resume_from_checkpoint=...) now restores the model directly after LightningModule.setup(), which is before LightningModule.configure_sharded_model() (#7652)",,0
1746,Create loggers property for Trainer and LightningModule (#11683),0.77706707,Optionally customize logging in LightningModule,,1
1747,PoC: Accelerator refactor (#5743),0.7464185,Refactored Accelerators and Plugins (#5743),,1
1748,Add BaseModelCheckpoint class to inherit from (#13024),0.67788005,"Base classes (#1326, #1877)",  copy app conf   ci + req.   script symlink   wip   keep only App   add also PL   lightning   artifact ,0
1749,Merge setup_tools into assistant (#15847),0.58277684,setup(,,0
1750,ref: move prepare_data to data connector (#3307),0.97664094,move prepare_data to data connector (#3307), add e2e cron job trigger workflow_dispatch  Co-authored-by: Mansy ahmed.mansy156@gmail.com Co-authored-by: thomas chaton thomas@grid.ai,1
1751,drop v0.10 deprecated (#3454),0.66566265,Removed deprecated: (#2760),,0
1752,Rename SingleDevicePlugin to SingleDeviceStrategy (#11181),0.7926798,    * Renamed the `SingleDevicePlugin` to `SingleDeviceStrategy` ([#11182](https://github.com/PyTorchLightning/pytorch-lightning/pull/11182)),Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Marc Skov Madsen masma@orsted.com Co-authored-by: Laverne Henderson laverne.henderson@coupa.com Co-authored-by: Felonious-Spellfire felonious.spellfire@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Mansy ahmed.mansy156@gmail.com Co-authored-by: Jirka jirka.borovec@seznam.cz,1
1753,Remove deprecated mode argument from ModelSummary (#10449),0.8411016,"Removed deprecated ModelCheckpoint arguments prefix, mode=""auto"" (#6162)",,1
1754,[App] Fix AutoScaler trying to replicate multiple works in a single machine (#15991),0.6662706,Porting fixes to autoscaler component (#16249),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
1755,Complete mocking Comet and remove dep (#3910),0.622705,Removed deprecated: (#2760), update CJ job names groups filter Apply suggestions from code review  Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
1756,Update version to 1.5.0dev (#8585),0.66856563,"    version=""0.0.1"",",,0
1757,Never pickle the Trainer with the LightningModule (#17133),0.83430916,Pickling the LightningModule no longer pickles the Trainer (#17133),,1
1758,Sg3 (#4082),0.595311,Updated SSIM metric (#4566)(#4656),Bumps tj-actions/changed-files from 28 to 29.0.1. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: tj-actions/changed-files   dependency-type: direct:production   update-type: version-update:semver-major ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
1759,added on_hpc_load and on_hpc_save hooks,0.64392287,The signature and behavior of the on_load_checkpoint and on_save_checkpoint callback hooks have changed (#14835):,Co-authored-by: awaelchli aedu.waelchli@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
1760,move profiler docs (#11431),0.7347948,Moved profilers to their own file (#7822),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
1761,Black format pytorch_lightning/core/lightning.py (#3576),0.80922073,+ # pytorch_lightning==1.7.0,Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
1762,ci: switch to utils check (#17122),0.59613156,Early stopping checks on_validation_end (#1458),  remove deprecated callback hook   changelog ,0
1763,Easier configurability of callbacks that should always be present in LightningCLI (#7964),0.7558783,LightningCLI.instantiate_trainer now takes a config and a list of callbacks (#8721),Co-authored-by: otaj ota@lightning.ai,1
1764,Remove obsolete pre_dispatch in DDPSpawnShardedPlugin (#10988),0.7055229,Merged the implementation of DDPSpawnStrategy into DDPStrategy and removed DDPSpawnStrategy (#14952),,1
1765,update NGC docker (#7787),0.657272,Changed to the NeptuneLogger (#16761):,,0
1766,update GPU to PT 1.5 (#2779),0.7439731,GPU training (#2704),,1
1767,"Update mlflow requirement from <=1.24.0,>=1.0.0 to >=1.0.0,<1.27.0 in /requirements (#13127)",0.69062155,Updated mlflow with using resolve_tags (#6746),Co-authored-by: rohitgr7 rohitgr1998@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
1768,distinguished between overfit_batches examples (#3298),0.7917849,overfit_pct in favour of overfit_batches,Bumps ravsamhq/notify-slack-action from 1 to 2. - Release notes - Commits  updated-dependencies: - dependency-name: ravsamhq/notify-slack-action   dependency-type: direct:production   update-type: version-update:semver-major ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,1
1769,"Remove the ""native"" suffix from the codebase (#16490)",0.7341871,"- ""Native"" suffix removal ([#16490](https://github.com/Lightning-AI/lightning/pull/16490))",Update mlflow requirement in /requirements Updates the requirements on mlflow to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: mlflow   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,1
1770,ref: device parser (#3400),0.9368509,"device parser (#3400, #3405)",,1
1771,make the error message readable (#2729),0.5746014,Enabled manual returns (#4089),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
1772,CI: adjust gatekeeper (#13604),0.5711982,Configuration Validator (#9779),"  disable non-blocking for mps due to race condition bug   fixed typo   fixed: unknown mps device for non arm systems   Removed unrobust test case   moved _MPS_DEVICES such that we used in apply_func   Resolve circular dependencies   Comment rewording   changed torchElasticEnvironment to a global import   simplified if statement to blocking device type   Added change to CHANGELOG   Update src/pytorch_lightning/utilities/apply_func.py   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   fixed mypy not detecting casting of device   Moved check into if statement to mainain original behavior   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",0
1773,Update lightning_module.rst (#3854),0.79543316,Update the Lightning App docs (#13537),,1
1774,ref: fixes logging for eval steps (#3763),0.7912319,moved eval loop logging to loggers (#3408),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: rohitgr7 rohitgr1998@gmail.com Co-authored-by: otaj ota@lightning.ai,1
1775,Fix RunningStage properties for sanity checking (#16774),0.72147727, - Sanity checking: `Trainer(num_sanity_val_steps>0)`,  let environment disable forking   add helper function and error messages   tests   changelog   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
1776,Change version in init.py 1.0.4 -> 1.1-dev (#4760),0.62197936,"    version=""0.0.1"",",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
1777,training loop refactor - move val loop  (#8120),0.8307562,Refactored training loop (#2336),Co-authored-by: rohitgr7 rohitgr1998@gmail.com Co-authored-by: otaj 6065855+otaj@users.noreply.github.com,1
1778,removed coverage file,0.6822573,Removed,,0
1779,Pass epoch argument to Comet Logger (#3438),0.68154526,Using .comet.config file for CometLogger (#1913),,0
1780,ci/hotfix: replace SD with Flashy (#16584),0.55809325,Changed fsspec to tuner (#4458),,0
1781,Deprecate LightningDataModule.on_save/load_checkpoint (#11893),0.87219834,LightningDataModule.load_from_checkpoint,,1
1782,Add docs about example dependencies (#2122),0.585338,Fixing critical bugs in newly added hooks and hparams assignment., Azure CPU pool Fix empty env LAI vars  Co-authored-by: manskx ahmed.mansy156@gmail.com,0
1783,[App] Refactor cloud dispatch and update to new API (#16456),0.97711927,Refactor cloud dispatch and update to new API (#16456),,1
1784,Fix a typo in logger arg in trainer.py (#17206),0.63872576,"$ python script.py fit --trainer.logger=WandbLogger --trainer.logger.name=""my_lightning_run""",Co-authored-by: Christian Schell christian.schell@uni-wuerzburg.de Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
1785,Do not reset Loops total counters (#8475),0.7337167,Do not reset the progress tracking dataclasses total counters (#8475),followup to #13929,1
1786,GPU Usage Logger (#2932),0.7905428,Log just the GPU stats,,1
1787,Fix manual optimization in pl_example (#6373),0.75022644,Removed deprecated Trainer argument enable_pl_optimizer and automatic_optimization (#6163),Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: otaj ota@lightning.ai,1
1788,Support re-instantiation for custom DataLoader in Lightning (#10680),0.752903,- Improved support for custom `DataLoader`s when instantiated in `*_dataloader` hook ([#12981](https://github.com/Lightning-AI/lightning/pull/12981)),"  Remove mps config for test   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",1
1789,Refactor tests to use BoringModel (#7401),0.59013134,Deprecated the TestTubeLogger (#9065),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
1790,3/n Move accelerator into Strategy (#11022),0.7900117,move specific accelerator code (#3457),Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
1791,fix warning (#3538),0.6962452,Improved error messages for invalid configure_optimizers returns (#3587),,0
1792,Add support for fsspec paths for CSVLoggers (#16880),0.643148,Used fsspec instead of gfile for all IO (#3320),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com,0
1793,fix(loggers): add best and latest aliases to wandb artifact in WandbLogger (#17121),0.6885159,"The default project name in WandbLogger is now ""lightning_logs"" (#14145)",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
1794,Fix replace_sampler missing the batch size under specific conditions (#9367),0.6392585,Reset the dataloaders on OOM failure in batch size finder to use the last successful batch size (#14372),Bagua trick needs to be replicated on everywhere applicable,0
1795,prevent internal call to has_prepared_data showing a warning (#8271),0.6567128,Avoid redundant callback restore warning while tuning (#13026),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
1796,update stale bot (#4769),0.5993694,- Removed deadlock detection / process reconciliation (`PL_RECONCILE_PROCESS=1`) ([#16204](https://github.com/Lightning-AI/lightning/pull/16204)),  release App 0.6.0 RC   req ,0
1797,Remove InternalDebugger.track_lr_schedulers_update (#9653),0.6937475,Removed pytorch_lightning.utilities.debugging.InternalDebugger (#9680),"  pinning starsessions   pinning starsessions   adding strict back to requirements.txt   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Duplicated   Basic implementation   Basic implementation   Basic implementation   Basic implementation   Common things moved to log helpers file   Decomposing logs reader classes for reusing   Setting colors for log levels   Manifest trimming   Changes added to CHANGELOG   Prettifications   Prettifications   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Logs function name change   Logs function name change   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   attempt to fix the pydanitc import   Tests + command name fixes   Extending tests   Adding limit argument   Unmerging CI fix   Unmerging CI fix   Adding fields for errors   Adding log level fixed field width   Adding absent typing + exeptions raising   Adding socket error logging   Addressing comments on cluster list function return value   Addressing comments on adding e2e tests   Adding version range for arrow package in reqs   New unit tests   arrow time parsing callback modified + unit tests   helpers updated   helpers updated   helpers updated   One more test   CMD test fix   CMD test fix   CMD test fix   CMD test fix   CMD test fix   LightningClient mocking   Flaky test removed   Co-authored-by: hhsecond sherin@grid.ai Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",0
1798,black: magic trailing comma (#8560),0.53668576,"    },",,0
1799,PyTorch 2.0 switched the set_to_none default (#16531),0.70130014,Drop PyTorch 1.9 support (#15347),enable testing of react e2e,1
1800,[Metrics] Detach bugfix (#4313),0.6994827,Remove MetricsHolder (#7909),  Update for M1 Mac installations   Apply suggestions from code review   Update PL installation   Update based on feedback   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
1801,move pytorch_lightning >> lightning/pytorch (#16594),0.75493586,- Added `LightningModule.lr_scheduler_step` ([#10249](https://github.com/PyTorchLightning/pytorch-lightning/pull/10249)),  Recreated the access_app_state file   Update the site's TOC to include the file   Update code sample file path   Minor formatting update   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Sherin Thomas sherin@grid.ai,1
1802,Cast only floating types with IPUs (#13983),0.7747841,Casted only floating point tensors to fp16 with IPUs (#13983),"  bump cuda in docker images to 11.6.1   PUSH TO HUB. REVERT THIS!   conda forge for 11.6   cuda 11.5   revert conda changes   11.6 back again   11.6 back again, all of them   maybe all passes now   maybe all passes now   final push   Revert ""PUSH TO HUB. REVERT THIS!""   This reverts commit 602bfce224cf22e24421448887844937e0aff9f0.  Apply suggestions from code review  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",1
1803,[bugfix] Apex never instantiated. (#7274),0.57183254,Resolved a bug where the work statuses will grow quickly and be duplicated (#13970),  chlog after App 0.5.6 & 0.5.7   . ,0
1804,enable highlight (#170),0.60289186,Highlights,fix collections.abc for py3.10 Co-authored-by: Sherin Thomas sherin@grid.ai,0
1805,Fix tpu spawn plugin test (#11131),0.660126,Updated logic for checking TPUs availability (#6767),  start CI   wip   matrix   name   wip   prune   rm   ls   cache   dir   ls   name   cleaning   clone   git   git   if   private   .   local_id   clean   var   group check   ci ,0
1806,resolve bug (#6781),0.6731186,Resolve bug with Finetuning (#5744),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
1807,feat: update pyproject.toml (#16430),0.67226845,"PyTorch 1.10 and Python 3.7 no longer supported (#16492, #16579)",  Add back support for logging in the gradient clipping hooks   Docs and CHANGELOG   Fix tests ,0
1808,Update lm_test_module.py,0.5924281,Skipped testing with PyTorch 1.7 and Python 3.9 on Ubuntu (#11217),Bumps tj-actions/changed-files from 24 to 28. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: tj-actions/changed-files   dependency-type: direct:production   update-type: version-update:semver-major ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
1809,Don't try to aggregate requirements/__pycache__/base.txt in setuptools (#15775),0.55299807,setup(,The recent release of starsessions 2.0 has broken lightning app as some of the arguments are removed. This PR also fixes a bug in our setup tools that prevents our internal # strict parameter being considered. Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
1810,Remove redundant find_unused_parameters=False in Lite (#16026),0.7476726,Changed the default of find_unused_parameters to False in DDP (#5185),,1
1811,Fix checkpointed state for lr_schedulers with step interval (#7877),0.7580894,Changed lr schedule step interval behavior to update every backwards pass instead of every forwards pass (#1477),,1
1812,[docs] Remove the redundant indents in trainer.py (#4720),0.66865623,Removed the deprecated TrainerTrainingTricksMixin class (#8679),,0
1813,dispatch cache clean (#16393),0.6322547,clean up data reset (#3161),,0
1814,Fix progress bar updates for Pod Training (#8258),0.7267436,"trainer.{logged,progress_bar,callback}_metrics are now updated on-demand (#7882)",,1
1815,Merge pull request #22 from williamFalcon/loading,0.5491322,refactored dataloader process hook (#3139),Co-authored-by: otaj 6065855+otaj@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
1816,ref: inner train loop (intermediate step) 6/n (#3366),0.7778492,"inner train loop (#3359, #3361, #3362, #3363, #3365, #3366, #3367, #3368, #3369, #3370, #3371, #3372, #3373, #3374, #3375, #3376, #3385, #3388, #3397)","The local app '/admin' route is being removed because it does not provide any value, so we need to remove references from it in the CLI.",1
1817,Raise an error when lightning replaces an existing sampler (#2020),1.0,Raise an error when lightning replaces an existing sampler (#2020),,1
1818,Removed deprecated property is_using_torchelastic from AcceleratorConnector (#9729),0.9041647,Removed deprecated property AcceleratorConnector.is_using_torchelastic in favor of TorchElasticEnvironment.is_using_torchelastic() (#9729),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
1819,add autopep8 to Contributions guide (#852),0.58156145,Porting fixes to autoscaler component (#16249),Bumps actions/upload-artifact from 2 to 3. - Release notes - Commits  updated-dependencies: - dependency-name: actions/upload-artifact   dependency-type: direct:production   update-type: version-update:semver-major ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
1820,ommit templates folder,0.50640965,  * `save_config_multifile`,Bumps actions/setup-node from 2 to 3. - Release notes - Commits  updated-dependencies: - dependency-name: actions/setup-node   dependency-type: direct:production   update-type: version-update:semver-major ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
1821,Add path filters to the TPU job (#14543),0.6527482,Enabled manual optimization for TPUs (#8458),Bumps actions/setup-python from 2 to 4. - Release notes - Commits  updated-dependencies: - dependency-name: actions/setup-python   dependency-type: direct:production   update-type: version-update:semver-major ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
1822,Tests: refactor models (#1691),0.6802578,"Refactored training_batch + tests to verify correctness (#2327, #2328)",Updates the requirements on ipython[all] to permit the latest version. - Release notes - Commits  updated-dependencies: - dependency-name: ipython[all]   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
1823,Remove undefined name from __all__ (#8468),0.47679248,Removed deprecated BaseProfiler.output_filename arg from it and its descendants in favor of dirpath and filename (#9214),,0
1824,Support checkpoint hooks on data module (#3563),0.7910902,DataModule hooks for loading and saving checkpoints,,1
1825,Support DataLoaders with missing arguments in replace_sampler (#8519),0.79051244,Improved error messages in replace_sampler when the DataLoader attributes are not included in the signature or the signature is missing optional arguments (#8519),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
1826,update training type plugin docs regarding result caching (#7261),0.66643655,Automatically set sync_batchnorm for training_type_plugin (#6536),  Update CODEOWNERS   Update .github/CODEOWNERS ,0
1827,update nightly & upgrade Twine (#5458),0.57665735,Key updates,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
1828,refactor python in GH actions (#5281),0.5490401,- `Loop.restarting=...` now sets the value recursively for all subloops ([#11442](https://github.com/PyTorchLightning/pytorch-lightning/pull/11442)),,0
1829,ref: test val flow steps (#3723),0.8153109,tests for val loop flow (#2605),Co-authored-by: otaj ota@lightning.ai Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
1830,Fix deepspeed keeping old sub-folders in same ckpt path (#12194),0.86847353,- Fixed deepspeed keeping old sub-folders in same ckpt path ([#12194](https://github.com/PyTorchLightning/pytorch-lightning/pull/12194)),  Fix access to logger attribute when multiple loggers are used   add changelog ,1
1831,Fixes various typing errors in pytorch_lightning/strategies/deepspeed.py (#13832),0.7411175,+ # pytorch_lightning==1.7.0,Update scikit-learn requirement in /requirements Updates the requirements on scikit-learn to permit the latest version. - Release notes - Commits  updated-dependencies: - dependency-name: scikit-learn   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,1
1832,resolve conflict,0.45588285,This conversation was marked as resolved by carmocca,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
1833,Option to provide seed to random generators to ensure reproducibility (#1572),0.6127232,pl.seed_everything will now also set the seed on the DistributedSampler (#7024),Fixes formatting for a note and makes headings for arguments and parameters stand out,0
1834,"Update typing-extensions requirement from <=4.1.1,>=4.0.0 to >=4.0.0,<4.2.1 in /requirements (#13054)",0.52449775,Improved exception message if rich version is less than 10.2.2 (#10839), Update torch requirement in /requirements  Updates the requirements on torch to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: torch   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com   Update base.txt   Update adjust_versions.py   Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com,0
1835,Fix amp autocast  (#6080),0.68071854,enable_pl_optimizer=False by default to temporarily fix AMP issues (#5163),,0
1836,Use an uniform call hook style in the loops (#12742),0.66335833,final inner eval loop hooks (#3154),Update tensorboard requirement in /requirements Updates the requirements on tensorboard to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: tensorboard   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
1837,release v0.5.1.3,0.8285334,"    version=""0.0.1"",",,1
1838,reorder deprecated args (#1230),0.66864365,"organize args (##3435, #3442, #3447, #3448, #3449, #3456)",,0
1839,fix nightly releasing (#5596),0.60336757,This release fixes that core issue,Co-authored-by: otaj 6065855+otaj@users.noreply.github.com,0
1840,"Update streamlit requirement from <1.16.1,>=1.13.0 to >=1.13.0,<1.22.1 in /requirements (#17589)",0.58830214,- Changed minimum supported version of `rich` from `10.14.0` to `12.13.0` ([#16798](https://github.com/Lightning-AI/lightning/pull/16798)),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
1841,Teardown sync-batchnorm after training (#11078),0.6736612,Changed teardown() in Accelerator to allow training_type_plugin to customize teardown logic (#7579),,0
1842,Add missing line. Add a test (#3594),0.53378403,"Removed output argument from *_batch_end hooks (#3965, #3966)",,0
1843,Fixing small typo in documentation. (#13215),0.5581231,Resolve bug with Finetuning (#5744),  CI: docker focus on PL only   group ,0
1844,Add support for Habana accelerator (HPU) (#11808),0.8260952,- Added support for Habana Accelerator (HPU) ([#11808](https://github.com/PyTorchLightning/pytorch-lightning/pull/11808)),  add a warning   add test   add test   add changelog   remove todo   clarify http won't work in cloud   Apply suggestions from code review   Co-authored-by: Sherin Thomas sherin@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Sherin Thomas sherin@grid.ai,1
1845,Fix DDP_SPAWN compatibility with bug_report_model.py (#6892),0.6651584,"Implemented {DDPShardedPlugin,DDPShardedSpawnPlugin}._setup_model_and_optimizers (#10028, #10064)",Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
1846,"Fix failing profiler tests for CI: conda (3.7, 1.8) (#6765)",0.56219447,Removed ProfilerConnector (#7654),"  Moved app.py to main app directory   updated docs   updated changelog   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",0
1847,"Update pandas requirement from <1.5.4,>1.0 to >1.0,<2.0.2 in /requirements (#17534)",0.71044624,Removed dependency on pandas (#736)," BYOC content  Content for the upcoming BYOC feature   First DRAFT of BYOC content   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  Update docs/source-app/index.rst  Co-authored-by: thomas chaton thomas@grid.ai  Update docs/source-app/workflows/byoc/index.rst  Co-authored-by: thomas chaton thomas@grid.ai  Update docs/source-app/workflows/byoc/index.rst  Co-authored-by: thomas chaton thomas@grid.ai   Updates based on feedback   Updates based on feedback   Update external ID with note   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  Update docs/source-app/workflows/byoc/index.rst  Co-authored-by: Raphael Randschau nicolai86@users.noreply.github.com  Update docs/source-app/workflows/byoc/index.rst  Co-authored-by: Raphael Randschau nicolai86@users.noreply.github.com  Update docs/source-app/workflows/byoc/index.rst  Co-authored-by: Raphael Randschau nicolai86@users.noreply.github.com  Update docs/source-app/workflows/byoc/index.rst  Co-authored-by: Raphael Randschau nicolai86@users.noreply.github.com  Update docs/source-app/workflows/byoc/index.rst  Co-authored-by: Raphael Randschau nicolai86@users.noreply.github.com  Updates for terraform mod  Updates for terraform mod and arg pram split  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci  Update docs/source-app/workflows/byoc/index.rst  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update docs/source-app/workflows/byoc/index.rst  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   Update index.rst   Update docs/source-app/workflows/byoc/index.rst   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update docs/source-app/workflows/byoc/index.rst  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update docs/source-app/workflows/byoc/index.rst  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update content with table  Changed bullets into table based on feedback  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Raphael Randschau nicolai86@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com",1
1848,Merge pull request #16 from williamFalcon/tests,0.51247585,"Changed setup() and teardown() stage argument to take any of {fit,validate,test,predict} (#6386)",,0
1849,Remove pytorch_lightning.core.memory.get_gpu_memory_map (#12644),0.8951062,"- Removed the deprecated `pytorch_lightning.core.memory.{get_gpu_memory_map,get_memory_profile}` ([#12659](https://github.com/Lightning-AI/lightning/pull/12659))",  CI: clean building docs   group   . ,1
1850,Mention that the Trainer has started in barebones mode (#16926),0.845521,Barebones Trainer mode (#16854),  add more issues types   Update .github/ISSUE_TEMPLATE/config.yml   Co-authored-by: Mansy ahmed.mansy156@gmail.com  typo  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Mansy ahmed.mansy156@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Laverne Henderson laverne.henderson@coupa.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com,1
1851,omitted mention of QuantizationAwareTraining callback (#17646),0.6840261,- Removed the `QuantizationAwareTraining` callback ([#16750](https://github.com/Lightning-AI/lightning/pull/16750)),,0
1852,Call super().__init__() in MilestonesFinetuning example (#7398),0.7324812,        super().init(),  fix install latest version of app/component   Add changelog   a better testcase   Update src/lightning_app/CHANGELOG.md   Co-authored-by: mansy mansy@lightning.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
1853,Remove the deprecated LightningCLI arguments (#16380),0.8737248,- Removed the deprecated `LightningCLI` arguments ([#16380](https://github.com/Lightning-AI/lightning/pull/16380)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Laverne Henderson laverne.henderson@coupa.com,1
1854,Ensure we check if we should use sharded amp plugin,0.5812642,        :param use_amp: Whether amp was requested or not,,0
1855,Moved env_vars_connector._defaults_from_env_vars to utilities.argsparse._defaults_from_env_vars (#10501),0.85944873,- Moved `trainer.connectors.env_vars_connector._defaults_from_env_vars` to `utilities.argsparse._defaults_from_env_vars` ([#10501](https://github.com/PyTorchLightning/pytorch-lightning/pull/10501)),,1
1856,ref: continue #3733 (#3767),0.60082924,[1.7.5] - 2022-09-06,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: otaj 6065855+otaj@users.noreply.github.com,0
1857,tests for legacy checkpoints (#5223),0.65976155,Resuming from checkpoints (#16167),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
1858,ci: update signaling (#15981),0.62647676,The preemption/termination signal is now configurable (#14626):,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
1859,Only allow one value for each plugin type in plugins flag (#12083),0.6593321,Made the Plugin.reduce method more consistent across all Plugins to reflect a mean-reduction by default (#6011),,0
1860,Update seed_everything() (#6843),0.7227199,pl.seed_everything will now also set the seed on the DistributedSampler (#7024),Co-authored-by: rohitgr7 rohitgr1998@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: awaelchli aedu.waelchli@gmail.com,1
1861,"Add on_predict_{batch,epoch}_{start,end} and Callback.on_predict_{start,end} (#7141)",0.71417344,"  * `Callback.on_pretrain_routine_{start,end}` in favor of `Callback.on_fit_start`", adjust CLI copy  Co-authored-by: RobertLaurella 99420295+RobertLaurella@users.noreply.github.com,1
1862,cleaning typing CI and config (#16860),0.6094909,"Cleaning (#5948, #5949, #5950)",Co-authored-by: Jirka jirka.borovec@seznam.cz,0
1863,Correct documentation examples of optimizer_step (#3326),0.81489086,    optimizer1.step(),,1
1864,1/n inter batch parallelism (#9020),0.89760184,Inter Batch Parallelism,Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
1865,Docs fixes (#6870),0.70304245,Docs improvements," [CLI] change cluster creation cost savings mode default  instead of having customers opt-into cost savings mode, we'll ask them to opt-out of cost savings mode.",1
1866,Add inference_mode flag to Trainer (#15034),0.7930046,- Added `inference_mode` flag to Trainer to let users enable/disable inference mode during evaluation ([#15034](https://github.com/Lightning-AI/lightning/pull/15034)),initial work adding drives to create work API from framework cloud dispatcher,1
1867,Add back clip_gradients(model) (#7231),0.6507138,"By overriding the LightningModule.configure_gradient_clipping hook, you can customize gradient clipping to your needs:",,0
1868,Wrap prepare_data and setup only once inside DataModule (#3654),0.76932526,"DataModules now avoid duplicate {setup,teardown,prepare_data} calls for the same stage (#7238)",,1
1869,Throw MisconfigurationError on unknown mode (#5255),0.6581025,Replace MisconfigurationException with warning in ModelCheckpoint Callback (#4560),,0
1870,missing (#2990),0.56251514,Removed deprecated: (#2760),,0
1871,Fix TrainsLogger doctest failing (switch to bypass mode in GitHub CI) (#1379),0.6025973,  * Removed the `LoggerConnector.on_train_split_start` method,,0
1872,updated required packages,0.5557487,Parsed local package versions (#13933),update,0
1873,Fix TensorBoardLogger creating redundant experiment when finalizing (#14762),0.71149933,Fix reset TensorRunningAccum (#5106),"  Add S3 protocol and optimization field to the drive object   Add a list of drives to the work specification   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   add only protocol for s3 drives, no optimization arguments, and add tests   added trailing slash criteria   allow slash in s3 drives   fix   fixed test issues   Co-authored-by: Panos Lantavos-Stratigakis default-email@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Rick Izzo rick@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rick Izzo rlizzo@users.noreply.github.com",1
1874,release v0.2.5,0.7847237,0.4.0,Updates the requirements on onnxruntime to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: onnxruntime   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,1
1875,Fix mypy in utilities.distributed (#8201),0.5368612,"- Fixed an issue where `DistributedSampler.set_epoch` wasn't getting called during `trainer.predict` ([#16785](https://github.com/Lightning-AI/lightning/pull/16785), [#16826](https://github.com/Lightning-AI/lightning/pull/16826))",Update gcsfs requirement in /requirements Updates the requirements on gcsfs to permit the latest version. - Release notes - Commits  updated-dependencies: - dependency-name: gcsfs   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
1876,Update ci badge (#9339),0.5131879,Key updates,,0
1877,fixes tpu data loader bug (#957),0.6578287,Updated logic for checking TPUs availability (#6767),,0
1878,Correct CWD for ddp subprocesses when using Hydra (#2719),0.6137666,Resolve interpolation bug with Hydra (#5406) ,,0
1879,ci typo in cofig (#4954),0.46149722,Pinned sphinx-autodoc-typehints with <v1.15 (#11400),,0
1880,Add missing codeowners for app package (#13542),0.63165295,Included app templates to the lightning and app packages (#13731),,0
1881,fix: keep pre versions (#14752),0.66246474,Set version as today (#13906),,0
1882,"Update lightning-utilities requirement from <0.5.0,>=0.4.2 to >=0.4.2,<0.6.0 in /requirements (#16371)",0.7589539,Gave warnings for unimplemented required lightning methods (#1317),  Update CODEOWNERS   Cleanup and remove old sections   pl focus   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
1883,Add back standalone task azure CI step (#14366),0.6391215,Reset metrics before each task starts (#9410),Remove duplicated classes,0
1884,formatting to PL utils (#5713),0.6443496,Simplify the PL examples structure (shallower and more readable) (#1247),Co-authored-by: Adam J. Stewart ajstewart426@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com,0
1885,[bugfix] Reduce memory leaks (#8490),0.7806363,Resolve memory leak for evaluation (#6326),  initial changes for lightning   Update .github/BECOMING_A_CORE_CONTRIBUTOR.md   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
1886,reduce parity to 0.22,0.55701876,reduced all simplified forward (#3126),,0
1887,fix TPU parsing and TPU tests (#2094),0.74506116,Updated logic for checking TPUs availability (#6767),,1
1888,typing error,0.5816078,Refactored setup for typing friendly (#6590),,0
1889,Add docs for IPUs (#7923),0.6062094,Graphcore IPU devices,"  append cuda version to tags   revertme: push to hub   Update docker readme   Build base-conda-py3.9-torch1.12-cuda11.3.1   Use new images in conda tests   revertme: push to hub   Revert ""revertme: push to hub""   This reverts commit 0f7d534b2ae41e4bd227961a929c333c88e35f59.  Revert ""revertme: push to hub""  This reverts commit 46a05fccbb9b596aa98d5d68424917b5811c5b4f.   Run conda if workflow edited   Run gpu testing if workflow edited   Use new tags in release/Dockerfile   Build base-cuda and PL release images with all combinations   Update release docker   Update conda from py3.9-torch1.12 to py3.10-torch.1.12   Fix ubuntu version   Revert conda   revertme: push to hub   Don't build Python 3.10 for now...   Fix pl release builder   updating version contribute to the error? https://github.com/docker/buildx/issues/456   Update actions' versions   Update slack user to notify   Don't use 11.6.0 to avoid bagua incompatibility   Don't use 11.1, and use 11.1.1   Update .github/workflows/ci-pytorch_test-conda.yml   Co-authored-by: Luca Medeiros 67411094+luca-medeiros@users.noreply.github.com   Update trigger   Ignore artfacts from tutorials   Trim docker images to distribute   Add an image for tutorials   Update conda image 3.8x1.10   Try different conda variants   No need to set cuda for conda jobs   Update who to notify ipu failure   Don't push   update filenaem   Co-authored-by: Luca Medeiros 67411094+luca-medeiros@users.noreply.github.com",0
1890,Add backward-compatibility for LightningLite in PL (#14735),0.7176038,LightningLite,,1
1891,update changelog after 1.0.6 (#4624),0.71826476,Full Changelog: https://github.com/Lightning-AI/lightning/compare/1.8.0...1.8.0.post1,,1
1892,Fix move_metrics_to_cpu with evaluation (#10631),0.7384217,Removed the Trainer(move_metrics_to_cpu=True) argument (#16358),Update windows,1
1893,Docs11 (#1054),0.7135122,Docs,  Rename workflow files   Update docs   Fix azure badges   Update the main readme   bad rebase   Update doc ,1
1894,Support sharded optimizer state dumping outside of sharded strategies (#14208),0.7393751,- Added support for saving sharded optimizer state dict outside of `DDPShardedStrategy` ([#14208](https://github.com/Lightning-AI/lightning/pull/14208)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
1895,fix(tests): use display_name in app/e2e (#16632),0.5257433,Updated app testing (#16000),Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: Kushashwa Ravi Shrimali kushashwaravishrimali@gmail.com Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
1896,Create index.md,0.4506557,Meta Module,,0
1897,Fix typo in CI/CD doc (#13196),0.5114824,Changed overwrite to True (#16009),"fix cluster creation CLI requiring instace-type selection we've marked instance_types as required=False, but the CLI calls split on the value. So if nothing is provided, we'll actually receive a runtime error, effectively rendering the flag as required. Co-authored-by: thomas chaton thomas@grid.ai",0
1898,Mark logger_connector as protected (#12195),0.7504563,- Marked `trainer.logger_connector` as protected ([#12195](https://github.com/PyTorchLightning/pytorch-lightning/pull/12195)),,1
1899,"Revert ""Remove limitation of batch scaler (#4006)"" (#4040)",0.7241317,Reset epoch progress with batch size scaler (#13846),,1
1900,Bump Lightning-AI/utilities from 0.7.1 to 0.8.0 (#17052),0.7077271,- Deprecated `LightningDeepSpeedModule` ([#14000](https://github.com/Lightning-AI/lightning/pull/14000)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
1901,Update warnings in TrainingTricksConnector (#9595),0.69105643,Removed Warning from trainer loop (#1634),,0
1902,Fix some missing code in step-by-step walk through (#10519),0.5470252,Resolve bug with Finetuning (#5744),Co-authored-by: Anton Shevtsov aeshevtsov@avito.ru Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
1903,Update Lite docs (#10347),0.6840259,Update the Lightning App docs (#13537),,0
1904,Update governance (#15478),0.7897868,Updated governance docs,,1
1905,rename docs/source-app & adjust docs links for lightning (#16676),0.81831026,Update the Lightning App docs (#13537),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: otaj 6065855+otaj@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
1906,Fixes #2936 (no fix needed) (#3892),0.65091586,This release fixes that core issue, Update docker images to build,0
1907,Test deepcopy for progress tracking dataclasses (#8265),0.6812344,"Connect the progress tracking dataclasses to the loops (#8244, #8362)",,0
1908,Automatic PyPi Release (#1324),0.7359319,PyTorch 1.5  support,Update CODEOWNERS,1
1909,Reuse existing commands when running connect more than once (#15471),1.0000002,Reuse existing commands when running connect more than once (#15471),Co-authored-by: awaelchli aedu.waelchli@gmail.com,1
1910,Fix compatibility with old checkpoints and fault-tolerance enabled (#11439),0.7190454,Refactor load in checkpoint connector (#4593),Update tqdm requirement in /requirements Updates the requirements on tqdm to permit the latest version. - Release notes - Commits  updated-dependencies: - dependency-name: tqdm   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
1911,Remove deprecated training type plugins (#14011),0.83600664,- Removed all deprecated training type plugins ([#14011](https://github.com/Lightning-AI/lightning/pull/14011)),,1
1912,remove message (#8163),0.60753644,Removed deprecated callbacks (#3979),,0
1913,ref: clean up hooks in run_evaluation (#3156),0.9809928,clean up hooks in run_evaluation (#3156),,1
1914,release v0.7.1,0.82562774,"    version=""0.0.1"",", Apply suggestions from code review  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: otaj 6065855+otaj@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
1915,Change structure of walkthrough (#2949),0.56522083,Renames model steps (#1051),,0
1916,Add descriptions to accelerator broadcast function/clean up all_gather (#6044),0.6637561,Accelerator all_gather supports collection (#5221),,0
1917,updated test runner,0.64385396,trainer.test(),,0
1918,ensure calling test multiple times does not change results (#3391),0.6504129,- fixed all the .test() calls,"  free requirements   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   typo   typo   ui   mypy   todo   mypy   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  mypy  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com",0
1919,Save / Load Hyperparameters with checkpoint (#415),0.7023293,Add ignore param to save_hyperparameters (#6056),,1
1920,add simple CircleCI (#2609),0.5771135,"adding compute environments (#3837, [#3842)","  add support for listing apps   update changelog with correct PR number   add tests for pagination   fix wrong mock on test_cli   ensure all enum values are accounted for   make AppManager and AppList protected, add limit to pagination calls   add restarting transition /w tests   add state transition not yet run with tests ",0
1921,Refa22 (#3388),0.65376747,Refactored EpochResultStore (#5522),,0
1922,docs: add hint about register_buffer (#2577),0.5986011,"Show a message when BuildConfig(dockerfile=""..."") is passed but a Dockerfile file is already present in the Work (#15799)",,0
1923,Replace custom AllGather implementation (#11531),0.6468962,Replaced _DataModuleWrapper with __new__ (#7289),,0
1924,Use LRScheduler for torch >= 1.14 otherwise use _LRScheduler (#15768),0.70318943,Disable torch.inference_mode with torch.compile in PyTorch 2.0 (#17215),,1
1925,[ci] Unpin pip==20.1 (#6375),0.53103423,The PrecisionPlugin.backward hooks no longer returns a value (#8328),,0
1926,"Update fsspec[http] requirement from !=2021.06.0,<2022.6.0,>=2021.05.0 to >=2021.05.0,!=2021.06.0,<2022.8.0 in /requirements (#14288)",0.5385617,Updated model_checkpoint's to_yaml to use fsspec open (#3801),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
1927,Remove the DeepSource integration (#10056),0.63772726,- Removed the deprecated `BaseProfiler` and `AbstractProfiler` classes ([#14404](https://github.com/Lightning-AI/lightning/pull/14404)),,0
1928,Use per-package imports in docs (#15124),0.66342,import argparse,update cuda_version 11.1 -> 11.1.1,0
1929,release v0.3.6.6,0.73938394,"    version=""0.0.1"",",Bumps JamesIves/github-pages-deploy-action from 4.1.4 to 4.4.0. - Release notes - Commits  updated-dependencies: - dependency-name: JamesIves/github-pages-deploy-action   dependency-type: direct:production   update-type: version-update:semver-minor ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,1
1930,Fixed TypeError on 1.7.6 when torch.distributed unavailable (#14809),0.6607088,- Fixed issue where the CLI fails with certain torch objects ([#13153](https://github.com/Lightning-AI/lightning/pull/13153)),Update mlflow requirement in /requirements Updates the requirements on mlflow to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: mlflow   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
1931,Avoid unnecessary list creation (#8595),0.5444398,Avoid optional Tracker attributes (#9320),,0
1932,Update setuptools requirement from <65.6.0 to <65.7.0 in /requirements (#15902),0.5955282,Setup: added requirement freeze for the next major version (#14480),,0
1933,Move TrainerCallbackHookMixin.on_save/load_checkpoint  to Trainer and rename for clarity (#11179),0.70141107,Resuming from a checkpoint is no longer done by specifying the filename in the Trainer constructor. The Trainer(resume_from_checkpoint=...) argument was renamed and moved to the individual Trainer methods.,Add --cluster-id flag which can be passed to lightning run app if the --cloud flag is present. This allows you to run your Lightning AI apps on Lightning AI BYOC clusters running on your own cloud provider infrastructure. Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: Laverne Henderson laverne.henderson@coupa.com,1
1934,fix apex gradient clipping (#2829),0.7275835,Ensure that clip gradients is only called if the value is greater than 0 (#6330),  Remove deprecated training type plugins   update changelog   DDP2Plugin   Update src/pytorch_lightning/CHANGELOG.md ,1
1935,Ddp2 (#261),0.823462,DDP(2) backend (#2796),,1
1936,Fix typo in LightningModule.configure_callbacks() (#9591),0.7421481,Improved the error message when the LightningWork is missing the run method (#14759), deprecate sheety  Co-authored-by: manskx ahmed.mansy156@gmail.com,1
1937,deprecate hprams setter method (#4813),0.6552024,Removed deprecated LightningModule hparams setter (#6207),,0
1938,(app): Add load_state_dict and state_dict (#14100),0.735214,"+def load_state_dict(self, state):",Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
1939,XLA Profiler integration (#8014),0.5561699,xla_device_utils >> xla_device,,0
1940,feat(wandb): support distributed modes (#11650),0.60820174,BFloat16 Support,Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: awaelchli aedu.waelchli@gmail.com,0
1941,Docs improvements around hparams (#8577),0.73141223,Docs improvements,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: otaj 6065855+otaj@users.noreply.github.com,1
1942,Update the link to jsonargparse's link_arguments (#12898),0.6246284,- `LightningCLI` changed to use jsonargparse native support for list append ([#13129](https://github.com/Lightning-AI/lightning/pull/13129)), adding explain notes for requirements Apply suggestions from code review  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
1943,fixes metrics pickle issue (#3921),0.69049966,Tested pickling (#1636), [App] Relax lightning app requirements  Co-authored-by: manskx ahmed.mansy156@gmail.com,0
1944,Setup extras (#2831),0.66198903,setup(,  Doc Terminology updates   API updates ,0
1945,Fix to avoid moving batch to device for DataParallel (#11780),0.7669289,"With DPStrategy, the batch is not explicitly moved to the device (#11780)",Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: otaj ota@lightning.ai Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
1946,release v0.4.3,0.80592227,0.4.0,Co-authored-by: otaj ota@lightning.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
1947,Keep global step update in the loop (#8856),0.6065469,+ global_step += 1,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
1948,ref: accelerator names (#4066),0.75593513,Selects the accelerator,,1
1949,None check for filepath in ModelCheckpoint (#1654),0.84196883,Deprecated filepath in ModelCheckpoint (#4213),Co-authored-by: rohitgr7 rohitgr1998@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
1950,Convert validation loop config warnings to PossibleUserWarning (#13377),0.8442068,-  Converted validation loop config warnings to `PossibleUserWarning` ([#13377](https://github.com/Lightning-AI/lightning/pull/13377)),,1
1951,fixing TensorBoard (#687),0.7944159,Fix reset TensorRunningAccum (#5106),Co-authored-by: rohitgr7 rohitgr1998@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
1952,Prepare 2.0.0 release (#17056),0.6960998,0.4.0,,0
1953,Delete unused CI scripts (#7152),0.56657296,Removed the SingleProcessRuntime (#15933),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
1954,Fix NVIDIA docker versions (#7834),0.6035228,nvidia/apex deprecation (#16039),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: thomas chaton thomas@grid.ai,0
1955,Fixed typo in docs for optimizer_idx (#5310),0.6951146,"    optimizer_idx,",,0
1956,Create 3 steps to Lightning guide to replace quick-start (#3055),0.7315955,Improved LightningTrainerScript start-up time (#15751),,1
1957,Remove Strategy.on_gpu (#11537),0.7714802,- Removed `Strategy.on_gpu` ([#11537](https://github.com/PyTorchLightning/pytorch-lightning/pull/11537)),,1
1958,Fix mypy errors  in pytorch_lightning/strategies/ddp.py (#13885),0.7339578,+ # pytorch_lightning==1.7.0,Bumps tj-actions/changed-files from 23 to 24. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: tj-actions/changed-files   dependency-type: direct:production   update-type: version-update:semver-major ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,1
1959,[Docs] Explain metric internals (#5899),0.71994257,"docs for all Metrics (#2184, #2209)",Bumps pypa/gh-action-pypi-publish from 1.5.0 to 1.5.1. - Release notes - Commits  updated-dependencies: - dependency-name: pypa/gh-action-pypi-publish   dependency-type: direct:production   update-type: version-update:semver-patch ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com,1
1960,Remove errnonoeous comma in logging call (#474),0.5505961,Improved verbose logging for EarlyStopping callback (#6811),  Prepare changelog for 1.7.0 release   update links   fix conflicts   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,0
1961,ref: decoupled ddp2 (#3816),0.95077515,decoupled DDP2 (#3816),,1
1962,update bug report issue template - include PL version (#8209),0.6096131,Updated error message for interactive incompatible plugins (#9896),,0
1963,Add support for multiple loggers (#903),0.8049766,Logging with multiple loggers,Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
1964,fixed lr scheduler tests,0.68086016,          lr_scheduler.step(),Co-authored-by: awaelchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: otaj ota@lightning.ai,0
1965,Integrate lightning_utilities.core.imports (#14475),0.78314364,from lightning.pytorch.utilities import CombinedLoader,  Make tbptt imports Python 3.10 compatible   add chlog ,1
1966,Removed deprecated pytorch_lightning.overrides.distributed.IndexBatchSamplerWrapper.batch_indices  (#13565),0.84945285,  * Removed the `pytorch_lightning.overrides.distributed.LightningDistributedModule` class,,1
1967,Add and update readme for docs and tests (#12348),0.55286574,Deprecated the TestTubeLogger (#9065)," Added support for HPU device stats monitor  Signed-off-by: Jerome janand@habana.ai  Update changelog  Signed-off-by: Jerome janand@habana.ai  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci  Apply suggestions from code review  Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com  Update reference  Signed-off-by: Jerome janand@habana.ai  Apply suggestions from code review  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   fix alignment   add descriptions   Update hpu_intermediate.rst   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com",0
1968,Change attributes of RichProgressBarTheme dataclass (#10454),0.64492166,from pytorch_lightning.callbacks import RichProgressBar,Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
1969,Fix toggle optimizer (#5775),0.7533484,Refactored optimizer (#4658),Bumps actions/cache from 2 to 3. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: actions/cache   dependency-type: direct:production   update-type: version-update:semver-major ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,1
1970,tests for val loop flow (#2605),1.0,tests for val loop flow (#2605),Bumps actions/setup-python from 2 to 4. - Release notes - Commits  updated-dependencies: - dependency-name: actions/setup-python   dependency-type: direct:production   update-type: version-update:semver-major ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,1
1971,support number for logging with sync_dist=True (#5080),0.9984006,Support number for logging with sync_dist=True (#5080),,1
1972,Tidy up IPU documentation (#8401),0.6400124,Graphcore IPU devices," PRs 909,910,911, and 912  moves last 4 commits to the private re;po to the OS repo  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci   Fix validation error   Fixes API links and validation issues   Update docs/source-app/examples/file_server/app.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   Fix Python validation errors   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",0
1973,Update torch_xla installation instructions in tpu_basic.rst (#16865),0.63897467,PyTorch 2.0 and torch.compile,,0
1974,Add MNIST dataset & drop torchvision dep. from tests (#986),0.8741977,Dropped torchvision dependency in tests and added own MNIST dataset class instead (#986),  pkg: parse local versions   offline   str   manifest   ci ,1
1975,CI: set probot timeout (#14455),0.5797465,Stopped optimizer_zero_grad from being called after IPU execution (#12913),,0
1976,[App] Rename gradio.py gradio_server.py (#16201),0.5933386,Changed lightning_app.components.serve.gradio to  lightning_app.components.serve.gradio_server (#16201), debug pulled version pre flags only meta,0
1977,Return early in reset_seed (#11983),0.6153959,seed_everything now fails when an invalid seed value is passed instead of selecting a random seed (#8787),Co-authored-by: Akihiro Nitta nitta@akihironitta.com,0
1978,README typo (#2177),0.5330364,Changed overwrite to True (#16009),,0
1979,require: adjust versions (#6363),0.61066926,Set version as today (#13906), prune func calls in meta pkg init move calling prune coped  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
1980,Refactor: legacy accelerators and plugins (#5645),0.837567,Refactored Accelerators and Plugins (#5743),,1
1981,[fix] Add a cluster environment teardown to clean up environment state (#6942),0.73905146,Cleanup cluster waiting (#16054),,1
1982,Remove unused devbot (#12338),0.69436854,Removed deprecated: (#2760),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
1983,Remove appropriate line from pyproject.toml as a followup to #13929 (#14397),0.6350541,Removed deprecated code in pytorch_lightning.utilities.meta (#16038),,0
1984,Fix typo in script name (#15724),0.58007073,Rename failed -> error in tables (#15608),"  update lightning links in docs   update links in chlog   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  Apply suggestions from code review  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update src/pytorch_lightning/README.md  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update src/pytorch_lightning/README.md  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   update   painful   badges   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  Update badges  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Akihiro Nitta nitta@akihironitta.com",0
1985,Remove deprecated functions from accelerator.py (#9019),0.73839164,Removed the deprecated pytorch_lightning.accelerators.GPUAccelerator in favor of pytorch_lightning.accelerators.CUDAAccelerator (#16050),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
1986,Join Horovod workers at the end of trainer.fit() to prevent race conditions following training (#1786),0.6980671,"Deprecated Trainer(strategy=""horovod"")",Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
1987,Fix pre-commit isort failure on tests/plugins/*.py (#5422),0.71671563,Dropped official support/testing for PyTorch <1.6 (#8288),,1
1988,Update docs index (#17246),0.6361337,Updated governance docs,Update MPS availability to include check for ARM chips,0
1989,Enable using any Sampler in distributed environment in Lite (#13646),0.80599785,- Enabled using any Sampler in distributed environment in Lite ([#13646](https://github.com/Lightning-AI/lightning/pull/13646)),Co-authored-by: awaelchli aedu.waelchli@gmail.com,1
1990,0.10.0 (#3954),0.7249658,[0.6.0] - 2022-09-08,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
1991,build more dockers & slack fails (#12675),0.57444894,clean up hooks in run_evaluation (#3156),Co-authored-by: Jirka jirka.borovec@seznam.cz,0
1992,fixed val_loss for early stopping,0.66014266,Disabled val and test shuffling (#1600), add UI for install all,0
1993,Fix warning in ModelCheckpoint (#3094),0.86211026,Replace MisconfigurationException with warning in ModelCheckpoint Callback (#4560),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
1994,Add additional test cases,0.5658527,"nb_test_batches to num_test_batches,",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,0
1995,cleaned up examples,0.65712535,some minor cleaning,Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
1996,split trainer tests (#956),0.7486608,trainer.test(),,1
1997,Set Loop.restarting recursively (#11442),0.8882548,Set Loop.restarting=False at the end of the first iteration (#8362),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
1998,ci: flagship flow - clean inputs (#16666),0.5910367,Changed the flow.flows to be recursive wont to align the behavior with the flow.works (#15466),Co-authored-by: otaj ota@lightning.ai Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
1999,Fix typing in pl.overrides.data_parallel (#10796),0.59258467,Raised a MisconfigurationException if batch transfer hooks are overriden with IPUAccelerator (13961),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2000,Logger consistency (#397),0.762258,Un-balanced logging properly supported (#5119),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
2001,"Make default logger name ""lightning_logs"" (#11762)",0.8453171,- Changed default logger name to `lightning_logs` for consistency ([#11762](https://github.com/PyTorchLightning/pytorch-lightning/pull/11762)),Update torchmetrics requirement in /requirements Updates the requirements on torchmetrics to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: torchmetrics   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,1
2002,More clear docstring for val_check_interval (#2802),0.6506686,Trainer(val_check_interval=100),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2003,Disable training when limit_train_batches=0 (#4371),0.9490599,Disabled training when limit_train_batches=0 (#4371),Co-authored-by: mansy mansy@lightning.ai,1
2004,CI: fix requirements freeze (#13441),0.7552519,Setup: added requirement freeze for the next major version (#14480),,1
2005,DeepSpeed ZeRO Update (#6546),0.9402765,"Updated DeepSpeed ZeRO (#6546, #6752, #6142, #6321)", Update torchtext requirement in /requirements  Updates the requirements on torchtext to permit the latest version. - Release notes - Commits  updated-dependencies: - dependency-name: torchtext   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com  Update requirements/pytorch/extra.txt  Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com,1
2006,quick patch code (#1352),0.580861,Precision Plugins (#5718),,0
2007,Call any trainer function from the LightningCLI (#7508),0.7722224,reference to the Trainer on the LightningDataModule (#3684),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
2008,Skip a few tests to reduce drone CI wait times,0.5203769,"As we continue to strengthen the codebase with more tests, we’re finally getting rid of annoying bugs that have been around for a bit now. Mostly around the inconsistent checkpoint and early stopping behaviour (amazing work @awaelchli  @jeremyjordan )",,0
2009,[Example] Add Pytorch Geometric Example (#4568),0.6860805,  model = PyTorchModel(...),,0
2010,Horovod tests do not make sense for 1 gpu (#12710),0.6518959,"refactored Horovod backend (#3121, #3122)",,0
2011,ci (#9522),0.5783533,[0.5.5] - 2022-08-9,,0
2012,Fix mypy for utilities.memory (#8744),0.6071329,Resolve memory leak for evaluation (#6326),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: otaj ota@lightning.ai,0
2013,[tests/models] refactor with BoringModel (#5507),0.62535894,Removed ModelSummary validation from train loop on_trainer_init (#6610),,0
2014,Un-balanced logging properly supported (#5119),0.99999994,Un-balanced logging properly supported (#5119),Co-authored-by: otaj 6065855+otaj@users.noreply.github.com,1
2015,Skip tuner algorithms on fast dev (#3903),0.8151587,Tuner algorithms will be skipped if fast_dev_run=True (#3903),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
2016,Add more trainer config tests (#10319),0.7496099,trainer.test(),,1
2017,readability deepspeed condition (#17232),0.72800386,Ensure we check deepspeed/sharded in multinode DDP (#6297),  Fix PyTorch spelling errors   more ,1
2018,Add LightningLite to top level imports (#15502),0.8442804,from lightning.lite import LightningLite,"  Rename GPUAccelerator to CUDAAccelerator   Add back GPUAccelerator and deprecate it   Remove temporary registration   accelerator connector reroute   accelerator_connector tests   update enums   lite support + tests   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   typo   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   move ""gpu"" support up before actual accelerator flag checks   Stupid arguments   fix tests   change exception type   fix registry test   pre-commit   CI: debug HPU flow (#13419)   Update the hpu-tests.yml to pull docker from vault  fire & sudo habana-gaudi-hpus Check the driver status on gaudi server (#13718)  Co-authored-by: arao arao@habana.ai Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Akarsha Rao 94624926+raoakarsha@users.noreply.github.com  Update typing-extensions requirement from <4.2.1,>=4.0.0 to >=4.0.0,<4.3.1 in /requirements (#13529)  Update typing-extensions requirement in /requirements Updates the requirements on typing-extensions to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: typing-extensions   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com  [pre-commit.ci] pre-commit suggestions (#13540)  updates: - github.com/psf/black: 22.3.0 → 22.6.0 Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com   [FIX] Native FSDP precision + tests (#12985)   Simplify fetching's loader types (#13111)   Include app templates to the lightning and app packages (#13731)   Include app templates to the package   Co-authored-by: mansy mansy@lightning.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Fix mypy typing errors in pytorch_lightning/callbacks/model_checkpoint.py (#13617)  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Fix typos initialize in docs (#13557)  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   Fix main progress bar counter when val_check_interval=int and check_val_every_n_epoch=None (#12832)   Fix mypy errors attributed to pytorch_lightning.loggers.tensorboard.py (#13688)   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Fix mypy errors attributed to pytorch_lightning.loggers.mlflow (#13691)  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: otaj 6065855+otaj@users.noreply.github.com  fix mypy errors for loggers/wandb.py (#13483)  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com   Fix gatekeeper minimum check (#13769)   changelog   changelog   fix order   move up again   add missing test   Co-authored-by: rohitgr7 rohitgr1998@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: arao arao@habana.ai Co-authored-by: Akarsha Rao 94624926+raoakarsha@users.noreply.github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com Co-authored-by: Sean Naren sean@grid.ai Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Mansy ahmed.mansy156@gmail.com Co-authored-by: mansy mansy@lightning.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Lee Jungwon 33821003+BongYang@users.noreply.github.com Co-authored-by: Nathaniel D'Amours 88633026+NathanielDamours@users.noreply.github.com Co-authored-by: Justin Goheen 26209687+JustinGoheen@users.noreply.github.com Co-authored-by: otaj 6065855+otaj@users.noreply.github.com Co-authored-by: Gautier Dagan s2234411@ed.ac.uk Co-authored-by: Akihiro Nitta nitta@akihironitta.com",1
2019,release v0.5.3.2,0.8003881,0.4.0,,1
2020,add properties to check for trainer state in pytorch profier (#12063),0.68153626,- Deprecated `Trainer.should_rank_save_checkpoint` Trainer property ([#11068](https://github.com/PyTorchLightning/pytorch-lightning/pull/11068)),Co-authored-by: awaelchli aedu.waelchli@gmail.com,0
2021,Refactor plugin tests whose assertions don't need to run in on_fit_start hook (#11149),0.6715946,"- The `dataloader_idx` argument is now optional for the `on_{validation,test,predict}_batch_{start,end}` hooks. Remove it or default it to 0 if you don't use multiple dataloaders ([#16753](https://github.com/Lightning-AI/lightning/pull/16753))",Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: otaj ota@lightning.ai,0
2022,Merge pull request #66 from williamFalcon/no_back,0.54742014,"merge backends (#3476, #3477, #3478, #3480, #3482)",,0
2023,Speed up subprocess launch (#15738),0.61496353,Speed up single-core TPU training by loading data using ParallelLoader (#2033),,0
2024,fix formatting typo in seed_everything docs (#8052),0.52938277,pl.seed_everything will now also set the seed on the DistributedSampler (#7024),,0
2025,Fix mypy errors attributed to pytorch_lightning.profilers.pytorch (#14405),0.72757095,+ # pytorch_lightning==1.7.0,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com,1
2026,test examples (#3643),0.6651746,test_percent_check in favour of limit_test_batches,Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
2027,fix lib paths after Wandb 0.10 (#3520),0.56346416,- Fixed an issue with the `TPUSpawnPlugin` handling the `XLA_USE_BF16` environment variable incorrectly ([#10990](https://github.com/PyTorchLightning/pytorch-lightning/pull/10990)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2028,Simplify backbone_image_classifier example (#7246),0.69377947,Changed computer_vision_fine_tunning example to use BackboneLambdaFinetuningCallback (#5377),,0
2029,revert backend types (#3788),0.66169417,move backends back to individual files (#3712),prune requirements Co-authored-by: Akihiro Nitta nitta@akihironitta.com,0
2030,ref: enable self.log from val step (#3701),0.7764634,Enabled self.log in most functions (#4969),,1
2031,Replace oldest versions also for Lite (#15337),0.6146916,Set version as today (#13906),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,0
2032,Update version.py to 1.8.0dev (#13999),0.7251042,"Following our 4 PyTorch release window, this release supports PyTorch 1.8 to 1.11. Support for PyTorch 1.7 has been removed.",,1
2033,Delete TrainingEpochLoop._dataloader_idx which always equals 0 (#8911),0.7221581,Removed trainer.reset_*_dataloader() methods (#16726),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
2034,Add validate logic for precision (#9080),0.73570806,"Simplified ""should run validation"" logic (#7682)",update macos ci,1
2035,[App] Add start_with_flow flag to works (#15591),0.5823686,Add support for printing application logs using CLI lightning show logs <app_name> [components] (#13634),,0
2036,app: update doctest_skip (#15997),0.62704945,Updated app testing (#16000),Co-authored-by: otaj 6065855+otaj@users.noreply.github.com,0
2037,fix logging error (#575),0.7465706,Removed LoggerStages (#5673),use correct python version in lightning component template Co-authored-by: manskx ahmed.mansy156@gmail.com,1
2038,Include hook's object name when profiling (#11026),0.57601243,- Changed profiler to index and display the names of the hooks with a new pattern []. ([#11026](https://github.com/PyTorchLightning/pytorch-lightning/pull/11026)),Co-authored-by: rohitgr7 rohitgr1998@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2039,Fix var name,0.5320183,Rename failed -> error in tables (#15608),Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: awaelchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: mansy mansy@lightning.ai Co-authored-by: manskx mansy@lightning.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Mansy ahmed.mansy156@gmail.com Co-authored-by: otaj ota@lightning.ai Co-authored-by: Sean Naren sean@grid.ai Co-authored-by: Keiichi Kuroyanagi kuroyanagi.keiichi@gmail.com Co-authored-by: Martino Sorbaro martinosorb@users.noreply.github.com Co-authored-by: Wang Ran (汪然) wangr@smail.nju.edu.cn Co-authored-by: Rhys Goodall rhys.goodall@outlook.com Co-authored-by: Siyuan Li siyuanli.s.c@gmail.com Co-authored-by: Ekagra Ranjan ekagra.ranjan@gmail.com Co-authored-by: S. Kumano 54502860+s-kumano@users.noreply.github.com Co-authored-by: otaj 6065855+otaj@users.noreply.github.com Co-authored-by: Gautier Dagan gautierdagan2017@u.northwestern.edu Co-authored-by: Sherin Thomas sherinct@live.com Co-authored-by: Cyprien Ricque 48893621+Cyprien-Ricque@users.noreply.github.com Co-authored-by: Masahiro Wada argon.argon.argon@gmail.com Co-authored-by: nitinramvelraj 98356761+nitinramvelraj@users.noreply.github.com Co-authored-by: donlapark 10988155+donlapark@users.noreply.github.com Co-authored-by: Justin Goheen 26209687+JustinGoheen@users.noreply.github.com Co-authored-by: Shantam Gilra 64306405+shantam-8@users.noreply.github.com Co-authored-by: Bibhabasu Mohapatra 68384968+bibhabasumohapatra@users.noreply.github.com Co-authored-by: Jimmy Yao jiahaoyao.math@gmail.com Co-authored-by: Nikhil Shenoy nikhilshenoy98@gmail.com Co-authored-by: Sanjay Aradhyamath 57592361+samz5320@users.noreply.github.com,0
2040,Minor fixes related to clipping (#10130),0.6481669,Resolve bug with Finetuning (#5744),,0
2041,Move inference_mode logic to the loops (#16704),0.6315675,Read our comprehensive introduction to loops,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com,0
2042,Simplify enabling CPU offload in FSDP (#15832),0.6945002,"  * `strategy=""fsdp_full_shard_offload""` is now `strategy=""fsdp_cpu_offload""`",Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: otaj 6065855+otaj@users.noreply.github.com,0
2043,Call on_predict_model_eval hook through method (#12741),0.6202171,ModelSummary Callback,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2044,Fix catimage import (#15712),0.58166945,Module imports are now catching ModuleNotFoundError instead of ImportError (#9867),,0
2045,Bump actions/setup-go from 3 to 4 (#17144),0.54632187,"- Removed the `fit_loop.{min,max}_steps` setters ([#16803](https://github.com/Lightning-AI/lightning/pull/16803))",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
2046,Update extensions doc (#10778),0.6295862,Update the Lightning App docs (#13537),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2047,Bump playwright from 1.28.0 to 1.29.1 in /requirements (#16294),0.5358836,[1.4.0] - 2021-07-27, Include app templates to the package  Co-authored-by: mansy mansy@lightning.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
2048,Update examples that require the run() method (#15096),0.6425588,The Fabric.run() method is no longer abstract (#14992),,0
2049,Add docstrings to Test Tube logger (#9110),0.7059603,Deprecated the TestTubeLogger (#9065),,1
2050,Remove checkpoint tracking from internal debugger (#9326),0.67536426,Deprecated checkpoint_callback from the Trainer constructor in favor of enable_checkpointing (#9754),updates: - github.com/psf/black: 22.3.0 → 22.6.0 Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
2051,fix NCCL error with non-consecutive trainer gpus (#8165),0.6417432,"Trainer now raises a MisconfigurationException when its methods are called with ckpt_path=""best"" but a checkpoint callback isn't configured (#9841)",Update typing-extensions requirement in /requirements Updates the requirements on typing-extensions to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: typing-extensions   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
2052,docs: build docs for specific tags (#17055),0.59574676,"docs for all Metrics (#2184, #2209)", Update the hpu-tests.yml to pull docker from vault fire & sudo habana-gaudi-hpus Check the driver status on gaudi server (#13718)  Co-authored-by: arao arao@habana.ai Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Akarsha Rao 94624926+raoakarsha@users.noreply.github.com,0
2053,"drop torchvision, tests only (#797)",0.78850394,Removed dependency on torchvision (#797),Co-authored-by: Akihiro Nitta nitta@akihironitta.com,1
2054,auto distribute datasets across nodes,0.6116554,Native Fully Sharded Data Parallel Strategy,,0
2055,release v0.2.4,0.8309072,0.4.0,,1
2056,Replace pull_request event from docs workflow (#15530),0.5911064,refactored dataloader process hook (#3139),,0
2057,Deprecate TrainerOptimizersMixin and move functionality to core/optimizer.py (#11155),0.7995328,- Deprecated `TrainerOptimizersMixin` and moved functionality to `core/optimizer.py`([#11155](https://github.com/PyTorchLightning/pytorch-lightning/pull/11155)),Co-authored-by: Akihiro Nitta nitta@akihironitta.com,1
2058,Update GitHub links to PL repo (#13849),0.6139451,Pkg: fix parsing versions by @Borda in https://github.com/Lightning-AI/lightning/pull/15401,"  Add change to src/lightning_app/*/   Update filter   Update config   Revert ""Add change to src/lightning_app/*/""   This reverts commit e411de153b5f3fb204b5c9be11f0a8d0d15c1269.  Examples  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",0
2059,ci: label issue with version (#17484),0.560165,Refactored EpochResultStore (#5522),revert skip jobs,0
2060,Remove checks for torch greater than 1.10 (#15846),0.6642777,Prevent modification of torch.backends.cudnn.benchmark when Trainer(benchmark=...) is not set (#13154),,0
2061,added module properties,0.7126527,New properties,  define app 0.6   req   fix link   txt   ex   gk   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   Apply suggestions from code review   lapp 0.5.2   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
2062,Remove reference to DistributedDataParallel from parallel plugin teardown (#8943),0.755736,Removed teardown from ParallelPlugin (#8943),,1
2063,ref: organize args 3/n (#3447),0.8764817,"organize args (##3435, #3442, #3447, #3448, #3449, #3456)",,1
2064,Update trainer profiler typehint to use Profiler instead of the deprecated BaseProfiler (#13084),0.6624939,Deprecated bool values in Trainer's profiler parameter (#3656),,0
2065,Add init_optimizers test for predict (#9440),0.5635754,Deprecated TrainerTrainingTricksMixin in favor of a separate utilities module for NaN/Inf detection for gradients and parameters (#6834), Pull request for fixing issue #13580 chlog and test disable track for epoch  Co-authored-by: rohitgr7 rohitgr1998@gmail.com,0
2066,Various Fabric documentation updates (#17236),0.7016525,Fabric,update docs README on how to build docs Co-authored-by: Jeroen Delcour jeroen@track32.nl,1
2067,fixing typos reported by community user (#16457),0.48428074,- pickling errors with loggers (txs @awaelchli),Bumps pypa/gh-action-pypi-publish from 1.4.1 to 1.5.0. - Release notes - Commits  updated-dependencies: - dependency-name: pypa/gh-action-pypi-publish   dependency-type: direct:production   update-type: version-update:semver-minor ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
2068,Update CODEOWNERS (#14119),0.57895964,Refactor cloud dispatch and update to new API (#16456), Add CI app e2e update UserID fix apps cleanup download frontend inside setup.py  Co-authored-by: mansy mansy@lightning.ai Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
2069,[rfc] Make Result.unpack_batch_size a static method (#4019),0.5355294,"        self.scale_batch_size(trainer, pl_module)",Update governance.rst,0
2070,Add ckpt_path option to LightningModule.test() (#2190),0.6963959,Deprecated on_{train/val/test/predict}_dataloader() from LightningModule and LightningDataModule (#9098),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2071,Enable and fix recent legacy checkpoint tests in CI (#12491),0.6224006,Refactored CheckpointConnector to offload validation logic to the CheckpointIO plugin (#9045),"  list pytest   docs   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   list   test   fix GK   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
2072,Maintain Backward compatibility for DeviceDtypeModuleMixin (#8474),0.75555366,Moved DeviceDtypeModuleMixin and HyperparametersMixin mixin to core (#8396),,1
2073,Adds back the slow spawn ddp implementation that people want (#2115),0.77305526,much faster ddp implementation. Old one was renamed ddp_spawn,,1
2074,mark several methods in evaluation loops as protected (#9516),0.72991645,"Marked several methods in EvaluationEpochLoop as protected: on_evaluation_batch_start, evaluation_step, evaluation_step_end (#9516)", add testing PT 1.12 Fix quantization tests Fix another set of tests Fix check since https://github.com/pytorch/pytorch/pull/80139 is only going to be available for 1.13 Skip this test for now for 1.12  Co-authored-by: SeanNaren sean@grid.ai,1
2075,update deprecation warning (#1258),0.87635076,Deprecation warning (#3844), update  Co-authored-by: Jirka jirka.borovec@seznam.cz,1
2076,CI: switch cloud e2e tests to Prod (#15369),0.53648996,Only check versions / env when not in the cloud (#15504),  Remove deprecated Strategy.post_dispatch   changelog   remove unused imports ,0
2077,Add periods to the documentation (#8252),0.5646906,Log epoch metrics before the on_evaluation_end hook (#7272),adjust gatekeeper,0
2078,Bugfix: LR finder max val batches (#17636),0.5832033,Batch Size Finder (#11089),  wip   wip   wip   wip   wip   wip   wip   wip   Update tests/tests_pytorch/serve/test_servable_module_validator.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update tests/tests_pytorch/serve/test_servable_module_validator.py  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update src/pytorch_lightning/serve/servable_module_validator.py  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update src/pytorch_lightning/serve/servable_module_validator.py  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update src/pytorch_lightning/serve/servable_module_validator.py  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   Typing improvements   wip   update doc   update   update   update   update   update   update   update   update   update   update   update   Update examples/pl_servable_module/production.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   update   update   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2079,Enable non-blocking for gpu device transfer (#1843),0.93577635,Enable non-blocking for device transfers to GPU (#1843),"  update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   update   update   update   update   update   update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update latest docs   remove image   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   ci   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   make   Update .gitignore   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update .github/workflows/docs-deploy.yml  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   update   update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   update   update   update   update   update   Update docs/source-app/_templates/theme_variables.jinja   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/api_reference/api_references.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/api_reference/api_references.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/api_reference/api_references.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/code_samples/convert_pl_to_app/train.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/core_api/lightning_app/communication_content.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/examples/model_server_app/model_server_app_content.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/get_started/training_with_apps.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/get_started/training_with_apps.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/get_started/training_with_apps.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/examples/hpo/hpo.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/core_api/lightning_app/communication_content.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/core_api/lightning_app/communication_content.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/core_api/lightning_app/dynamic_work_content.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/examples/github_repo_runner/github_repo_runner_content.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/examples/github_repo_runner/github_repo_runner_content.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/examples/github_repo_runner/github_repo_runner_step_2.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/examples/github_repo_runner/github_repo_runner_step_4.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/examples/model_server_app/model_server.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   update   Update docs/source-app/core_api/lightning_app/dynamic_work_content.rst   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/examples/github_repo_runner/app.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/examples/github_repo_runner/app.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   update   update   update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com",1
2080,Adds Sampler Wrappers for custom samplers in distributed environment (#12959),0.78987265,It is now possible to use custom samplers in a distributed environment without the need to set replace_ddp_sampler=False and wrap your sampler manually with the DistributedSampler.,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
2081,Remove unused on_train_epoch_end hook in accelerator (#9035),0.8889001,Removed on_train_epoch_end from Accelerator (#9035),,1
2082,set min PT 1.3 (#1917),0.59684134,[1.3.7post0] - 2021-06-23,,0
2083,Document how to use multiple models and optimizers in Fabric (#16952),0.7713649,"model, optimizer = fabric.setup(model, optimizer)",,1
2084,tests to ensure correct dataloader calls (#3221),0.6687893,Run ddp_spawn dataloader checks on Windows (#6930),,0
2085,switched cpu amp order,0.515285,Apex mixed precision gets replaced with AMP (#16149),,0
2086,Adds is last batch (#13550),0.5682562,The default value of accumulate_grad_batches has changed from 1 to None (#9652).,,0
2087,Expose sharding context manager in Lite (#15169),0.4779352,Support SLURM and torchelastic global rank environment variables (#5715),,0
2088,Bump Lightning Cloud to 0.5.7 (#14757),0.70799696,- Fixed a bug with a default CloudCompute for Lightning flows ([#15371](https://github.com/Lightning-AI/lightning/pull/15371)),Co-authored-by: Kaushik B kaushikbokka@gmail.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,1
2089,Enforce eval shuffle warning only for default samplers (#12653),0.94085205,Enforced eval shuffle warning only for default samplers in DataLoader (#12653),  Increase typing_extensions requirement   it's rich! ,1
2090,Sequential CombinedLoader to flatten the eval and predict loops (#16726),0.6714395,expand eval loop out (#3165),,0
2091,"Refactor tests, skip if sklearn not available (#12093)",0.60872823,Deprecated the TestTubeLogger (#9065),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2092,Fix requirements-extra.txt Trains package to release version (#1229),0.5797829,Removed support for EvalResult and TrainResult (#3968),This reverts commit cd31ba3f87d51b1e09e466c34b9b530809b38ddd.,0
2093,fixes logger crash on ddp (#2388),0.7763392,DDP + loggers should be fixed,"  update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   update   update   update   update   update   update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update latest docs   remove image   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   ci   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   make   Update .gitignore   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update .github/workflows/docs-deploy.yml  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   update   update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   update   update   update   update   update   Update docs/source-app/_templates/theme_variables.jinja   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/api_reference/api_references.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/api_reference/api_references.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/api_reference/api_references.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/code_samples/convert_pl_to_app/train.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/core_api/lightning_app/communication_content.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/examples/model_server_app/model_server_app_content.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/get_started/training_with_apps.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/get_started/training_with_apps.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/get_started/training_with_apps.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/examples/hpo/hpo.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/core_api/lightning_app/communication_content.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/core_api/lightning_app/communication_content.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/core_api/lightning_app/dynamic_work_content.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/examples/github_repo_runner/github_repo_runner_content.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/examples/github_repo_runner/github_repo_runner_content.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/examples/github_repo_runner/github_repo_runner_step_2.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/examples/github_repo_runner/github_repo_runner_step_4.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/examples/model_server_app/model_server.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   update   Update docs/source-app/core_api/lightning_app/dynamic_work_content.rst   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/examples/github_repo_runner/app.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source-app/examples/github_repo_runner/app.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   update   update   update   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   update   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com",1
2094,Wandb bug/wandb multi (#1360),0.60018885,"The default project name in WandbLogger is now ""lightning_logs"" (#14145)",mypy hotfix,0
2095,CI for pre-release (#7220),0.5692072,[0.6.0] - 2022-09-08,  Remove max_steps=None   Update changelog   Update docs   Unused import   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2096,Add a GKE cleanup workflow to run once per hour. (#2682),0.5818696,"Cleaning (#5948, #5949, #5950)",,0
2097,Remove pytorch lightning.callbacks.lr monitor.learning rate monitor.lr_sch_names (#13353),0.9005632,- Removed deprecated `pytorch_lightning.callbacks.lr_monitor.LearningRateMonitor.lr_sch_names` ([#13353](https://github.com/Lightning-AI/lightning/pull/13353)),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
2098,Fix double precision support in Lite (#14827),0.6380131,Precision Plugins (#5718),,0
2099,Fix sigterm signal handling (#10189),0.69619846,the default signal is SIGUSR1,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2100,added tb docs,0.6737517,Docs improvements,,0
2101,makes checkpoint process safe (#431),0.6823977,Resuming from checkpoints (#16167),  Remove deprecated ClustertEnvironment methods   update changelog   ignore typing error   Co-authored-by: Akihiro Nitta nitta@akihironitta.com,0
2102,Fast defaults (#2185),0.6312986,"Changed default for DeepSpeed CPU Offload to False, due to prohibitively slow speeds at smaller scale (#6262)",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2103,[feat] Add StochasticWeightAveragingCallback (#5640),0.7355423,Deprecated passing stochastic_weight_avg to the Trainer constructor in favor of adding the StochasticWeightAveraging callback directly to the list of callbacks (#8989),,1
2104,Fix: handle logical CUDA device IDs for GPUStatsMonitor if CUDA_VISIBLE_DEVICES set (#8260),0.67637134,"device = ""cuda"" if torch.cuda.is_available() else ""cpu""",Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: otaj ota@lightning.ai,0
2105,Remove amp check as guard now upstream,0.5910962,"Removed the on_epoch guard from the ""should stop"" validation check (#7701)",  Removed the deprecated   method   Removed deprecated  IndexBatchSamplerWrapper.batch_indices   Update src/pytorch_lightning/CHANGELOG.md   Missed code   Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2106,Fix logging to loggers with multiple eval dataloaders (#12454),0.8408298,- Fixed logging to loggers with multiple eval dataloaders ([#12454](https://github.com/PyTorchLightning/pytorch-lightning/pull/12454)),Bumps docker/setup-buildx-action from 1 to 2. - Release notes - Commits  updated-dependencies: - dependency-name: docker/setup-buildx-action   dependency-type: direct:production   update-type: version-update:semver-major ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,1
2107,prune metric: accuracy 4/n (#6515),0.56057113,    precision=16,Bumps actions/upload-artifact from 2 to 3. - Release notes - Commits  updated-dependencies: - dependency-name: actions/upload-artifact   dependency-type: direct:production   update-type: version-update:semver-major ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
2108,[feat] Allow overriding optimizer_zero_grad and/or optimizer_step when using accumulate_grad_batches (#7980),0.766925,    optimizer1.zero_grad(),,1
2109,Add CODEOWNERS for progress dataclasses (#8196),0.7747455,"Add {,load_}state_dict to the progress tracking dataclasses (#8140)",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
2110,refactor training loop (#2336),0.9768672,Refactored training loop (#2336),  CI: hotfix gatekeeper   no min   min 1 ,1
2111,cache,0.46940622,Checkpoint saving and loading extensibility:,Remove redundant test,0
2112,checkout,0.40886042,Increased TPU check timeout from 20s to 100s (#5598),Update labeler,0
2113,Param printing (#336),0.63405085,        :param device_ids:,fix typo,0
2114,Replacing latest tag with 0.4.0 for lightning-bolts links (#10440),0.6963519,- Deprecated LightningCLI's registries in favor of importing the respective package ([#13221](https://github.com/Lightning-AI/lightning/pull/13221)),  Enable dependabot on GHA   Update comment   Update PR limit   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
2115,Exclude lightning.py for vulture (#8379),0.6294795,+ # pytorch_lightning==1.7.0,Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2116,Deprecate TrainerModelHooksMixin (#7422),0.8391243,Removed the deprecated TrainerTrainingTricksMixin class (#8679),Co-authored-by: Akihiro Nitta nitta@akihironitta.com,1
2117,Fix log_graph in TensorBoardLogger (#3092),0.7671659,Changed the default logger to TensorBoardLogger (#609), Refactor docker builds in CI Reduce duplicate and merge two workflows push: bool expression Extend timeout for ipu builds Update concurrency group Define env for push to hub Rename workflow Fix bool expressions Remove unnecessary trigger paths Remove unused env var Update job names Trim timeout rename  Co-authored-by: Jirka jirka.borovec@seznam.cz,1
2118,update chlog after 1.8.2 (#15754),0.66253066,0.4.0,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2119,define distributed as a type (#3740),0.5661943,Distributed group defaults to WORLD if None (#5125),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,0
2120,CI: revert skip jobs (#13715),0.5515716,Reset metrics before each task starts (#9410),  Removal of depreciated code from decorators   Update CHANGELOG.md   Removed imports ,0
2121,Merge pull request #1642 from PyTorchLightning/new-master,0.6534527,- Removed `configure_sync_batchnorm` from `ParallelStrategy` and all other strategies that inherit from it ([#11754](https://github.com/PyTorchLightning/pytorch-lightning/pull/11754)),,0
2122,DeepSpeed integration tests for Lite (#14901),0.7226083,Support for manual optimization with DeepSpeed (#7970),  Fix pyproject.toml   Add TODO   Update mypy workflow ,1
2123,Add  williamfalcon as owner for API changes (#4610),0.5147567,API changes to the trainer,  Add pr labeler   Triger on docs change   Make mutually exclusive   Add requirements   files   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
2124,Support skipping to validation (#9681),0.7074393,"Simplified ""should run validation"" logic (#7682)",  assist   split   need   inverse   checkout   json   include   unzip   mirror   fire   twine   for tar -xf   clean   3.8   ls ,1
2125,fixed drop prob import,0.6951819,import argparse,  Fix TPU circleci tests   Fix TPU circleci tests   Fix TPU circleci tests   Fix TPU circleci tests   Fix TPU circleci tests   Fix rank issue   Fix rank issue   debug alternative fix   Revert properties   Co-authored-by: awaelchli aedu.waelchli@gmail.com,0
2126,Keep track of the best model's path saved by ModelCheckpoint (#1799),0.86273694,Attribute best_model_path to ModelCheckpoint for storing and later retrieving the path to the best saved model file (#1799) ,  Decouple schema check   Update workflow name   Don't run if dir not found ,1
2127,set find_unused_parameters=False in DDP as in pytorch (#5435),0.8410456,Changed the default of find_unused_parameters to False in DDP (#5185),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com,1
2128,Fix mypy errors attributed to pytorch_lightning.loggers.neptune (#13692),0.7189811,Deprecated pytorch_lightning.logging (#767),Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,1
2129,added downloads badge,0.49036315,CheckpointIO Plugins,Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
2130,add pep8speaks (#842),0.49854577,adding Trainer.tune() (#3293),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: awaelchli aedu.waelchli@gmail.com,0
2131,Retrieve last logged val from result by key (#3049),0.9999999,Retrieve last logged val from result by key (#3049),,1
2132,Fix deepspeed default precision plugin amp_level to O2 (#13897),0.84504175,- Fixed default `amp_level` for `DeepSpeedPrecisionPlugin` to `O2` ([#13897](https://github.com/Lightning-AI/lightning/pull/13897)), fix typing in strategies/ddp2.py Use quotes instead of future.annotations for forward references  Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,1
2133,Remove the deprecated pytorch_lightning.profiler module (#16359),0.9082104,- Removed the deprecated `pytorch_lightning.profiler` module ([#16359](https://github.com/Lightning-AI/lightning/pull/16359)), fix typing in strategies/single_device.py Make assert statement more explicit  Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,1
2134,fixes typing errors in auto_restart.py (#13904),0.59469706,Removed mode='auto' from EarlyStopping (#6167),  condensers for App   pkg   TestsUpdate .github/CODEOWNERS   Co-authored-by: Mansy ahmed.mansy156@gmail.com Co-authored-by: Mansy ahmed.mansy156@gmail.com,0
2135,Fixes train outputs (#2428),0.6554463,Moved TrainsLogger to Bolts (#2384), Add CI for app examples  Co-authored-by: manskx mansy@lightning.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz,0
2136,improve partial Codecov (#1172),0.552331,Truncated long version numbers in progress bar (#2594),,0
2137,Delete unused autopep8 config (#5904),0.63726735,Removed mode='auto' from EarlyStopping (#6167),"  Update lightning_app src   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update lightning app tests   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   add CI   update tests   requirements   fix version tests   todo   fix tests   fix tests   fix tests   fix tests   fix formatting   Co-authored-by: mansy mansy@lightning.ai Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: awaelchli aedu.waelchli@gmail.com",0
2138,Fix/typo (#880),0.56371653,"Removed PyTorch 1.6 support (#10367, #10738)",,0
2139,Ensure the existence of DDPPlugin._sync_dir in reconciliate_processes (#8939),0.99999994,Ensure the existence of DDPPlugin._sync_dir in reconciliate_processes (#8939),Fix typing in _load_py_module function,1
2140,Fix docstring (#2884),0.6213974,Docs,,0
2141,Raise an error if batch transfer hooks are overridden with IPUAccelerator (#13961),0.96699536,Raised a MisconfigurationException if batch transfer hooks are overriden with IPUAccelerator (13961),,1
2142,Refactor: name modules (#548),0.6911257,Renamed utils modules (#5199),,0
2143,Pad experiment version with zero for easier listing (#355),0.54123485,The default value of accumulate_grad_batches has changed from 1 to None (#9652).,"  add lightning app examples   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   fix CI   rm init   restucture app examples   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  img  Co-authored-by: mansy mansy@lightning.ai Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz",0
2144,ruff: enable & fixing RET (#17540),0.64391947,Avoid relpath bug on Windows (#16164),"  tests   ci   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update tests   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: mansy mansy@lightning.ai",0
2145,"Remove deprecated on_{save,load}_checkpoint signature (#8697)",0.8060136,Removed support for the deprecated on_save_checkpoint signature. The hook now takes a checkpoint positional parameter (#8697),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
2146,add slack badge (#583),0.5434459,adding Trainer.tune() (#3293)," More clear docs for LightningDataModule  More clear docs for the methods add_argparse_args and from_argparse_args of the class LightningDataModule.  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
2147,PL: update changelog post 1.7.5 release (#14570),0.725678,Full Changelog: https://github.com/Lightning-AI/lightning/compare/1.8.0...1.8.0.post1,Typo,1
2148,fix typing in BaguaStrategy (#12692),0.5381017,Resolve bug with Finetuning (#5744),Remove unused argument model in the doc of verify_loop_configurations.,0
2149,[App] Implement ready for components (#16129),0.8873689,Implemented ready for components (#16129),Typo,1
2150,flake8 fixes (#3064),0.6380284,This release fixes that core issue,Typo,0
2151,_launch refactor and types [1/n] (#7232),0.62065196,"Base classes (#1326, #1877)",Typo,0
2152,ref: checkpoint connector methods 3/n,0.8050837,Refactor load in checkpoint connector (#4593), cut actions sequence ver,1
2153,[App] Wait for full file to be transferred in Path / Payload (#15934),0.9352003,Wait for full file to be transferred in Path / Payload (#15934),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
2154,bye bye Drone (#5901),0.49884906,"Cleaning (#5948, #5949, #5950)",,0
2155,Implement freeze batchnorm with freezing track running stats (#15063),0.8331952,Implement freeze batchnorm with freezing track running stats by @PososikTeam in https://github.com/Lightning-AI/lightning/pull/15063, fix doc of timer,1
2156,Bump actions/setup-python from 2 to 4 (#14287),0.54859567,"- Removed the `fit_loop.{min,max}_steps` setters ([#16803](https://github.com/Lightning-AI/lightning/pull/16803))",,0
2157,Avoid patching common DataHooks to the LightningModule (#10603),0.719272,"Removed automatic patching of {train,val,test,predict}_dataloader() on the LightningModule (#9764)",Co-authored-by: Jirka jirka.borovec@seznam.cz,1
2158,CI: precommit - docformatter (#8584),0.5651279,Docs,,0
2159,Bump sphinx-toolbox from 3.2.0 to 3.4.0 in /requirements (#16959),0.5485492,0.4.0,Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2160,remove default tensor,0.63893425,Move tensorboardX to extra dependencies. Use the CSVLogger by default (#16349),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
2161,Backward compatibility for checkpoint loading (#1132),0.7298539,Checkpoint saving and loading extensibility:,Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
2162,Update warning if ckpt directory is not empty (#5209),0.5408248,"    'path/to/checkpoint.ckpt',",Update comet-ml requirement in /requirements Updates the requirements on comet-ml to permit the latest version.  updated-dependencies: - dependency-name: comet-ml   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
2163,Fix mypy typing errors in pytorch_lightning/trainer/trainer.py (#14204),0.71365964,Removed pytorch_lightning/trainer/training_loop.py (#7985),Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
2164,Revert new Hydra launch behavior (#15737),0.7781724,Temporarily removed support for Hydra multi-run (#15737),Co-authored-by: Jirka jirka.borovec@seznam.cz,1
2165,Update old device flags (#12471),0.5942718,"Raise MisconfigurationException when the accelerator is available but the user passes invalid ([]/0/""0"") values to the devices flag (#12708)",  Remove protobuf from base req   Update tensorboard version   Co-authored-by: Jirka jirka.borovec@seznam.cz,0
2166,Add compatibility when torch.distributed is not available (#14454),0.65307975,- Raised exception in `init_dist_connection()` when torch distributed is not available ([#10418](https://github.com/PyTorchLightning/pytorch-lightning/pull/10418)),,0
2167,(app) Remove ClickRunner (#14147),0.5595787,Removed ProfilerConnector (#7654),,0
2168,[Docs] Example for auto_insert_metric_name (#12649),0.6105091,- Added class name prefix to metrics logged by `DeviceStatsMonitor` ([#12228](https://github.com/Lightning-AI/lightning/pull/12228)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2169,remove deprecated train_loop (#10482),0.7991938,Removed deprecated TrainResult (#5323),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: awaelchli aedu.waelchli@gmail.com,1
2170,Fix typo in Trainer.test() (#5226),0.7339524,Trainer is now raising a MisconfigurationException instead of a warning if Trainer.{validate/test} is missing required methods (#10016),  Change the path of the command execution folder from mnist_examples to convert_from_pt_to_pl   Add a guide to add PYTHONPATH   Fix Lightning Lite link   Remove duplicate   Add note   Co-authored-by: Akihiro Nitta nitta@akihironitta.com,1
2171,Single source for the mypy version (#15224),0.5738502,Parsed local package versions (#13933),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Jirka jirka.borovec@seznam.cz,0
2172,fix changelog (#1583),0.7215407,Complete changelog,,1
2173,finished dist (#911),0.59330165,[1.4.6] - 2021-09-10,"  allow freeze   ci   typo   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  ipu  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
2174,Add missing highlighting for Python snippets (#8411),0.5372716,- Removed support for Python 3.6 ([#11117](https://github.com/PyTorchLightning/pytorch-lightning/pull/11117)),,0
2175,added gpu check for each gpu test,0.65462565,GPU training (#2704),,0
2176,Pin horovod to <0.24 (#12234),0.5743156,"refactored Horovod backend (#3121, #3122)",,0
2177,Fix PyPI releasing (#15243),0.74180675,Drop PyTorch 1.9 support (#15347),Update numpy requirement in /requirements Updates the requirements on numpy to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: numpy   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,1
2178,update examples (#4233),0.5844892,[1.3.0] - 2021-05-06,Update neptune-client requirement in /requirements Updates the requirements on neptune-client to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: neptune-client   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
2179,Update docs for train_bn in Basefinetuning.filter_params (#13938),0.5880678,"Refactor RunningStage and TrainerState usage (#4945, #7173)","  Remove endpoint after collaborate app/dht CLI   Fix references, change filename   Add CHANGELOG.md   Address review   Co-authored-by: Jirka jirka.borovec@seznam.cz",0
2180,removed file,0.757727,Removed,Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
2181,Raise better error when calling Trainer.save_checkpoint without a model attached (#12772),0.8008764,Removed passing a ModelCheckpoint instance to Trainer(checkpoint_callback) (#6166),  placeholder   move setup_tools & abstract about   adjust lightning-app   notes   lightning about   lightning init   CI check   ci   install   adjust manifest & mv chlog   manifest   pkg   mv setup   parse_requirements   lit   ci - pytorch   wrap func   ci   cd draft   generate lit   pkg   utf-8   root pkg   req.   ver   mypy   try check   meta pkg   meta pkg - vars   meta pkg - pruning   meta pkg - fixing   fix PL for meta   multi-line wrapper   hack manifest   ci   fix docstr   fixing   ci & mypy   links ,1
2182,Allow Horovod teardown() to complete gracefully if exception thrown in callback setup (#11752),0.7011813,- Fixed an issue where `HorovodStrategy.teardown()` did not complete gracefully if an exception was thrown during callback setup [#11752](https://github.com/PyTorchLightning/pytorch-lightning/pull/11752),,1
2183,Restructure parsing flow in the LightningCLI (#8721),0.7212349,LightningCLI changes:,MPS Accelerator,1
2184,"update hparams, allow OmegaConf (#2047)",0.5873628,Support **DictConfig for hparam serialization (#2519),Fix docs link,0
2185,Fix mypy errors attributed to pytorch_lightning/profilers/advanced.py (#13792),0.72267085,+ # pytorch_lightning==1.7.0,,1
2186,Fix LightningOptimizer.step signature (#9289),0.739599,Check LightningOptimizer doesn't delete optimizer hooks (#6305),,1
2187,fixed typos in style guide (#4181),0.4644546,Refactored setup for typing friendly (#6590),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com,0
2188,Clean cuda.empty_cache usage (#8199),0.48344675,"device = ""cuda"" if torch.cuda.is_available() else ""cpu""",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com,0
2189,Update tests/tuner/*.py to use devices instead of gpus or ipus (#11520),0.6385887,Skipped testing with PyTorch 1.7 and Python 3.9 on Ubuntu (#11217), fix typo in ci cd doc fix path to release-docker.yaml Update instance type Update gpu instance type  Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
2190,Update DDPShardedPlugin precision handling after moving PrecisionPlugin (#10658),0.6726507,"Implemented {DDPShardedPlugin,DDPShardedSpawnPlugin}._setup_model_and_optimizers (#10028, #10064)",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
2191,Fix broken link (#16442),0.624656,- Fixed a bug where the upload files endpoint would raise an error when running locally ([#14924](https://github.com/Lightning-AI/lightning/pull/14924)),Update fsspec[http] requirement in /requirements Updates the requirements on fsspec[http] to permit the latest version. - Release notes - Commits  updated-dependencies: - dependency-name: fsspec[http]   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
2192,Fixes colab links for lightning in 2steps documentation (#9038),0.7857208,Introduce lightning connect (#14452),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka jirka.borovec@seznam.cz,1
2193,Fix inconsistent exceptions raised with no rich installed (#11360),0.69591635,Improved exception message if rich version is less than 10.2.2 (#10839),Update numpy requirement in /requirements Updates the requirements on numpy to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: numpy   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
2194,Update pre-commit and add new hooks (#7781),0.6905385,Updated hooks arguments - breaking for setup and teardown (#2850),Update omegaconf requirement in /requirements Updates the requirements on omegaconf to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: omegaconf   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
2195,Move warning before deepcopying the hyperparameters (#15132),0.6958283,Flattening Wandb Hyperparameters (#2459),Update hydra-core requirement in /requirements Updates the requirements on hydra-core to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: hydra-core   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
2196,Add example table to loop docs (#10058),0.62090147,"For a full tutorial and running example, visit our docs. TODO: add to docs",Update horovod requirement in /requirements Updates the requirements on horovod to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: horovod   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
2197,Add check for unique device ids  (#8666),0.62073016,... or the device IDs,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
2198,update mergify - master only (#5682),0.5793545,"    version=""0.0.1"",",Update gcsfs requirement in /requirements Updates the requirements on gcsfs to permit the latest version. - Release notes - Commits  updated-dependencies: - dependency-name: gcsfs   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
2199,update docker base on PT 1.7 (#6931),0.5669278,Changed to the NeptuneLogger (#16761):,Update gym[classic_control] requirement in /requirements Updates the requirements on gym[classic_control] to permit the latest version. - Release notes - Commits  updated-dependencies: - dependency-name: gym[classic_control]   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
2200,Update Fabric docs based on user feedback (#16460),0.55683964,Learn more about Fabric and what it can do in the new docs!,Update wandb requirement in /requirements Updates the requirements on wandb to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: wandb   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,0
2201,[bugfix] Fix signature mismatch in DDPCPUHPCAccelerator's model_to_device (#5505),0.65128654,Fix hanging in DDP HPC accelerators (#5157), add xla environment class add api reference integrate use xenv remove properties  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Kaushik B kaushikbokka@gmail.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
2202,[fix] Add barriers before and after setup hook is run (#7202),0.66577,Updated hooks arguments - breaking for setup and teardown (#2850),Update changelog,0
2203,Avoid rich 10.15.0 and 10.15.1 (#12293),0.6618173,Improved exception message if rich version is less than 10.2.2 (#10839),Co-authored-by: Raymond G Schireman raymond.schireman@uvm.edu Co-authored-by: Akihiro Nitta nitta@akihironitta.com,0
2204,"Update torchmetrics requirement from <0.9.3,>=0.7.0 to >=0.7.0,<0.10.1 in /requirements (#15135)",0.76125455,"Set PyTorch 1.4 as min requirements, also for testing and examples torchvision>=0.5 and torchtext>=0.5 (#5418)",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com,1
2205,fixing bug in testing for IterableDataset (#547),0.737014,Disabled sampler replacement when using IterableDataset (#11507),Update torchmetrics requirement in /requirements Updates the requirements on torchmetrics to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: torchmetrics   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com,1
2206,Rename the DDPSpawnShardedPlugin to DDPSpawnShardeedStrategy (#11210),0.8503493,Merged the implementation of DDPSpawnStrategy into DDPStrategy and removed DDPSpawnStrategy (#14952),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
2207,Update changelog after 1.5.6 release (#11094),0.72479016,Full Changelog,Build new hpu docker image,1
2208,fix accumulated grad norm fixes #87 (#88),0.6555527,The default value of accumulate_grad_batches has changed from 1 to None (#9652).,,0
2209,Add reinforcement learning example for Fabric (#16506),0.58482474,"2. Instantiate Fabric directly, without subclassing",Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com,0
2210,Make trainer.state a read-only property (#3109),0.65997,Deprecated Trainer.terminate_on_nan public attribute access (#9849),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2211,Bump tj-actions/changed-files from 24 to 28 (#14337),0.5161628,moved TPU xxx_step to backend (#3118),Co-authored-by: Jirka jirka.borovec@seznam.cz,0
2212,Fix max_batches with fast_dev_run. (#2581),0.6486719,Updated fast_dev_run to accept integer representing num_batches (#4629),,0
2213,added clean slurm save load test,0.65991175,Slurm resubmit with apex + ddp,,0
2214,The torchdynamo inline cache is fixed in 2.1 (#16934),0.6602951,Keep torch.backends.cudnn.benchmark=False by default (unlike in v1.6.{0-4}) after speed and memory problems depending on the data used. Please consider tuning Trainer(benchmark) manually. (#13154),,0
2215,Have the outputs match the loops format (#12182),0.6472254,Moved result teardown to the loops (#8245),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2216,"Remove incorrect ""template"" information (#13911)",0.65514076,Remove MetricsHolder (#7909),,0
2217,CI: fixture for global rank variable reset (#6839),0.67411435,Enabled prepare_data from correct processes - clarify local vs global rank (#2166),,0
2218,Adds tests to make sure logging doesn't happen multiple times (#3899),0.7583111,Cleaning up stale logger tests (#3490),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
2219,Move predictions to CPU before accumulating (#9085),0.6879821,CPU stats monitoring,,0
2220,fix tpu docs (#886),0.60421795,Updated logic for checking TPUs availability (#6767),,0
2221,remove obsolete todo in pl_examples (#6475),0.58036256,Removed deprecated EvalResult (#5633),Updated link,0
2222,anti-colision isort config (#5511),0.5817372,Deprecated access to the AcceleratorConnector.configure_slurm_ddp method and marked it as protected (#10101),,0
2223,Improvements to checkpoint migration (#16233),0.7027038,Resuming from checkpoints (#16167),"  edit   docs   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   fixing   clean generated   ignore   pre-commit   ci   ci   Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",1
2224,[App] Use environment variables from args when loading apps for the CloudRuntime,0.563159,The utility lightning.app.utilities.cloud.is_running_in_cloud now returns True during the loading of the app locally when running with --cloud (#16045),setup,0
2225,Improve mechanism to reset the seed after sanity check (#11870),0.6337522,Check environ before selecting a seed to prevent warning message (#4743),"  CI: renames azure   check   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   skip lightning_app   ignore   .   true block   ci   ci   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
2226,Fixes #120 (#210),0.6102433,"refactored Horovod backend (#3121, #3122)",,0
2227,Fix result gathering with varying tensor shapes (#3020),0.67427695,Auto convert tensors to contiguous format when gather_all (#4907),,0
2228,Update version for rc0 release (#13877),0.638381,"    version=""0.0.1"",",,0
2229,Amp2 (#2505),0.6749828,Apex mixed precision gets replaced with AMP (#16149),,0
2230,added dp reduce out test,0.63652205,"Enables DP, but with many limitations","  add main page   update   move   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   fix link   copy over from main repo   formatting   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz",0
2231,Follow up of #2892 (#3202),0.65217644,[1.2.2] - 2021-03-02,"  Update README and approval config   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
2232,ENG-1404: Hide region CLI flag for cluster creation (#15277),0.710488,Enabled custom clusters (#4048),  CI: block app contrib.   format   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
2233,Add experiment_key argument to online mode example (#4997),0.5335039,Inference mode support,,0
2234,Deprecate PrecisionPlugin.on_save/load_checkpoint (#11978),0.74881727,"- Deprecated `PrecisionPlugin.on_{save,load}_checkpoint` in favor of `PrecisionPlugin.{state_dict,load_state_dict}` ([#11978](https://github.com/PyTorchLightning/pytorch-lightning/pull/11978))",Delete pytorch_lightning/demos directory,1
2235,Remove unused param tpu_core_idx (#1948),0.62274826,"Removed process_idx from the {DDPSpawnPlugin,TPUSpawnPlugin}.new_process methods (#10022)","  introduce Lightning App   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz",0
2236,dummy logger (#1836),0.6555476,for logger in loggers:,"  GH org rename Lightning-AI   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  repo name  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
2237,Protect Flow from exception during request processing (#15187),0.5602896,- Added a try / catch mechanism around request processing to avoid killing the flow ([#15187](https://github.com/Lightning-AI/lightning/pull/15187),"  move: legacy >> test/   move: tests >> test/   rename unittests   update CI   tests4pl   tests_pytorch   proxi   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   ci   link   cli   standalone   fixing   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   .   Apply suggestions from code review   Co-authored-by: Akihiro Nitta nitta@akihironitta.com  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci   alone   test -> tests   Standalone fixes   ci   Update   More fixes   Fix coverage   Fix mypy   mypy   Empty-Commit   Fix   mypy just for pl   Fix standalone   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com",0
2238,Add ddp_cpu to DistributedType Enum (#8596),0.5391572,DDP custom implementation support (override these hooks):,,0
2239,support python 3.9 (#4944),0.77129585,Drop Python 3.6 support, Update wandb requirement in /requirements  Updates the requirements on wandb to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: wandb   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com,1
2240,last config try,0.6047808,"Automatically reload the ""last"" checkpoint",  docs: rename source >> source-PL   docs: fix typing   readthedocs   update paths & codeowners   source-pytorch   ci   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2241,update BYOC documentation with AWS details (#16044),0.6089374,Add support to see Lightning AI BYOC cluster logs (#14334),  move: pl_examples >> src/   convert pl_examples package to plain examples   update CI for examples   ci   missing   install ,0
2242,Allow string plugins (#4888),0.67132515,Enabled plugins (#4041),"  move: pytorch_lightning >> src/   update setup & install   update CI   ci   update CI for examples   Self review   mypy   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   ci   make   docs   typo   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   ci: gpu   .   hpu   typing   docs   tpu   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
2243,Bugfix/_has_len (#2293),0.6119425,tons of bug fixes :wink:,  Drop PyTorch 1.8 support   Missed update   Skip profiler test until supported   Upgrade ipu dockerfile pytorch version   Update XLA version ,0
2244,Add typing to some utility files (#11316),0.6014632,Add support for printing application logs using CLI lightning show logs <app_name> [components] (#13634),Co-authored-by: Jirka jirka.borovec@seznam.cz,0
2245,Example docs formatting (#1364),0.5936028,Docs,unify torch.Tensor >> Tensor Co-authored-by: Jirka jirka.borovec@seznam.cz,0
2246,change STEP_OUTPUT type from dict to mapping (#17387),0.5856279,Removed legacy code to include step dictionary returns in callback_metrics. Use self.log_dict instead. (#6682),,0
2247,Backwards compatibility for get_init_args (#16851),0.7553545,      init_args:, Update rich requirement in /requirements  Updates the requirements on rich to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: rich   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com,1
2248,Avoid creating CUDA stream if not running on CUDA (#17499),0.6475792,"device = ""cuda"" if torch.cuda.is_available() else ""cpu""", drop mamba use legacy GPU machines,0
2249,Update API references in doc (#11357),0.66303825,Refactor cloud dispatch and update to new API (#16456),  Create  hpu-ci-runner Dockerfile   Add ENTRYPOINT script 'start.sh' to hpu-ci-runner   rename dirs   ci   add docker   Fix build failure   Fix build failure   Fix title of nightly ci runner build   Fix comments   Fix comments   Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Akihiro Nitta nitta@akihironitta.com,0
2250,Replace DataLoader sampler once for IPUs (#8858),0.6910665,Disabled sampler replacement when using IterableDataset (#11507),CI: Remove simple test ci_test-base.yml,0
2251,Add trainer.predict config validation (#6543),0.7191807,trainer = pl.Trainer(plugins=SLURMEnvironment(auto_requeue=False)),Fixing typo in documentation.,1
2252,Fix docs (#4174),0.6084672,Docs improvements,,0
2253,added broadcast option to tpu (#3814),0.62883365,Enabled MultiNode Components to support state broadcasting (#15607),,0
2254,"Removed dependency on pandas, instead use generic csv (#736)",0.82201445,Removed dependency on pandas (#736),Co-authored-by: Akihiro Nitta nitta@akihironitta.com,1
2255,Drop breaking sphinx docs build (#8934),0.6300895,Pinned sphinx-autodoc-typehints with <v1.15 (#11400), Add pull-legacy-checkpoints action Replace pulls with the new action and script Simplify,0
2256,Deprecate LightningModule.loaded_optimizer_states_dict (#8229),0.98110455,Deprecated LightningModule.loaded_optimizer_states_dict (#8229), update docker Apply suggestions from code review  Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
2257,Remove deprecated callback hooks (#14834),0.8779327,We removed some Callback hooks that were ambiguous to use Removed deprecated callback hooks (#14834):,,1
2258,Update test for ckpt+val_check_interval (#7084),0.6275228,"trainer.test(model, ckpt_path=""/path/to/checkpoint.ckpt"")",  LightningCLI natively support callback list append.   Update jsonargparse version   Handle case where callbacks is not a list.   Fix PEP8 issue.   Handle mypy false positive   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2259,Introduce parameter to fix deepspeed crash for RNNS (#9489),0.7375445,Expose DeepSpeed loss parameters to allow users to fix loss instability (#6115),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
2260,ref: accelerator connector methods x/n (#3470),0.9625209,"accelerator connector methods x/n (#3469, #3470, #3474)",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
2261,notes on Bug fixing (#2053),0.7798333,Tons of bug fixes,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com,1
2262,initialize poptorch_models based on trainer_fn (#10149),0.61879385,"def on_train_epoch_start(self, trainer, pl_module):",,0
2263,Minor doc fixes (#2893),0.68347573,Docs improvements,  refactor:removed the close instance from the LoggerCollection class   Also logger.close()   Update CHANGELOG   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2264,Update torch version matrix (#11649),0.6783222,- Fixed an issue with comparing torch versions when using a version of torch built from source ([#17030](https://github.com/Lightning-AI/lightning/pull/17030)),  batchfinder clarification   fix according to precommit hook ,0
2265,Update training_tricks.py (#3151),0.6987202,Removed pytorch_lightning/trainer/training_loop.py (#7985),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2266,Docs2 (#1028),0.7236507,Docs,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
2267,Black format pytorch_lightning/core/hooks.py (#3575),0.788286,+ # pytorch_lightning==1.7.0,Co-authored-by: rohitgr7 rohitgr1998@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
2268,"Revert ""Load app before setting LIGHTNING_DISPATCHED"" (#16064)",0.69750047,Update the Lightning App docs (#13537),,0
2269,Porting latest App docs update (#13680),0.7543229,Update the Lightning App docs (#13537),Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
2270,hydra ddp support + generalized ddp gpu flags (#2212),0.6233501,Made DDP the default if no backend specified with multiple GPUs (#1789),add @akihironitta,0
2271,Remove rank 0 restrictions from logger (#8608),0.8498131,Removed restrictions in the Trainer that loggers can only log from rank 0; the existing logger behavior has not changed (#8608),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
2272,Updated basic debugging (#14488),0.68984693,DDP Debugging Improvements,,0
2273,Add check to fairscale override,0.55904746,Add function to remove checkpoint to allow override for extended classes (#16067),This reverts commit e19babc9869a9de584731d0106bee41f02c71584.,0
2274,Integrate lightning_utilities.core.rank_zero (#14556),0.73449385,- Deprecated `pytorch_lightning.utilities.distributed.rank_zero_only` in favor of `pytorch_lightning.utilities.rank_zero.rank_zero_only` ([#11747](https://github.com/PyTorchLightning/pytorch-lightning/pull/11747)),,1
2275,Remove duplicated native AMP + LBFGS check (#9748),0.59189427,Trainer now raises an exception when requesting amp_level with native amp_backend (#9755),,0
2276,updated docs (#2999),0.6985989,Docs improvements,,0
2277,Fix pre-commit blacken-docs failures (#8624),0.53513014,- Fixed a bug where the upload files endpoint would raise an error when running locally ([#14924](https://github.com/Lightning-AI/lightning/pull/14924)),  Add README.md to .github/workflows/   Link CI/CD doc page from contributing.md   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
2278,[docs] Add activation checkpointing information (#9165),0.63554585,CheckpointIO Plugins, Update typing-extensions requirement in /requirements  Updates the requirements on typing-extensions to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: typing-extensions   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com,0
2279,Update index.rst (#3201),0.57432866,Changed epoch indexing from 1 instead of 0 (#2206), Update deepspeed requirement from <0.6.0 to <0.7.0 in /requirements  Updates the requirements on deepspeed to permit the latest version. - Release notes - Commits  updated-dependencies: - dependency-name: deepspeed   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com,0
2280,Fix to ensure the checkpoint states are saved in a common filepath with deepspeed (#12887),0.74108726,DeepSpeed single file saving (#6900), Update tensorboard requirement in /requirements  Updates the requirements on tensorboard to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: tensorboard   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com,1
2281,Add SSH key management to CLI (#15291),0.8226783,- Added support for managing SSH-keys via CLI ([#15291](https://github.com/Lightning-AI/lightning/pull/15291)), Update jsonargparse[signatures] requirement in /requirements  Updates the requirements on jsonargparse[signatures] to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: jsonargparse[signatures]   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com,1
2282,Remove legacy Result parameters (#6016),0.6906258,Removed legacy code to include step dictionary returns in callback_metrics. Use self.log_dict instead. (#6682), Update matplotlib requirement in /requirements  Updates the requirements on matplotlib to permit the latest version. - Release notes - Commits  updated-dependencies: - dependency-name: matplotlib   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com,0
2283,"Update requirements, update test (#9345)",0.6142477,Refactored setup_training and remove test_mode (#5388), Update neptune-client requirement in /requirements  Updates the requirements on neptune-client to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: neptune-client   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com,0
2284,[LAI] Solve circular imports for combined package (#15234),0.57850623,"Separated utils: imports & enums (#5256, #5874)", Update mlflow requirement in /requirements  Updates the requirements on mlflow to permit the latest version. - Release notes - Changelog - Commits  updated-dependencies: - dependency-name: mlflow   dependency-type: direct:production ... Signed-off-by: dependabot[bot] support@github.com Co-authored-by: dependabot[bot] 49699333+dependabot[bot]@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz,0
2285,adding explain notes for requirements (#13872),0.61375546,Pruned requirements duplicity (#13739),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2286,Typing tuner.auto_gpu_select (#9292),0.6865405,Support auto_select_gpus with the accelerator and devices API (#12608),  Fix torchelastic detection under Mac   CHANGELOG ,0
2287,Update Changelog post v1.7.7 (#14851),0.7203138,Full Changelog,fix tutorial Co-authored-by: Jiny491 jiny491@gmail.com,1
2288,make checkboxes (#473),0.5305635,Configuration Validator (#9779),"  refactor strategies to use accelerator   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci  Apply suggestions from code review  Co-authored-by: Aki Nitta nitta@akihironitta.com  Apply suggestions from code review  Co-authored-by: Aki Nitta nitta@akihironitta.com   update changelog   remove changes to strategies   update changelog   update device-specific teardowns   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   partially revert   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Update strategy.py   swap hpu   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   add missing log   unused import   order matters for tpu?   Co-authored-by: edward-io me@edward.io Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Aki Nitta nitta@akihironitta.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com",0
2289,drop duplicated guides (#864),0.6017158,Drop duplicate metrics (#5014),,0
2290,Remove outputs from on_predict_epoch_end (#16655),0.73568785,- Removed the `outputs` argument from the `on_predict_epoch_end` hook. You can access them via `trainer.predict_loop.predictions` ([#16655](https://github.com/Lightning-AI/lightning/pull/16655)), rename min_gpus to min_cuda_gpus,1
2291,fix prints for py3.5,0.64816904,"PyTorch 1.10 and Python 3.7 no longer supported (#16492, #16579)",,0
2292,Update fastapi dependency pins (#17173),0.55262476,- The `pyDeprecate` dependency is no longer installed ([#14472](https://github.com/Lightning-AI/lightning/pull/14472)),  Fix trainer profiler typehint   Remove unused import of deprecated BaseProfiler   Update CHANGELOG.md   Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2293,Disable strict loading in multiprocessing launcher (#16365),0.85821724,"Disabled strict loading in multiprocessing launcher (""ddp_spawn"", etc.) when loading weights back into the main process (#16365)",,1
2294,E2E image version using from environment variable instead of hardcoded one (#16968),0.46563032,Refactored EpochResultStore (#5522),,0
2295,[bugfix] Add set_default_tensor_type to torch.DoubleTensor with precision=64 (#7108),0.60927093,Disable torch.inference_mode with torch.compile in PyTorch 2.0 (#17215),,0
2296,fix imports / isort / flake8,0.7059491,| Import pl.core.mixins.DeviceDtypeModuleMixin                                                           | 1.10             | No longer supported |,,1
2297,Add tpu_spawn_debug to plugin registry (#7933),0.6422628,Removed Plugin in base_plugin.py in favor of accessing TrainingTypePlugin and PrecisionPlugin directly instead (#9066),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2298,[doc] Add Zero Grad set_to_none=True trick (#6548),0.61865574,The default value of accumulate_grad_batches has changed from 1 to None (#9652).,,0
2299,Remove error when test dataloader used in test (#1495),0.7282798,Removed test_dataloaders parameter from Trainer.fit() (#1434),,1
2300,Set val_check_interval default to 1.0. (#145),0.64316267,Disabled val and test shuffling (#1600),,0
2301,[App] Enable uploading files to a project. (#16631),0.7567732,Enabled cp (upload) at project level (#16631),,1
2302,ddp backend refactor (#3209),0.93331957,refactored DDP backend forward (#3119), CI: Azure - multiple configs names benchmark Apply suggestions from code review  Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
2303,fix enumerate usage (#12949),0.52174866,"organize args (##3435, #3442, #3447, #3448, #3449, #3456)",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2304,Print the logs when TPU tests fail (#15533),0.6637696,Cleaning up stale logger tests (#3490), Don't set materialized child to child's child Update CHANGELOG  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
2305,Resolve Boring App Race Condition (#15324),0.5570936,Resolved a bug about a race condition when sending the work state through the caller_queue (#14074),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2306,Fix broadcast for Windows minimal (#8331),0.5725937,Enabled MultiNode Components to support state broadcasting (#15607),,0
2307,"Add stronger typing, skip ddp test on windows",0.5960969,Run ddp_spawn dataloader checks on Windows (#6930),,0
2308,LR scheduler + train refactor (#103),0.68469214,- Deprecated `Trainer.lr_schedulers` in favor of `Trainer.lr_scheduler_configs` which returns a list of dataclasses instead of dictionaries ([#11443](https://github.com/PyTorchLightning/pytorch-lightning/pull/11443)),,0
2309,Avoid non-blocking GPU->CPU copies. (#11288),0.8246852,Enable non-blocking for device transfers to GPU (#1843),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
2310,Unpin protobuf version and update tensorboard version (#13259),0.6454338,Switch from tensorboard to tensorboardx in TensorBoardLogger (#15728),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2311,Add support for DDP fork (#13405),0.77090156,- Added support for DDP Fork ([#13405](https://github.com/Lightning-AI/lightning/pull/13405)), Use hpu hmp for bf16 alone Update changelog  Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
2312,Configure isort (#2136),0.5980939,Renamed utils modules (#5199),"  freeze versions   unfreeze   dependabot   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   fix all req   ...   use base   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   fix refs   Apply suggestions from code review   Co-authored-by: Akihiro Nitta nitta@akihironitta.com   Apply suggestions from code review   dockers   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com",0
2313,Remove magic monitor support for ModelCheckpoint (#8293),0.82440746,Allow ModelCheckpoint monitor to be None (#3633),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
2314,update chlogs after 1.8.1 (#15632),0.62578166,0.4.0,Co-authored-by: rohitgr7 rohitgr1998@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2315,fix loss param order (#2169),0.5491599,Refactor Model backward (#2276),Co-authored-by: Akihiro Nitta nitta@akihironitta.com,0
2316,set dp as default backend,0.67035794,moves configure ddp to each backend (#3924),,0
2317,Black format the model checkpoint callback (#3559),0.6391666,Enable None model checkpoint default (#3669),,0
2318,remove ddp procs collection from script launcher (#12029),0.64534473,"Removed process_idx from the {DDPSpawnPlugin,TPUSpawnPlugin}.new_process methods (#10022)",,0
2319,adds setup+teardown hook (#2229),0.8243239,Updated hooks arguments - breaking for setup and teardown (#2850),  Add documentation for collaborative_training   Add strategies   Fix formatting   use accelerator API   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   Fix link   Try to fix label   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   Fix sequence   Apply suggestions from code review   Co-authored-by: RobertLaurella 99420295+RobertLaurella@users.noreply.github.com   Address reviews   Address code reviews   Update docs/source/strategies/collaborative_training_expert.rst   Co-authored-by: Akihiro Nitta nitta@akihironitta.com  Update docs/source/strategies/collaborative_training_intermediate.rst  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: RobertLaurella 99420295+RobertLaurella@users.noreply.github.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com,1
2320,Bump carmocca/probot from 1 to 2 (#14336),0.5041206,Deprecated max_nb_epochs and min_nb_epochs (#567),  Removed lines pertinent to checkpoint_callback   removed checkpoint callback flag   Updated Change Log   Removed deprecation test for checkpoint_callback argument   updated line in the simple_classif_training.py   Updated docs   updated simple_classif_training.py removing enable_checkpointing ,0
2321,PyTorch 1.7 Stable support (#3821),0.86684376,PyTorch 1.5  support,Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Kaushik B kaushikbokka@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
2322,Fixing a small issue in trainer logging  (#1563),0.7239803,Removed restrictions in the Trainer that loggers can only log from rank 0; the existing logger behavior has not changed (#8608)," HPU doesn't support torch.inference_mode  Signed-off-by: Jerome janand@habana.ai  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci  Update doc and changelog  Signed-off-by: Jerome janand@habana.ai   Update pytorch_lightning/trainer/trainer.py   Revert back to HPU available   Address reviews   Signed-off-by: Jerome janand@habana.ai   Update pytorch_lightning/trainer/trainer.py   Update pytorch_lightning/trainer/trainer.py   Add TPU accelerator condition   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Kaushik B kaushikbokka@gmail.com",1
2323,Fix  COMET_EXPERIMENT_KEY environment variable usage in comet logger (#4230),0.7292315,Using .comet.config file for CometLogger (#1913),  parse strategies as own extras   prune devel   Update Makefile   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  revert parse_requirements  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
2324,Only output IPU report on request (#8340),0.59935975,Stopped optimizer_zero_grad from being called after IPU execution (#12913),,0
2325,Trainer cleanup (#934),0.8050792,Trainer.fit hook clean up (#3198),,1
2326,Merge branch 'master' into feature/fairscale-817-6n,0.48977935,Moved DeviceDtypeModuleMixin and HyperparametersMixin mixin to core (#8396),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2327,Removed deprecated Trainer.num_processes property in favour of Trainer.num_devices (#14423),0.7713588,Removed trainer.reset_*_dataloader() methods (#16726),,1
2328,Remove outdated trainer argument videos (#17071),0.6590755,Removed the deprecated TrainerTrainingTricksMixin class (#8679),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2329,fix lr scheduler docs (#1446),0.67995006,lr_scheduler now activated after epoch    , unpin CUDA docker image for GPU CI Apply suggestions from code review  Co-authored-by: Aki Nitta nitta@akihironitta.com Co-authored-by: Akihiro Nitta akihiro@pytorchlightning.ai Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
2330,Adding appends to some of the pseudocode blocks (#8427),0.5831692,"adding compute environments (#3837, [#3842)",Co-authored-by: Kaushik B kaushikbokka@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2331,Ports (#338),0.68287677,"# if user gave a port number, use that one instead",,0
2332,Add ModelPruning(prune_on_train_epoch_end) to choose when to apply pruning (#7704),0.60703963,python trainer.py fit --model=Model1 --optimizer=CustomOptimizer,,0
2333,Fix false positive deprecation warning from register_ddp_comm_hook (#12846),0.7035362,Silenced some warnings. verified ddp refactors (#3483),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
2334,Tests: refactor cleanup (#1744),0.6959157,"Refactored training_batch + tests to verify correctness (#2327, #2328)",Co-authored-by: otaj ota@grid.ai Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2335,Update links to latest PL docs (#17031),0.61242956,Update the Lightning App docs (#13537),Co-authored-by: manjirou maxim.mametkulov@halbestunde.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
2336,Tutorials (#998),0.58347976,Integrated TrainingEpochLoop.total_batch_idx (#8598),  Update Strategy doc   Update strategy references ,0
2337,Hacky fix for mlflow logger (#277),0.6497443,bug fix with logging val epoch end + monitor (#3812),Separate strategies' requirements,0
2338,Issue #657 - Call on_train_end after early stopping (#723),0.67722845,Changed EarlyStopping callback from by default running EarlyStopping.on_validation_end if only training is run. Set check_on_train_epoch_end to run the callback at the end of the train epoch instead of at the end of the validation epoch (#7069),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
2339,Delay PyPI releasing (#4730),0.65361375,"Resolved schedule step bug for PyTorch Profiler (#6674, #6681)",,0
2340,update test for resume_from_checkpoint on missing file (#7255),0.8534342,Changed resume_from_checkpoint warning to an error when the checkpoint file does not exist (#7075),,1
2341,Add formulas and references to metrics docs (#4823),0.73700845,"docs for all Metrics (#2184, #2209)",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
2342,Fix Broken Link in lightning_app.core.work.LightningWork (#15032),0.8043426,Improved the error message when the LightningWork is missing the run method (#14759),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: thomas chaton thomas@grid.ai,1
2343,Remove skipping logic in PL CI (#14565),0.5404799,Casted only floating point tensors to fp16 with IPUs (#13983),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2344,Docs/changelog for 1.0.3 (#4267),0.7341763,Full Changelog,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
2345,Enable custom apex and amp plugins (#4355),0.6693275,apex plugin (#3502),  Fix fully sharded mixed precision setter   Add CHANGELOG.md ,0
2346,Removed the deprecated the trainer.lr_schedulers (#14408),0.8406871,Removed the deprecated TrainerLoggingMixin class (#8609),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
2347,Fabric: do set_epoch for batch_sampler.sampler (#16841),0.6670146,fabric.launch(),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: otaj ota@grid.ai,0
2348,Add deprecation warning to ModelCheckpoint when logging val_loss with no monitor (#6012),0.84774154,Deprecated using 'val_loss' to set the ModelCheckpoint monitor (#6012),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
2349,Fix broken links after reverse mirror changes (#16600),0.54974747,  * Removed `Loop.connect()` ([#16384](https://github.com/Lightning-AI/lightning/pull/16384)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2350,Remove NaNs from loss in LRFinder (#1862),0.7386265,Removed non-finite values from loss in LRFinder (#1862),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
2351,Clean existing logging tests (#7760),0.8289534,Cleaning up stale logger tests (#3490),,1
2352,docs: migration guide to the latest [1/n] (#17034),0.6874901,You can find a migration guide for this change in this PR's description.,,0
2353,add UI for install all (#13732),0.52401924,| Import pl.core.mixins.DeviceDtypeModuleMixin                                                           | 1.10             | No longer supported |,,0
2354,add ddp sync for logging in result step (#2822),0.71222377,Support number for logging with sync_dist=True (#5080),,1
2355,tests: Remove usage of --flake8 flag (#5909),0.6586325,Deprecated flags: (#2213),"  Fuse_modules in a qat-respecting way   Add compatibility for PyTorch <1.11   In older pytorch versions, fuse_modules used the Module.training flag to determine wheter fusion should be QAT-compliant or not, refer https://github.com/pytorch/pytorch/releases/tag/v1.11.0   Add CHANGELOG for pull #12891   Fix conditional import of fuse_modules_qat   torch.ao.quantization.fuse_modules_qat was actually added in torch 1.11.  Update CHANGELOG.md  Co-authored-by: Akihiro Nitta nitta@akihironitta.com",0
2356,testing file init,0.58639514,      init_args:, Update gpg key Use curl instead of wget Install key manually,0
2357,change backbone to self (#8762),0.5830094,Updated references to self.forward() to instead use the __call__ interface. (#1211),Co-authored-by: Sean Naren sean@grid.ai Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
2358,Move pl/core/mixins/device_dtype_mixin.py to lite/utilities/device_dtype_mixin.py (#14511),0.7264881,- Removed deprecated `pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin` in favor of `pytorch_lightning.core.mixins.device_dtype_mixin.DeviceDtypeModuleMixin` ([#10442](https://github.com/PyTorchLightning/pytorch-lightning/pull/10442)),,1
2359,Fixed failed import warning for torch.distributed.group (#3553),0.6738506,import torch,  allow pickling of KFoldLoop   Update pl_examples/loop_examples/kfold.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Nicolas Berger nicolas.berger@inait.ai Co-authored-by: Kushashwa Ravi Shrimali kushashwaravishrimali@gmail.com,0
2360,system info (#1234),0.5007279,(#16002),,0
2361,Update issue templates,0.6107914,Use correct python version in lightning component template (#13790),  last checkpoint versioning   changelog   Simplify test   Update CHANGELOG.md   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update CHANGELOG.md  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2362,"pkg, strict version (#14814)",0.627304,Moved accelerators and plugins to its legacy pkg (#5645),,0
2363,Merge isinstance calls (#8469),0.59944665,"merge backends (#3476, #3477, #3478, #3480, #3482)",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2364,"Fix ""Get Started"" at the top being 404 (#12210)",0.54885817,Renamed reset_on_epoch to reset_on_run (#9658),  first fix   full bugfix + tests   Apply Adrian's suggestion   Add test with tensor(0)   Minor code simplification   change sorting to make the comment correct   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2365,Minor fixes/improvements in Metric docs (#6114),0.775203,"docs for all Metrics (#2184, #2209)",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
2366,Enable more shorthand strategy names in the Fabric CLI (#16485),0.7380582,- Enabled all shorthand strategy names that can be supported in the CLI ([#16485](https://github.com/Lightning-AI/lightning/pull/16485)),,1
2367,Correct SLURM wrong import (#10842),0.63469565,import argparse,Update docs conf with renamed lightning extension,0
2368,Remove get_gpu_memory_map deprecated since v1.5 (#15617),0.67626923,"- Removed the deprecated `pytorch_lightning.core.memory.{get_gpu_memory_map,get_memory_profile}` ([#12659](https://github.com/Lightning-AI/lightning/pull/12659))",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2369,Hash values in LightningEnum instead of name. (#8421),0.6360078,Updated metrics to use LightningEnum (#5689),,0
2370,Avoid adding None loss values in training_epoch_end (#7772),0.7722349,Made training_epoch_end behave like validation_epoch_end (#1357),,1
2371,Disable quantization aware training observers (#8540),0.89853644,Quantization aware training observers are now disabled by default during validating/testing/predicting stages (#8540),,1
2372,Rename ptl to pl,0.5297301,    * Renamed the `TPUSpawnPlugin` to `TPUSpawnStrategy` ([#11190](https://github.com/PyTorchLightning/pytorch-lightning/pull/11190)),,0
2373,Allow newer torch versions (#269),0.69003713,- Fixed an issue with comparing torch versions when using a version of torch built from source ([#17030](https://github.com/Lightning-AI/lightning/pull/17030)),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
2374,Tests: clean metrics (#4152),0.65612745,test_percent_check in favour of limit_test_batches,Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Kaushik B kaushikbokka@gmail.com,0
2375,Cleaner datadir management for some tests (#15791),0.6772337,Cleaner API to accommodate the various research use cases   ,  Fix ddp_comm_hook tests   Refactor ddp_comm_hook tests   Co-authored-by: Akihiro Nitta akihiro@pytorchlightning.ai,0
2376,update docs (#9903),0.63580513,Update the Lightning App docs (#13537),,0
2377,Bugfix/all gather (#5221),0.65577346,Tons of bug fixes,Co-authored-by: Akihiro Nitta nitta@akihironitta.com,0
2378,Improve the error message for installing tensorboardx (#17053),0.979391,Improved the error message for installing tensorboard or tensorboardx (#17053),Co-authored-by: carmocca carlossmocholi@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com,1
2379,Fix typing in pl.overrides.distributed (#10797),0.57732975,Raised a MisconfigurationException if batch transfer hooks are overriden with IPUAccelerator (13961),,0
2380,[feat] Add Trainer(stochastic_weight_avg=True/False) (#6038),0.7546892,"trainer = Trainer(callbacks=[FineTuneBatchSizeFinder(milestones=(5, 10))])",Co-authored-by: Mauricio Villegas mauricio_ville@yahoo.com,1
2381,Added support for downloading wandb artifacts in the WandbLogger  (#14551),0.68134546,- Added `WandbLogger.download_artifact` and `WandbLogger.use_artifact` for managing artifacts with Weights and Biases ([#14551](https://github.com/Lightning-AI/lightning/pull/14551)),,0
2382,Fixed total number of batches (#439),0.7391174,set validation to a fix number of batches,  add error message   add test   changelog   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
2383,release v0.4.4,0.8254814,0.4.0,  Fix deepspeed installation   Adapt to deepspeed>=0.5.9   Fix fairscale installation   Co-authored-by: Akihiro Nitta akihiro@pytorchlightning.ai,1
2384,ddp backend refactor (#3204),0.92825615,refactored DDP backend forward (#3119),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
2385,[App] Expose Run Work Executor (#15561),0.6878943,Expose RunWorkExecutor to the work and provides default ones for the MultiNode Component (#15561),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2386,Remove optimizer_connector.py (#10120),0.8388133,Removed pytorch_lightning.trainer.connectors.OptimizerConnector (#10120),  Use new rank_zero_debug   Fix and move import statement to the top ,1
2387,Fixes #3945 (#3947),0.6366935,Resolve bug with Finetuning (#5744),  remove deprecated test_tube logger   remove testube from logger init   remove relevant testtube tests   update CHANGELOG with removal of deprecated TestTubeLogger ,0
2388,Remove explicit flush from tensorboard logger (#2126),1.0000001,Remove explicit flush from tensorboard logger (#2126),,1
2389,Add Progress Bar to docs (#11359),0.7734803,progress bar,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
2390,Remove explicit isinstance checks in strategies for checkpoint io (#11177),0.667459,Refactored CheckpointConnector to offload validation logic to the CheckpointIO plugin (#9045),Co-authored-by: Raymond G Schireman raymond.schireman@uvm.edu Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2391,updated reqs,0.63084674,Moved base req. to root (#4219),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2392,Lazy import tensorboard (#15762),0.7674135,Switch from tensorboard to tensorboardx in TensorBoardLogger (#15728),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
2393,Update tests/models/*.py to use devices instead of gpus or ipus (#11470),0.6313649,Dropped official support/testing for PyTorch <1.6 (#8288),Co-authored-by: carmocca carlossmocholi@gmail.com,0
2394,More explicit exception message when testing with fast_dev_run=True (#6667),0.6871472,Tuner algorithms will be skipped if fast_dev_run=True (#3903),,0
2395,docs update and follow up of #2789 (#2797),0.6757399,"docs for all Metrics (#2184, #2209)",,0
2396,Update HPU Dockerfile to latest version (#13344),0.6168135,Release LAI docs as stable (#14250),  remove val_transform from datamodule.py   remove val_transforms from tests   update docs   update changelog   remove unused imports   Co-authored-by: Raymond G Schireman raymond.schireman@uvm.edu Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2397,clean up unused distributed sampler logic in trainer (#5975),0.6818042,Automatic distributed samplers,Co-authored-by: Akihiro Nitta akihiro@pytorchlightning.ai,0
2398,Finishing touches to the graveyard (#15123),0.5118681,[1.4.8] - 2021-09-22,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2399,apply_func.py: from torchtext.legacy.data import Batch (#6211),0.7206779,"  from torch.utils.data import DataLoader, Dataset",Co-authored-by: Raymond G Schireman raymond.schireman@uvm.edu Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com,1
2400,updated output of test models,0.71578455,- Full tests that run multiple models in different configs,Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
2401,Update CHANGELOG.md following patch releases (#9536),0.65706336,Here is a selection of important changes that are not backward compatible with versions < 1.5. The full list of changes and removals are listed in the changelog at the bottom.,  docs refactor 4/n (fix broken links)   docs refactor 4/n (fix broken links)   docs refactor 4/n (fix broken links) ,0
2402,Fix epoch logging on train epoch end (#13025),0.71661735,The TQDM progress bar now correctly shows the on_epoch logged values on train epoch end (#11069), Enable inference mode for evaluation better name Update CHANGELOG.md  Co-authored-by: Akihiro Nitta nitta@akihironitta.com,1
2403,Use PickleError base class to detect all pickle errors (#6917),0.6714824,Tested pickling (#1636),Co-authored-by: rohitgr7 rohitgr1998@gmail.com,0
2404,fix deprecated call (#6005),0.73516905,Removed deprecated early_stop_callback (#3982),Co-authored-by: Akihiro Nitta nitta@akihironitta.com,1
2405,"fixed hpc save, load. cleaned apu",0.586292,Fix hanging in DDP HPC accelerators (#5157),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2406,"Revert/Fix: epoch indexing from 1, to be from 0 (#2289)",0.8962674,Changed epoch indexing from 0 instead of 1 (#2289),  add distributed.is_available checks to avoid errors when not available   Update CHANGELOG   Update pytorch_lightning/strategies/ddp.py   Co-authored-by: Akihiro Nitta nitta@akihironitta.com  Update pytorch_lightning/strategies/ddp.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
2407,Force NVML-based CUDA check in PyTorch 1.14+ (#15110),0.6405834,"- Trainer queries the CUDA devices through NVML if available to avoid initializing CUDA before forking, which eliminates the need for the `PL_DISABLE_FORK` environment variable introduced in v1.7.4 ([#14631](https://github.com/Lightning-AI/lightning/pull/14631))",Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
2408,[Bug-Fix]:properties current_epoch and global_step between model and trainer same always  (#3785),0.7394562,The current_epoch and global_step attributes now get restored irrespective of the Trainer task (#9413),,1
2409,simplify CI horovod (#4951),0.7018709,horovod deprecation (#16141),,1
2410,Update mising CODEOWNERS for the PL package (#14280),0.56988466,"- Deprecated the internal `pl.core.mixins.DeviceDtypeModuleMixin` class ([#14511](https://github.com/Lightning-AI/lightning/pull/14511), [#14548](https://github.com/Lightning-AI/lightning/pull/14548))","  updated titles + css   updated titles + css   levels structure   levels structure   levels structure   adding level indexes   finished intro guide layout   finished intro guide layout   general titles   general titles   added movie   added movie   finished 15 mins   levels   added core levels   added core levels   fixed api reference on the left   gpu guides   gpu guides   gpu guides   gpu guides   precision   hpu guide   added ipu   added ipu   added ipu   added ckpt docs   finished basic logging   intermediate   intermediate   intermediate   fixed   fixed margins   fixed margins   fixed margins   fixed margins   fixed margins   fixed margins   fixed margins   fixed margins   fixed margins   added logger stuff   added logger stuff   added logger stuff   added logger stuff   added logger stuff   ic   added inconsolata   added inconsolata   added inconsolata   added inconsolata   added inconsolata   added inconsolata   added inconsolata   updated menu   added basic cloud docs   added basic cloud docs   added basic cloud docs   added basic cloud docs   ic   ic   ic   ic   ic   ic   ic   ic   ic   ic   ic   ic   added demos folder   added demos folder   added demos folder   added demos folder   added demos folder   added demos folder   twocolumns directive   twocols   twocols   registry   registry   registry   registry   registry   registry   registry   registry   registry   registry   registry   registry   registry   registry   registry   registry   registry   registry   registry   cleaning up   cleaning up   cleaning up   cleaning up   cleaning up   cleaning up   cleaning up   cleaning up   cleaning up   updated titles + css   levels structure   adding level indexes   finished intro guide layout   general titles   added movie   finished 15 mins   levels   added core levels   fixed api reference on the left   gpu guides   precision   hpu guide   added ipu   added ckpt docs   finished basic logging   intermediate   fixed margins   added logger stuff   ic   added inconsolata   updated menu   added basic cloud docs   ic   added demos folder   twocolumns directive   registry   cleaning up   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   deconflict   deconflict   deconflict   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Add testsetup sections wherever needed; fix errors in building docs   pre-commit fixes   Fix duplicate label   minor nit with pre-commit   Fix labels   More changes...   require   debug & cli   prec & model & visu   fix references   fix references   fix refs   fix refs - model_parallel   fix references   prune testsetup with global   refs in index   Fix duplicate label errors   Update orphan docs   Update orphan docs   Update orphan docs   fix links   Fix genindex and search index   fix refs   fix refs   Fix index rst related issues   fix refs   inc to rst   Fix links ref   fix more references   fix refs   deconflict   errors   errors   errors   fix refs   fix refs   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   fix warnings   Fix LightningCLI errors   Fix LightningCLI errors   Fix LightningCLI errors   Fix LightningCLI errors   fix doc build   Duplicate Label fix (docs) (#12800)   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   ignore typing in demo folder   Ignore demos for mypy   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Kushashwa Ravi Shrimali kushashwaravishrimali@gmail.com Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: rohitgr7 rohitgr1998@gmail.com Co-authored-by: Kaushik B kaushikbokka@gmail.com Co-authored-by: otaj ota@grid.ai",0
2411,Removing unecessary early stopping calls (#1863),0.68586695,Removed deprecated early_stop_callback (#3982),,0
2412,Removed from_argparse_args tests in test_cli.py (#14597),0.67552495,"  * Removed functions from `lightning.pytorch.utilities.argparse`: `from_argparse_args()`, `parse_argparser()`, `parse_env_variables()`, `get_init_arguments_and_types()`, `add_argparse_args()`",,0
2413,ref: group prepare data hook (6) (#3212),0.9550376,group prepare data hook (#3212), remove dataloader_idx fix tests Update CHANGELOG.md  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
2414,[IPU] Add reset dataloader hooks to training type plugin 3/n (#7861),0.7453823,Removed on_reset_*_dataloader hooks in TrainingType Plugins and Accelerators (#8858),update slack link,1
2415,[feat] Add restore to base loop (#8247),0.6895624,Set Loop.restarting=False at the end of the first iteration (#8362),,0
2416,ref: device to gpus (#3405),0.70595896,"device parser (#3400, #3405)",Co-authored-by: Akihiro Nitta nitta@akihironitta.com,1
2417,Remove conda installation guide from lightning docs (#13727),0.6637739,Removed the deprecated LightningDeepSpeedModule (#16041),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
2418,fix title levels (#12470),0.5552149,Refactored EpochResultStore (#5522),,0
2419,Fix tests on single-GPU machine (#16911),0.64443874,GPU training (#2704),,0
2420,Upgrade Ubuntu version from 18.04 to 20.04 (#11395),0.50579846,"    version=""0.0.1"",", build dockers add slack Apply suggestions from code review  Co-authored-by: Akihiro Nitta nitta@akihironitta.com,0
2421,[App][CLI] Fix lightning cli --version (#14433),0.79996204,cli = LightningCLI(),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
2422,Fix mathjax in RTD build (#10889),0.5722181,Monir bug fix with print issues and data_loader (#1080),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2423,Accelerator model state dict (#7474),0.68195426,Ensure we set the eval/train flag correctly on accelerator model (#6877),Co-authored-by: rohitgr7 rohitgr1998@gmail.com,0
2424,update changelogs,0.8467312,Complete changelog,,1
2425,release v0.1.dev18,0.65064025,"    version=""0.0.1"",",,0
2426,remove trainer hidden state | sanity refactor [1 / n] (#7437),0.69731796,Removed deprecated property Trainer.running_sanity_check in favor of Trainer.sanity_checking (#9209),,0
2427,clarify Trainer running state atribs. (#5589),0.753233,"Refactor RunningStage and TrainerState usage (#4945, #7173)","  remove more code   update tests   remove unsupported test   remove unsupported test   remove dead enum values   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   add changelog   fix pep   add xfail test   remove comment   Remove support for passing strategy name to plugins   remove unused import   chlog   improve comment   update chlog   fix merge error   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",1
2428,[App] Fixing Sigterm Handler causing thread lock which caused KeyboardInterrupt to hang (#15881),0.5554742,Deprecated on_keyboard_interrupt callback hook in favor of new on_exception hook (#9260),Co-authored-by: Kushashwa Ravi Shrimali kushashwaravishrimali@gmail.com,0
2429,faster CI testing (#1323),0.59368575,Add a notebook example to reach a quick baseline of ~94% accuracy on CIFAR10 using Resnet in Lightning (#4818),Co-authored-by: Raymond G Schireman raymond.schireman@uvm.edu Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
2430,Update module path for LightningDeprecationWarning in setup.cfg (#11793),0.75268507,Update the Lightning App docs (#13537),,1
2431,refactor: simplify Tensor import (#15959),0.6830466,Fix reset TensorRunningAccum (#5106),,0
2432,Run main progress bar independent of val progress bar in TQDMProgressBar (#12563),0.9364292,Run main progress bar updates independent of val progress bar updates in TQDMProgressBar (#12563), check docker requires ci update bagua conda cuda,1
2433,prune ecosystem example (#5085),0.58985454,Sanitize None params during pruning (#6836),,0
2434,Standalone Lite: Launchers (#14555),0.5898289,There were two different ways of importing Lite in <= 1.9.0,  remove unused AcceleratorConnector argument   update lite ,0
2435,CI: fix examples - patch download MNIST (#6357),0.5893549,Fixing critical bugs in newly added hooks and hparams assignment.,Co-authored-by: carmocca carlossmocholi@gmail.com,0
2436,Update onnx requirement from <1.14.0 to <1.15.0 in /requirements (#17587),0.50106597,- Do not update on-plateau schedulers when reloading from an end-of-epoch checkpoint ([#14702](https://github.com/Lightning-AI/lightning/pull/14702)),,0
2437,added .gitignore,0.5605108,- Added support for adding descriptions to commands either through a docstring or the `DESCRIPTION` attribute ([#15193](https://github.com/Lightning-AI/lightning/pull/15193),,0
2438,Fix mypy typing errors attributed to pytorch_lightning/demos/mnist_datamodule.py (#13929),0.6512631,+ # pytorch_lightning==1.7.0,Co-authored-by: Akihiro Nitta nitta@akihironitta.com,0
2439,[bugfix] Remove warning for distributed values (#7132),0.6036073,Silenced some warnings. verified ddp refactors (#3483),,0
2440,Clarify default epoch and step values in ModelCheckpoint (#16651),0.76253754,"Removed epoch and step arguments from ModelCheckpoint.format_checkpoint_name(), these are now included in the metrics argument (#7344)",,1
2441,Replace IPU with external implementation (#17075),0.6462673,"- When training with `precision=16` on IPU, the cast has been moved off the IPU onto the host, making the copies from host to IPU cheaper ([#13880](https://github.com/Lightning-AI/lightning/pull/13880))",Co-authored-by: Aki Nitta nitta@akihironitta.com Co-authored-by: Raymond G Schireman raymond.schireman@uvm.edu Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,0
2442,Fix attribute error in SWA when running with Tuner (#14836),0.7013482,- Fixed an attribute error when running the tuner together with the `StochasticWeightAveraging` callback ([#14836](https://github.com/Lightning-AI/lightning/pull/14836)),Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
2443,[IPU] Allow poptorch.Options to override Trainer (#8233),0.6297546,trainer = pl.Trainer(plugins=SLURMEnvironment(auto_requeue=False)),Co-authored-by: akihiro@grid.ai akihiro@grid.ai Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
2444,Create single file in TensorBoardLogger (#777),0.9999999,Create single file in TensorBoardLogger (#777),Co-authored-by: kd li_jide_ok@126.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
2445,Add state_dict to loops (#8197),0.6963184,Improve Loop API to better handle children state_dict and progress (#8334),,0
2446,update appveyor badge (#378),0.5696609,Update the Lightning App docs (#13537),,0
2447,added stage param to LightningDataModule.setup example (#7483),0.68418443,Deprecated LightningDataParallel in favor of new wrapper module LightningParallelModule (#5670),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
2448,Remove outdated Trainer animation (#17060),0.77152133,Removed the deprecated TrainerTrainingTricksMixin class (#8679),"  Deprecate: initial LightningLoggerBase to Logger, tests/loggers pass   Refactor moved LightningLoggerBase to logger.py and removed base.py in loggers   Recreated base.py for safer backwards compatibility   Renamed test_base.py to test_logger.py and added test_base.py to test deprecation warning.   Renamed tests/loggers/test_base.py to tests/loggers/test_logger.py   Order all list in loggers/init alphabetically   minor: change deprecation warning of loggers.logger.base   fixed failing tests and formating   Update the documentation   move deprecation test to deprecated_api files   backward compatibility and deprecations tests for all functionality in loggers/logger.py   fix PEP8 issues.   Update CHANGELOG.md   Skip mypy on renamed file   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Aki Nitta nitta@akihironitta.com Co-authored-by: carmocca carlossmocholi@gmail.com",1
2449,Avoid firewall message from find_free_network_port (#13113),0.5222994,The LoadBalancer now uses internal ip + port instead of URL exposed (#16119),,0
2450,fixed amp bug,0.7755451,enable_pl_optimizer=False by default to temporarily fix AMP issues (#5163),Co-authored-by: manjirou maxim.mametkulov@halbestunde.com Co-authored-by: Kushashwa Ravi Shrimali kushashwaravishrimali@gmail.com,1
2451,Remove argparse utils (#16708),0.72086537,argparse_utils >> argparse, Update ci_pr-gatekeeper.yml,1
2452,Bump ravsamhq/notify-slack-action from 1 to 2 (#14290),0.57777107,- Fixed main progress bar counter when `val_check_interval=int` and `check_val_every_n_epoch=None` ([#12832](https://github.com/Lightning-AI/lightning/pull/12832),Co-authored-by: carmocca carlossmocholi@gmail.com,0
2453,Finish Ananthsub patch 1 (enable prepare_data from correct processes). clarify local vs global rank (#2166),0.801275,Enabled prepare_data from correct processes - clarify local vs global rank (#2166),Co-authored-by: carmocca carlossmocholi@gmail.com,1
2454,fabric: test with tbX (#16511),0.6834471,Fabric,"  clearing env vars in a test to allow compatibility with ""make test""   added clear=True to more mock environments in testcases   Co-authored-by: Akihiro Nitta nitta@akihironitta.com",0
2455,Remove deprecated max_steps=None (#13591),0.7243066,Changed default value of the max_steps Trainer argument from None to -1 (#9460),This reverts commit 184518c2fab188a9679a5b9d73ba95e3a8097280.,1
2456,Fix typo in API evolution (#9160),0.5847163,Removed deprecated API (#2073),"  add PR Gatekeeper   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  Apply suggestions from code review  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com",0
2457,new way of passing dataloaders (#759),0.7891648,"Accessing dataloaders (#16726, #16800)",,1
2458,"When running DDP without DistributedSampler, throw warning instead of exception (#91)",0.6983186,Changed warnings and recommendations for dataloaders in ddp_spawn (#6762),,0
2459,Enforce Lightning module as source of truth for automatic optimization (#7130),0.7006862,Add automatic optimization property setter to lightning module (#5169),  Clarification on omegaconf's interpolation support in LightningCLI.   Update docs/source/common/lightning_cli.rst   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com,1
2460,Remove ABC from LightningModule (#9517),0.73620236,Removed the deprecated LightningDeepSpeedModule (#16041),,1
2461,Enable timeout for DDPStrategy (#13244),0.6411568,"- Added a `timeout` argument to `DDPStrategy` and `DDPSpawnStrategy`. ([#13244](https://github.com/Lightning-AI/lightning/pull/13244), [#13383](https://github.com/Lightning-AI/lightning/pull/13383))",,0
2462,relax packaging versions (#16508),0.5739463,Implemented ready for components (#16129),,0
2463,release 0.7.6 (#1813),0.6749157,"    version=""0.0.1"",",,0
2464,release v0.3.6,0.7571784,"    version=""0.0.1"",",,1
2465,diable val and test shuffling (#1600),0.830185,Disabled val and test shuffling (#1600),,1
2466,args should come after the last positional argument (#1807),0.9587032,Args should come after the last positional argument (#1807),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
2467,Address feedback for new Lite docs (#16330),0.58717346,"docs for all Metrics (#2184, #2209)",,0
2468,Remove AccleratorConnector.device_type (#12081),0.7073829,- Removed the `AcceleratorConnector.device_type` property ([#12081](https://github.com/PyTorchLightning/pytorch-lightning/pull/12081)),,1
2469,Fix add_argparse_args raising TypeError with Python 3.6 (#9554),0.72026944,Trainer.add_argparse_args classmethod fixed. Now it adds a type for the arguments (#1147),  wip plugins and strategies   updates   updates   update   update   reset   remove transition   update references   update references to plugins   update registry usage   mention default setting for plugins ,1
2470,drop CircleCI,0.4920208,Drop duplicate metrics (#5014), ci(Mergify): configuration update  Signed-off-by: Jirka Borovec    draft   team   code    eden    Apply suggestions from code review   Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com   Update .github/mergify.yml   +krshrimali   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Apply suggestions from code review  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2471,Add an issue template for Code Improvement (#8960),0.59878784,Fixing critical bugs in newly added hooks and hparams assignment.,,0
2472,Drone: use nightly build cuda docker images (#3658),0.45093453,- Added native AMP support for `ddp_fork` (and associated alias strategies) with CUDA GPUs ([#14983](https://github.com/Lightning-AI/lightning/pull/14983)),,0
2473,Merge different gpu backends with accelerator='gpu' (#13642),0.7405349,"    accelerator=""gpu"", ",,1
2474,"Update wandb requirement from <0.12.20,>=0.10.22 to >=0.10.22,<0.13.2 in /requirements (#14080)",0.55771476,Setup: added requirement freeze for the next major version (#14480),,0
2475,Delete TensorBoardLogger experiment before spawning the processes. (#10777),0.7692572,Remove explicit flush from tensorboard logger (#2126),,1
2476,Update kfold example to avoid ci failures (#10019),0.47232252,Resolve bug with Finetuning (#5744),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
2477,Rename ParallelPlugin to ParallelStrategy (#11123),0.764414,- ParallelPlugin.teardown, bump black version to 22.3.0,1
2478,xfail flaky quantization test blocking CI (#13177),0.58467376,Disabled val and test shuffling (#1600),,0
2479,Fix wrong error message in ModelPruning (#13820),0.78533936,Replace MisconfigurationException with warning in ModelCheckpoint Callback (#4560), remove deprecated prepare_data_per_node from Trainer remove deprecated test for prepare_data_per_node remove doc for deprecated prepare_data_per_node remove inconsistency test remove deprecated prepare_data_per_node remove doc mentioning Trainer(prepare_data_per_node) update changelog remove unused code  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: rohitgr7 rohitgr1998@gmail.com,1
2480,[App] Fix cluster logic (#15383),0.72857463,Enabled custom clusters (#4048),,1
2481,Fix Deprecation warning in DDPSpawn (#8193),0.77486074,Silenced some warnings. verified ddp refactors (#3483),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
2482,bugfix/3185 transpose (#3252),0.62745583,Porting fixes to autoscaler component (#16249),Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: ananthsub 2382532+ananthsub@users.noreply.github.com,0
2483,Device (#1790),0.6285723,    devices=1, Remove progress_bar_refresh_rate from Trainer constructor changelog,0
2484,Automatically set sync_batchnorm for training_type_plugin (#6536),1.0,Automatically set sync_batchnorm for training_type_plugin (#6536),,1
2485,Allow for multiple example inputs when creating summary (#543),0.60210556,    # 2. Add the outputs to the list,,0
2486,Add notes to Trainer docs when devices flag is not defined (#12155),0.5959206,adding Trainer.tune() (#3293), updated titles + css,0
2487,Fix LightningOptimizer step and toggling logic (#9958),0.7205616,Check LightningOptimizer doesn't delete optimizer hooks (#6305),,1
2488,Steps (#1051),0.6721214,Renames model steps (#1051),,0
2489,Clean-up after logger connector redesign 1/2 (#7909),0.7574769,Dramatically simplify the LoggerConnector (#7882),,1
2490,Refactor test modules (#180),0.68785834,"Refactored training_batch + tests to verify correctness (#2327, #2328)",,0
2491,[App] Enable running with spawn context (#15923),0.56868446,"    * All spawn-based plugins now spawn processes immediately upon calling `Trainer.{fit,validate,test,predict}`",Update index.rst,0
2492,Add Lightning Ecosystem to docs (#11399),0.72874177,Update the Lightning App docs (#13537), Remove TPU Availability check from parse devices Update tests,1
2493,[App] Fix hanging CI (#15913),0.57539165,Introducing CLI commands for apps (#13602)!,,0
2494,[1/4] Add get_device_stats to accelerator interface (#9586),0.6275279,Refactored Accelerators and Plugins (#5743),,0
2495,Bump tj-actions/changed-files from 28 to 29.0.1 (#14430),0.5251864,moved TPU xxx_step to backend (#3118),Co-authored-by: rohitgr7 rohitgr1998@gmail.com,0
2496,Move model to cuda before creating optimizer (#554),0.642048,        Must return a model and list of optimizers,,0
2497,Use trainer.call_hook in the evaluation loop (#7626),0.7666732,Refactored trainer _run_* functions and separate evaluation loops (#8065),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
2498,easy import for lightningModule,0.8528904,from pytorch_lightning import LightningModule,Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.comk-Pro.local,1
2499,default logger is now tensorboard (#609),0.90183955,Changed the default logger to TensorBoardLogger (#609),,1
2500,ref: decouple apex second attemp part 5/n (#4058),0.6385236,"Epoch 8:  53%|█████    | 17/32 [5.13/s, v_num=2, loss=0.5643]",,0
2501,Update new-project.rst (#3734),0.5448665,Refactored EpochResultStore (#5522),,0
2502,fix docs links (#6057),0.604759,- Fixed a bug where the upload files endpoint would raise an error when running locally ([#14924](https://github.com/Lightning-AI/lightning/pull/14924)),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
2503,Fix resuming the tqdm progress bar  (#13962),0.80668175,Deprecated ProgressBar callback in favor of TQDMProgressBar (#10134)," set_epoch for prediction and evaluation minor fix in the test, warning msg was changed  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com",1
2504,release v0.0.2,0.79427093,"    version=""0.0.1"",",,1
2505,Auto convert to contiguous format for all_gather (#4907),0.7770977,Auto convert tensors to contiguous format when gather_all (#4907),,1
2506,updated docs,0.81916624,Docs improvements,,1
2507,FSDP with full state dict (#7487),0.7200186,Native FSDP implementation,,1
2508,Use raise .. from .. to explicitly chain exceptions (#3750),0.9870126,Used raise .. from .. to explicitly chain exceptions (#3750),Co-authored-by: rohitgr7 rohitgr1998@gmail.com,1
2509,Docs badges / images (#1202),0.5482967,Docs,Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
2510,Prevent last checkpoint being deleted after resumed training with changed dirpath (#12225),0.7755489,- Fixed an issue where `ModelCheckpoint` could delete last checkpoint from the old directory when `dirpath` has changed during resumed training ([#12225](https://github.com/PyTorchLightning/pytorch-lightning/pull/12225)),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz,1
2511,Add version header to CLI config files (#12532),0.7545526,A header with the version that generated the config is now included.,Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com,1
2512,update usage of deprecated checkpoint_callback (#5006),0.7210853,Saved checkpoints will no longer use the type of a Callback as the key to avoid issues with unpickling (#6886),,1
2513,[1/2] Deprecate outputs in on_train_epoch_end hooks (#7339),0.8014637,Deprecated outputs in both LightningModule.on_train_epoch_end and Callback.on_train_epoch_end hooks (#7339),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
2514,Fix TensorBoardLogger's validation of example input when logging graph (#15323),0.70293164,Auto log the computational graph for loggers that support this (#3003),,1
2515,update changelog after 1.4.9 release (#9762),0.7068223,Full Changelog: https://github.com/Lightning-AI/lightning/compare/1.8.0...1.8.0.post1,,1
2516,Fixes #2678 - enables training_step to return None (#3862),0.7104911,Changed default value of the max_steps Trainer argument from None to -1 (#9460),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.comk-Pro.local,1
2517,"Fix ""line too long"" PEP8 complaint (#8957)",0.5115515,Resolve bug with Finetuning (#5744),,0
2518,Create CORE_CONTRIBUTOR_GUIDELINES,0.67550707,core decorator data_loader,,0
2519,BYOT example (#16938),0.5872194,(#16002),,0
2520,Merge pull request #12723 from PyTorchLightning/req/strategies,0.669729,- Removed `configure_sync_batchnorm` from `ParallelStrategy` and all other strategies that inherit from it ([#11754](https://github.com/PyTorchLightning/pytorch-lightning/pull/11754)),,0
2521,Fix amp ddp test in Fabric (#16862),0.59903836,enable_pl_optimizer=False by default to temporarily fix AMP issues (#5163), Run HPU tests only with yml (#12469)  Execute supported tests serially Signed-off-by: Jerome janand@habana.ai,0
2522,"Revert ""Fix PL docs build on readthedocs.org (#15511)"" (#15565)",0.5514481,Docs improvements,,0
2523,Group all the logged gradients under the same sub-folder (#7756),0.5911056,    # 2. Inspect the (unscaled) gradients here,,0
2524,Fix _compare_version and add _TORCH_GREATER_EQUAL_DEV_1_10 (#9960),0.6771698,- Removed deprecated support for old torchtext versions ([#14375](https://github.com/Lightning-AI/lightning/pull/14375)),,0
2525,[docs] updated docs for min_delta parameter in early stopping callback (#9513),0.6036706,Avoid redundant callback restore warning while tuning (#13026),Co-authored-by: bruno.cabado bruno.cabado@cinfo.es Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
2526,ci: merge pytest with slow (#16689),0.66890347,"Resolved schedule step bug for PyTorch Profiler (#6674, #6681)",,0
2527,Accelerator Refactor: Precision Plugins (#5718),0.91017437,Refactored Accelerators and Plugins (#5743),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
2528,Feature GRID-9731: Update Lightning Cloud.py Backend to Accept Drive Specs (2/2) (#14106),0.6781821,- Fixed a bug with a default CloudCompute for Lightning flows ([#15371](https://github.com/Lightning-AI/lightning/pull/15371)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
2529,Fix gatekeeper minimum check (#13769),0.69106483,Early stopping checks on_validation_end (#1458),,0
2530,tests pep8,0.4917913,test_end >> test_epoch_end,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2531,integrated tensorboardx test-tube,0.68189865,Switch from tensorboard to tensorboardx in TensorBoardLogger (#15728),Co-authored-by: Aki Nitta nitta@akihironitta.com,0
2532,More clear docs for LightningDataModule (#13464),0.7494317,Updated LightningTemplateModel to look more like Colab example (#1577),,1
2533,remove deprecated callbacks (#3979),0.981863,Removed deprecated callbacks (#3979),,1
2534,fix hparams assign in init (#4189),0.7161364,Allowing decorate model init with saving hparams inside (#4662),,1
2535,Add a introduction documents for using Intel Neural Compressor to conduct post-training quantization (#16085),0.6046919,Refactor GPUStatsMonitor to improve training speed (#3257),,0
2536,Remove redundant require_backward_grad_sync=False in sharded plugins (#10065),0.61597353,Updated error message for interactive incompatible plugins (#9896),,0
2537,Fix mypy errors for model summary utilities (#13384),0.58007467,Deprecated LightningModule.summarize() in favor of pytorch_lightning.utilities.model_summary.summarize() (#8513),,0
2538,add test for model hooks (#4010),0.68137705,Removed deprecated model hooks (#3980),,0
2539,Tests: fix deprecated TM mape (#8830),0.603941,Updated logic for checking TPUs availability (#6767),,0
2540,add testing PT 1.12 (#13386),0.63514626,support for latest test-tube logger optimized for PT 1.2.0.   ,,0
2541,updated trainer docs (#4571),0.7324898,"Refactor RunningStage and TrainerState usage (#4945, #7173)",Co-authored-by: Ivan Svogor ivan.svogor@iarai.ac.at Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
2542,Remove deprecated distributed_backend from Trainer (#10017),0.9999998,Remove deprecated distributed_backend from Trainer (#10017),,1
2543,[skip ci] metric docs (#3992),0.7780442,"docs for all Metrics (#2184, #2209)",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: Aki Nitta nitta@akihironitta.com Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: four4fish 88516121+four4fish@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Kaushik B kaushikbokka@gmail.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: jjenniferdai 89552168+jjenniferdai@users.noreply.github.com Co-authored-by: Kushashwa Ravi Shrimali kushashwaravishrimali@gmail.com Co-authored-by: Akarsha Rao 94624926+raoakarsha@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Carlos Mocholí carlossmocholi@gmail.comk-Pro.local,1
2544,Refactor PL examples to examples/pytorch/ (#16925),0.65985173,Simplify the PL examples structure (shallower and more readable) (#1247),  Collect and run all ipu tests   Update azure pipeline   Increase pytest verbosity   Update RunIf   Co-authored-by: Akihiro Nitta nitta@akihironitta.com,0
2545,docs refactor 4/n (fix broken links) (#12825),0.62212133,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",,0
2546,replace MD template with Yaml - feature (#15908),0.6354955,Replace mata_tags.csv with hparams.yaml (#1271), Remove devbot,0
2547,NumPy to Torch: For LR Finder (#17264),0.6717749,"Native torch metrics (#1488, #2062)",,0
2548,Call _cuda_clearCublasWorkspaces on teardown (#16907),0.5664196,Deprecated @data_loader decorator  (#926),,0
2549,encourage draft PR submission (#4274),0.5373298,Contributors,,0
2550,Remove deprecated datamodule lifecycle properties (#10350),0.67683107,Removed deprecated property LightningModule.datamodule in favor of Trainer.datamodule (#9233),,0
2551,docs updates 1/n (#15473),0.6441519,Docs improvements,,0
2552,Loop specialization (#8226),0.75177467,Loop Customization,,1
2553,CI: adjust doctests for lightning.pytorch (#15469),0.7425181,from pytorch_lightning import LightningModule,,1
2554,Fix setter usage for checkpoint io and precision in TTP (#11071),0.61456656,Refactor load in checkpoint connector (#4593),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
2555,Native torch metrics (#1488),0.98591185,"Native torch metrics (#1488, #2062)",Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com,1
2556,Add MPI cluster environment (#16570),0.7525935,Enabled custom clusters (#4048),,1
2557,fix logging on rank 0 only (#2425),0.74947464,Removed restrictions in the Trainer that loggers can only log from rank 0; the existing logger behavior has not changed (#8608),,1
2558,"Update comet-ml requirement from <=3.28.2,>=3.1.12 to >=3.1.12,<3.31.6 in /requirements (#13414)",0.63477004,Using .comet.config file for CometLogger (#1913),,0
2559,Remove deprecated device attributes from Trainer (#14829),0.73985577,Deprecated the Trainer.disable_validation property in favor of not Trainer.enable_validation (#8291),,1
2560,Update docs for base Loop class with examples (#9993),0.62664,"Base classes (#1326, #1877)",Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
2561,[docs] Update FSDP instructions and add DeepSpeed evaluate/predict example (#8713),0.72282946,"Implemented DeepSpeedPlugin._setup_model_and_optimizers (#10009, #10064)",  Update logging docs   fix text size issue   put back Log Writing Frequency section ,1
2562,Add support for specifying process group backend to relevant distributed strategies (#11745),0.7500106,- Added support to explicitly specify the process group backend for parallel strategies ([#11745](https://github.com/PyTorchLightning/pytorch-lightning/pull/11745)),,1
2563,Mark evaluation epoch loops attributes as protected (#8420),0.62894946,"Marked several methods in EvaluationLoop as protected: get_max_batches, on_evaluation_model_eval, on_evaluation_model_train, on_evaluation_start, on_evaluation_epoch_start, on_evaluation_epoch_end, on_evaluation_end, reload_evaluation_dataloaders (#9516)",,0
2564,Fix/mismatched toggle optimizer (#7563),0.7083209,Refactored optimizer (#4658),,1
2565,ref: reduced all simplified_forward (#3126),0.9188852,reduced all simplified forward (#3126),,1
2566,Added args parameter to LightningCLI to ease running from within Python (#14596),0.8975228,- Added `args` parameter to `LightningCLI` to ease running from within Python ([#14596](https://github.com/Lightning-AI/lightning/pull/14596)),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
2567,Refactor LightningDistributedDataParallel (#5185),0.83896315,Deprecated LightningDistributedDataParallel in favor of new wrapper module LightningDistributedModule (#5185),,1
2568,ci: trigger on action edit (#16514),0.5040968,refactored dataloader process hook (#3139),,0
2569,tests for val step flow and logging (#3731),0.66829807,tests for val loop flow (#2605),,0
2570,Collect and run all IPU tests (#11170),0.6000787,The CPU stats are gathered using the psutil package.,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2571,App tests hang on Windows with Python 3.9 (#15385),0.6885861,Updated app testing (#16000),Co-authored-by: thomas chaton thomas@grid.ai,0
2572,Remove the redundant precision attribute from LightningModule (#16203),0.7878437,- Removed the `LightningModule.precision` attribute ([#16203](https://github.com/Lightning-AI/lightning/pull/16203)), Add needs triage to issues by default,1
2573,ref: adding compute environments (1/n) (#3837),0.9096887,"adding compute environments (#3837, [#3842)",,1
2574,Remove the deprecated auto_select_gpus Trainer argument (#16184),0.86092037,  * Removed the `Trainer(auto_select_gpus=...)` argument,,1
2575,log metrics for correct dataloader only (#10522),0.7231292,Metric reduction with Logging (#5150),Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
2576,add linked badge (#2983),0.5400226,adding Trainer.tune() (#3293),,0
2577,drop unused variable in API (#6308),0.6446972,Removed deprecated API (#2073),  do not mark LightningModule methods as abstract   add concrete test ,0
2578,support num_sanity_val_steps=-1 (#2246),0.7523157, - Sanity checking: `Trainer(num_sanity_val_steps>0)`," Pin setup-gcloud to v0 instead of master.  setup-gcloud will be updating the branch name from master to main in a future release. Even though GitHub will establish redirects, this will break any GitHub Actions workflows that pin to master. This PR updates your GitHub Actions workflows to pin to v0, which is the recommended best practice. Co-authored-by: Akihiro Nitta nitta@akihironitta.com",1
2579,Set num_nodes and sync_batchnorm From Trainer for Manually Passed Training Type Plugin (#7026),0.9080727,Changed resolve_training_type_plugins to allow setting num_nodes and sync_batchnorm from Trainer setting (#7026),,1
2580,Add teardown method to BaseProfiler. (#6370),0.65616167,Changed teardown() in Accelerator to allow training_type_plugin to customize teardown logic (#7579),,0
2581,Refactor base profilers 3/5 (#6621),0.66821104,Split profilers module (#6261),,0
2582,fix mypy typing for model summary (#9447),0.53682214,import my_code.models,Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
2583,CI cleaning (#4941),0.81490004,"Cleaning (#5948, #5949, #5950)",,1
2584,release v0.3.6.7,0.7566881,"    version=""0.0.1"",",,1
2585,updated required deps,0.58757895,DDP Debugging Improvements,Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Kaushik B kaushikbokka@gmail.com,0
2586,Typo fix in upgrade from 1.9.x to 2.0 docs: use_distributed_sample-> use_distributed_sampler (#17113),0.5920282,Allow setting replace_sampler_ddp=True with a distributed sampler already added (#4273),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
2587,Replace ColossalAIStrategy with external implementation (#16757),0.6165447,Refactored EpochResultStore (#5522),,0
2588,Do not prefetch when possible (#12101),0.6186874,Do not override PYTHONWARNINGS (#4700),,0
2589,[App] Removing single quote (#16079),0.5464682,Removed the SingleProcessRuntime (#15933),,0
2590,Move s-rog (myself) from core to alumni (#13694),0.46227235,training forward refactor (#3134),  Refactor TorchElasticEnvironment.detect to use native utility from torch.distributed   fix version and tests   fix version   Update tests/accelerators/test_accelerator_connector.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
2591,Add test assertion (#9309),0.6183498,  validation_if_necessary(),,0
2592,prune Results usage in notebooks (#3911),0.54794276,Sanitize None params during pruning (#6836),Co-authored-by: Aki Nitta nitta@akihironitta.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
2593,Fix docs typo (#2778),0.5323434,Rename failed -> error in tables (#15608),,0
2594,"[App] Add status endpoint, enable ready (#16075)",0.54331315,Updated app testing (#16000),,0
2595,Expectopatronum implement #89 (#182),0.594556,Refactored EpochResultStore (#5522),,0
2596,[fix] Attach train+val dataloaders to trainer in trainer loop (#7207),0.80010366,train_dataset = trainer.train_dataloader.dataset,,1
2597,fix resuming from checkpoint for fault-tolerant in case of no failure (#9371),0.68130195,Refactor load in checkpoint connector (#4593),,0
2598,Set accelerator through CLI only if set explicitly (#16818),0.66805214,Enabled passing in custom accelerators (#4050),,0
2599,enabling gpu size = 1 to run without data parallel,0.72216684,Changed min-max GPU memory to be on their own plots (#1358),  Update rich version   Update requirements/extra.txt   Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,1
2600,Fix mypy errors attributed to pytorch_lightning/loops/epoch/training_epoch_loop.py (#13555),0.7177808,Removed pytorch_lightning/trainer/predict_loop.py (#8094), Horovod w. MPI nccl_built fix,1
2601,Multi opts tests and clarification  (#4016),0.67933804,Generalized Optimizer validation to accommodate both FSDP 1.x and 2.x (#16733),,0
2602,[App] Update multi-node examples (#15700),0.6756886,Updated Multinode Warning (#16091),,0
2603,Fix enforce_datamodule_dataloader_override() for iterable datasets (#2957),0.69015133,Disabled sampler replacement when using IterableDataset (#11507),,0
2604,add copyright to tests (#5143),0.487529,@property,,0
2605,make model checkpointing test deterministic (#9457),0.673061,"Deprecated model steps training_end, validation_end and test_end (#1051, #1056)", Pin docker image sha,0
2606,Simplify deprecations (#6620),0.7745233,Simplify optimization Logic (#4984),,1
2607,Makes automatic optimization a model attribute (#4602),0.95248735,Changed automatic_optimization to be a model attribute (#4602), fix fire horovod assistant cmake u20 cuda -j2 fix mypy  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
2608,Bump hivemind from 1.0.1 to 1.1.2 in /requirements (#15839),0.6229984,- Hivemind Strategy,Co-authored-by: edward-io me@edward.io Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Aki Nitta nitta@akihironitta.com,0
2609,ci: flagship disable pr (#16354),0.61062014,Removed ProfilerConnector (#7654),,0
2610,change print to logging (#457),0.85374296,switched from print to logging,Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
2611,Fail the test when a DeprecationWarning is raised (#9940),0.7462177,Deprecation warning (#3844), nightly release min version fire,1
2612,Fix wrong num padding for RichProgressBar (#14296),0.6057975,Resolve bug with Finetuning (#5744), CI: sanity check for req. pkgs scripts rename gcsfs ? rich ! install extra move set -e  Co-authored-by: Aki Nitta nitta@akihironitta.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
2613,add Greeting action (#843),0.4552346,Support for user-defined callbacks (#889 and #950), enable PT 1.11 horovod Apply suggestions from code review  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Aki Nitta nitta@akihironitta.com,0
2614,Docs 4/n (#15628),0.6705109,"docs for all Metrics (#2184, #2209)", try skip horovod 0.24.0 only HOROVOD_BUILD_CUDA_CC_LIST fix test  Co-authored-by: Aki Nitta nitta@akihironitta.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2615,Add module wrapper code,0.6199425,  * Removed the deprecated `pl_module` argument from the distributed module wrappers,,0
2616,Add support for custom cloud compute configurations for Flows (#14831),0.76827735,- Added support for configuring flow cloud compute ([#14831](https://github.com/Lightning-AI/lightning/pull/14831)),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
2617,Fixes #2407 (#2981),0.6165844,Resolve bug with Finetuning (#5744),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Aki Nitta nitta@akihironitta.com,0
2618,Fix incorrect code-snippet in optimizers doc (#7598),0.69765925,Refactored optimizer (#4658),,0
2619,Support true 16-bit precision with deepspeed (#17576),0.71430945,Support for manual optimization with DeepSpeed (#7970),,1
2620,fix typo in readme,0.48660037,Changed overwrite to True (#16009),,0
2621,"Fix incorrect precision=""mixed"" being used with DeepSpeedStrategy and IPUStrategy (#14041)",0.7360071,Updated precision attributes in DeepSpeedPlugin (#10164),,1
2622,Update CHANGELOG (#6156),0.72948307,Full Changelog,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
2623,Checkout the HEAD SHA for docs builds (#15510),0.5541787,"docs for all Metrics (#2184, #2209)",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2624,Pinning starsessions to 1.x (#14333),0.99999994,Pinning starsessions to 1.x (#14333),,1
2625,LAI: creating mirror package (#15105),0.54706997,Dropped torchvision dependency in tests and added own MNIST dataset class instead (#986),,0
2626,fix retruning returns (#1431),0.6426883,Set Loop.restarting=False at the end of the first iteration (#8362),Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: rohitgr7 rohitgr1998@gmail.com Co-authored-by: Kushashwa Ravi Shrimali kushashwaravishrimali@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
2627,Update lightning_template model to save hparams. (#2665),0.77673066,Removed deprecated LightningModule hparams setter (#6207),,1
2628,waiting on feedback (#15893),0.5472144,[0.6.0] - 2022-09-08,,0
2629,Fix locally failing lite tests (#15137),0.6071975,Improved the error message when the LightningWork is missing the run method (#14759),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
2630,Remove model.trainer call inside of dataloading mixin (#7317),0.99999994,Remove model.trainer call inside of dataloading mixin (#7317),,1
2631,added test docs,0.69690657,Docs improvements,,0
2632,LightningCLI natively support callback list append (#13129),0.8041675,LightningCLI.instantiate_trainer now takes a config and a list of callbacks (#8721),,1
2633,CI: add mdformat (#8673),0.56030464,"adding compute environments (#3837, [#3842)",,0
2634,Fix min-epochs and early-stopping triggering too many validation runs (#16719),0.7443782,Early stopping checks on_validation_end (#1458),,1
2635,Use high progress_bar_refresh_rate on Google Colab (#4654),0.76199806,Changed the default value for the progress_bar_refresh_rate Trainer argument in Google COLAB notebooks to 20 (#5516),,1
2636,Add current_score to ModelCheckpoint.on_save_checkpoint (#4721),0.74438894,"The ModelCheckpoint.save_on_train_epoch_end attribute is now computed dynamically every epoch, accounting for changes to the validation dataloaders (#15300)",,1
2637,Rename _SupportsStateDict --> _Stateful Protocol (#11469),0.54951745,Replaced _DataModuleWrapper with __new__ (#7289),Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
2638,show progressbar only on progress_rank 0 on ddp_slurm (#4437),0.75824094,Better progress bar (#16695),,1
2639,Rewrite accelerator_connector (#11448),0.8897151,AcceleratorConnector rewrite,,1
2640,Update CI to CUDA 11.7.1 (#16123),0.5139676,"device = ""cuda"" if torch.cuda.is_available() else ""cpu""",,0
2641,remove deprecated test (#3820),0.7735764,Removed deprecated: (#2760),,1
2642,fix path in CI for release & python version in all dockers & duplicated badges (#3765),0.52298844,Renamed and moved core/step_result.py to trainer/connectors/logger_connector/result.py (#7736),,0
2643,[metrics] Update SSIM (#4566),0.97827995,Updated SSIM metric (#4566)(#4656),Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Kaushik B kaushikbokka@gmail.com,1
2644,fixed multiprocessing import,0.68717194,from setuptools import setup,,0
2645,Support val_check_interval values higher than number of training batches  (#11993),0.80193985,Trainer(val_check_interval=100),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
2646,loggers: revert useless exception (#15789),0.7534948,Removed LoggerStages (#5673),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,1
2647,[App] Enable debugger with LightningApp (#15590),0.7580942,Lightning App,Co-authored-by: rohitgr7 rohitgr1998@gmail.com,1
2648,Construct the hook kwargs inside each loop (#12100),0.67863154,moved hooks around in eval loop (#3195),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
2649,Reduce number of times optimizers are instantiated with FSDP (#12267),0.7400079,Refactored optimizer (#4658),Co-authored-by: rohitgr7 rohitgr1998@gmail.com,1
2650,Avoid underscore suffix in filenames (#15189),0.5306703,Deprecated Profiler(output_filename) in favor of dirpath and filename (#6621),,0
2651,Fix iterating over a DummyLogger when fast_dev_run > 0 (#10232),0.68199635,Tuner algorithms will be skipped if fast_dev_run=True (#3903),Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
2652,Fix test_quantization with Pytorch 1.10 (#9808),0.70640063,Skipped testing with PyTorch 1.7 and Python 3.9 on Ubuntu (#11217),,1
2653,missing changelog (#2464),0.70374906,Complete changelog,,1
2654,Update deepspeed precision plugin for Lite (#10164),0.8154124,Updated precision attributes in DeepSpeedPlugin (#10164),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
2655,Continue Jeremy's early stopping PR #1504 (#2391),0.5315101,[1.1.7] - 2021-02-03,Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
2656,Update CODEOWNER of CI/CD (#14676),0.5049966,Configuration Validator (#9779),,0
2657,Remove the deprecated code in pl.core.mixins (#16424),0.70125496,Removed unused mixin attributes (#6487),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
2658,Delete leftover directory after rename (#13302),0.5861132,  * `save_config_filename`,,0
2659,fixing build meta pkg flow (#13926),0.6031486,"Resolved schedule step bug for PyTorch Profiler (#6674, #6681)",,0
2660,CI: resume testing with py3.8 (#6516),0.67304164,Skipped testing with PyTorch 1.7 and Python 3.9 on Ubuntu (#11217),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2661,Remove redundant test (#6466),0.6477412,Refactored setup_training and remove test_mode (#5388),,0
2662,fixed TPU docs (#5958),0.677484,Updated logic for checking TPUs availability (#6767),,0
2663,Deprecate the FairScale integration (#16353),0.732379,FairScale deprecation (in favor of PyTorch's FSDP implementation) (#16353),,1
2664,Allow modifying run_examples.sh defaults (#8769),0.54472274,Reuse existing commands when running connect more than once (#15471),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2665,Remove trainer.fit return value [2/n] (#7237),0.9392991,Removed trainer.fit() return value of 1. It has no return now (#7237),Co-authored-by: Aki Nitta nitta@akihironitta.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
2666,Fix mypy errors attributed to pytorch_lightning. strategies.sharded_spawn (#14102),0.7410363,+ # pytorch_lightning==1.7.0,,1
2667,Fixing tests (#936),0.68824565,- fixed all the .test() calls,,0
2668,Tests/App: refactor examples - structure (#15770),0.6686713,Updated app testing (#16000),,0
2669,Update docs for sync_dist logging option (#10186),0.7425823,Support number for logging with sync_dist=True (#5080),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
2670,convert warning cache usage to rank_zero_only in WandbLogger (#8764),0.64768136,Prevent WandbLogger from dropping values (#5931),,0
2671,Fix gradient clipping (#1438),0.73538274,Gradient Clipping Customization,,1
2672,Sampler (#1328),0.58370966,"Base classes (#1326, #1877)",,0
2673,fix legacy creation (#16282),0.5695671,Refactored EpochResultStore (#5522),,0
2674,CI: skip horovod in testing docs (#5702),0.6266315,horovod deprecation (#16141),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
2675,freeze PT 1.5 for Horovod issue (#2744),0.65267044,"refactored Horovod backend (#3121, #3122)",,0
2676,Use a standalone test symlink for Lite (#14502),0.6151471,- Enabled using any Sampler in distributed environment in Lite ([#13646](https://github.com/Lightning-AI/lightning/pull/13646)),,0
2677,[CLI] Fix cluster logs with over 5000 entries (#14458),0.69412315,Fixing 5000 log line limitation for Lightning AI BYOC cluster logs (#14458),  remove AccleratorConnector.device_type   Update CHANGELOG.md   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2678,Add MpModelWrapper in TPU Spawn (#7045),0.6485057,Renamed TPUHalfPrecisionPlugin to TPUBf16PrecisionPlugin (#10026),,0
2679,hpc restore takes priority over non hpc weights (#419),0.5740707,Deprecated the use of CheckpointConnector.hpc_load() in favor of CheckpointConnector.restore() (#7652),Co-authored-by: rohitgr7 rohitgr1998@gmail.com,0
2680,Raise MisconfigurationException if trainer.eval is missing required methods (#10016),0.8575789,Trainer is now raising a MisconfigurationException instead of a warning if Trainer.{validate/test} is missing required methods (#10016),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
2681,fix gpus default for Trainer.add_argparse_args (#6898),0.71469986,Improved Argparse usability with Trainer,Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
2682,Merge pull request #23 from williamFalcon/lkhphuc-lr_sched,0.5246583,refactored dataloader process hook (#3139),,0
2683,remove import of datasets separately since unused (#10668),0.61571795,"  from torch.utils.data import DataLoader, Dataset",Co-authored-by: Aki Nitta nitta@akihironitta.com Co-authored-by: Jirka jirka.borovec@seznam.cz,0
2684,remove frame inspection on self.hparams (#2253),0.5512996,Remove MetricsHolder (#7909),,0
2685,[Docs] hparams is not a dictionary for now (#1026),0.56029725,Docs,,0
2686,Lazy import check for neptune dependency (#13477),0.6706843,Wrapped imports for traceability (#13924),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2687,[FEAT] Refactor logging 3/3 [v1] (#4552),0.8402418,Refactored logging,,1
2688,Add basic SSH documentation for CLI (#15316),0.6605065,- Added support for managing SSH-keys via CLI ([#15291](https://github.com/Lightning-AI/lightning/pull/15291)),,0
2689,add version_ prefix to log_dir (#706),0.63876987,Changed the behaviour when logging evaluation step metrics to no longer append /epoch_* to the metric name (#7351),,0
2690,Remove deprecated XLAStatsMonitor (#12688),0.75054425,- Removed the deprecated `XLAStatsMonitor` callback ([#12688](https://github.com/Lightning-AI/lightning/pull/12688)),Co-authored-by: rsokl ryan.soklaski@ll.mit.edu Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,1
2691,Add AcceleratorRegistry (#12180),0.782597,"AcceleratorRegistry.register(""sota_accelerator"", SOTAAccelerator, x=123)",Co-authored-by: Danielle Pintz 38207072+daniellepintz@users.noreply.github.com,1
2692,Remove opt from manual_backward in docs (#6267),0.5837817,  * Removed arguments `optimizer` and `optimizer_idx` from `LightningModule.backward`,"  fix is_interactive_compatible   improve tests   update message   address review   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
2693,attempting to remove some speed issues (#1482),0.6344934,Removed DeepSpeed FP16 Exception as FP32 is now supported (#8462),,0
2694,added docs (#944),0.6586236,Docs improvements,,0
2695,Update dataloaders params in example (#8191),0.72533506,Enhanced load_from_checkpoint to also forward params to the model (#1307),,1
2696,[doc] Improve Multiple Val/Test Dataloaders with simultaneous batches option (#6320),0.7096171,"for data: val_dataloader, test_dataloader, train_dataloader",,1
2697,val and test are optional now (#95),0.753934,"validation_step, val_dataloader are now optional.   ",,1
2698,Implement TensorboardLogger (#607),0.84990007,Switch from tensorboard to tensorboardx in TensorBoardLogger (#15728),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com,1
2699,Create required group for app examples (#15332),0.61442864,group prepare data hook (#3212),,0
2700,Move device parser tests inside Lite (#14586),0.67234236,"device parser (#3400, #3405)",Co-authored-by: ananthsub ananth.subramaniam@gmail.com,0
2701,Make BYOT imports forward compatible (#16997),0.5862232,import argparse,Co-authored-by: Danielle Pintz 38207072+daniellepintz@users.noreply.github.com,0
2702,Fix access to logger attribute when multiple loggers are used (#14234),0.8393201,Logging with multiple loggers,Co-authored-by: Chaddie chaddie.paik@webtoonscorp.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
2703,Update Lightning Lite docs (1/n) (#16250),0.86114275,Update the Lightning App docs (#13537),,1
2704,Set smarter default for DDP sharded for performance optimization (#6937),0.72473997,"Now, you get the best speed by default for all ddp variants",,1
2705,Add a performance section to TPU docs to address FAQ  (#5445),0.64567065,Updated logic for checking TPUs availability (#6767),Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Ethan Harris ethanwharris@gmail.com Co-authored-by: thomas chaton thomas@grid.ai,0
2706,added 16 bit training support with --use_amp flag,0.5703894,Trainer now raises an exception when requesting amp_level with native amp_backend (#9755),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
2707,Update CHANGELOG.md,0.7592224,Complete changelog,,1
2708,Remove the deprecated profile_iterable (#14864),0.73613113,- Removed the deprecated `SimpleProfiler.profile_iterable` and `AdvancedProfiler.profile_iterable` attributes ([#14864](https://github.com/Lightning-AI/lightning/pull/14864)),,1
2709,[App] Add ServeStreamlit work (#15400),0.60947186,Add support for Lightning App Commands through the configure_commands hook on LightningFlow and ClientCommand  (#13602),,0
2710,enable single gpu per node (#218),0.6107852,Made DDP the default if no backend specified with multiple GPUs (#1789),,0
2711,increase profiler test coverage (#1208),0.57491744,test_percent_check in favour of limit_test_batches,Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk,0
2712,[ci skip] NeMo Documentation for PyTorch Lightning (#3707),0.713007,+ # pytorch_lightning==1.7.0,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
2713,Add callout items to the Docs landing page (#12196),0.63182384,Read more about callback entry points in our docs.,,0
2714,Ensure we add the condition to the case statement,0.56585443,1. Add the arguments you need,,0
2715,dockers nightly (#3615),0.5606569,"Mocking loggers (#3596, #3617, #3851, #3859, #3884, #3853, #3910, #3889, #3926)",,0
2716,Standalone Lite: Accelerators (#14578),0.72773445,Refactored Accelerators and Plugins (#5743),Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,1
2717,prune metrics: info retrieval (#6649),0.580937,Pruned deprecated classif. metrics from pytorch_lightning.metrics.functional.classification (#7499),,0
2718,Clarify work.stop() limitation (#16073),0.57459164,Resolved a bug where the work statuses will grow quickly and be duplicated (#13970),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2719,add missing req.,0.48182043,Moved base req. to root (#4219),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2720,update codeowners for lit (#16596),0.58596957,- Removed deprecated support for old torchtext versions ([#14375](https://github.com/Lightning-AI/lightning/pull/14375)),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: thomas chaton thomas@grid.ai,0
2721,Fix incorrect handling of on_batch_end edge cases in run_training_batch (#509),0.6902386,"Overriding the on_train_batch_{start,end} hooks in conjunction with taking a dataloader_iter in the training_step no longer errors out and instead shows a warning (#16062)",Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
2722,"update docs for ""overfit_batches"" (#2324)",0.66860247,overfit_pct in favour of overfit_batches,,0
2723,fix links to Docs (#744),0.60002613,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",,0
2724,ignore argparse from example for tests,0.644276,argparse_utils >> argparse,,0
2725,Remove _StrategyType (#16328),0.64554286,Renamed Strategy.reduce to Strategy.all_reduce in all strategies (#16370),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2726,fix pretty print (#1441),0.622154,Monir bug fix with print issues and data_loader (#1080),,0
2727,Fix accumulated_grad_batches typehint (#9071),0.6138756,Removed deprecated model hooks (#3980),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
2728,Start version suffixes at 1 (#5008),0.7384687,Changed ModelCheckpoint version suffixes to start at 1 (#5008),,1
2729,"Revert ""Add check to ensure 1.6""",0.58613026,Early stopping checks on_validation_end (#1458),,0
2730,Fix root node resolution (#1954),0.72026885,    root_node = '127.0.0.2',,1
2731,Support DeepSpeed <0.7.0 (#13859),0.78290963,Support for manual optimization with DeepSpeed (#7970),,1
2732,fix: MNIST minor bug (#5075),0.64804375,Resolve bug with Finetuning (#5744),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
2733,"Support calling fit and test scripts using ""python -m"" module syntax with DDP (#8073)",0.6261051,python script.py test,,0
2734,Update changelog after 1.5.8 release (#11336),0.71646106,Full Changelog,,1
2735,docs: minor spelling tweaks (#5022),0.5719694,Docs improvements,Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com,0
2736,updated hooks (#2850),0.775543,Updated hooks arguments - breaking for setup and teardown (#2850),,1
2737,Avoid loading dataloaders if limit_batches=0 (#11576),0.7686529,- Disable loading dataloades if corresponding `limit_batches=0` ([#11576](https://github.com/PyTorchLightning/pytorch-lightning/pull/11576)),,1
2738,update CHANGELOG (#897),0.7257906,Full Changelog,,1
2739,added badge,0.5006726,New features,,0
2740,Distributed communication docs for Lite (#16373),0.5753514,Changed default setting for communication of multi-node training using DDPShardedPlugin (#6937),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
2741,Torch Elastic DDP DeadLock bug fix (#8655),0.6130106,- Fixed issue where the CLI fails with certain torch objects ([#13153](https://github.com/Lightning-AI/lightning/pull/13153)),,0
2742,"docs: use ref for anchor links, fix a few typo (#3486)",0.57336795,Fixing critical bugs in newly added hooks and hparams assignment.,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
2743,add test for none checkpoint in ddp_spawn (#2845),0.65383637,Enable None model checkpoint default (#3669),  pip install --user   add checks   rm unrelated comment   consistent format   Fail if horovod not found   Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2744,remove wrong annotation (#2349),0.6121313,Removed deprecated: (#2760),,0
2745,Move optimizer creation after device placement for ddp backends. (#2904),0.83164513,Moved optimizer creation after device placement for DDP backends (#2904](https://github.com/PyTorchLightning/pytorch-lighting/pull/2904)),,1
2746,Forward extra keyword arguments in LightningDataModule.from_datasets (#14185),0.7400119,- Added support for passing extra init-parameters to the `LightningDataModule.from_datasets` ([#14185](https://github.com/Lightning-AI/lightning/pull/14185)),,1
2747,Finish PR #2432: Imagenet example updates + basic testing (#2889),0.56143147,Updated logic for checking TPUs availability (#6767),Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Danielle Pintz 38207072+daniellepintz@users.noreply.github.com,0
2748,update (#7056),0.6360431,Update the Lightning App docs (#13537),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
2749,Set the logger explicitly in tests (#15815),0.7168249,for logger in loggers:,,1
2750,Callbacks [wip] (#889),0.70492953,  callbacks:,Co-authored-by: louie.kim louie.kim@kakaocorp.comlouie.kim@kakaocorp.com Co-authored-by: Jirka jirka.borovec@seznam.cz,1
2751,Mean Average Precision metric for Information Retrieval (1/5) (#5032),0.69670373,Renaming of precision recall metric (#3308),,0
2752,clear memory cache before train starts (#418),0.64873356,Resolve memory leak for evaluation (#6326),,0
2753,add epoch option (#933),0.63808906,epoch can now log independently (#3843),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
2754,Add docs for fsdp_native (#14108),0.71925014,Native FSDP implementation,,1
2755,fix(docs): test_dataloader use transformed dataset (#3090),0.6851928,refactored dataloader process hook (#3139),Co-authored-by: rohitgr7 rohitgr1998@gmail.com,0
2756,Organize app examples (#17045),0.6940037,App,,0
2757,Disable batch_size extraction for torchmetric instances (#10815),0.9177631,Disabled batch_size extraction for torchmetric instances because they accumulate the metrics internally (#10815),,1
2758,Revert property as this is incorrect.=,0.5236537,Refactor Model backward (#2276),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
2759,update chlog + legacy chpt (#7954),0.5865427,0.4.0,,0
2760,Improve collision check on hparams between LightningModule and DataModule  (#9496),0.7073543,Deprecated on_{train/val/test/predict}_dataloader() from LightningModule and LightningDataModule (#9098),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
2761,Added doc strings to base logger file (#9232),0.61340797,Return the default_root_dir as the log_dir when the logger is a LoggerCollection (#8187),Co-authored-by: ananthsub ananth.subramaniam@gmail.com,0
2762,[App] Improve lightning connect experience (#16035),0.84611595,Lightning App,,1
2763,Pin myst-parser<0.17,0.47749826,The PrecisionPlugin.backward hooks no longer returns a value (#8328),,0
2764,group all loop tests in a folder (#7394),0.49458355,Enabling val/test loop disabling (#2692),,0
2765,Allow log to an existing run ID in MLflow with MLFlowLogger (#12290),0.85776925,- Allow logging to an existing run ID in MLflow with `MLFlowLogger` ([#12290](https://github.com/PyTorchLightning/pytorch-lightning/pull/12290)),Co-authored-by: Aki Nitta nitta@akihironitta.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
2766,[Hot Fix] Ensure process_dataloader is called when tpu_cores > 1 to use Parallel DataLoader (#6015),0.9486971,Ensured process_dataloader is called when tpu_cores > 1 to use Parallel DataLoader (#6015),Co-authored-by: rohitgr7 rohitgr1998@gmail.com,1
2767,hide doctest decoration (#2194),0.6317425,Allowing decorate model init with saving hparams inside (#4662), Centralize rank_zero_only utilities into their own module  Fixes #11746   PossibleUserWarning   Update test_warnings.py   update imports   more imports   Update CHANGELOG.md   Update mlflow.py   Update cli.py   Update api_references.rst   Update meta.py   add deprecation tests   debug standalone   fix standalone tests   Update CHANGELOG.md ,0
2768,[CLI] Move storage from app prefix to project/app prefix (#14583),0.8968927,Application storage prefix moved from app_id to project_id/app_id (#14583), small improvements to TB and CSV loggers addr comments remove redundant lines and update tests  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Aki Nitta nitta@akihironitta.com,1
2769,Refactor XLA and TPU checks across codebase (#14550),0.6766518,Updated logic for checking TPUs availability (#6767),,0
2770,Use user executable for pip installation (#15230),0.7227591,pip install rich,,1
2771,Neptune integration (#648),0.66275954,Changed to the NeptuneLogger (#16761):,,0
2772,Remove deprecated items from api reference (#16151),0.79497635,Removed deprecated API (#2073),,1
2773,Parametrize deepspeed hook test (#11308),0.6795676,DeepSpeed Stage 1,,0
2774,added tbptt test for logging (#3850),0.6767746,support for latest test-tube logger optimized for PT 1.2.0.   ," Use trainer.strategy.root_device in favor of LightningModule.device in DeviceStatsMonitor  Minor refactor to use the strategy's own root_device instead of the LightningModule's device property. Attempts at manual model parallelization by extending this plugin will face difficulties with the assumption that the LightningModule has all of its parameters on the same device.  For those use cases, it is critical to remove the assumption that the module has a device property (device in general goes against PyTorch module's design principles: - https://github.com/pytorch/pytorch/issues/7460 - https://github.com/PyTorchLightning/pytorch-lightning/pull/1790#discussion_r423459412",0
2775,"Remove pytorch_lightning.profiler.{AbstractProfiler,BaseProfiler} deprecated since v1.6 (#15637)",0.94188046,Removed deprecated pytorch_lightning.profiler.base.AbstractProfiler in favor of pytorch_lightning.profilers.profiler.Profiler (#15637),,1
2776,Docs BYOC content (#13976),0.58896554,"docs for all Metrics (#2184, #2209)",Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
2777,Improve typing for loops (#10780),0.6693283,Loop customization improvements,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2778,fixes test issues on ddp (#1017),0.7200844,Silenced some warnings. verified ddp refactors (#3483),,1
2779,deprecate on_{train/val/test/predict}_dataloader() from DataHooks (#9098),0.7668439,"for data: val_dataloader, test_dataloader, train_dataloader",,1
2780,Standardize tensor annotation (#13292),0.64606035,Changed (renamed and refactored) TensorRunningMean -> TensorRunningAccum: running accumulations were generalized. (#1278),,0
2781,release v0.3.3,0.7418796,0.4.0,Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com,1
2782,Ban tensorboard==2.5.0 and deepspeed==0.3.15 (#7159),0.6321132,DeepSpeed Stage 1,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2783,Update with GitHub Discussions (#6186),0.59067047,Full Changelog: https://github.com/Lightning-AI/lightning/compare/1.8.0...1.8.0.post1,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Sean Naren sean@grid.ai Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: thomas chaton thomas@grid.ai,0
2784,multi processing warnings (#1602),0.7841422,Updated Multinode Warning (#16091),,1
2785,Prepare changelog for 1.8 (#15111),0.80372375,Complete changelog,,1
2786,clarify gpu / process (#6049),0.64774716,GPU training (#2704),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
2787,fixing master (#17268),0.55795014,This release fixes that core issue,,0
2788,[IPU] Do not use DistributedSampler (#12114),0.7109654,- No longer set a `DistributedSampler` to the `poptorch.DataLoader` when IPUs are used ([#12114](https://github.com/PyTorchLightning/pytorch-lightning/pull/12114)),Co-authored-by: Jirka jirka.borovec@seznam.cz,1
2789,Run on_validation_end only on main process in DDP (#1125),0.6305417,Ensure the existence of DDPPlugin._sync_dir in reconciliate_processes (#8939),,0
2790,Remove dead code in eval loop output tracking (#8625),0.6673179,Enabled no returns from eval (#2446),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2791,ref: bug fix with logging val epoch end + monitor (#3812),0.98810184,bug fix with logging val epoch end + monitor (#3812),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
2792,0.9.0 readme (#3075),0.5641761,"device parser (#3400, #3405)",,0
2793,add more issues types (#14174),0.6084902,Fixing critical bugs in newly added hooks and hparams assignment.,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
2794,Fix global step update when the epoch is skipped (#7677),0.6201732,Changed progress bar epoch counting to start from 0 (#3061),,0
2795,[2/2] Remove outputs from evaluation epoch end hooks (#7338),0.76877844,Removed evaluation loop legacy returns for *_epoch_end hooks (#6973),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka jirka.borovec@seznam.cz,1
2796,ignore types in files (#3409),0.5026258,remove _evaluate fx (#3197),Co-authored-by: Aki Nitta nitta@akihironitta.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2797,Move logger and profiler finalization to trainer's teardown (#8685),0.70507646,"trainer.{logged,progress_bar,callback}_metrics are now updated on-demand (#7882)",,1
2798,tensorboarX to tensorboardX (#136),0.9058635,Switch from tensorboard to tensorboardx in TensorBoardLogger (#15728),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
2799,Fix commands and API test (#14947),0.6479282,- fixed all the .test() calls,,0
2800,Add tests for functions in utilities/data.py (#8785),0.6825162,python script.py test,,0
2801,Revert #16401 and user proper CSVLogger (#16405),0.5953989,refactored dataloader process hook (#3139),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka jirka.borovec@seznam.cz,0
2802,"Removed old eval logic, added eval tests",0.58610773,refactored inner eval loop (#3141),,0
2803,Add Accelerator.is_available() interface requirement (#11797),0.6741246,"Custom Accelerator implementations must now implement two new abstract methods: is_available() (#11797) and auto_device_count() (#10222). The latter determines how many devices get used by default when specifying Trainer(accelerator=..., devices=""auto"").",,0
2804,Deprecate TrainerCallbackHookMixin (#11148),0.7914224,Removed the deprecated TrainerLoggingMixin class (#8609),"A small typo in one of the code blocks. ""self.encoded"" should be replaced by ""self.encoder""",1
2805,Encapsulate extracting reference model within the plugin to allow custom wrapper logic to live within the plugin/accelerators,0.75714236,We introduced a new plugin class for wrapping layers of a model with synchronization logic for multiprocessing.,,1
2806,testing new speed (#1587),0.6164869,DeepSpeed Stage 1,,0
2807,Remove redundant special case for disabling the progress bar on TPU (#11061),0.73915654,Deprecated ProgressBar callback in favor of TQDMProgressBar (#10134),,1
2808,Only enable pulse for infinite datasets (#10305),0.5328866,"Refactor dataloading, supports infinite dataloader (#955)",,0
2809,ref: remove inner train loop 1/n (#3397),0.69633365,"train loop refactor - moving train loop to own object (#3310, #3312, #3313, #3314)",Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
2810,Add docstring to HyperparametersMixin (#9253),0.66743904,Flattening Wandb Hyperparameters (#2459),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Danielle Pintz 38207072+daniellepintz@users.noreply.github.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
2811,Fix mypy typing errors in pytorch_lightning/strategies/tpu_spawn.py (#13813),0.7067895,+ # pytorch_lightning==1.7.0,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
2812,Allow log_momentum for adaptive optimizers (#5333),0.84187514,Allowed log_momentum for adaptive optimizers in LearningRateMonitor (#5333),,1
2813,Fix TPU cleaning job (#9301),0.7249409,"Cleaning (#5948, #5949, #5950)",Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
2814,Override broadcast_object_list for torch<1.8 (#7592),0.5969089,Disable torch.inference_mode with torch.compile in PyTorch 2.0 (#17215),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: Jirka jirka.borovec@seznam.cz,0
2815,[App][CI] Fix psutil requirement CI (#14413),0.58537656,The psutil package is now required for CPU monitoring (#17010),,0
2816,Sharded DDP Docs (#4920),0.68197227,DDP2 implementation (inspired by parlai and @stephenroller).,,0
2817,trigger,0.5094876,2. Create the Tuner,,0
2818,Cleaning requirement + git fix (#14863),0.6278435,"Cleaning (#5948, #5949, #5950)",Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Jirka jirka.borovec@seznam.cz,0
2819,Resolve workers being forcelly deleted with persistent_workers=True (#10434),0.49806252,Resolved a bug where the work statuses will grow quickly and be duplicated (#13970),,0
2820,aggregate multiple helper scripts to single CLI (#11147),0.58096516,Introducing CLI commands for apps (#13602)!,Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
2821,Remove the deprecated agg_and_log_metrics (#14840),0.7838342,- Removed the deprecated `Logger.agg_and_log_metrics` hook in favour of `Logger.log_metrics` and the `agg_key_funcs` and `agg_default_func` arguments. ([#14840](https://github.com/Lightning-AI/lightning/pull/14840)),Co-authored-by: Aki Nitta nitta@akihironitta.com Co-authored-by: Jirka jirka.borovec@seznam.cz,1
2822,Versioning of last checkpoins (#12902),0.6434138,Early stopping checks on_validation_end (#1458),,0
2823,Allow to deactivate GPU memory logging in Trainer (#190),0.736785,Changed min-max GPU memory to be on their own plots (#1358),,1
2824,Remove unnecessary use of comprehension (#8471),0.5203083,"Simplified ""should run validation"" logic (#7682)",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2825,update changelog after 1.3.8 patch release (#8239),0.6901365,Full Changelog: https://github.com/Lightning-AI/lightning/compare/1.8.0...1.8.0.post1,,0
2826,replace MD template with Yaml - refactor (#15907),0.66682947,Replace mata_tags.csv with hparams.yaml (#1271),,0
2827,Fix typo in progress bar docs (#1680),0.61927646,Deprecated ProgressBar callback in favor of TQDMProgressBar (#10134),,0
2828,missing change (#1591),0.5647421,Replaced _DataModuleWrapper with __new__ (#7289),,0
2829,tests: drop slow flag (#16692),0.6522045,Disabled val and test shuffling (#1600),  empty commit   Specify apex installation target directory   pip install --user ,0
2830,docs: unify PL ref in changelog (#17032),0.65705353,Full Changelog,,0
2831,Fix batch_outputs with optimizer frequencies (#3229),0.75680757,Working with multiple optimizers (#16539),Pin coverage<6.3,1
2832,Adding tests for legacy checkpoints - 1.8.x (#17374),0.6500988,Resuming from checkpoints (#16167),,0
2833,Drop all result docs. Make the separation between flow and logging clear (#3744),0.7068708,- This means dataflow and logging have been decoupled,,1
2834,New skip conditions for unpickle-patching tests (#15329),0.6052269,Enabling val/test loop disabling (#2692),,0
2835,added multiple outputs to LightningTestModel,0.72387683,LightningCLI additions:,,1
2836,Add auto_insert_metric_name to ModelCheckpoint docstring. (#8310),0.67206526,Changed ModelCheckpoint version suffixes to start at 1 (#5008),,0
2837,Resultdocs (#2793),0.6255231,Completely overhaul the Result object in favor of ResultMetric (#7882),,0
2838,Fixes #2551 (#3858),0.62346464,Resolve bug with Finetuning (#5744),,0
2839,[CLI] Fix bug that forces overriding configure_optimizers (#11672),0.8120121,- Fixed bug that forced overriding `configure_optimizers` with the CLI ([#11672](https://github.com/PyTorchLightning/pytorch-lightning/pull/11672)),,1
2840,Validate SRUN variables when launching in SLURM (#15011),0.67944306,- Added a sanity check that scripts are executed with the `srun` command in SLURM and that environment variables are not conflicting ([#15011](https://github.com/Lightning-AI/lightning/pull/15011)),,0
2841,Remove fp16 restriction in the docstring for DeepSpeedStrategy (#13919),0.7411489,Removed DeepSpeed FP16 Exception as FP32 is now supported (#8462),,1
2842,:hammer: minor refactor in trainer. (#4801),0.67932945,adding Trainer.tune() (#3293),Co-authored-by: rohitgr7 rohitgr1998@gmail.com,0
2843,ref: refactored inner eval loop (#3141),0.96752524,refactored inner eval loop (#3141),,1
2844,Create CODEOWNERS,0.49127638,core decorator data_loader,,0
2845,[Metrics] AUROC error on multilabel + improved testing (#3350),0.63950765,Deprecated reorder parameter of the auc metric (#4237),,0
2846,add contrib questions (#5691),0.51481646,@rohitgr7 @tchaton ,,0
2847,Fix: freeze Jinja2 version (#12444),0.6248365,Setup: added requirement freeze for the next major version (#14480),,0
2848,Update docs structure,0.7320273,Docs improvements,,1
2849,Fixes #2972 #2946 (#2986),0.5981729,Resolve bug with Finetuning (#5744),,0
2850,add parity test for sync batchnorm (#12021),0.62593967,Allow metrics logged together with hparams (#1630),,0
2851,Merge pull request #4856 from PyTorchLightning/feature/plug,0.6738069,- Removed `configure_sync_batchnorm` from `ParallelStrategy` and all other strategies that inherit from it ([#11754](https://github.com/PyTorchLightning/pytorch-lightning/pull/11754)),Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,0
2852,loop customization docs  (#9609),0.7968534,Loop customization improvements,,1
2853,Add functools.wraps support for is_overridden (#8296),0.5989325,- Added support for auto wrapping for `DDPFullyShardedNativeStrategy` ([#14252](https://github.com/Lightning-AI/lightning/pull/14252)),,0
2854,yapf examples (#5709),0.46413553,FairScale deprecation (in favor of PyTorch's FSDP implementation) (#16353),Co-authored-by: Aki Nitta nitta@akihironitta.com,0
2855,add CircleCI,0.4833988,"adding compute environments (#3837, [#3842)",,0
2856,refactor trainer checks (#1651),0.7949875,"Refactor RunningStage and TrainerState usage (#4945, #7173)",,1
2857,Fix TPU tests (#16628),0.7743043,Updated logic for checking TPUs availability (#6767), Reset on_step flag to True in log_grad_norm updated change log  Co-authored-by: Aki Nitta nitta@akihironitta.com,1
2858,fix qconfig import for pytorch 1.10 (#9899),0.6611612,| Import pytorch_lightning.callbacks.base.Callback                                                           | 1.9             | pytorch_lightning.callbacks.callback.Callback |,,0
2859,Fix torchelastic detection with non-distributed installations (#13142),0.6200509,Removed dependency on torchvision (#797),,0
2860,[fix] Add barrier to accelerator's teardown (#6814),0.7182709,Refactored accelerator backends:, update tests for v2 Update Pass devices to kwargs add accelerator to kwargs Fix testing with cpu on GPU env  Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: Aki Nitta nitta@akihironitta.com,1
2861,skip files in coverage (#3944),0.5234971,Split callbacks in multiple files (#849),,0
2862,SequentialMode and dataloader_iter improvements (#16784),0.7253215,Make the _LiteDataLoader an iterator and add supports for custom dataloader (#10279),Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Aki Nitta nitta@akihironitta.com,1
2863,[App] Add configure_layout method for works (#15926),0.68439424,Expose RunWorkExecutor to the work and provides default ones for the MultiNode Component (#15561),,0
2864,Use global_step while restoring logging step for old checkpoints (#13645),0.8632519,- Used `global_step` while restoring logging step for old checkpoints ([#13645](https://github.com/Lightning-AI/lightning/pull/13645)),,1
2865,fix PyPI releasing (#5605),0.7582736,"Removed PyTorch 1.6 support (#10367, #10738)",,1
2866,Clean up CODEOWNERS for PL and Lite (#14942),0.563238,"- Deprecated the internal `pl.core.mixins.DeviceDtypeModuleMixin` class ([#14511](https://github.com/Lightning-AI/lightning/pull/14511), [#14548](https://github.com/Lightning-AI/lightning/pull/14548))",,0
2867,changelog: fix links & section (#16396),0.63719153,Full Changelog, Set Loop.restarting recursively Docs CHANGELOG Update pytorch_lightning/loops/epoch/training_epoch_loop.py Co-authored-by: Aki Nitta nitta@akihironitta.com,0
2868,Refactor: clean trainer device & distrib getters (#5300),0.94891214,Refactor: clean trainer device & distributed getters (#5300),,1
2869,update docs for ModelCheckpoint save_last (#12332),0.7244614,Changed defaults of save_top_k and save_last to None in ModelCheckpoint (#3680),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
2870,Update Error message for ProfileConnector (#7204),0.7147568,Removed ProfilerConnector (#7654),,1
2871,release v0.111,0.7830206,"    version=""0.0.1"",",,1
2872,Improved Deepspeed Imports (#13223),0.7058896,"Implemented DeepSpeedPlugin._setup_model_and_optimizers (#10009, #10064)",,1
2873,Future 1/n: package in src/ folder (#13293),0.548047,Enabled ls and cp (download) at project level (#16622),,0
2874,Fix auto scaling mode when calling tune method on trainer. (#7321),0.70498097,trainer.tune() now returns the tuning result (#7258),,1
2875,Supporting Adding DDP Communication Hooks (#6736),0.726292,DDP custom implementation support (override these hooks):,,1
2876,Update docstrings for backward methods (#13886),0.58690566,Docs improvements,,0
2877,"Add Trainer(strategy=""bagua"") (#11146)",0.78526896,adding Trainer.tune() (#3293),Co-authored-by: rohitgr7 rohitgr1998@gmail.com,1
2878,Move data fetcher ownership to the loops (#11621),0.73492026,- Moved ownership of the data fetchers from the DataConnector to the Loops ([#11621](https://github.com/PyTorchLightning/pytorch-lightning/pull/11621)),,1
2879,save last model after saving top_k when save_last=True (#2881),0.8282528,Changed defaults of save_top_k and save_last to None in ModelCheckpoint (#3680),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
2880,docs: enable syntax highlight (#109),0.5123583,Docs,,0
2881,Self-review of the recent Trainer changes (#14916),0.776324,New Trainer Arguments: Strategy and Devices,Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
2882,Make gradients available for all_gather on TPU (#15003),0.5989176,Enabled manual optimization for TPUs (#8458),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
2883,[App] Resolve some bugs from the Training Studio scaling (#16114),0.637704,Trainer.fit now raises an error when using manual optimization with unsupported features such as gradient_clip_val or accumulate_grad_batches (#7788),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
2884,"[TPU] Do not delete jobs with ""keepalive"" in the name  (#17411)",0.5364039,Removed collisions with logger versions by tying it to job id.,Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Roger Shieh sh.rog@protonmail.ch Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
2885,allow using apex with any PT version (#2865),0.5648775,Allow user to select individual TPU core to train on (#1729),,0
2886,Mark the connectors as protected (#17008),0.71084523,"- Marked the `{Accelerator,Signal,Callback,Checkpoint,Data,Logger}Connector` classes as protected ([#17008](https://github.com/Lightning-AI/lightning/pull/17008))",Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com,1
2887,DDP sampler (#1513),0.69383955,DDP Debugging Improvements,,0
2888,cleaning up demos (#313),0.68076247,"Cleaning (#5948, #5949, #5950)",Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2889,Require neptune 1.0 (#16888),0.5169579,Initial plugin server (#16523),,0
2890,Tpu save (#4309),0.61413395,Enabled manual optimization for TPUs (#8458),,0
2891,Rename TPUSpawnPlugin to TPUSpawnStrategy (#11190),0.8077436,    * Renamed the `TPUSpawnPlugin` to `TPUSpawnStrategy` ([#11190](https://github.com/PyTorchLightning/pytorch-lightning/pull/11190)),Update mergify team name,1
2892,fix info message when max training time reached (#7780),0.67196804,Disabled training when limit_train_batches=0 (#4371),,0
2893,Mnodes (#5020),0.62981284,(#16002),,0
2894,Learning rate stepping option (#941),0.7769898,Learning Rate Finder (#13802),,1
2895,Add doc strings to tensorboard logger class (#9093),0.71669614,Changed the default logger to TensorBoardLogger (#609),,1
2896,name typo (#5612),0.53908473,Rename failed -> error in tables (#15608),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2897,test dockers & add AMP in pt-1.6 (#1584),0.6207138,enable_pl_optimizer=False by default to temporarily fix AMP issues (#5163),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
2898,Refactor deterministic and benchmark logic (#16653),0.69706345,Determinism,,0
2899,replace obj.copy() with copy.copy(obj) (#701),0.50416255,"deprecated results obj, added support for simpler comms (#3681)",,0
2900,ci: print existing candidates (#16077),0.50136673,[1.7.5] - 2022-09-06,,0
2901,fix: e2e with short form after signup (#14689),0.48805216,| Enum RunningStage.TUNING                                    | 1.10             | No longer supported         |,,0
2902,MAINTAINER has been deprecated (#7683),0.785857,Removed deprecated: (#2760),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
2903,Remove the deprecated code in pl.utilities.optimizer (#16439),0.7625574,- Removed the deprecated code in:,,1
2904,E2E fix for custom base image (#14468),0.5303467,Refactored EpochResultStore (#5522),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: thomas chaton thomas@grid.ai,0
2905,Merge pull request #42 from williamFalcon/warning,0.52533126,"merge backends (#3476, #3477, #3478, #3480, #3482)",,0
2906,Ensure imports are not required explicitly for type casting,0.54279876,import argparse,,0
2907,Allow force run app on cloud if loading locally errors (#15019),0.60466444,- Added support to start lightning app on cloud without needing to install dependencies locally ([#15019](https://github.com/Lightning-AI/lightning/pull/15019),,0
2908,Minor BYOT Follow-up (#17076),0.5828315,(#16002),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
2909,Add base code,0.6368669,A base Checkpoint class for extra customization,Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
2910,Enable purely iteration-based training (#5726),0.8990619,Extended support for purely iteration-based training (#5726),,1
2911,Updating docs and error message: half precision not available on CPU (#7384),0.6588984,Default to precision=bf16 on CPU when precision=16 is passed (#10033),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Aki Nitta nitta@akihironitta.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2912,"Fix logger, tensorboard (#610)",0.8171023,Changed the default logger to TensorBoardLogger (#609),Co-authored-by: Mauricio Villegas mauricio_ville@yahoo.com,1
2913,Add support for local connect terminal context (#15241),0.61438644,- Added Lightning CLI Connection to be terminal session instead of global ([#15241](https://github.com/Lightning-AI/lightning/pull/15241),,0
2914,shutdown workers on failure (#10463),0.43451554,- Removed deadlock detection / process reconciliation (`PL_RECONCILE_PROCESS=1`) ([#16204](https://github.com/Lightning-AI/lightning/pull/16204)),Co-authored-by: rohitgr7 rohitgr1998@gmail.com,0
2915,Unpin CUDA docker image for GPU CI (#12373),0.6482634,"- `accelerator=""gpu""` now automatically selects an available GPU backend (CUDA and MPS currently) ([#13642](https://github.com/Lightning-AI/lightning/pull/13642))",,0
2916,release v0.3.2,0.78417647,0.4.0,Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: thomas chaton thomas@grid.ai,1
2917,Add parity test for simple RNN (#1351),0.53136754,"nb_test_batches to num_test_batches,",,0
2918,resolve test,0.5967784,test_percent_check in favour of limit_test_batches,,0
2919,Remove metrics & cluster_environments from CODEOWNERS (#9376),0.7197964,Remove MetricsHolder (#7909),Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,1
2920,Removed process_position argument from Trainer Class (#13071),0.7425005,Removed the Trainer(move_metrics_to_cpu=True) argument (#16358),Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
2921,added sample input for summary,0.5665788,Overview,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
2922,fixing examples (#6600),0.66533214,Resolve bug with Finetuning (#5744),,0
2923,Replace _TORCH_GREATER_EQUAL_DEV_1_10 with _TORCH_GREATER_EQUAL_1_10 (#10240),0.6199756,Disable torch.inference_mode with torch.compile in PyTorch 2.0 (#17215),,0
2924,[App] Add env variables to desactivate pull and push of the App State (#15367),0.4954812,- Added a layout endpoint to the Rest API and enable to disable pulling or pushing to the state ([#15367](https://github.com/Lightning-AI/lightning/pull/15367),,0
2925,removed checkpoint save_function option,0.7426071,Removed the deprecated save_function property in ModelCheckpoint (#8680),,1
2926,Fault Tolerant: Add support for fault tolerant dataloader validator (#10465),0.75993925,- Fault Tolerant Manual,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
2927,Disable XSRF protection in StreamlitFrontend to support upload in localhost (#15684),1.0,Disable XSRF protection in StreamlitFrontend to support upload in localhost (#15684),,1
2928,Remove deprecated dataloader_idx argument from on_train_batch_start/end callback hooks (#12769),0.7389661,"- Removed deprecated `dataloader_idx` argument from `on_train_batch_start/end` hooks `Callback` and `LightningModule` ([#12769](https://github.com/Lightning-AI/lightning/pull/12769), [#12977](https://github.com/Lightning-AI/lightning/pull/12977))",,1
2929,Added optimizer_idx to backward call (#733),0.7228441,"    optimizer_idx,",,1
2930,Fix torch bfloat import version (#9089),0.7326559,import torch,,1
2931,ensure reset method works in notebooks (#1093),0.5898391,Use completed over processed in reset_on_restart (#9656),,0
2932,rename flow _exit (#16378),0.6465675,Changed the flow.flows to be recursive wont to align the behavior with the flow.works (#15466),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
2933,added amp level option,0.7026173,Trainer now raises an exception when requesting amp_level with native amp_backend (#9755),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
2934,clean up docs around loggers (#304),0.7563913,Cleaning up stale logger tests (#3490),,1
2935,Invoke parent DDP configuration for torch>1.10.2 (#12912),0.7065801,separate torchelastic from DDP (#3810),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
2936,[Docs] Fix README.md in lightning/examples/pl_basics (#13380),0.6654296,Improved the error message when the LightningWork is missing the run method (#14759),Update CODEOWNERS for pl/strategies,0
2937,Remove TODO,0.78543735,Removed,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
2938,Remove unused Lite code (#15000),0.6456338,Removed deprecated code in pytorch_lightning.utilities.meta (#16038),,0
2939,Avoid partial for apply to collection (#8529),0.51777655,Allow dataloaders without sampler field present (#1907),,0
2940,[CLI] adjust command description (#14130),0.6738368,Introducing CLI commands for apps (#13602)!,Co-authored-by: Kaushik B kaushikbokka@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
2941,update codecov,0.51775587,"- Fixed security vulnerability ""CWE-94: Improper Control of Generation of Code (Code Injection)"" ([#12212](https://github.com/PyTorchLightning/pytorch-lightning/pull/12212))",,0
2942,docs: fix links in fabric (#17611),0.59148467,fabric.launch()," Revert ""[CI] Comment flaky tests (#10084)""  This reverts commit ed9802643c5485a0f07d8376009410ae76076cc4.",0
2943,Fix hang in DDP HPC accelerators (#5157),0.98869306,Fix hanging in DDP HPC accelerators (#5157),,1
2944,Enables reload of dataloaders on every n epochs from every epoch (#5043),0.6833677,"Refactor dataloading, supports infinite dataloader (#955)",Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
2945,Default benchmark based on deterministic flag (#11944),0.658323,- Make `benchmark` flag optional and set its value based on the deterministic flag ([#11944](https://github.com/PyTorchLightning/pytorch-lightning/pull/11944)),,0
2946,refactor 3/n (#2709),0.6848786,Refactor Model backward (#2276),,0
2947,setup: set default metadata (#13571),0.5542304,Did not interfere with a default sampler (#1318),,0
2948,Reset the current progress tracking state during double evaluation (#11119),0.74364763,Progress tracking,,1
2949,add contributing guide to readme,0.5000419,Contributors,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
2950,Add ddp_notebook alias for ddp_fork (#13744),0.6283942,DDP custom implementation support (override these hooks):,,0
2951,ref: add .log to lightning module (1/n) (#3686),0.9289509,"add .log to lightning module (#3686, #3699, #3701, #3704, #3715)",,1
2952,rename old Trainer.train_loop -> Trainer.fit_loop  (#8025),0.877634,Deprecated the Trainer.train_loop property in favor of Trainer.fit_loop (#8025),,1
2953,Support arbitrary Optimizables as optimizers (#16189),0.86731863,"            optimizer,",Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
2954,Renamed the DDP2Plugin to DDP2Strategy (#11185),0.82363343,    * Renamed the `DDP2Plugin` to `DDP2Strategy` ([#11185](https://github.com/PyTorchLightning/pytorch-lightning/pull/11185)),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
2955,Set find_unused_parameters=False as the default (#16611),0.80232596,Changed the default of find_unused_parameters to False in DDP (#5185),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
2956,Make monitor required arg of EarlyStopping callback (#10328),0.79653597,Deprecated default value of monitor argument in EarlyStopping callback to enforce monitor as a required argument (#7907),,1
2957,Replace HPU with external implementation (#17067),0.67754906,Moveed HPU broadcast override to the HPU strategy file (#17011),,0
2958,Drop support for PyTorch 1.10 (#16492),0.9731355,Drop PyTorch 1.9 support (#15347),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
2959,Merge pull request #64 from williamFalcon/cov,0.48463523,"merge backends (#3476, #3477, #3478, #3480, #3482)",Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
2960,dev release 1,0.6648659,"    version=""0.0.1"",",Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: thomas chaton thomas@grid.ai,0
2961,update building latest XLA 1.8 (#7359),0.5329242,xla_device_utils >> xla_device,Co-authored-by: rohitgr7 rohitgr1998@gmail.com,0
2962,adds tensorboard hparams logging test (#2342),0.69283396,Allow logging of metrics together with hparams (#1630),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
2963,"Update docs for Strategy, Accelerator, Plugins (#12582)",0.7815208,Refactored Accelerators and Plugins (#5743),,1
2964,Sync module states during non-fit (#17370),0.60674304,Remove .item which causes sync issues (#1254),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
2965,working on single gpu init speed,0.6416442,Refactor GPUStatsMonitor to improve training speed (#3257),Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
2966,disable auto gpu loading when restoring weights to avoid OOM (#242),0.69615257,"Loads all models on CPU when restoring weights to avoid OOM issues in PyTorch. User now needs to move to GPU manually. However, if using Lightning, lightning will move to correct GPUs automatically.   ",,0
2967,[App] Update app URLs to latest format (#16568),0.9603671,Updated app URLs to the latest format (#16568),,1
2968,[Metrics] Add multiclass auroc (#4236),0.7330342,Deprecated reorder parameter of the auc metric (#4237),,1
2969,Update configs with new GitHub labels (#10532),0.56585574,Full Changelog: https://github.com/Lightning-AI/lightning/compare/1.8.0...1.8.1,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
2970,[HOTFIX] ModelCheckpoint - Don't increase current_epoch and global_step if not trained (#4291),0.6660454,The current_epoch and global_step attributes now get restored irrespective of the Trainer task (#9413),,0
2971,Update setup logic in training type plugins [1 / n] (#9994),0.7602724,The accelerator and training type plugin setup hooks no longer have a model argument (#8536),,1
2972,CI: update signalling (#15887),0.6289147,The preemption/termination signal is now configurable (#14626):,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
2973,Add doc strings for Neptune logger (#9111),0.59280014,Changed to the NeptuneLogger (#16761):,,0
2974,Consolidate state when retrieving sharded state dict in Lite (#10746),0.536916,"def load_state_dict(self, state):",,0
2975,Improve LightningDataModule hook test and fix dataloader_idx argument (#7941),0.77580875,Deprecated on_{train/val/test/predict}_dataloader() from LightningModule and LightningDataModule (#9098),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
2976,Follow E231 [flake8] (#6110),0.6180234,[1.1.8] - 2021-02-08,,0
2977,Document Exceptions in profilers (#6229),0.58037496,Split profilers module (#6261),,0
2978,Remove Accelerator.parallel_device_ids and deprecate Trainer.data_parallel_device_ids (#12072),0.7563275,Decoupled device parsing logic from Accelerator connector to Trainer (#8180),,1
2979,"Warn when self.log(..., logger=True) is called without a logger (#15814)",0.7783735,    self.log(...),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
2980,[2/n] Directly call TrainingTypePlugin APIs instead of going through the Accelerator (#9901),0.83699393,"The Trainer now calls TrainingTypePlugin collective APIs directly instead of going through the Accelerator reference (#9677, #9901)",,1
2981,"[Fault Tolerance] Don't check the len of a dataset, but its instance. (#10432)",0.61693156,Manual Fault-tolerance,Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
2982,fix deprecated warnings (#1048),0.71548414,Silenced some warnings. verified ddp refactors (#3483),  Raname DDPPlugin to DDPStrategy   Change ddp_plugin to ddp_strategy   update changelog   rename occurences in docs   rename more occurrences   fix line too long   more fixes   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
2983,Update quick start guide with latest info (#14880),0.5668932,Set version as today (#13906),,0
2984,Sharded Plugin 3/n: Expose step input to DDP plugin (#4686),0.6439286,DDP custom implementation support (override these hooks):,,0
2985,update docs deploy: stable/latest (#15460),0.6725115,Release LAI docs as stable (#14250),,0
2986,ref: inner train loop (intermediate step) 1/n (#3361),0.78862554,"inner train loop (#3359, #3361, #3362, #3363, #3365, #3366, #3367, #3368, #3369, #3370, #3371, #3372, #3373, #3374, #3375, #3376, #3385, #3388, #3397)",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
2987,Clean up (#2467),0.83133173,"Cleaning (#5948, #5949, #5950)",Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
2988,convert state to tuple explicitly when setting python random state (#9401),0.98411983,Converted state to tuple explicitly when setting Python random state (#9401),,1
2989,Fix self.optimizers() not returning a single LightningOptimizer (#8326),0.8169856,Check LightningOptimizer doesn't delete optimizer hooks (#6305),,1
2990,Revert saving the dataloader and result collection by default (#11842),0.70360863,      return DataLoader(...),  Rename DDPFullyShardedPlugin to DDPFullyShardedStrategy   update fsdp_plugin to fsdp_strategy   update changelog   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
2991,Remove reference to genindex and search in index.rst (#12919),0.53874373,remove _evaluate fx (#3197),"  Add required states for resumed ModelCheckpoint GC   Add backwards compatibility with legacy cktps   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com  Add test to check if attrs are written to ckpt  Note that we do not yet check for proper loading/reinstantiation of ModelCheckpooint based on the ckpt written to disk   Test if attributes are restored properly from ckpt   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  Fix broken test_callbacks_state_fit_ckpt_path  ModelCheckpoint is configured to save after every epoch, but trainer.fit is called with max_steps = 1 Note there may be a better way of doing this, where ModelCheckpoint is called after training_step   Update test_restore.py   Update test_restore.py   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Check that all attributes are restored properly   revert changes, use fix on master   Convert to proper unit test   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Refactor test_mode_checkpoint_saveload_ckpt   First save, then load ckpt.  Instantiate ModelCheckpoint twice.  Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com",0
2992,Fix loop examples after Accelerator API removals (#10514),0.6440538,Refactored Accelerators and Plugins (#5743),,0
2993,[Fix] Ensure we set the eval/train flag correctly on accelerator model  (#6877),0.9818824,Ensure we set the eval/train flag correctly on accelerator model (#6877),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: four4fish 88516121+four4fish@users.noreply.github.com,1
2994,added gan template (#115),0.54174495,Allowing decorate model init with saving hparams inside (#4662),,0
2995,update stale bot (#4205),0.5949441,- Removed deadlock detection / process reconciliation (`PL_RECONCILE_PROCESS=1`) ([#16204](https://github.com/Lightning-AI/lightning/pull/16204)),,0
2996,Better error message when trying to re-initialize CUDA in forked subprocess (#14709),0.7672451,- Added a more descriptive error message when attempting to fork processes with pre-initialized CUDA context ([#14709](https://github.com/Lightning-AI/lightning/pull/14709)),,1
2997,Fix issues when RichProgressBar disabled (#15376),0.71253484,- Changed `MisconfigurationException` to `ModuleNotFoundError` when `rich` isn't available ([#11360](https://github.com/PyTorchLightning/pytorch-lightning/pull/11360)),,1
2998,Fix ModelCheckpoint period (#3630),0.7496167,Replace MisconfigurationException with warning in ModelCheckpoint Callback (#4560),,1
2999,[Refactor] Improve loops API 1/n (#8334),0.6814214,"Refactored evaluation loop interface; added new classes DataLoaderLoop, EvaluationLoop, EvaluationEpochLoop (#7990, #8077)",,0
3000,Sync working directory with all processes when using Hydra (#5629),0.83990955,Re-introduced fix for Hydra directory sync with multiple process (#5993),,1
3001,Logging Non-matching keys when loading from checkpoint in non-strict … (#8152),0.63595605,Saved checkpoints will no longer use the type of a Callback as the key to avoid issues with unpickling (#6886),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
3002,Resolve increased time. (#14074),0.6096792,"To resolve fault-tolerance issues, we changed where the current epoch value gets increased.",,0
3003,Add typing for trainer.logger (#11114),0.71552205,adding Trainer.tune() (#3293),,1
3004,Move mixin to core (#8396),0.67919886,Moved DeviceDtypeModuleMixin and HyperparametersMixin mixin to core (#8396),,0
3005,Add log_device_info to Trainer (#8079),0.74502,"trainer.{logged,progress_bar,callback}_metrics are now updated on-demand (#7882)",,1
3006,Update TPU Spawn to use root_device instead of LightningModule's device (#11750),0.64738405,  * Deprecated the `XLADeviceUtils.tpu_device_exists` staticmethod in favor of `pytorch_lightning.accelerators.TPUAccelerator.is_available()`,,0
3007,docs: replacement of method type_as in docs to Tensor.to (#15027),0.59281635,Switch from tensorboard to tensorboardx in TensorBoardLogger (#15728),,0
3008,changed lbfgs test min acc,0.5690355,"Refactored training_batch + tests to verify correctness (#2327, #2328)",Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
3009,Add a warning to the detect_anomaly flag. (#17380),0.6851014,warning_utils >> warnings,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
3010,Merge pull request #12 from cinjon/commafix,0.45894393,refactored dataloader process hook (#3139),,0
3011,restrict public interface of training loop (#8024),0.6418699,Restricted public access to several internal functions (#8024),,0
3012,temporary drop metrics tests while speeding them up (#4071),0.6738715,Drop duplicate metrics (#5014),,0
3013,"Update tqdm requirement from <4.65.0,>=4.57.0 to >=4.57.0,<4.66.0 in /requirements (#17630)",0.6110995,Updated logic for checking TPUs availability (#6767),,0
3014,typing: fix App's core API - Flow (#16947),0.57671803,app = L.LightningApp(Flow()),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
3015,Create the loss accumulator directly on the device (#12430),0.5737445,    devices=1,,0
3016,Remove is_global_zero check in training_epoch_loop (#12134),0.695633,Made training_epoch_end behave like validation_epoch_end (#1357),,0
3017,Fix backwards compatibility for logging (#799),0.6985094,Un-balanced logging properly supported (#5119),,0
3018,Update init_ddp_connection's name and log (#10295),0.67738605,callback system and init DDP (#3836),Add security contact,0
3019,Add check groups for specific workflow changes (#15503),0.5974997,System customization syncing for jobs run (#16932),,0
3020,update to _has_prepared_data (#9411),0.6446098,"- Removed deprecated `has_prepared_data`, `has_setup_fit`, `has_setup_validate`, `has_setup_test`, `has_setup_predict`, `has_teardown_fit`, `has_teardown_validate`, `has_teardown_test` and `has_teardown_predict` datamodule lifecycle properties  ([#10350](https://github.com/PyTorchLightning/pytorch-lightning/pull/10350))",,0
3021,Update configure_optimizers docs (#7390),0.7163416,Refactored optimizer (#4658),,1
3022,fix import in Tensorboard example (#193),0.76300144,Improved the error message for installing tensorboard or tensorboardx (#17053),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
3023,update version for Fabric CLI (#16556),0.5590219,"Renamed the class LightningLite to Fabric (#15932, #15938)",Co-authored-by: tchaton thomas@grid.ai Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
3024,feat(wandb): add sync_step (#5351),0.6373291,Removed the deprecated sync_step argument from WandbLogger (#8763),,0
3025,update (#6403),0.61963403,0.4.0,,0
3026,Switch from tensorboard to tensorboardx in logger (#15728),0.9340613,Switch from tensorboard to tensorboardx in TensorBoardLogger (#15728),,1
3027,Update tests/accelerators/*.py to use devices instead of gpus or ipus (#11817),0.63091564,"The new devices argument is now agnostic to all accelerators, but the previous arguments gpus, tpu_cores, ipus are still available and work the same as before. In addition, it is now also possible to set devices=""auto"" or accelerator=""auto"" to select the best accelerator available on the hardware.",,0
3028,pytorch 1.6 (#2745),0.81776583,PyTorch 1.5  support,,1
3029,Fix result transfer in multiprocessing launcher on multi-node (#15567),0.6443196,"Disabled strict loading in multiprocessing launcher (""ddp_spawn"", etc.) when loading weights back into the main process (#16365)","  remove training_step() from accelerator   remove test, val, predict step   move   wip   accelerator references   cpu training   rename occurrences in tests   update tests   pull from adrian's commit   fix changelog merge pro   fix accelerator_connector and other updates   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   fix doc build and some mypy   fix lite   fix gpu setup environment   support customized ttp and accelerator   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   fix tpu error check   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   fix precision_plugin initialization to recognisze cusomized plugin   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Update bug_report_model.py   Update accelerator_connector.py   update changelog   allow shorthand typing references to pl.Accelerator   rename helper method and add docstring   fix typing   Update pytorch_lightning/trainer/connectors/accelerator_connector.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update tests/accelerators/test_accelerator_connector.py  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update tests/accelerators/test_cpu.py  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci   fix pre commit complaint   update typing to long ugly path   spacing in flow diagram   remove todo comments   docformatter   Update pytorch_lightning/plugins/training_type/training_type_plugin.py   revert test changes   improve custom plugin examples   remove redundant call to ttp attribute   it is no longer a property  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci  Apply suggestions from code review  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com",0
3030,Enable Probot CheckGroup v3 (#15622),0.59657997,Configuration Validator (#9779),,0
3031,Tests/install pkg (#2835),0.6815897,Dropped official support/testing for PyTorch <1.6 (#8288),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
3032,Update removal version of argparse_utils.py from 1.4 to 2.0 for backwards compatibility (#9162),0.67240965,Removed deprecated pytorch_lighting.utilities.argparse_utils module (#9166),,0
3033,Fix unintended zero-only target in iou example (#4262),0.6368689,Changed IoU score behavior for classes absent in target and pred (#3098),,0
3034,Save test predictions on multiple GPUs (#2926),0.7109252,GPU training (#2704),Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: thomas chaton thomas@grid.ai,1
3035,Bump actions/cache from 2 to 3 (#13619),0.5327269,Update the logic to check for accumulation steps with deepspeed (#9826),,0
3036,"Update docker requirement from <6.0.2,>=5.0.0 to >=5.0.0,<6.1.2 in /requirements (#17585)",0.531029,"    version=""0.0.1"",",,0
3037,temporarily suspend all mergify rules (#5112),0.5103651,Disabled val and test shuffling (#1600),,0
3038,DP device fix (#3196),0.68881744,"refactor DDP backend (#3204, #3207, #3208, #3209, #3210)",Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
3039,Ampt (#2572),0.6773888,        :param amp:,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
3040,missing tests default_root_dir=tmpdir (#6314),0.5773631,    root_node = '127.0.0.2',,0
3041,[App] Support running on multiple clusters (#16016),0.92669433,Support running on multiple clusters (#16016),,1
3042,Typing: callback base (#5919),0.6529129,Callback hooks,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Sean Naren sean@grid.ai Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
3043,Add a png image for dark mode support (#9518),0.48895437,- Update `RichProgressBarTheme` styles after detecting light theme on colab ([#10993](https://github.com/PyTorchLightning/pytorch-lightning/pull/10993)),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com,0
3044,Fix PyTorch versions in Lite CI (#15338),0.750424,Enable PyTorch 1.7 compatibility (#3541),Co-authored-by: Aki Nitta nitta@akihironitta.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
3045,"Update fsspec[http] requirement from !=2021.06.0,<=2022.2.0,>=2021.05.0 to >=2021.05.0,!=2021.06.0,<2022.6.0 in /requirements (#13366)",0.53738624,Updated model_checkpoint's to_yaml to use fsspec open (#3801),,0
3046,Added Wandb entity attribute (#783),0.59174335,"The WandbLogger.name property no longer returns the name of the experiment, and instead returns the project's name (#14145)",Co-authored-by: four4fish 88516121+four4fish@users.noreply.github.com,0
3047,"""sequential"" mode for CombinedLoader (#16743)",0.6450819,"combined_loader = CombinedLoader(iterables, mode=""min_size"")",,0
3048,"Update inquirer requirement from <=3.1.2,>=2.10.0 to >=2.10.0,<=3.1.3 in /requirements (#17258)",0.5127234,[1.3.0] - 2021-05-06,,0
3049,Use typing forward references (#7770),0.55172896,Updated references to self.forward() to instead use the __call__ interface. (#1211),,0
3050,Add typing to callbacks (#10001),0.70266604,Changed callbacks argument in Trainer to allow Callback input (#5446),,1
3051,Merge pull request #9438 from PyTorchLightning/feature/neptune-code-owners,0.67589015,pytorch_lightning.loggers.neptune.NeptuneLogger is now consistent with the new neptune-client API; the old neptune-client API is supported by NeptuneClient from the neptune-contrib repo (#6867),"  first commit   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update pr #   test filterwarnings   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   add a todo comment   updates   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  `` Update pytorch_lightning/core/saving.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  `` Update pytorch_lightning/core/saving.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci  model --> LightningModule Update pytorch_lightning/core/saving.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  model --> LightningModule Update pytorch_lightning/core/saving.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com",0
3052,Update isort config (#5142),0.5942786,This release fixes that core issue,,0
3053,fix LightningTemplateModel (#1577),0.8348203,Updated LightningTemplateModel to look more like Colab example (#1577),,1
3054,added travis,0.43807095,"    },",,0
3055,app/tests: skip instead of fail (#17461),0.7136371,Updated app testing (#16000),,1
3056,v1.2.0rc1 (#5946),0.6613202,"    version=""0.0.1"",",Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
3057,fix MNIST download (#1044),0.5629398,- Fixed a bug where the upload files endpoint would raise an error when running locally ([#14924](https://github.com/Lightning-AI/lightning/pull/14924)),,0
3058,Use base version check before calling _register_load_state_dict_pre_hook (#17030),0.676625,Called on_load_checkpoint before loading state_dict (#4057),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
3059,Refine the pytorch profiler (#11268),0.7647505,PyTorch,,1
3060,Fix strict torch_xla availability check (#16476),0.63956964,- Fixed issue where the CLI fails with certain torch objects ([#13153](https://github.com/Lightning-AI/lightning/pull/13153)),,0
3061,drop usage of deprecated distributed_backend (#5009),0.6976757,Distributed Backend,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,0
3062,tests for multiple optimizers and dataloader combinations (#3937),0.80973935,Working with multiple optimizers (#16539),,1
3063,dp tests,0.59953475,DDP Debugging Improvements,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
3064,fixes ddp bugs (#1819),0.6701191,"We had a few (subtle) bugs that affected DDP and a few key things in 0.7.2 so we released 0.7.3 to fix them because they are critical for DDP. sorry about that! still, no API changes, but please do skip straight to  0.7.3 upgrade for those fixes",,0
3065,Fix some flaky tests in tuner/lr_finder (#9766),0.7389832,tuner.lr_find(...),,1
3066,added epoch flag back,0.67853343,Deprecated flags: (#2213),,0
3067,ref: decouple apex second attemp part 1/n (#4052),0.61981016,remove _evaluate fx (#3197),,0
3068,Bump version of black to 22.3.0 (#12542),0.57417625,"    version=""0.0.1"",",,0
3069,Fix yielding from iterator in LiteDataLoader (#10304),0.7007342,Make the _LiteDataLoader an iterator and add supports for custom dataloader (#10279),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
3070,Fix typo (#1224),0.58055437,Changed overwrite to True (#16009),,0
3071,Reference trainer properties in docs (#16969),0.6182966,Trainer.num_devices and Trainer.device_ids,Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
3072,Expose scaler in amp plugin (#4737),0.6366397,training AMP scaling refactor (#3135),,0
3073,ref: (2/n) fix no log in epoch end (#3699),0.7383594,bug fix with logging val epoch end + monitor (#3812),,1
3074,Improved the jsonargparse[signatures] availability variable (#13035),0.5378083,Improved error messages in replace_sampler when the DataLoader attributes are not included in the signature or the signature is missing optional arguments (#8519),,0
3075,Update Lightning Lite docs (4/n) (#16246),0.857224,Update the Lightning App docs (#13537),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
3076,Fix signal teardown outside main thread (#11124),0.61711,Connect and Disconnect node (#16700),Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
3077,Support strategy argument being case insensitive (#12528),0.9999998,Support strategy argument being case insensitive (#12528),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
3078,Deprecate LightningModule.use_amp (#12315),0.76952475,Deprecated LightningModule.model_size (#8343),,1
3079,Small code simplification in training_epoch_loop.py (#10146),0.68255615,Extracted ManualOptimization logic from TrainingBatchLoop into its own separate loop class (#9266),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
3080,TPU Spawn Rank & root device Error  (#7074),0.5964228,TPU core selection,,0
3081,CI: yesqa (#8564),0.5226115,"    },",,0
3082,refactor on_gpu handling in checkpoint connector (#7860),0.8184076,Refactor load in checkpoint connector (#4593),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
3083,"Allow disabling default logger, checkpoint_callback, and early_stop_callback (#360)",0.68504053,Callback hooks for loading and saving checkpoints, Update docs.txt Update conf.py Update .readthedocs.yml,0
3084,Improve LightningModule hook tests (#7944),0.74197674,- Added profiling of `LightningDataModule` hooks ([#12971](https://github.com/Lightning-AI/lightning/pull/12971)),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
3085,docs: adding hivemind (#17038),0.6777637,- Hivemind Strategy,Co-authored-by: thomas chaton thomas@grid.ai,0
3086,update old references to filepath in ModelCheckpoint docs (#4339),0.8141097,Deprecated filepath in ModelCheckpoint (#4213),Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk,1
3087,added clarifying comments,0.5989009,NOTE,,0
3088,added lr scheduler test using dev debugger (#3004),0.6464522,"def lr_scheduler_step(self, scheduler, optimizer_idx, metric):","  Update tpu tp share same logic with ttp   run test   Update tpu_spawn.py   debug   Add changelog   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   Update training_type_plugin.py   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  Update training_type_plugin.py  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
3089,Fixed a small issue in the documentation for FeatureExtractorFreezeUnfreeze (#12049),0.6298485,Resolve bug with Finetuning (#5744),  3/n Move Accelerator into strategy - remove model_sharded_context()   update ttp function   update changelog   update changelog   Co-authored-by: ananthsub ananth.subramaniam@gmail.com,0
3090,Add trainer attribute to datamodule (#3749),0.76720417,adding Trainer.tune() (#3293),  Remove precision_plugin pre_dispatch() method   update changelog ,1
3091,[Doc] Lightning Module (#4123),0.7717931,Introduce lightning connect (#14452),,1
3092,[App] Introduce basic auth to Lightning CLI (#16105),0.7834456,CLI Commands for Lightning Apps,Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
3093,Clean docs (#725),0.6434377,Docs improvements,,0
3094,Fix default CloudCompute for flows (#15371),0.73479176,- Fixed a bug with a default CloudCompute for Lightning flows ([#15371](https://github.com/Lightning-AI/lightning/pull/15371)),,1
3095,"Apply dynamo to training_step, validation_step, test_step, predict_step (#15957)",0.67924,"This will compile forward and {training,validation,test,predict}_step",,0
3096,Added support for custom parameters in subclasses of SaveConfigCallback (#14998),0.8207177,- Added support for custom parameters in subclasses of `SaveConfigCallback` ([#14998](https://github.com/Lightning-AI/lightning/pull/14998)),Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
3097,Coverage (#1058),0.6262262,[1.5.5] - 2021-12-07,,0
3098,Update lightning logo (#4490),0.7600893,Update the Lightning App docs (#13537),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
3099,Update test for logging a metric object and state reset (#4825),0.66661525,Changed the behaviour when logging evaluation step metrics to no longer append /epoch_* to the metric name (#7351),,0
3100,pin actions/checkout version to v2 (#1617),0.50969505,Key updates,,0
3101,"Update old ""module_arguments"" and ""hparams"" references in docs (#4417)",0.6490098,Fixing critical bugs in newly added hooks and hparams assignment.,Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
3102,Update CI setup (#13291),0.55773515,The psutil package is now required for CPU monitoring (#17010),,0
3103,Add lightning apps to info_packages of collect_env_details (#13815),0.70564616,- Integrate the `lightning_utilities` package (,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
3104,simplify skip-if tests >> 0/n (#5920),0.66993517,test_percent_check in favour of limit_test_batches,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
3105,nightly release to tests (#3718),0.5561309,[0.6.0] - 2022-09-08,"  fix typing   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
3106,add Stale action (#905),0.5392103,Reset metrics before each task starts (#9410),Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,0
3107,[1/2] Add support for early stopping during training epoch end (#6944),0.7830452,EarlyStopping now runs at the end of the training epoch by default (#8286),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
3108,ci: update doctest for installed packages (#16574),0.6448808,Docs improvements,,0
3109,Call set_epoch for distributed batch samplers (#13396),0.8438009,The loops now call .set_epoch() also on batch samplers if the dataloader has one wrapped in a distributed sampler (#13396),,1
3110,Remove unnecessary use of comprehension (#8149),0.50391793,"Simplified ""should run validation"" logic (#7682)",Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
3111,Mention skipping steps in docs (#4108),0.5936351,Remove unnecessary intermediate layers in Dockerfiles (#5697),  Skip hanging spawn tests   Docstring fix   Add back to TPU spawn ,0
3112,Update requirements.txt (#664),0.5451014,Updated logic for checking TPUs availability (#6767),,0
3113,update CodeFactor badge (#779),0.6003581,Configuration Validator (#9779),,0
3114,Exclude Graveyard from noqa (#15084),0.45979583,Removed,,0
3115,CI/CD: Refactor building docker images (#13576),0.5505165,Refactored EpochResultStore (#5522),,0
3116,Add before_batch_transfer and after_batch_transfer hooks (#3671),0.6438681,"Removed output argument from *_batch_end hooks (#3965, #3966)",,0
3117,Fix off-by-one epoch length (#377),0.67334867,Reset epoch progress with batch size scaler (#13846),,0
3118,Bug/4319 ddp checkpoint (#4323),0.63678974,Refactor load in checkpoint connector (#4593),,0
3119,release update (#1405),0.6881163,This release includes:,,0
3120,Remove unused post_optimizer_step (#9746),0.8170011,"Removed {Accelerator,TrainingTypePlugin,PrecisionPlugin}.post_optimizer_step (#9746)",Co-authored-by: Danielle Pintz 38207072+daniellepintz@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
3121,ref: separate flow vs log tests (#3704),0.6436699,Cleaning up stale logger tests (#3490),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
3122,Remove old testing artifacts (#15052),0.6949446,Deprecated the TestTubeLogger (#9065),"  1/n Integrate Device Specific Accelerator Logic with strategy - move batch_to_device to strategy   add changelog   add model is not none check   Apply suggestions from code review   Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   Update CHANGELOG.md   Update test_datamodules.py   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Update test_hooks.py   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  Update dp.py  Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
3123,formatting tests: 5/5 (#5848),0.52900636,test_percent_check in favour of limit_test_batches,,0
3124,switch agents pool (#6270),0.46381325,# use slurm job id for the port number,,0
3125,Rename the DataParallelPlugin to DataParallelStrategy (#11183),0.7671026,    * Renamed the `DataParallelPlugin` to `DataParallelStrategy` ([#11183](https://github.com/PyTorchLightning/pytorch-lightning/pull/11183)),,1
3126,Make move_metrics_to_cpu work recursively (#6007),0.6054052,Removed the Trainer(move_metrics_to_cpu=True) argument (#16358),,0
3127,Bump json5 from 2.2.1 to 2.2.3 in /src/lightning_app/cli/react-ui-template/ui (#16237),0.6396457,- Changes in `LightningCLI` required for the new major release of jsonargparse v4.0.0 ([#10426](https://github.com/PyTorchLightning/pytorch-lightning/pull/10426)),"  update typing in fairscale   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
3128,Add deprecation warning & test for distributed_backend flag (#8575),0.64971334,Silenced some warnings. verified ddp refactors (#3483),"  fix typing in hparams mixin   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  unused import  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
3129,Document how to run apps with a local version of Lightning on the cloud (#16163),0.8044728,- Added support to start lightning app on cloud without needing to install dependencies locally ([#15019](https://github.com/Lightning-AI/lightning/pull/15019),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
3130,Refactor setup_training and remove test_mode (#5388),0.9851419,Refactored setup_training and remove test_mode (#5388),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
3131,Fix ModelCheckpoint example (#2321),0.7670169,Replace MisconfigurationException with warning in ModelCheckpoint Callback (#4560),,1
3132,Remove pep8speaks (#8392),0.5983573,Removed deprecated: (#2760),,0
3133,"Deprecate agg_key_funcs, agg_default_func, and update_agg_funcs from LightningLoggerBase (#11871)",0.84385294,- Deprecated `agg_key_funcs` and `agg_default_func` parameters from `LightningLoggerBase` ([#11871](https://github.com/PyTorchLightning/pytorch-lightning/pull/11871)),Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
3134,fix missing images on pypi (#1407),0.6574322,"Removed PyTorch 1.6 support (#10367, #10738)",,0
3135,add freeze for development and full range for install (#12994),0.79314876,Setup: added requirement freeze for the next major version (#14480),Co-authored-by: thomas chaton thomas@grid.ai,1
3136,merged,0.50604194,"merge backends (#3476, #3477, #3478, #3480, #3482)",Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
3137,docs: fix link in guide (#17090),0.6234586,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ","  improve typing in pytorch_lightning/lite   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  include lite again  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
3138,fix examples running in DP (#4764),0.5543004,DDP Debugging Improvements,,0
3139,Fix typo in OS name (#15318),0.5794143,Changed overwrite to True (#16009),,0
3140,CI: rename Azure workflow file (#15097),0.58722985,Refactor cloud dispatch and update to new API (#16456),  Improve typing for loops   Free memory ,0
3141,fix internal call to deprecated train_loop (#8434),0.6025365,Removed deprecated early_stop_callback (#3982),Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,0
3142,Restructure Lite examples and add GAN (#16240),0.57218134,    # Let Lite setup your model and optimizer,  Avoid optional instances in Loops   More cleanup ,0
3143,📝Update Readme.md (Code Error Minimal Example) (#3416),0.5849039,Improved exception message if rich version is less than 10.2.2 (#10839),,0
3144,[Doc] Fix on_train_batch_end description (#4330),0.7117512,| on_batch_end               | on_train_batch_end           |,,1
3145,[TEST] Min steps override early stopping (#4283),0.62029994,Changed default value of the max_steps Trainer argument from None to -1 (#9460),,0
3146,Update default filename (#4500),0.6233905,Deprecated Profiler(output_filename) in favor of dirpath and filename (#6621),,0
3147,Implement finalize for WandbLogger (#4341),0.7065988,Removed wandb logger's finalize method (#1193),,1
3148,CI: update poplar sdk version (#12226),0.5400385,This release fixes that core issue,,0
3149,Add documentation for trainer.datamodule and dataloaders of a trainer object (#14600),0.7728972,train_dataset = trainer.train_dataloader.dataset,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
3150,made root note address individually testable,0.49989015,# figure out the root node addr,  Ignore mypy only for failing files   Comment ,0
3151,ref: separate trainer docstrings poc (#3418),0.7045238,"Refactor RunningStage and TrainerState usage (#4945, #7173)",  mypy install deps   fix deps   add examples   fix type errors   fix type error   fix   fix   update pyproject.toml ,1
3152,fixed extra dataloader bug (#1196),0.6836456,"Accessing dataloaders (#16726, #16800)",,0
3153,Handle the case with no queries in GPUStatsMonitor (#9014),0.68796396,Parsing of GPU Argument,,0
3154,Loop Refactor 1/N - Training Loop (#7871),0.8931968,Refactored training loop (#2336),Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,1
3155,Fix formatting,0.52983594,Refactoring,Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
3156,Improve the checkpoint upgrade utility script (#15333),0.6787825,- Checkpoint saving and loading redesign ([#16434](https://github.com/Lightning-AI/lightning/pull/16434)),,0
3157,Add a reference to the Trainer on the LightningDataModule (#3684),0.96512985,reference to the Trainer on the LightningDataModule (#3684),"  use precision type   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",1
3158,Restructure Fabric docs (2/n) (#17126),0.6177677,Learn more about Fabric and what it can do in the new docs!,Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
3159,autoplay (#2968),0.5942526,Removed mode='auto' from EarlyStopping (#6167),,0
3160,Avoid inference_mode with FSDP (#17064),0.66109806,Avoided requiring the FairScale package to use precision with the fsdp native strategy (#14092),,0
3161,Update documentation about configuration files structure (#16956),0.5673407,setup(,,0
3162,Remove unnecessary generator (#8154),0.6037644,- Removed `AcceleratorConnector.num_gpus` property ([#12384](https://github.com/PyTorchLightning/pytorch-lightning/pull/12384)),,0
3163,Make DDP and Horovod batch_size scaling examples explicit (#9813),0.5845511,Tune batch size,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
3164,fix missing PyPI images & CI badges (#853),0.61904067,"Removed PyTorch 1.6 support (#10367, #10738)",,0
3165,[hotfix] ddp + manual_optimisation (#4976),0.73779434,"Implemented {DDPShardedPlugin,DDPShardedSpawnPlugin}._setup_model_and_optimizers (#10028, #10064)",,1
3166,Standalone Lite: Update LightningLite (#14726),0.78541124,Updated compatibility for LightningLite to run with the latest DeepSpeed 0.7.0 (13967),,1
3167,Add missing logging tests (#8195),0.75370955,Cleaning up stale logger tests (#3490),,1
3168,Add typings for evaluation_loop.py and remove some dead code (#7015),0.70268744,Removed pytorch_lightning/trainer/evaluation_loop.py (#8056),,1
3169,update NGC (#10770),0.5943389,Changed to the NeptuneLogger (#16761):,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com,0
3170,fix of issue 600 (#625),0.628551,Resolve bug with Finetuning (#5744),,0
3171,add fault-tolerance for global random state in map-style datasets (#8950),0.5636797,Fault-tolerance improvements,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
3172,Fix App tests (#14922),0.8213718,Updated app testing (#16000),,1
3173,Changed smoothing in tqdm to decrease variability of time remaining between training / eval (#1194),0.9769906,Changed smoothing in TQDM to decrease variability of time remaining between training/eval (#1194),,1
3174,Cache docker builds (#3659),0.5770543,"Redesigned multi-dataloader support (#16743, #16784, #16939)",,0
3175,move GH docs (#168),0.5736513,move lr_finder (#3434),,0
3176,docs - anchor links (#848),0.5348425,"docs for all Metrics (#2184, #2209)",,0
3177,"Disable {save,check}_on_train_epoch_end with check_val_every_n_epoch>1 (#9156)",0.6588302,"- `ModelCheckpoint(save_last=True, every_n_epochs=N)` now saves a ""last"" checkpoint every epoch (disregarding `every_n_epochs`) instead of only once at the end of training ([#12418](https://github.com/PyTorchLightning/pytorch-lightning/pull/12418))",,0
3178,Skepticleo trainer argparser (#1023),0.72731304,"trainer/separate argparse (#3421, #3428, #3432)",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
3179,Type trainer.connectors.checkpoint_connector (#9419),0.69482434,trainer = Trainer(plugins=[SLURMEnvironment(requeue_signal=signal.SIGHUP)]),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
3180,adding support for interrupt signals,0.56559634,The preemption/termination signal is now configurable (#14626):,,0
3181,clean docs new guide (#3270),0.65911996,Docs improvements,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
3182,Updates docs to clarify train_dataloader usage by batch_size_finder (#13171),0.6891581,train_dataset = trainer.train_dataloader.loaders.dataset,,0
3183,Fix dangerous default argument (#8164),0.5640073,Did not interfere with a default sampler (#1318),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: thomas chaton thomas@grid.ai,0
3184,make experiment param in trainer optional (#77),0.6813799,No need for experiment object in trainer.   ,,0
3185,Grep for potential errors in standalone tests (#15341),0.5601469,"Removed experimental fault-tolerance support (#16516, #16533)",,0
3186,Use accelerator instead of dist backend for missing horovod warning (#8319),0.68493134,"refactored Horovod backend (#3121, #3122)",Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
3187,Add automatic GPU choice to trainer (#1426),0.7650635,GPU training (#2704),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
3188,[WIP] Reduction when batch size < num gpus (#1609),0.8967886,Reduction when batch_size < num_gpus (#1609),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
3189,Fix dataloaders are not reset when tuning the model (#7566),0.6936102,Replace MisconfigurationException with warning in ModelCheckpoint Callback (#4560),,0
3190,every_n_val_epochs -> every_n_epochs (#8383),0.68518114,Deprecated ModelCheckpoint(every_n_val_epochs) in favor of ModelCheckpoint(every_n_epochs) (#8383),,0
3191,fixed bag callback=False or None at trainer_io.py (#409),0.5770627,Trainer is now raising a MisconfigurationException instead of a warning if Trainer.{validate/test} is missing required methods (#10016),,0
3192,Merge the slow and regular test workflows (#15331),0.60256517,System customization syncing for jobs run (#16932),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
3193,Remove unnecessary NotImplementedError used in Training Type Plugin (#10235),0.7355559,Removed Plugin in base_plugin.py in favor of accessing TrainingTypePlugin and PrecisionPlugin directly instead (#9066),,1
3194,Deprecate AllGatherGrad (#15364),0.61546564,Deprecation warning (#3844),Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
3195,[App] Add ready property to the flow (#15921),0.61241084,app = L.LightningApp(Flow()),,0
3196,add mypy ci check for Lite (#10289),0.5523772,"cli = LightningCLI(MyModel, run=False)",,0
3197,fix GH release badges (#5040),0.5250223,This release fixes that core issue,,0
3198,[docs] Update Bolts link (#6743),0.6326127,Update the Lightning App docs (#13537),,0
3199,deprecate metrics pkg (#6505),0.6402688,Removed deprecated metrics (#6161),"  1/n move precision plugin into strategy - update reference   update precision plugin reference in tpu_spawn   add missing reference in error message   add back removed license line   update references in tests   update reference in trainer   update return annotation for precision_plugin property on TTP   simplify access to precision plugin reference in sharded plug   add changelog   remove precision property from ttp and add deprecation message   fix make doc and update precision reference   simplify a reference to precision   accidentally overridden Adrian's change, now add it back  Update CHANGELOG.md  add Adrian's change back  Update accelerator precision  Add Adrian's change back  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci  Add none check for precision plugin  just to be safe   Update ipu.py   update precision_plugin param deprecation message   Update accelerator.py   Remove deprecated warning    Tests will fail after 9940 Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
3200,re-enabled naming metrics in ckpt name (#3060),0.9998658,Re-enabled naming metrics in ckpt name (#3060),,1
3201,docs clean up (#4068),0.6544145,Docs improvements,,0
3202,Remove redundant quotes in an error message (#9392),0.5222114,Removed output argument from *_epoch_end hooks (#3967),Co-authored-by: tchaton thomas@grid.ai Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
3203,Fix TPU CI (#12419),0.6798719,Updated logic for checking TPUs availability (#6767),,0
3204,Update speed docs (#11044),0.6760235,"many speed improvements (how we move data, adjusted some flags & PL now adds 300ms overhead per epoch only!)",,0
3205,removed bilstm,0.65318096,Removed,,0
3206,Do not configure launcher if processes are launched externally (#12431),0.5714041,"Disabled strict loading in multiprocessing launcher (""ddp_spawn"", etc.) when loading weights back into the main process (#16365)",,0
3207,Disallow batch sampler with multiple IPU devices (#13854),0.59199566,- Disallowed using `BatchSampler` when running on multiple IPUs ([#13854](https://github.com/Lightning-AI/lightning/pull/13854)),,0
3208,Additional hooks (#598),0.7335139,New Hooks,,1
3209,fix attribute access in LightningModule.toggle_optimizer (#6513),0.7595048,"- Fixed `LightningModule.{un,}toggle_model` when only 1 optimizer is used ([#12088](https://github.com/PyTorchLightning/pytorch-lightning/pull/12088))",,1
3210,simplify trainer output,0.68623704,Allow easy trainer re-instantiation (#7508),,0
3211,Replace CallbackHookNameValidator with FxValidator [3/n] (#7627),0.67348444,Replaced _DataModuleWrapper with __new__ (#7289),,0
3212,hotfix: drop pyyaml 5.4.* [feat-1.2] (#5609),0.66629493,Drop PyTorch 1.9 support (#15347),,0
3213,drop logging level (#1015),0.7519453,Metric reduction with Logging (#5150),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
3214,swaps lr sched order (#2356),0.50897264,Refactored EpochResultStore (#5522),,0
3215,removed forkedpdb,0.6208038,Removed,,0
3216,[App] Authentication for HTTP queue (#15202),0.7533444,- Added authentication to HTTP queue ([#15202](https://github.com/Lightning-AI/lightning/pull/15202)),,1
3217,Fix valid accuracy computed with train accuracy in docs (#3983),0.61016047,"Refactored training_batch + tests to verify correctness (#2327, #2328)",,0
3218,Remove ModelSummary validation from train loop on_trainer_init (#6610),0.99033,Removed ModelSummary validation from train loop on_trainer_init (#6610),,1
3219,Prepare 1.1.5 release (#5576),0.6935099,"    version=""0.0.1"",",Co-authored-by: thomas chaton thomas@grid.ai,0
3220,3/n integrate new LightningDataFetcher into loop (#8953),0.71723056,LightningDataModule.load_from_checkpoint,,1
3221,Fix cloudcomputes registration for structures (#15964),0.6770951,Refactor cloud dispatch and update to new API (#16456),Co-authored-by: Danielle Pintz 38207072+daniellepintz@users.noreply.github.com,0
3222,Fabric: auto default (#16842),0.7269411,"- Fabric now chooses `accelerator=""auto"", strategy=""auto"", devices=""auto""` as defaults ([#16842](https://github.com/Lightning-AI/lightning/pull/16842))",,1
3223,Minor CLI improvements [1/3] (#9553),0.60547686,Minor changes,,0
3224,Pt 1.9 breaking fix: iter type hint (#7993),0.58504814,- Fixed issue where the wrapped dataloader `iter()` would be called twice ([#16841](https://github.com/Lightning-AI/lightning/pull/16841)),,0
3225,Fix Pruning callback and add a few features (#5825),0.65982497,Avoid redundant callback restore warning while tuning (#13026),,0
3226,Revert namespace package search to normal package search (#1545),0.5392648,Parsed local package versions (#13933),,0
3227,Use full yaml loader (load is deprecated) (#3357),0.5755467,"Disabled strict loading in multiprocessing launcher (""ddp_spawn"", etc.) when loading weights back into the main process (#16365)",Co-authored-by: thomas chaton thomas@grid.ai,0
3228,ref: restore on_eval_start hook (#3183),0.6818459,Call reset_on_restart in the loop's reset hook instead of when loading a checkpoint (#9561),,0
3229,Clear reference to training loss at the end of train step (#9336),0.7160827,Move training_output validation to after train_step_end (#7868),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
3230,[Refactor] Remove should_raise_exception (#8202),0.61904514,remove _evaluate fx (#3197),Co-authored-by: Aki Nitta nitta@akihironitta.com,0
3231,native amp (#2373),0.6666621,Trainer now raises an exception when requesting amp_level with native amp_backend (#9755),,0
3232,fix 2333 (#2360),0.6297999,This release fixes that core issue,"  Drop torch 1.6 support   Drop 1.6 support   Update CHANGELOG   Fixes   Split change   Undo change   1.7 -> 1.7.1   https://github.com/pytorch/pytorch/issues/47354   Force trigger nightly   Update .github/workflows/events-nightly.yml   Co-authored-by: Aki Nitta nitta@akihironitta.com   Revert 1.7.1 change - try wildcard   Update adjust versions and test it   Undo test changes   Revert ""Undo test changes""   This reverts commit 3a6acadd115e86f02d83a788f1978372ab6764f3.  Update CHANGELOG.md  Co-authored-by: Aki Nitta nitta@akihironitta.com",0
3233,fix optimizer loop with frequencies (#9507),0.72114784,- Extended optimizer support with particular frequency,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
3234,Bump tj-actions/changed-files from 29.0.3 to 29.0.4 (#14650),0.53852826,"- Removed deprecated `LightningModule.log(tbptt_reduce_fx, tbptt_reduce_token, sync_dist_op)` ([#10423](https://github.com/PyTorchLightning/pytorch-lightning/pull/10423))",Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
3235,Default to precision=bf16 on CPU when precision=16 is passed (#10033),1.0000001,Default to precision=bf16 on CPU when precision=16 is passed (#10033),,1
3236,Remove NumPy from Callback scripts (#17267),0.5786892,"- Removed deprecated arguments `num_nodes` and `sync_batchnorm` from `DDPPlugin`, `DDPSpawnPlugin`, `DeepSpeedPlugin` ([#10357](https://github.com/PyTorchLightning/pytorch-lightning/pull/10357))",  remove deprecated train_loop   chlog ,0
3237,truncate long version number in progress bar (#2594),0.95042884,Truncated long version numbers in progress bar (#2594),Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,1
3238,Bump google-github-actions/setup-gcloud from 0 to 1 (#15671),0.5802269,- Removed deprecated `GPUStatsMonitor` callback ([#12554](https://github.com/Lightning-AI/lightning/pull/12554)),,0
3239,"Update scikit-learn requirement from <=1.1.1,>0.22.1 to >0.22.1,<1.1.3 in /requirements (#14276)",0.7348268,Removed dependency on scikit-learn (#801),,1
3240,Update backward hook for PrecisionPlugin  (#10008),0.8132414,The PrecisionPlugin.backward hooks no longer returns a value (#8328),,1
3241,Setup: add requirement freeze for next major version (#14480),0.9860751,Setup: added requirement freeze for the next major version (#14480),,1
3242,Lite: setting extras & fix CI (#15192),0.63398796,- Fixed bug that forced overriding `configure_optimizers` with the CLI ([#11672](https://github.com/PyTorchLightning/pytorch-lightning/pull/11672)),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
3243,Fix ShardedDataParallel has no attribute require_backward_grad_sync (#6915),0.59653366,Remove .item which causes sync issues (#1254),,0
3244,Update init.py (#4308),0.57974935,"PyTorch 1.10 and Python 3.7 no longer supported (#16492, #16579)",  Missing Changelogs   Add 1.5.1 entry to changelog   Co-authored-by: Kaushik B kaushikbokka@gmail.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,0
3245,[bugfix] Prevent deepcopy of dataloaders / Trainer in SWA Callback (#8472),0.6180119,refactored dataloader process hook (#3139),  solve combinedloader   update   update changelog   update on comments   resolve iterable dataset support   update test description   update   update on comments   update   Accelerator auto   Address review   Refactor   Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
3246,Logger tests and fixes (#1009),0.84379154,Cleaning up stale logger tests (#3490),,1
3247,black formatting and migrated to self.log logging in finetuning example (#5229),0.719733,Refactored logging,,1
3248,Reset val_dataloader in tuner/batch_size_scaling (#9857),1.0000001,Reset val_dataloader in tuner/batch_size_scaling (#9857),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
3249,Do not set PYTHONHASHSEED #2156 (#3745),0.70284617,Do not override PYTHONWARNINGS (#4700),  Do not autodetach extras   Update CHANGELOG   Use foo ,1
3250,optimize logging,0.75922215,Refactored logging,Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
3251,CI: add e2e cron job (#14402),0.49092114,Introducing CLI commands for apps (#13602)!,,0
3252,Fixes note formatting and more (#14264),0.5624939,Resolve bug with Finetuning (#5744),,0
3253,Update the TQDM progress bar on_train_epoch_end (#11069),0.876196,The TQDM progress bar now correctly shows the on_epoch logged values on train epoch end (#11069),,1
3254,revert building docs with redirect (#17016),0.54923666,refactored dataloader process hook (#3139),  fix   less code   add test case   add test cases   update input   add test cases   add type hint   add changelog note   Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,0
3255,Remove unnecessary intermediate layers in base-conda Dockerfile (#5697),0.91192544,Remove unnecessary intermediate layers in Dockerfiles (#5697),,1
3256,doc update (#3894),0.7104192,Docs improvements,  disable step logging in epoch hooks   chlog   Apply suggestions from code review   chlog ,1
3257,Remove implicit frontend testing from testing.run_app_in_cloud (#16741),0.7825501,Removed implicit ui testing with testing.run_app_in_cloud in favor of headless login and app selection (#16741),  Raise exceptions when torch distributed is not avalible   add changelog ,1
3258,Remove manual optimization find_unused_parameters override (#12425),0.6956953,  * Removed `optimizer_idx` argument from `PrecisionPlugin.optimizer_step` and all of its overrides in subclasses,Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
3259,Integrate lightning_utilities get_all_subclasses (#14575),0.7155321,- Integrate the `lightning_utilities` package (,Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
3260,Fix logging on_train_batch_end in a callback with multiple optimizers (#5521),0.6958306,    --trainer.callbacks.logging_interval=epoch,"  rename occurrences of master port, master address, maser node, master process   rename properties   add property decorators   occurrences in docs   update changelog   update changelog   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   add lost method   create deprecation   add changelog   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   fix typo (but it was already there!!!)   Apply suggestions from code review   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com   add todo   update more occurences   add types   add missing import   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com",0
3261,enables samplers which don't need set epoch (or when ppl don't need a sampler) (#254),0.6228455,Did not interfere with a default sampler (#1318),,0
3262,Document how to use TensorBoardLogger with fsspec (#16320),0.7343052,Switch from tensorboard to tensorboardx in TensorBoardLogger (#15728),,1
3263,Save and load with CheckpointIO in DDPSpawn plugins (#10846),0.7398567,CheckpointIO Plugins,  Don't store csv.Dictwriter in ExperimentWriter   Add test for pickle after .save()   Add entry in changelog ,1
3264,fix returning logged metrics instead of callback metrics during evaluation (#12224),0.7868149,- Fixed returning logged metrics instead of callback metrics during evaluation ([#12224](https://github.com/PyTorchLightning/pytorch-lightning/pull/12224)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
3265,[bugfix] TPU test hangs to barrier on 1 process (#6272),0.6722986,Updated logic for checking TPUs availability (#6767),  Only import PostLocalSGD related modules when it's needed   Only import PostLocalSGD related modules when it's needed   Only import PostLocalSGD related modules when it's needed ,0
3266,[CLI] Shorthand notation to instantiate callbacks [3/3] (#8815),0.67588603,Support shorthand notation to instantiate datamodules (#10011),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
3267,[Docs] update docs for resume_from_checkpoint (#5164),0.77327096,Skip restore from resume_from_checkpoint while testing (#5161),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
3268,fixing LBFGS test (#1678),0.59351975,BFloat16 Support,  Remove deprecated configure_slurm_ddp   Update CHANGELOG   Remove deprecated tests from test suite ,0
3269,Remove the deprecated logger.close (#13149),0.79873586,Removed LoggerStages (#5673),"  move parser function to utils   fix types   keep static method   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",1
3270,bananas (#494),0.7111161,# `outputs` is a list of all bananas returned in the epoch," Remove deprecated sync_batchnorm and num_nodes attributes in DDPPlugin  Part of #10312 test_v1_6_0_ddp_num_nodes() test_v1_6_0_ddp_sync_batchnorm()  Remove deprecated sync_batchnorm and num_nodes attributes in DDPPlugin  Part of #10312 test_v1_6_0_ddp_num_nodes() test_v1_6_0_ddp_sync_batchnorm()   remove deprecation warnings   apply removal to spawn plugin   update changelog   remove num_nodes in deepspeed   remove unused imports   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",1
3271,Default seed_everything(workers=True) in the LightningCLI (#7504),1.0,Default seed_everything(workers=True) in the LightningCLI (#7504),  Update Trainer(precision) docs   Update ,1
3272,Fix BF16 teardown for TPU precision plugin (#10990),0.662904,Renamed TPUHalfPrecisionPlugin to TPUBf16PrecisionPlugin (#10026),  Add more trainer config tests   Add more trainer config and ttp register tests   Add more trainer config and ttp register tests ,0
3273,clean docs (#967),0.63268375,"docs for all Metrics (#2184, #2209)","  Remove deprecated property _slurm_managing_tasks from accelerator connector   Update CHANGELOG   Update Changelog   Removed is_slurm_managing_tasks from AcceleratorConnector   resolve merge conflict   add back accidentally removed lines   remove test   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
3274,Update call to amp.autocast from fast_dtype to dtype (#9211),0.5224755,Deprecated LightningDistributed and moved the broadcast logic to DDPPlugin and DDPSpawnPlugin directly (#9691),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
3275,exclude mpi run from auto-detection of horovod (#8610),0.58524233,"refactored Horovod backend (#3121, #3122)",Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Roger Shieh sh.rog@protonmail.ch,0
3276,Remove MANIFEST reference in docs job (#15584),0.6146549,Remove unnecessary intermediate layers in Dockerfiles (#5697),,0
3277,Add fairscale install msg for Sharded Plugins (#7213),0.5401251,Precision Plugins (#5718),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
3278,"Remove deprecated utilities.distributed.rank_zero_{warn,deprecation} (#10451)",0.80747503,- Removed deprecated `utilities.distributed.rank_zero_{warn/deprecation}` ([#10451](https://github.com/PyTorchLightning/pytorch-lightning/pull/10451)),"  add test, + add change to data loading batch sample method   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  Refactor and CHANGELOG  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com",1
3279,Clean up optimizer code (#3587),0.8017232,Refactored optimizer (#4658),,1
3280,Add timeout argument for FSDPStrategy (#17274),0.6090912,"- Added a `timeout` argument to `DDPStrategy` and `DDPSpawnStrategy`. ([#13244](https://github.com/Lightning-AI/lightning/pull/13244), [#13383](https://github.com/Lightning-AI/lightning/pull/13383))",Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
3281,Remove the deprecated pl.strategies.utils.on_colab_kaggle function (#16437),0.8034925,- Removed the deprecated `pl.strategies.utils.on_colab_kaggle` function ([#16437](https://github.com/Lightning-AI/lightning/pull/16437)),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
3282,Refactor: runif for spec 6/6 (#6307),0.6345408,clean up hooks in run_evaluation (#3156),,0
3283,Swap torch.load for fsspec load in ddp spawn backend (#3787),0.981891,Swaped torch.load for fsspec load in DDP spawn backend (#3787),,1
3284,Implement double optimizer closure for hook structure consistency (#10167),0.69645727,Made optimization steps for hooks (#2363),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
3285,Fix repository links (#13304),0.6537754,- Fixed a bug where the upload files endpoint would raise an error when running locally ([#14924](https://github.com/Lightning-AI/lightning/pull/14924)),,0
3286,Add Ready to Go label only after two approvals (#8484),0.5165867,"Marked several methods in EvaluationEpochLoop as protected: on_evaluation_batch_start, evaluation_step, evaluation_step_end (#9516)",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
3287,Add 1.2.6 section to CHANGELOG (#6732),0.68241924,Full Changelog,Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
3288,Remove warning,0.7444215,warning_utils >> warnings,,1
3289,"add on_train_batch_start/end, chpt hooks in hooks docs (#4023)",0.651172,- Deprecated `on_batch_start` and `on_batch_end` callback hooks in favor of `on_train_batch_start` and `on_train_batch_end` ([#11577](https://github.com/PyTorchLightning/pytorch-lightning/pull/11577)),,0
3290,Add explicit checkpoints for tests,0.66779435, - Checkpointing: `Trainer(enable_checkpointing=True)`,  update ref to 1.5 as stable   Fix CHANGELOG   Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
3291,Small typo correction on CONTRIBUTING.md (#4625),0.52567905,Fixing critical bugs in newly added hooks and hparams assignment.,  update changelog   update version in about   update references to 1.5 in readme   Co-authored-by: thomas chaton thomas@grid.ai,0
3292,Fabric: Test PyTorch 2.0 pre-release on CPU and CUDA (#16905),0.6479068,Dropped official support/testing for PyTorch <1.6 (#8288),"  introduce , udpate tests   update CHANGELOG.md   change staticmethod and hook attribute naming   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   fix typo   remove non-essential comment   fix merge error and comment format   try to fix test_tpu.py failure   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci   update on comments   chlog   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   chlog   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   try fix   Revert back TPUSpawn changes   Update test   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Kaushik B kaushikbokka@gmail.com",0
3293,Support hierarchical dict (#1152),0.59386516,diff,  Add display_every_n_epochs argument to RichProgressBar   Add tests   Update test   Update test   Update changelog   use leave argument instead   Update pytorch_lightning/callbacks/progress/rich_progress.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
3294,more early stopping options (convergence and divergence threshold) (#6868),0.5470374,[0.6.3] - 2022-10-07,Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
3295,ref: part 8 of #3733 (#3806),0.7078867,[1.1.8] - 2021-02-08,,1
3296,Rename _distrib_type to _strategy_type (#11328),0.7037011,"All TrainingTypePlugins have been renamed to Strategy (#11120). Strategy is a more appropriate name because it encompasses more than simply training communcation. This change is now aligned with the changes we implemented in 1.5, which introduced the new strategy and devices flags to the Trainer.","  fix yielding form iterator   update description   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  remove unused code  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",1
3297,Fix LightningCLI parse_env and description in subcommands (#15138),0.895186,- Fixed `LightningCLI` parse_env and description in subcommands ([#15138](https://github.com/Lightning-AI/lightning/pull/15138)),,1
3298,Update to Mypy>0.9 (#8386),0.5722705,"PyTorch 1.10 and Python 3.7 no longer supported (#16492, #16579)",Co-authored-by: Kaushik B kaushikbokka@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: SeanNaren sean@grid.ai Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: four4fish 88516121+four4fish@users.noreply.github.com Co-authored-by: Nicki Skafte Detlefsen skaftenicki@gmail.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: Pietro Lesci 61748653+pietrolesci@users.noreply.github.com,0
3299,[App] Fix env variable login (#16339),0.475903,| Enum RunningStage.TUNING                                    | 1.10             | No longer supported         |,,0
3300,better use of void (#7809),0.50827974,Enabling val/test loop disabling (#2692),,0
3301,Remove optimizer_idx arg in manual optimization (#6093),0.8313002,Removed the deprecated optimizer_idx from training_step as an accepted argument in manual optimization (#8576),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
3302,Replace eval() with ast.literal_eval() for security (#12212),0.44201952,Enabled no returns from eval (#2446),,0
3303,"new LightningModule hook ""configure_callbacks"" (#5621)",0.7822887,- Added support for returning a single Callback from `LightningModule.configure_callbacks` without wrapping it into a list ([#11060](https://github.com/PyTorchLightning/pytorch-lightning/pull/11060)),Co-authored-by: rohitgr7 rohitgr1998@gmail.com,1
3304,update contrib notes (#4514),0.5990344,[0.6.0] - 2022-09-08,,0
3305,Refactor early stopping test (#11866),0.7622304,Early stopping checks on_validation_end (#1458),,1
3306,[CLI] add support to run app on a specific cluster (#13894),0.7568547,Enabled custom clusters (#4048),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
3307,fix for pyTorch 1.2 (#549),0.82138515,"Removed PyTorch 1.6 support (#10367, #10738)",Co-authored-by: tchaton thomas@grid.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
3308,Remove legacy dead code in DDP script launch (#11678),0.60892856,Removed logger_connector legacy code (#6733),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
3309,Update tests/core/*.py to use devices instead of gpus or ipus (#11433),0.6521244,Dropped official support/testing for PyTorch <1.6 (#8288),,0
3310,[App] Check for NOT_STARTED app status in cloud tests (#16344),0.7032751,Updated app testing (#16000),Co-authored-by: tchaton thomas@grid.ai,1
3311,ci: fix Azure trigger pattern (#16972),0.5623864,Updated mlflow with using resolve_tags (#6746),,0
3312,Control automatic resubmission on SLURM (#10601),0.83179057,Control SLURM's re-queueing,,1
3313,fix(wandb): allow use of sweeps (#1512),0.83248055,Allow use of sweeps with WandbLogger (#1512),Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
3314,Update docs for multiple optimizers in 2.0 (#16588),0.81355107,Working with multiple optimizers (#16539),,1
3315,CI: Use self-hosted Azure GPU runners (#14632),0.67953527,GPU training (#2704),,0
3316,Refactor unnecessary else / elif when if block has a raise statement (#8403),0.5382525,Used raise .. from .. to explicitly chain exceptions (#3750),,0
3317,Typo in major heading seen by newcomers (#14501),0.44250965,"    },",,0
3318,"Update horovod requirement from !=0.24.0,<=0.24.3,>=0.21.2 to >=0.21.2,!=0.24.0,<0.25.1 in /requirements (#13363)",0.6626928,"refactored Horovod backend (#3121, #3122)",,0
3319,Fix merge conflicts,0.5296441,Fixing critical bugs in newly added hooks and hparams assignment.,,0
3320,Add info message for Ampere GPUs to enable tf32 matmuls (#16037),0.56235516,"This release adds support for training models on Tensor Processing Units (TPU). We can now train models on GPUs and TPUs by changing a single parameter in Trainer (see docs). We are also bringing the flexibility of Bearer into Lightning by allowing for arbitrary user-defined callbacks, see docs. ",,0
3321,CI: debug TPU failing tests (#13679),0.68663335,Updated logic for checking TPUs availability (#6767),,0
3322,Upgrade HPU image to release 1.6.1 (#14932),0.58167875,"    version=""0.0.1"",",,0
3323,[Changelog] 1.0.4 (#4440),0.7558306,Full Changelog,,1
3324,Parametrize fit hook test with manual optimization (#8071),0.7269717,Made optimization steps for hooks (#2363),,1
3325,introduce has_len_all_ranks() to check the length of dataloader across ranks (#9827),0.62017083,"Raised MisconfigurationException when total length of dataloader across ranks is zero, and give warning when total length is non-zero, but only local rank length is zero. (#9827)",,0
3326,Update uvicorn requirement from <0.21.2 to <0.22.1 in /requirements (#17586),0.5219486,Only check versions / env when not in the cloud (#15504),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
3327,Unify rank zero messaging utilities (#14116),0.67753464,Rank-zero only EarlyStopping messages,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
3328,Fabric RL example: add torchmetrics and minor fixes (#16543),0.6086662,"Native torch metrics (#1488, #2062)",  deprecate training tricks connector   fixes ,0
3329,added init to test folder,0.5205762,      init_args:,,0
3330,Merge pull request #4880 from PyTorchLightning/better_simple_profiler,0.65427536,- Removed unnecessary `_move_optimizer_state` method overrides from `TPUSpawnPlugin` and `SingleTPUPlugin` ([#10849](https://github.com/PyTorchLightning/pytorch-lightning/pull/10849)),,0
3331,improve simple profiler output (#11414),0.64515984,Split profilers module (#6261),,0
3332,"Deprecate log_gpu_memory, gpu_metrics, and util funcs in favor of DeviceStatsMonitor callback (#9921)",0.98486936,"Deprecated log_gpu_memory, gpu_metrics, and util funcs in favor of DeviceStatsMonitor callback (#9921)",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
3333,Type Hints for Trainer (#912),0.78651243,"trainer = Trainer(callbacks=[FineTuneBatchSizeFinder(milestones=(5, 10))])",,1
3334,move deprecated keys check sooner in checkpoint loading (#9494),0.6366917,Checkpoint and early stopping now work without val. step (#1041),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
3335,"F.cross_entropy(y_hat, y)(y_hat, y) typo. (#137)",0.40102485,"| ""bf16-mixed""                      | ""bf16""     |",,0
3336,Deprecate CLI registries and update documentation (#13221),0.60785025,"The new version renders the registries and the auto_registry flag, introduced in 1.6.0, unnecessary, so we have deprecated them.",,0
3337,prune deprecated EvalResult (#5633),0.5691719,Removed deprecated EvalResult (#5633),This reverts commit 6429de8944b27beeeb25dcff5b423ecd26fd3265.,0
3338,ref: trainer 1/n (#3412),0.8263954,"trainer/separate argparse (#3421, #3428, #3432)",,1
3339,Use root_device in DeviceStatsMonitor callback  (#11748),0.6914036,device_stats = DeviceStatsMonitor(),,0
3340,Remove the *_step_end hooks (#16791),0.808094,The *_epoch_end hooks were removed (#16520),,1
3341,[DeepSpeed] fix flag forwarding in DeepSpeedPlugin (#10899),0.6823392,- Fixed deepspeed keeping old sub-folders in same ckpt path ([#12194](https://github.com/PyTorchLightning/pytorch-lightning/pull/12194)),Co-authored-by: tchaton thomas@grid.ai,0
3342,Optimize precision conversion in forward of Fabric module wrapper (#16903),0.5756293,Made optimization steps for hooks (#2363),,0
3343,Update links for zero_grad to PyTorch docs (#8618),0.6637101,Drop PyTorch 1.9 support (#15347),,0
3344,[Docs] checkpoints (#4034),0.69884044,Resuming from checkpoints (#16167),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
3345,removed dep,0.8115934,Removed,,1
3346,update tests for v2 (#11485),0.6814108,Updated app testing (#16000),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
3347,docs dpp warn (#1835),0.6384722,Deprecation warning (#3844),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
3348,Set spawn_method on initialization (#11162),0.62920815,"We redesigned the process creation for spawn-based strategies such as DDPSpawnStrategy and TPUSpawnStrategy (#10896). All spawn-based strategies now spawn processes immediately upon calling Trainer.{fit,validate,test,predict}, which means the hooks/callbacks prepare_data, setup, configure_sharded_model and teardown all run under an initialized process group. These changes align the spawn-based strategies with their non-spawn counterparts (such as DDPStrategy).","  WIP   Progress   Undo test change   Fix plugin closure execution order   Update CHANGELOG   Fix manual optimization on AMP and skipping backward   Fix for deepspeed   Typo   Hook test for manual closure   Add skipping test with AMP   You are hideous, apex   Add deepspeed test   Update CHANGELOG   Fix for broken master   Add RunIf   FIXMEs   Rename   Fix grad norm   add a simple test   update test   update  test   update test   fix merge conflicts   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Sea of changes   Undo change   Introduce TPUPrecisionPlugin   Undo changes   Undo changes   Resolve FIXME   Undo change   Undo change   Undo change   Fix FIXMEs   Fix FIXME   Correct value   Bad merge   Fix circular imports   WIP   Fixing clipping   Fixes   Bad merge   Move optimizer step and clipping into the PrecisionPlugin   Fix AMP   Update CHANGELOG   Fix tests   Underscore   Progress   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Remove pre_optimizer_step   Missed one   Progress   Progress   Fix test   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Update FIXMEs   Fix test   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Fix test   DeepSpeed warning. mypy   Rename   Finish tests   Update CHANGELOG   Dumb fixes   accelerator=auto   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   Update on comments   Use ClassifModule   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com",0
3349,Fix default value in documentation (#2452),0.57632756,Resolve bug with Finetuning (#5744),Co-authored-by: thomas chaton thomas@grid.ai,0
3350,torchmetrics: bump min version v0.10 in examples (#16936),0.75022876,"Native torch metrics (#1488, #2062)",Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: tchaton thomas@grid.ai,1
3351,Remove shown_warnings dead code from Trainer (#9230),0.7630942,Removed Warning from trainer loop (#1634),,1
3352,"Update mlflow requirement from <1.27.0,>=1.0.0 to >=1.0.0,<1.28.0 in /requirements (#14024)",0.67354107,Updated mlflow with using resolve_tags (#6746),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
3353,[bugfix] Changed CometLogger to stop modifying metrics in place (#9150),0.65982115,Remove MetricsHolder (#7909),,0
3354,fix typos in datamodule (#8949),0.6133224,Replaced _DataModuleWrapper with __new__ (#7289),,0
3355,"Remove the deprecated LightningDataModule.size, LightningDataModule.dims (#12780)",0.85401803,Removed the deprecated LightningDeepSpeedModule (#16041),Co-authored-by: Kaushik B kaushikbokka@gmail.com,1
3356,v1.4.0rc2 (#8553),0.6979756,0.4.0,Co-authored-by: thomas chaton thomas@grid.ai,0
3357,"Add ""monitor"" to saved ModelCheckpoints (#4383)",0.8297889,"Allow ModelCheckpoint monitor to be None, meaning it will always save ([3630)",,1
3358,flatten Wandb hyperparameters dict  (#2459),0.9400338,Flattening Wandb Hyperparameters (#2459),,1
3359,Decouple pulling legacy checkpoints from existing GHA workflows and docker files (#13185),0.6068773,Removed deprecated checkpoint argument filepath (#5321),,0
3360,"Rename log_save_interval, row_log_interval (#3748)",0.78492665,Rename Trainer arguments row_log_interval >> log_every_n_steps and log_save_interval >> flush_logs_every_n_steps (#3748),,1
3361,Add Support for Non-primitive types in TensorboardLogger (#1130),0.7660244,Switch from tensorboard to tensorboardx in TensorBoardLogger (#15728),,1
3362,[IPU] Fix Custom Poptorch options to IPUPlugin (#8241),0.6843721,- No longer set a `DistributedSampler` to the `poptorch.DataLoader` when IPUs are used ([#12114](https://github.com/PyTorchLightning/pytorch-lightning/pull/12114)),Co-authored-by: tchaton thomas@grid.ai,0
3363,Remove legacy support for the magic log/progress_bar keys in dict returns (#6734),0.82895887,"Removed legacy code to log or include metrics in the progress bar by returning them in a dict with the ""log""/""progress_bar"" magic keys. Use self.log instead (#6734)",,1
3364,ref: trainer argparse 1/n (#3421),0.91193545,"trainer/separate argparse (#3421, #3428, #3432)",,1
3365,Nested metrics dictionaries now can be passed to the loggers (#1582),0.66579324,Allow logging of metrics together with hparams (#1630),,0
3366,Update auto-opt docs (#6037),0.5819653,Improved error messages for invalid configure_optimizers returns (#3587),,0
3367,adding  @lantiga as code-owner (#16394),0.5631839,@property,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
3368,Update docstring for early_stop_callback default Trainer argument (#3641),0.8049945,Deprecate early_stop_callback Trainer argument (#3845),Co-authored-by: tchaton thomas@grid.ai,1
3369,release v0.4.5,0.78364086,0.4.0,Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
3370,fix flushing loggers (#1459),0.77378476,Removed LoggerStages (#5673),,1
3371,Revert across accelerators,0.80009997,Refactored accelerator backends:,Co-authored-by: tchaton thomas@grid.ai Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
3372,temporarily fixes early stopping bug (#2119),0.6455793,Early stopping checks on_validation_end (#1458),,0
3373,Add torch log API usage for lightning module instantiation (#3603),0.68868065,"add .log to lightning module (#3686, #3699, #3701, #3704, #3715)",,0
3374,[App] Add --zip option to cp command (#16798),0.64514285,- Added `--zip` option to the `lightning cp` command to copy content from the Cloud Platform Filesystem as a zipfile,Co-authored-by: tchaton thomas@grid.ai,0
3375,Exclude the CHANGELOG from the pre-commit size check (#12931),0.58629143,Full Changelog,Co-authored-by: tchaton thomas@grid.ai Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
3376,Promote the CLI out of utilities (#13767),0.55874205,- Enabled all shorthand strategy names that can be supported in the CLI ([#16485](https://github.com/Lightning-AI/lightning/pull/16485)),Co-authored-by: tchaton thomas@grid.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
3377,feat: allow root path to run the app on /path (#14972),0.9726617,Allowed root path to run the app on /path (#14972),,1
3378,Better handling connection interruption (#15267),0.70325637,Connect and Disconnect node (#16700),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: rohitgr7 rohitgr1998@gmail.com,1
3379,update setup,0.5958448,setup(,,0
3380,Feat: Add BackboneLambdaFinetunningCallback (#5377),0.7595186,Changed computer_vision_fine_tunning example to use BackboneLambdaFinetuningCallback (#5377),,1
3381,removed callback metrics from test results obj (#2994),0.9967936,Removed callback metrics from test results obj (#2994),,1
3382,update changelog after 1.0.5 (#4505),0.73314136,Full Changelog,Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,1
3383,secrets docs (#14951),0.5966464,"Introducing encrypted secrets (#14612), a feature requested by Lightning App users :tada:!",,0
3384,Refactor 2/n (#2708),0.6784189,Mixed precision overhaul (#16783),,0
3385,Update GHA job names (#14400),0.45912227,    # use the last 4 numbers in the job id as the id,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
3386,Update FairScale on CI (#7017),0.6586043,"For reference, FairScale's implementation can be used with",,0
3387,[App] Refactor plugins to be a standalone LightningPlugin (#16765),0.7380485,Included app templates to the lightning and app packages (#13731),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: tchaton thomas@grid.ai Co-authored-by: rohitgr7 rohitgr1998@gmail.com,1
3388,Fix default int values being float (#12989),0.58686614,Changed iou [func] to allow float input (#4704),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: tchaton thomas@grid.ai,0
3389,Fix logging interval (#2694),0.74832,Un-balanced logging properly supported (#5119),"  remove dead code in accelerator connector   remove slurm ""fake_slurm_managing_tasks"" dead code ",1
3390,Minor fix to example in docstring of lr_finder (#4104),0.6554493,tuner.lr_find(...),Co-authored-by: tchaton thomas@grid.ai,0
3391,Disable eval dataloaders replacement during overfitting (#10877),0.61737186,"Refactor dataloading, supports infinite dataloader (#955)",,0
3392,Make sure train doesn't crash when called at max_epoch (#608),0.629744,- Added a warning that shows when `max_epochs` in the `Trainer` is not set ([#10700](https://github.com/PyTorchLightning/pytorch-lightning/pull/10700)),Co-authored-by: tchaton thomas@grid.ai,0
3393,Update starlette requirement from <=0.22.0 to <0.24.0 in /requirements (#16471),0.5654087,Pinning starsessions to 1.x (#14333),,0
3394,use print for INFO and lower levels summarize() (#580),0.5374187,Overview,,0
3395,updated amp use,0.66745126,        :param use_amp: Whether amp was requested or not,,0
3396,add svg logo [SKIP CI] (#3988),0.4790318,Allowing decorate model init with saving hparams inside (#4662),,0
3397,Document exceptions in callbacks (#5541),0.57260466,  callbacks:,,0
3398,Input validation for Fabric.launch (#17423),0.719127,fabric.launch(),,1
3399,[bugfix] Resolve lost reference to meta object in ResultMetricCollection (#8932),0.5805588,Remove EpochResultStore and HookResultStore in favor of ResultCollection (#7909),,0
3400,CI: update TPU docker (#14448),0.7116579,Updated logic for checking TPUs availability (#6767),,1
3401,Centralize rank_zero_only utilities into their own module (#11747),0.7441801,- Added `rank_zero` module to centralize utilities ([#11747](https://github.com/PyTorchLightning/pytorch-lightning/pull/11747)),,1
3402,"Parse Union[bool, str] arguments (#3235)",0.564705,Improved string conversion for ResultCollection (#8622),Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: thomas chaton thomas@grid.ai,0
3403,Dim 0 warning (#256),0.5635426,Updated Multinode Warning (#16091),,0
3404,Make the trainer a required loop argument (#16771),0.7119541,Refactored result handling in training loop (#7506),This reverts commit f0e6f1b58aeb81f461491fd4818dd487a507ad42.,1
3405,Move TrainingBatchLoop.build_kwargs to utilities (#9198),0.6400715,- Moved `batch_to_device` method from `Accelerator` to `TrainingTypePlugin` ([#10649](https://github.com/PyTorchLightning/pytorch-lightning/pull/10649)),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
3406,use XLA base image for TPU testing (#2536),0.6043527,TPU training (#2708),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
3407,tests for pytorch 1.5 (#1552),0.77984136,Skipped testing with PyTorch 1.7 and Python 3.9 on Ubuntu (#11217),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
3408,Loader docs (#1416),0.5765345,"Show a message when BuildConfig(dockerfile=""..."") is passed but a Dockerfile file is already present in the Work (#15799)",,0
3409,Enable TPU support (#868),0.7356863,Support 8-core TPU on Kaggle,,1
3410,"Update wandb requirement from <0.12.19,>=0.8.21 to >=0.8.21,<0.12.20 in /requirements (#13415)",0.5691051,Removed wandb logger's finalize method (#1193),,0
3411,Fix tbptt_reduce_fx when non-floating tensors are logged (#3796),0.6249863,Casted only floating point tensors to fp16 with IPUs (#13983),,0
3412,Save and load sharded checkpoints with FSDP in Fabric (#17323),0.6797664,DataModule hooks for loading and saving checkpoints,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
3413,fix model arguements (#5653),0.6869428,Deprecated prefix argument in ModelCheckpoint (#4765),Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
3414,Rename LiteMultiNode to FabricMultiNode (#16505),0.68542564,"Renamed the class LightningLite to Fabric (#15932, #15938)",,0
3415,Update docs README on how to build docs (#13465),0.6454668,Docs improvements,,0
3416,Surface Neptune installation problems to the user (#14715),0.58926463,Changed to the NeptuneLogger (#16761):,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: rohitgr7 rohitgr1998@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
3417,Resolve interactions between CUDA tests (#15042),0.5752096,"- Trainer queries the CUDA devices through NVML if available to avoid initializing CUDA before forking, which eliminates the need for the `PL_DISABLE_FORK` environment variable introduced in v1.7.4 ([#14631](https://github.com/Lightning-AI/lightning/pull/14631))",Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
3418,Bump google-github-actions/get-gke-credentials from 0 to 1 (#15843),0.53365326,- Removed deprecated `GPUStatsMonitor` callback ([#12554](https://github.com/Lightning-AI/lightning/pull/12554)),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
3419,Reduce log output size in special tests (#7481),0.741012,Metric reduction with Logging (#5150),,1
3420,Avoid raising the sampler warning if num_replicas=1 (#14097),0.9999999,Avoid raising the sampler warning if num_replicas=1 (#14097),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
3421,Fix import in dp,0.6272541,from setuptools import setup,,0
3422,Unify current_fx_name and current_hook_fx_name [2/n] (#7594),0.48717704,"merge backends (#3476, #3477, #3478, #3480, #3482)",,0
3423,ci: parametrize publish (#16343),0.5922174,[1.3.7post0] - 2021-06-23,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
3424,Deprecate BaseProfiler.profile_iterable (#12102),0.72235626,- Deprecated `BaseProfiler.profile_iterable` ([#12102](https://github.com/PyTorchLightning/pytorch-lightning/pull/12102)),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com,1
3425,use public format checkpoint method (#9818),0.5125568,"Versioning of ""last"" checkpoints",Co-authored-by: Sean Naren sean@grid.ai Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
3426,CI: docker focus on PL only (#14246),0.5944324,Casted only floating point tensors to fp16 with IPUs (#13983),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
3427,Add method to TPUSpawn plugin to override how models are setup (#10039),0.6945118,Implemented DataParallelPlugin._setup_model (#10010),Co-authored-by: thomas chaton thomas@grid.ai,0
3428,Remove unnecessary pytest.param usage (#9760),0.6829817,Removed deprecated profiled_functions argument from PyTorchProfiler (#9178),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
3429,Reformat iou [func] and add IoU class (#4704),0.73434246,Changed iou [func] to allow float input (#4704),,1
3430,extend arg parser (#1842),0.7850727,args = parser.parse_args(),,1
3431,Do not mark LightningModule methods as abstract (#12381),0.73051214,Gave warnings for unimplemented required lightning methods (#1317),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
3432,fix make clean command (#16823),0.60238254,clean up hooks in run_evaluation (#3156),,0
3433,Future 2/n: stand-alone examples (#13294),0.6469357,[1.2.2] - 2021-03-02,,0
3434,Update version after the 1.6.0 release (#12512),0.7204336,"    version=""0.0.1"",",  rm distributed_backend from Trainer   unused   chlog   internal distributed_backend   Docstring   Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
3435,Raise AttributeError in lightning_getattr and lightning_setattr when attribute not found (#6024),0.6675459,Deprecated .get_model() with explicit .lightning_module property (#6035),,0
3436,Update README.md (#2117),0.52123785,Updated mlflow with using resolve_tags (#6746),Co-authored-by: thomas chaton thomas@grid.ai,0
3437,docs,0.9012184,Docs,,1
3438,Catch exceptions when optional dependencies are missing (#442),0.59940434,Improved support for running apps when dependencies aren't installed (#15711),,0
3439,Update contributing docs with pointers for legacy checkpoint testing (#8892),0.6062621,Refactored CheckpointConnector to offload validation logic to the CheckpointIO plugin (#9045),,0
3440,disabled early stopping by default (#1022),0.6792716,EarlyStopping now runs at the end of the training epoch by default (#8286),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
3441,Remove TPU Availability check from parse devices (#12326),0.762718,Updated logic for checking TPUs availability (#6767),,1
3442,[Metrics] Unification of FBeta (#4656),0.7952336,Renamed class metric Fbeta >> FBeta (#4656),,1
3443,Fix failing GPU tests (#722),0.7258079,GPU training (#2704),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
3444,Update root Makefile to run all test projects (#14881),0.5264071,Updated app testing (#16000),  fix   update tests   chlog   skip windows ,0
3445,Fix fine-tuning callback test (#5643),0.715021,- fixed all the .test() calls,Co-authored-by: thomas chaton thomas@grid.ai,1
3446,Add support for command descriptions (#15193),0.74756706,- Added support for adding descriptions to commands either through a docstring or the `DESCRIPTION` attribute ([#15193](https://github.com/Lightning-AI/lightning/pull/15193),,1
3447,releasing feature as nightly (#5233),0.57213426,This release includes:,,0
3448,added testing for metrics,0.64897454,test_percent_check in favour of limit_test_batches,,0
3449,[App] Install exact version whn upgrading and not when testing (#15984),0.67806494,Updated app testing (#16000),,0
3450,Simplify strategy installation in CI (#17347),0.71474206,Simplify optimization Logic (#4984),,1
3451,Convert warning message to debug-level info in spawn plugins (#10864),0.6286004,- Changed the info message for finalizing ddp-spawn worker processes to a debug-level message ([#10864](https://github.com/PyTorchLightning/pytorch-lightning/pull/10864)),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com,0
3452,Add resuming from specific checkpoint (#516),0.8262398,Resuming from checkpoints (#16167),,1
3453,Fixed error message and test docstring (#1698),0.59085697,- fixed all the .test() calls,,0
3454,Pkg: fix parsing versions (#15401),0.76072085,Pkg: fix parsing versions by @Borda in https://github.com/Lightning-AI/lightning/pull/15401,Co-authored-by: ronif ronif@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
3455,Lite: Support self.log from a LightningModule (#16311),0.72647786,Optionally customize logging in LightningModule,,1
3456,Update inference doc (#11428),0.5549787,"Refactored prediction loop interface; added new classes PredictionLoop, PredictionEpochLoop (#7700, #8077)",,0
3457,Migrate TPU tests to GitHub actions (#14687),0.6564821,Updated logic for checking TPUs availability (#6767),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
3458,CI: add flake8 (#4239),0.57035816,adding Trainer.tune() (#3293),,0
3459,use tmpdir in tests when writing predictions to disk (#3561),0.53677034,Updated logic for checking TPUs availability (#6767),,0
3460,hotfix for PT1.6 and torchtext (#6323),0.63118124,- Removed deprecated support for old torchtext versions ([#14375](https://github.com/Lightning-AI/lightning/pull/14375)),Co-authored-by: tchaton thomas@grid.ai,0
3461,hotfix: move process_dataloader to plugins (#6023),0.61125803,refactored dataloader process hook (#3139),Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
3462,revamp entire metrics (#3868),0.7550181,Classification metrics overhaul (#4837),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: thomas chaton thomas@grid.ai,1
3463,Update Lightning Lite docs (6/n) (#16342),0.86579746,Update the Lightning App docs (#13537),Co-authored-by: tchaton thomas@grid.ai,1
3464,Run all tests in master (#15288),0.59981704,- fixed all the .test() calls,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Nicki Skafte Detlefsen skaftenicki@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
3465,Fix NeptuneLogger.log_text(step=None) (#7194),0.66951984,Ignored step param in Neptune logger's log_metric method (#5510),Co-authored-by: tchaton thomas@grid.ai,0
3466,[TPU] Rename classes to use XLA instead of TPU (#17383),0.59466946,Renamed TPUHalfPrecisionPlugin to TPUBf16PrecisionPlugin (#10026),"  LR logging works even with no lr scheduler, wrote few extra tests as well   updated changelog   modified code as suggested by DeepSource   added helper functions   opt with no scheduler   rename   chlog   update test   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: rohitgr7 rohitgr1998@gmail.com",0
3467,ddp fix for trainer.test() + add basic ddp tests (#2997),0.69999075,trainer.test(),Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Kaushik B kaushikbokka@gmail.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,0
3468,[TPU CI] Use timestamp+pythonVersion to form the docker image tag. (#3779),0.55860126,    pil_image = T.ToPILImage()(image),  init hook   docs   dep train args   update tests   doc   doc   .gitignore   not dep   add trainer args   add & update tests   fix tests   pre-commit   docs   add docs   add exception   code review   deepspeed   update tests   not   try fix   Apply suggestions from code review   update deepspeed   disable some tests   disable some tests   enable all tests ,0
3469,Update trainer.py,0.69901836,Removed pytorch_lightning/trainer/training_loop.py (#7985),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
3470,Renamed Mount root_dir Argument to mount_path (#15228),0.6375411,Changed Checkpoint path parameter from filepath to dirpath (#1016),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Kaushik B kaushikbokka@gmail.com,0
3471,Use the setter in the children recursively (#14724),0.52204144,Can be set explicitly,,0
3472,Move fetchers classes with the loops (#16678),0.69753885,"Connect the progress tracking dataclasses to the loops (#8244, #8362)",,0
3473,make cluster creation/deletion async by default (#16185),0.9866526,Made cluster creation/deletion async by default (#16185),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
3474,Add back sanity checks (#3846),0.6372139, - Sanity checking: `Trainer(num_sanity_val_steps>0)`,  remove type error handling in _configure_checkpoint_callbacks   rm test   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
3475,testing -1 gpu option,0.66054857,GPU training (#2704),,0
3476,Enable any ML experiment tracking framework (#223),0.56359285,MLflowLogger now uses the env variable MLFLOW_TRACKING_URI as default tracking URI (#7457),Co-authored-by: Chris Chow cchow@nianticlabs.com Co-authored-by: thomas chaton thomas@grid.ai,0
3477,ref: unify slurm and TE under backendPlugin 2/n (#4580),0.7235957,"Unify sLURM/TorchElastic under backend plugin (#4578, #4580, #4581, #4582, #4583)",,1
3478,[App] update app testing (#16000),0.97176886,Updated app testing (#16000),  update docs and add tests   update docs and add tests   Update pytorch_lightning/callbacks/gradient_accumulation_scheduler.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Apply suggestions from code review  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
3479,Fix ci for IPU (#13340),0.6485311,Graphcore IPU devices,,0
3480,Add Danielle Pintz and Siyu Wang as core maintainers (#12515),0.47046787,@akihironitta @awaelchli @leezu,  add check   chlog   Apply suggestions from code review   Co-authored-by: ananthsub ananth.subramaniam@gmail.com  Apply suggestions from code review  Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: thomas chaton thomas@grid.ai,0
3481,Fix multinode cloud component (#15965),0.65742767,Refactor cloud dispatch and update to new API (#16456),Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,0
3482,"added cpu, gpu tests",0.7086201,GPU training (#2704),,1
3483,fix docs path,0.6102447,- all the file path errors with loggers (txs @awaelchli),,0
3484,Modified python version check to accommodate for legacy version styles (#13420),0.6698518,Compatibility for Python 3.10,"  enable_chekpointing   update codebase   chlog   update tests   fix warning   Apply suggestions from code review   Co-authored-by: ananthsub ananth.subramaniam@gmail.com  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci  Apply suggestions from code review  Co-authored-by: ananthsub ananth.subramaniam@gmail.com  Apply suggestions from code review  Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
3485,Prune CHANGELOG.md (#5151),0.52261686,[1.3.7post0] - 2021-06-23,,0
3486,Remove AccleratorConnector.num_ipus and deprecate Trainer.ipus (#12386),0.73193645,Removed the deprecated TrainerTrainingTricksMixin class (#8679),,1
3487,Clarify logger flag (#7190),0.69806874,Dramatically simplify the LoggerConnector (#7882),,0
3488,Bugfix/Multiple dataloaders (#7433),0.6918949,"Working with multiple dataloaders (#16800, #16753)",,0
3489,update list of fist party packages (#16859),0.5259758,Key updates,,0
3490,[feat] Enable self.log in callbacks (#5094),0.98095036,Enabled self.log in callbacks (#5094),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
3491,Allow Callback instance as an argument of callbacks in Trainer (#5446),0.8965649,Changed callbacks argument in Trainer to allow Callback input (#5446),,1
3492,Fix Imagenet example (#10179),0.58299077,Resolve bug with Finetuning (#5744),  Update README.md   Update README.md   Create evaluation.py   Update README.md   Update evaluation.py   Create evaluation.py   Create evaluation.py   Update evaluation.py   Create nlp.py   Update evaluation.py   Create evaluation.py   Update nlp.py   Update nlp.py   Update evaluation.py   Create evaluation.py   Update nlp.py   Update nlp.py   Update requirements.txt   Update evaluation.py   Create data_loader.py   Update nlp.py   Update evaluation.py   Update data_loader.py   Update nlp.py   Update data_loader.py   Update requirements.txt   Update model_checkpoint.py   Delete evaluation.py   Delete data_loader.py   Delete nlp.py   Update requirements.txt   Update model_checkpoint.py   Update README.md   Update pytorch_lightning/callbacks/model_checkpoint.py   Update CHANGELOG.md   Update test_model_checkpoint.py   Update model_checkpoint.py   update   update   chlog update   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
3493,Remove deprecated progress_bar_refresh_rate from Trainer constructor (#12514),0.8645329,"Deprecated passing progress_bar_refresh_rate to the Trainer constructor in favor of adding the ProgressBar callback with refresh_rate directly to the list of callbacks, or passing enable_progress_bar=False to disable the progress bar (#9616)",  use predefined logic   patch init_optimizers   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
3494,repair CI for Win (#2358),0.5168189,Porting fixes to autoscaler component (#16249),,0
3495,Update lr_monitor.py (#4826),0.6288043,"Resolved schedule step bug for PyTorch Profiler (#6674, #6681)",Co-authored-by: tchaton thomas@grid.ai Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
3496,Reset all results on epoch end (#14061),0.6711157,Reset current progress counters when restarting an epoch loop that had already finished (#9371),,0
3497,Add Transformers Question Answering to community examples (#2603),0.5738238,Introduce lightning connect (#14452),,0
3498,Ensure global seed exists before passing into env subprocess.Popen call (#3904),0.57676923,Check environ before selecting a seed to prevent warning message (#4743),  use public method   document   Apply suggestions from code review ,0
3499,Add root README (#13313),0.62692976,# figure out the root node addr,  reset val   chlog ,0
3500,update PR template (#5206),0.56760347,Use correct python version in lightning component template (#13790),,0
3501,All gatherwith grads (#5012),0.5261935,"@Borda, @NumesSanguis, @rohitgr7, @williamFalcon",Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
3502,Fix command line run for refinforce_learn_qnet in pl_examples  (#5414),0.49989408,Fixing critical bugs in newly added hooks and hparams assignment.,,0
3503,Fix import in doctest example (#14067),0.66337156,import argparse,  Add support for detect_anomaly   Update CHANGELOG.md ,0
3504,Remove optimizer_idx from toggle/untoggle_optimizer methods (#16560),0.7723025,Removed the deprecated optimizer_idx from training_step as an accepted argument in manual optimization (#8576),  deprecate hooks   dep todo   explicit   Apply suggestions from code review   Apply suggestions from code review   code review   base ,1
3505,Optimize non-empty directory warning check in model checkpoint callback (#9615),0.66238546,Removed a redundant warning with ModelCheckpoint(monitor=None) callback (#9875),,0
3506,Fix failing master due to an interction between PRs (#10627),0.5864928,Raised a MisconfigurationException if batch transfer hooks are overriden with IPUAccelerator (13961),  support_dict   chlog   fix test   epochs ,0
3507,Fix mypy for utilities.xla_device (#8755),0.5658997,xla_device_utils >> xla_device,  reload state on fit   trainer.state   add test   chlog   revert   review   review   rev and ammend   fix test and logic   update   code review   Apply suggestions from code review   better assertions   better assertions   Apply suggestions from code review   add loop test   Apply suggestions from code review   Split for typing   review comments   review comments   use if_else   code review   code review   code review   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   Remove unnecessary pieces from the test   move test   Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
3508,ci/hotfix: legacy workflow (#16721),0.5589572,Resolved a bug where the work statuses will grow quickly and be duplicated (#13970), rename CB Tune arg round  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
3509,link blog (#5740),0.46773767,[1.3.7post0] - 2021-06-23,Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
3510,Review APIs experimental status (#17012),0.6369412,A new stateful API,  Fix missing arguments when saving hyperparams from parent class only   fix antipattern ,0
3511,Added doc strings to wandb logger (#9109),0.66384524,Removed wandb logger's finalize method (#1193),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
3512,updated dict keys,0.8136054,Key updates,,1
3513,Fix apex version in Docker due to broken upstream (#7146),0.6021124,Resolved a bug where the install command was not installing the latest version of an app/component by default (#14181),,0
3514,rewrite and improve tests for truncated back-propagation (#9369),0.6316154,Refactor Model backward (#2276),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
3515,Refactor GPUStatsMonitor to improve training speed (#3257),1.0,Refactor GPUStatsMonitor to improve training speed (#3257),  @RunIf(min_gpus=1)   dtype -> fast_dtype ,1
3516,Remove the Trainer.prediction_writer_callbacks property (#16759),0.79559416,- Removed the `Trainer.prediction_writer_callbacks` property ([#16759](https://github.com/Lightning-AI/lightning/pull/16759)),"  Add warnings regarding unsupported keys in optim config and OneCycleLR   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Fix docstring   Update CHANGELOG.md   Split  into two parts   Use difference operator to find extra keys   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",1
3517,Fix bug in upload file endpoint (#14924),0.8205174,- Fixed a bug where the upload files endpoint would raise an error when running locally ([#14924](https://github.com/Lightning-AI/lightning/pull/14924)),,1
3518,ref: separate te from ddp (#3810),0.6815368,decoupled DDP2 (#3816),,0
3519,Move CheckpointConnector.fault_tolerant_auto_save_path out of CheckpointConnector.hpc_resume_path (#11092),0.82273954,Deprecated the use of CheckpointConnector.hpc_load() in favor of CheckpointConnector.restore() (#7652),,1
3520,filter drafts (#2897),0.5505153,Refactored optimizer (#4658),  add exception   chlog   code review   Apply suggestions from code review   Co-authored-by: thomas chaton thomas@grid.ai,0
3521,[App] Resolve race condition to move ui files (#15398),0.5280465,Resolved a bug about a race condition when sending the work state through the caller_queue (#14074),  update tests   fix more tests ,0
3522,refactor 4 (#2711),0.7099421,Refactor Model backward (#2276),,1
3523,Clean-up after logger connector redesign 2/2 (#7631),0.752037,Dramatically simplify the LoggerConnector (#7882),add Rohit Gupta to default codeowners,1
3524,"Update Introduction docs page to ""Lightning in 2 Steps"" (#12357)",0.7577076,Update the Lightning App docs (#13537),,1
3525,Remove custom log events (#16707),0.74264073,Removed LoggerStages (#5673),  re-add changes   Update test_data_parallel.py   Update CHANGELOG.md   Update test_legacy_checkpoints.py   Update test_horovod.py   Update test_horovod.py   Update accelerator_connector.py   update tests ,1
3526,Fix support for CombinedLoader while checking for warning raised with eval dataloaders (#10994),0.6654362,Removed deprecation warnings being called for on_{task}_dataloader (#9279),,0
3527,Add is_last_batch to progress tracking (#9657),0.74224013,Progress tracking,,1
3528,"[App] Fix e2e CI, use display name in show logs command (#16679)",0.6015657,Add support for printing application logs using CLI lightning show logs <app_name> [components] (#13634),,0
3529,Introduce strategies directory for Training Strategies (#11226),0.6605726,trainer = pl.Trainer(strategy=DDPFullyShardedNativeStrategy()),,0
3530,Unblock GPU CI (#8456),0.7857716,Enable non-blocking for device transfers to GPU (#1843),,1
3531,ref: decoupled ddp spawn (#3746),0.84081495,"decoupled DDP, DDP spawn (#3733, #3817, #3819, #3927)",,1
3532,add more profiler,0.63410187,Moved profilers to their own file (#7822),,0
3533,Add lightning-geometric (#4771),0.71523505,Updated metrics to use LightningEnum (#5689),,1
3534,CI: support PT 1.10 (#8133),0.5765193,The psutil package is now required for CPU monitoring (#17010),,0
3535,Docker/nvidia (#7109),0.5793168,Changed to the NeptuneLogger (#16761):,,0
3536,add copyr (#6661),0.49685293,Add code_dir argument to tracer run (#15771),,0
3537,Fix device placement when .cuda() called without specifying index (#14128),0.62653625,"device = ""cuda"" if torch.cuda.is_available() else ""cpu""",Co-authored-by: thomas chaton thomas@grid.ai,0
3538,Fix TPU Spawn gather (#6896),0.6843174,Resolve TPU miss rendezvous (#6781),,0
3539,Update cloud_io.py (#4936),0.69305724,Refactor cloud dispatch and update to new API (#16456),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
3540,[App] Fix e2e tests (#16146),0.6252794,Updated app testing (#16000),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
3541,feat(trainer): add enable_benchmarking option (#803),0.6775216,Allow easy trainer re-instantiation (#7508),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
3542,Fix mypy errors in pytorch_lightning/strategies/sharded.py (#14184),0.7579747,+ # pytorch_lightning==1.7.0,,1
3543,[feat] pp 1/n (#5016),0.6841517,[1.1.5] - 2021-01-19,Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
3544,generalize reinstantiation of dataloader (#1346),0.7865366,"Refactor dataloading, supports infinite dataloader (#955)",Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
3545,Fix metric state reset (#5273),0.76815784,Metric compute() method will no longer automatically call reset() (#5409), Make HorovodPlugin.all_gather consistent with other plugins,1
3546,Fix mypy errors attributed to pytorch_lightning.loggers.tensorboard.py (#13688),0.6786181,Deprecated pytorch_lightning.logging (#767),"  wip   reset _notebooks   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   reset _notebooks   testing with mock   update test with mock   update test   update tests   update test   remove track_load_dataloader_calls   update last test   remove unused imports   remove InternalDebugger   update changelog   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com",0
3547,Fix ImportErrors on Multinode if package not present (#15963),0.717247,Module imports are now catching ModuleNotFoundError instead of ImportError (#9867),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
3548,Add support for functions to be parsed by the Lightning CLI in addition to Types (#8400),0.72156334,"LightningCLI now supports registries for callbacks, optimizers, learning rate schedulers, LightningModules and LightningDataModules. This greatly improves the command line experience as only the class names and arguments are required as follows:",,1
3549,Deprecate LightningDataModule lifecycle properties (#7657),0.7609844,Deprecated LightningModule.model_size (#8343),  dump global_step   add test   chlog ,1
3550,"Update s3fs requirement from <2022.8.3,>=2022.5.0 to >=2022.5.0,<2022.11.1 in /requirements (#16198)",0.57386005,Generalized Optimizer validation to accommodate both FSDP 1.x and 2.x (#16733),  Deprecate stochastic_weight_avg from the Trainer constructor   Update CHANGELOG.md   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
3551,Update docs for devices flag (#10293),0.57303154,"Raise MisconfigurationException when the accelerator is available but the user passes invalid ([]/0/""0"") values to the devices flag (#12708)",  update warnings   add tests   comments   Apply suggestions from code review   Apply suggestions from code review ,0
3552,Fixed typo Havana -> Habana for HPUs (#15589),0.47192872,Support **DictConfig for hparam serialization (#2519),  Deprecate LightningDistributed and keep logic in ddp/ddpSpawn directly   Deprecate LightningDistributed and keep logic in ddp/ddpSpawn directly   Deprecate LightningDistributed and keep logic in ddp/ddpSpawn directly   Deprecate LightningDistributed and keep logic in ddp/ddpSpawn directly   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Apply suggestions from code review  Co-authored-by: ananthsub ananth.subramaniam@gmail.com   Deprecate LightningDistributed and keep logic in ddp/ddpSpawn directly   Update pytorch_lightning/distributed/dist.py   Co-authored-by: ananthsub ananth.subramaniam@gmail.com   Deprecate LightningDistributed and keep logic in ddp/ddpSpawn directly   Apply suggestions from code review   Co-authored-by: ananthsub ananth.subramaniam@gmail.com   Apply suggestions from code review   Apply suggestions from code review   Deprecate LightningDistributed and keep logic in ddp/ddpSpawn directly   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com,0
3553,CI: Update XLA from 1.9 to 1.12 (#14013),0.5107214,Used XLA utility API to move data to CPU (Single TPU core) (#8078),"  pt1 dir empty check   clean imports   bring back resolve mkdir?   original doc   warningcache   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   cp callback after resolve   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  move global_zero check outside warn fn  Co-authored-by: ananthsub ananth.subramaniam@gmail.com  move global_zero check outside warn fn 2  Co-authored-by: ananthsub ananth.subramaniam@gmail.com  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com",0
3554,clarify forward (#3611),0.6668955,[1.1.7] - 2021-02-03,  unique filename   chlog   update tests ,0
3555,Update versions after recent PyTorch releases (#9623),0.7635003,"Following our 4 PyTorch release window, this release supports PyTorch 1.8 to 1.11. Support for PyTorch 1.7 has been removed.",,1
3556,pin virtualenv (#822),0.5062766,Initial plugin server (#16523),,0
3557,Enable Probot CheckGroup v4 (#15649),0.5948013,Configuration Validator (#9779),,0
3558,WandbLogger to log model topology by default (#8662),0.7487793,Changed WandbLogger(log_model={True/'all'}) to log models as artifacts (#6231),  fix accumuate_grad_batches on init   chlog   update error   move to callback connector   add test with callback   fix tests   Update pytorch_lightning/trainer/connectors/callback_connector.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   update ipu logic   rev   rev   rev   pls work   code review   Co-authored-by: Rohit Gupta goku@rmac.local Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
3559,Add TRAINS experiment manager support (#1122),0.63719606,"Refactor RunningStage and TrainerState usage (#4945, #7173)",Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
3560,add forgotten change logs (#1380),0.75092775,Complete changelog,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
3561,Move the CombinedLoader to an utility file (#16819),0.6390568,from lightning.pytorch.utilities import CombinedLoader,"  wip   reset _notebooks   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   reset _notebooks   testing with mock   update test with mock   update test   update tests   update test   remove track_load_dataloader_calls   update last test   remove unused imports   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
3562,ref: separate properties (#3432),0.5800315,New properties,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
3563,added community examples (#1030),0.503896,Contributors,,0
3564,Copy batch for local forward (#532),0.51432955,Pass batch outputs to on_train_batch_end instead of epoch_end outputs (#4369),,0
3565,Prevent bug when launching apps on multiple clusters (#15226),0.78641534,- Fixed a bug when launching an app on multiple clusters ([#15226](https://github.com/Lightning-AI/lightning/pull/15226)),,1
3566,Merge pull request #1634 from PyTorchLightning/remove_warning,0.679651,- Removed `configure_sync_batchnorm` from `ParallelStrategy` and all other strategies that inherit from it ([#11754](https://github.com/PyTorchLightning/pytorch-lightning/pull/11754)),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
3567,Fault Tolerant Manual: Add loading to reload the states (#10699),0.7077767,- Fault Tolerant Manual,,1
3568,readme typo (#17098),0.52797365,Changed overwrite to True (#16009),,0
3569,Docs: sync chlog 1.3.1 (#7478),0.64668536,Remove .item which causes sync issues (#1254),,0
3570,Fix argparse docs (#7148),0.68406844,argparse_utils >> argparse,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
3571,CI: fix ignored paths (#13733),0.6216308,Avoid relpath bug on Windows (#16164),,0
3572,Apply isort to pl_examples/ (#5291),0.63989985,Simplify the PL examples structure (shallower and more readable) (#1247), reset metrics,0
3573,Add log output for slurm (#1657),0.6454122,To log:,,0
3574,CI: move azure-pipelines config to separate directory (#7276),0.6216116,Changed Checkpoint path parameter from filepath to dirpath (#1016),,0
3575,validation and training loops run the partial dataset (#1192),0.7433281,Refactored result handling in training loop (#7506),,1
3576,add missing typing to trainer properties (#5974),0.67997855,adding Trainer.tune() (#3293),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
3577,"Deprecate callback hooks on_pretrain_routine_{start,end} (#11794)",0.73584294,Removed deprecated early_stop_callback (#3982),,1
3578,"Modify accelerator reference model to property, change name to reflect func",0.5430356,Refactored into accelerator module:,,0
3579,Rename IPUPlugin to IPUStrategy (#11193),0.7995019,    * Renamed the `IPUPlugin` to `IPUStrategy` ([#11193](https://github.com/PyTorchLightning/pytorch-lightning/pull/11193)),,1
3580,Basic examples fixes (#5912),0.6701262,Fixing critical bugs in newly added hooks and hparams assignment.,,0
3581,Add step index in checkpoint name (#3807),0.58480585,Checkpoint and early stopping now work without val. step (#1041),,0
3582,"Fix setting device when creating ""inf"" monitor value in ModelCheckpoint (#10118)",0.77790105,Allow ModelCheckpoint monitor to be None (#3633),  Fix gradient accumulation for ShardedDataParallel   Update changelog   Update pytorch_lightning/plugins/training_type/sharded.py   add test   Update test_sharded_plugin.py   Update test_sharded_plugin.py   Update test_sharded_plugin.py ,1
3583,Remove unused rank_zero_deprecation in WandB logger (#9034),0.7263034,Removed restrictions in the Trainer that loggers can only log from rank 0; the existing logger behavior has not changed (#8608),,1
3584,added tests for the training epoch end (#3967),0.7156478,"Deprecated model steps training_end, validation_end and test_end (#1051, #1056)",,1
3585,[App] Enable listing at project level 2/n (#16622),0.5495356,Add support for listing Lightning AI apps (#13987),,0
3586,Update test_tensorboard.py,0.5907369,Improved the error message for installing tensorboard or tensorboardx (#17053),,0
3587,"Fix LightningModule.{un,}toggle_model when only 1 optimizer is used (#12088)",0.9039327,"- Fixed `LightningModule.{un,}toggle_model` when only 1 optimizer is used ([#12088](https://github.com/PyTorchLightning/pytorch-lightning/pull/12088))",,1
3588,Fix test configuration check and testing (#1804),0.72565454,Refactored setup_training and remove test_mode (#5388),"  deprecate loggerbase.close   deprecate warning   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   add to changelog   fix import   fix import alphabetize   spacing?   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   copy-paste avoid pre-commit.ci?   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   literally match the other comment   unindent   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   suggest finalize instead of save   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Update tests/loggers/test_base.py   format but to be formatted   Update pytorch_lightning/loggers/base.py   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Update pytorch_lightning/loggers/base.py   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  Update pytorch_lightning/loggers/base.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",1
3589,Skip CircleCI trigger for forks (#14930),0.5004229,Dropped the LightningCLI ArgumentParser when pickling (#8017),,0
3590,Add back GPUAccelerator and deprecate it,0.8023186,"When everything works, switch back to GPU by changing only the accelerator. Check our documentation for more useful debugging tricks.",,1
3591,relax redis requirement (#14008),0.5333035,Refactor load in checkpoint connector (#4593),,0
3592,Remove more deprecated methods from base Accelerator class (#10448),0.8551496,- Removed deprecated passthrough methods and properties from `Accelerator` base class:,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: thomas chaton thomas@grid.ai,1
3593,Deprecate stochastic_weight_avg from the Trainer constructor (#8989),0.883723,Deprecated passing stochastic_weight_avg to the Trainer constructor in favor of adding the StochasticWeightAveraging callback directly to the list of callbacks (#8989),  Document exceptions in ipu.py   Document exceptions in tpu.py   Document exceptions in gpu.py ,1
3594,"Update fairscale requirement from <=0.4.6,>=0.4.5 to >=0.4.5,<0.4.13 in /requirements (#15842)",0.5948527,Avoided requiring the FairScale package to use precision with the fsdp native strategy (#14092),Co-authored-by: Sean Naren sean@grid.ai,0
3595,Pass outputs from all dataloaders to test_end and validation_end (#203),0.6991626,validation_end >> validation_epoch_end, fix hyperparameter logging between LightningModule and DataModule,0
3596,Support special test parametrizations (#10569),0.6884364,test_percent_check in favour of limit_test_batches,Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
3597,updated readme,0.51689994,Full Changelog,,0
3598,releasing,0.77437156,This release includes:,,1
3599,Remove AcceleratorConnector.devices (#12435),0.78603876,- Removed `AcceleratorConnector.devices` property ([#12435](https://github.com/PyTorchLightning/pytorch-lightning/pull/12435)),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
3600,Add required states for resumed ModelCheckpoint GC (#10995),0.70560974,Forced ModelCheckpoint callbacks to run after all others to guarantee all states are saved to the checkpoint (#5731),,1
3601,fix typo error in docstring of LightningLoggerBase.after_save_checkpoint (#8737),0.67075795,Improved the error message when the LightningWork is missing the run method (#14759),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
3602,Merge pull request #44 from Borda/extend-CI,0.59315443,"merge backends (#3476, #3477, #3478, #3480, #3482)",,0
3603,Add ColossalAI strategy (#14224),0.66885126,Colossal-AI, Add test Accept TypeError for arg_type.args being None  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
3604,Remove redundant optimizer_idx reset on epoch (#9285),0.7120632,Removed the deprecated optimizer_idx from training_step as an accepted argument in manual optimization (#8576),,1
3605,ruff: autofix PT (#17541),0.69664645,Porting fixes to autoscaler component (#16249),,0
3606,Remove deprecated HPC model hooks (#14315),0.8407111,Removed deprecated model hooks (#3980),Co-authored-by: Ethan Harris ethanwharris@gmail.com,1
3607,Improve typing for Lite (#10743),0.7180628,Refactored setup for typing friendly (#6590),,1
3608,add trainer attribute to denote if interrupted (#1368),0.6341269,Deprecate early_stop_callback Trainer argument (#3845),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
3609,Add flash[image] dependency in Active learning example (#13442),0.53686833,"        image, _ = self.trainer.train_dataloader.loaders.dataset[0]",,0
3610,Add Ethan as Lightning App CodeOwners (#15626),0.6703731,Lightning App,Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
3611,pypi azure badges - tags (#6068),0.5713399,- Marked `trainer.checkpoint_connector` as protected ([#11550](https://github.com/PyTorchLightning/pytorch-lightning/pull/11550)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
3612,Skips DDP parameter sync (#4301),0.7069471,Ensure the existence of DDPPlugin._sync_dir in reconciliate_processes (#8939),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
3613,Doc Fix: Passing datamodule argument to trainer.tune is supported (#12406),0.6798266,Removed support for passing a bool value to profiler argument of Trainer (#6164),,0
3614,Remove ProfilerConnector class (#7654),0.9142746,Removed ProfilerConnector (#7654),,1
3615,Fix error handling in learning rate finder (#13845),0.8305218,Learning Rate Finder (#13802), [RFC] Skip reconciliate_processes if used within a cluster environment that creates processes externally,1
3616,fix amp wrong call,0.70489466,enable_pl_optimizer=False by default to temporarily fix AMP issues (#5163),,1
3617,Update trainer.rst (#4722),0.7233857,"Refactor RunningStage and TrainerState usage (#4945, #7173)", Free references to data fetcher in data connector teardown,1
3618,[Fix] Call clip gradients if clip val greater than 0 (#6330),0.8986771,Ensure that clip gradients is only called if the value is greater than 0 (#6330),Co-authored-by: ananthsub ananth.subramaniam@gmail.com,1
3619,Fix typing in pl.trainer.config_validator (#10803),0.67896295,trainer = pl.Trainer(plugins=SLURMEnvironment(auto_requeue=False)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
3620,Change PR template (#2224),0.54149103,Use correct python version in lightning component template (#13790),,0
3621,fix install dtrun (#6025),0.59965074,"refactor DDP backend (#3204, #3207, #3208, #3209, #3210)",,0
3622,Remove the deprecated DDP2 strategy (#14026),0.8931852,Removed support for the DDP2 strategy,,1
3623,add osx to Travis (#202),0.49015957,adding Trainer.tune() (#3293),,0
3624,prepare_data before spawns (#1067),0.6258534,use prepare_data to download and process the dataset.,,0
3625,Remove hardcoding of rank_zero_only.rank in accelerator connector (#6878),0.8963872,Remove hardcoding of local rank in accelerator connector (#6878),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
3626,Feature: LightningDataModule.from_datasets(...) (#5133),0.7689888,LightningDataModule.load_from_checkpoint,,1
3627,Assistant for Unified Package (#15207),0.6006012,Renamed utils modules (#5199),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
3628,Added SaveConfigCallback.save_config (#17475),0.67141587,  * `save_config_overwrite`,,0
3629,DeepSpeed support for device IDs (#9847),0.7155001,Ensure we check deepspeed/sharded in multinode DDP (#6297),  Handle collision of user argument   Add CHANGELOG.md ,1
3630,"Added basic get model tests, add better typing",0.65071225,- Full tests that run multiple models in different configs,,0
3631,Added Vulture dead code checker (#5654),0.47471485,Deprecated @data_loader decorator  (#926),,0
3632,test_cpu and test_gpu EvalModelTemplate deprecation (#4820),0.6375591,"trainer = pl.Trainer(callbacks=DeviceStatsMonitor(cpu_stats=False), accelerator=""gpu"")",,0
3633,Deprecate pl/core/mixins/device_dtype_mixin and update imports (#14511),0.837564,| Import pl.core.mixins.DeviceDtypeModuleMixin                                                           | 1.10             | No longer supported |,,1
3634,Disable attaching samplers when using IterableDataset (#11507),0.87629527,Disabled sampler replacement when using IterableDataset (#11507),,1
3635,Added atomic checkpoint creation (#689),0.5575124,Resuming from checkpoints (#16167),,0
3636,[App] Improve the autoscaler UI (#16063),0.6429455,Porting fixes to autoscaler component (#16249),,0
3637,Merge branch 'hparams_from_checkpoint' of https://github.com/neggert/pytorch-lightning,0.6811367,- Added profiling for `on_load_checkpoint`/`on_save_checkpoint` callback and LightningModule hooks ([#12149](https://github.com/PyTorchLightning/pytorch-lightning/pull/12149)),  add predict   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
3638,docfix (#3463),0.7333838,Docs,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
3639,[docs] Add Cotratron to community examples (#2130),0.49829578,Allow logging of metrics together with hparams (#1630),,0
3640,Add support for pluggable Accelerators (#12030),0.79959726,- Added support for pluggable Accelerators ([#12030](https://github.com/PyTorchLightning/pytorch-lightning/pull/12030)),  Small doc fixes   remove space   small fix   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
3641,bug free release,0.5876212,0.4.0 is the first public release after a short period testing with public users. Thanks for all the help ironing out bugs to get Lightning to run on everything from notebooks to local to server machines.,,0
3642,add training loop docs,0.6934111,Refactored training loop (#2336),Fixes #9431,0
3643,ci/gpu: fix install future & use local cache (#16929),0.53412443,Fix hanging in DDP HPC accelerators (#5157),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: tchaton thomas@grid.ai Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
3644,Deprecate GPUStatsMonitor and XLAStatsMonitor in favor of DeviceStatsMonitor (#9924),0.94718397,Deprecated GPUStatsMonitor and XLAStatsMonitor in favor of DeviceStatsMonitor callback (#9924),,1
3645,Fix warning message formatting in save_hyperparameters (#12498),0.66307,Moved save_hyperparameters to its own function (#7119),Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,0
3646,Remove methods with unnecessary super delegation. (#8148),0.581782,Add function to remove checkpoint to allow override for extended classes (#16067),add neptune staff as code owners for neptune.py,0
3647,Sphinx generated documentation (#521),0.60258603,Pinned sphinx-autodoc-typehints with <v1.15 (#11400),"  Initial split to NeptuneLegacyLogger and NeptuneLogger   Adapt NeptuneLogger to neptune-pytorch-lightning repo version   Fix stylecheck and tests   Fix style and PR suggestions   Expect Run object in NeptuneLogger.init   Model checkpoint support and restructured tests   Reformat code - use "" instead of '   Fix logging INTEGRATION_VERSION_KEY   Update CHANGELOG.md   Fix stylecheck   Remove NeptuneLegacyLogger   updated neptune-related docstrings   PR suggestions   update CODEOWERS file  move import logic to imports.py  minor neptune.py improvements   formatting fixes and minor updates   Fix generation of docs   formatting fixes and minor updates   fix   PR fixes vol. 2   define return type of _dict_paths method   bump required version of neptune-client   Enable log_* functions   Update pytorch_lightning/loggers/neptune.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Revert ""Enable log_* functions""  This reverts commit 050a436899b7f3582c0455dc27b171335b85a3a5.   Make global helper lists internal   Logger's name and version methods return proper results   Initialize Run and its name and id at logger init level   Make _init_run_instance static   Add pre-commit hook code changes.   Fix blacken-docs check   Fix neptune doctests and test_all   added docs comment about neptune-specific syntax   added docs comment about neptune-specific syntax in the loggers.rst   fix   Add pickling test   added myself to neptune codeowners   Enable some of deprecated log_* functions   Restore _run_instance for unpickled logger   Add step parameter to log_* functions   Fix stylecheck   Fix checkstyle   Fix checkstyle   Update pytorch_lightning/loggers/neptune.py   Co-authored-by: thomas chaton thomas@grid.ai   Fix tests   Fix stylecheck   fixed project name   Fix windows tests   Fix stylechecks   Fix neptune docs tests   docformatter fixes   De-duplicate legacy_kwargs_msg   Update .github/CODEOWNERS   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: kamil-kaczmarek kamil.kaczmarek@neptune.ml Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: thomas chaton thomas@grid.ai",0
3648,Replace yapf with black (#7783),0.51020896,Renamed TPUHalfPrecisionPlugin to TPUBf16PrecisionPlugin (#10026),,0
3649,Deprecate Trainer.verbose_evaluate (#10931),0.77195275,"trainer = Trainer(callbacks=GradientAccumulationScheduler({""1"": 5, ""10"": 3}))",,1
3650,Add outputs param for on_val/test_epoch_end hooks (#6120),0.64535046,test_end >> test_epoch_end,,0
3651,Update Lightning Lite docs (2/n) (#16239),0.8637446,Update the Lightning App docs (#13537),Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
3652,[CLI] Support custom trainers without callbacks (#13138),0.81225955,- Added support for using custom Trainers that don't include callbacks using the CLI ([#13138](https://github.com/Lightning-AI/lightning/pull/13138)),,1
3653,Fix type hints of tuner/batch_size_scaling.py (#13518),0.77825165,tuner.scale_batch_size(...),,1
3654,warn user when dropping unpicklable hparams (#2874),0.7335563,Don't raise a warning when nn.Module is not saved under hparams (#12669),Co-authored-by: Jirka jirka.borovec@seznam.cz,1
3655,Small cleanup when dataloader states are saved (#11843),0.7449462,clean up data reset (#3161),Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
3656,Merge DDPStrategy and DDPSpawnStrategy in PL (#16809),0.86279786,Merged the implementation of DDPSpawnStrategy into DDPStrategy and removed DDPSpawnStrategy (#14952), [loops] Reset reference to dataloader iterator on run end,1
3657,ref: modular is_overridden (#3290),0.9745927,modular is_overridden (#3290),,1
3658,Fix mypy checks for double precision plugin (#7151),0.6685022,precision plugins (#3504), Move get_progress_bar_dict from lightning module to progress bar callback,0
3659,fix mypy typing errors in pytorch_lightning.utilities.data.py (#13901),0.74108726,+ # pytorch_lightning==1.7.0,  convert state to tuple explicitly   update changelog ,1
3660,[App] Hot fix: Resolve detection of python debugger (#16068),0.62271094,Removed pytorch_lightning.utilities.debugging.InternalDebugger (#9680),,0
3661,Fix master merge conflict (#11858),0.5711233,Fixing critical bugs in newly added hooks and hparams assignment.,,0
3662,Group argument wandb (#1760),0.5552486,group prepare data hook (#3212),,0
3663,Fix mypy typing for utilities.apply_func (#8781),0.5165416,- Added an error message when attempting to launch processes with `python -i` and an interactive-incompatible strategy ([#15293](https://github.com/Lightning-AI/lightning/pull/15293)),,0
3664,Default value for ModelCheckpoint filepath (#1548),0.8796668,Deprecated filepath in ModelCheckpoint (#4213), Fix logging of nan parameters,1
3665,Fix property raised instead of returned (#17595),0.56414104,    # returning a value here is no longer supported,Co-authored-by: ananthsub ananth.subramaniam@gmail.com,0
3666,Increase TPU Check timeout (#7706),0.90711606,Increased TPU check timeout from 20s to 100s (#5598),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: thomas chaton thomas@grid.ai,1
3667,"Fixed configure optimizer from dict without ""scheduler"" key (#1443)",0.68159974,"    def configure_optimizers(lightning_module, optimizer, lr_scheduler=None):",,0
3668,[HotFix] Logging - One epoch delay on training epoch metrics. (#4913),0.7350434,Log epoch metrics before the on_evaluation_end hook (#7272),,1
3669,moved dataloaders after amp and optimizers,0.61997855,"Moved the optimizer_step and clip_gradients hook from the Accelerator and TrainingTypePlugin into the PrecisionPlugin (#10143, #10029)",,0
3670,Added basic file logger (#2721),0.6942744,moved eval loop logging to loggers (#3408),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
3671,Fix non autoplaying videos on Safari iOS (#3343),0.4854517,Running an app without a UI locally no longer opens the browser (#15875),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: thomas chaton thomas@grid.ai,0
3672,Remove the deprecated Trainer device arguments (#16171),0.7631675,Removed the deprecated TrainerTrainingTricksMixin class (#8679),,1
3673,Add support for torch.use_deterministic_algorithms (#9121),0.69441164,Set Torch inference mode for prediction (#15719),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
3674,Fix broken code in docs (#5859),0.5764849,Resolve bug with Finetuning (#5744),,0
3675,fix imports in examples,0.7196179,import argparse,,1
3676,build dockers XLA 1.7 (#4891),0.6130607,xla_device_utils >> xla_device,Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
3677,fix dp reduction test (#6404),0.6887928,DDP Debugging Improvements,,0
3678,fix: training_step_end doesn't work as stated in docs (#8188),0.6867034,Move training_output validation to after train_step_end (#7868),"Without clearing this reference, the loss tensor stays live through the next training step. This can be a problem for memory intensive models that produce very deep backward graphs such as neural ODEs. For these models, keeping the backward graph of the previous loss in memory can lead to OOM errors in the next training step even though the step might have succeeded if we had cleared (and thus GC'd) the previous backward graph. Co-authored-by: tchaton thomas@grid.ai Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com",0
3679,PR for Issue#3114 (#3741),0.6165542,[1.7.4] - 2022-08-31, CI: precommit - docformatter fix deprecated  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
3680,Docs clean up 1/n (#12477),0.6884974,"Cleaning (#5948, #5949, #5950)",,0
3681,Fixes automatic parser bug (#1585),0.5924884,"device parser (#3400, #3405)",,0
3682,fix metric name to work with default earlystopping (#628),0.662164,Change Metrics persistent default mode to False (#4685),,0
3683,Remove the deprecated GPUAccelerator (#16050),0.77500844,- Removed the deprecated automatic GPU selection ([#16184](https://github.com/Lightning-AI/lightning/pull/16184)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
3684,"cleaned docs, fixed argparse generator (#1075)",0.5949149,Removed deprecated BaseProfiler.output_filename arg from it and its descendants in favor of dirpath and filename (#9214),"  Update parsing.py   add todo (for single arg)   unblock non container single arg   init test   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Update CHANGELOG.md   pep8 line length   Update pytorch_lightning/utilities/parsing.py   remove dict namespace conversion   add omegaconf support   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   add dict test   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   add omegaconf test   Update CHANGELOG.md   Update pytorch_lightning/utilities/parsing.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/utilities/parsing.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",0
3685,move pl/utilities/cloud_io.py to lite/utilities/cloud_io.py (#14515),0.7061262,- Deprecated all functions in `pytorch_lightning.utilities.cloud_io` in favor of `lightning_lite.utilities.cloud_io` ([#14515](https://github.com/Lightning-AI/lightning/pull/14515)),,1
3686,fix py 1.1.0 for now,0.74958503,PyTorch 1.5  support, reset progress updates update docs add test  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
3687,Support for multiple val_dataloaders (#97),0.7126628,"for data: val_dataloader, test_dataloader, train_dataloader",  replace dev debugger in early stopping   remove unused imports ,1
3688,add state_dict/load_state_dict to base Callback (#11998),0.6982515,Called on_load_checkpoint before loading state_dict (#4057), Avoid deprecation warnings being called when hooks are not implemented Update tests & changelog Apply suggestions from code review  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
3689,update by flake8,0.5396029,Update the Lightning App docs (#13537),  add on_exception callback hook   deprecate on_keyboard_interrupt   Apply suggestions from code review   raise keyboard interrupt   Delete cluster   update changelog   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
3690,CI: prune dependency for benchmarks (#15879),0.5353465,Pruned requirements duplicity (#13739),,0
3691,Added getstate/setstate method for torch.save serialization (#4127),0.5811397,Support **DictConfig for hparam serialization (#2519),,0
3692,Integrate lightning_utilities is_overridden (#14620),0.7617735,Introduce lightning connect (#14452),Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
3693,Make print_nan_grads print grad (#208),0.7531524,print(trainer.num_devices),Co-authored-by: SeanNaren sean@grid.ai Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
3694,Remove check_checkpoint_callback (#7724),0.71152925,Removed deprecated early_stop_callback (#3982),,1
3695,[docs] Add step to ensure sync_dist is adding to logging when multi-gpu enabled (#4817),0.649871,Support number for logging with sync_dist=True (#5080),,0
3696,Fix wandb save_dir is not overridden by None dir when using CLI (#14878),0.855849,- Fixed wandb `save_dir` is overridden by `None` `dir` when using CLI ([#14878](https://github.com/Lightning-AI/lightning/pull/14878)),,1
3697,Removed a redundant warning with ModelCheckpoint(monitor=None) callback (#9875),1.0,Removed a redundant warning with ModelCheckpoint(monitor=None) callback (#9875),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com,1
3698,Add missing self to setup hook example (#11041),0.6227311,Explicitly disallow calling self.log(on_epoch=False) during epoch-only or single-call hooks (#7874),,0
3699,CI: Enable Python 3.10 in full CPU testing (#13829),0.61196357,Skipped testing with PyTorch 1.7 and Python 3.9 on Ubuntu (#11217),,0
3700,Fairscale import updates (#14721),0.6545081,import argparse,,0
3701,Remove redundant None check from spawn plugins (#10855),0.6409503,"    * All spawn-based plugins now spawn processes immediately upon calling `Trainer.{fit,validate,test,predict}`",,0
3702,move accelerator_connector.py to the connectors subfolder (#6033),0.98450357,Moved accelerator_connector.py to the connectors subfolder (#6033),,1
3703,DDP Parity tests as standalone task (#17503),0.61960995,"ddp = pl.strategies.DDPStrategy(process_group_backend=""fairring"")",,0
3704,added summary flag,0.61882144,"In the previous release, we added shorthand notation support for registered components. In this release, we added a flag to automatically register all available components:",Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
3705,Refactor _get_rank utility to take strategy instead of trainer (#14546),0.7333901,"trainer = pl.Trainer(strategy=BaguaStrategy(algorithm=""gradient_allreduce"")  # default",  Allow easy CLI trainer re-instantiation   Update CHANGELOG   Allow passing any trainer argument   Do not modify the previous config ,1
3706,save checkpoints and profiler output to the first logger (#14325),0.78278077,"- When using multiple loggers, by default checkpoints and profiler output now get saved to the log dir of the first logger in the list ([#14325](https://github.com/Lightning-AI/lightning/pull/14325))",  added doc strings to base logger   updated docs ,1
3707,Avoid calling average_parameters multiple times per optimizer step (#12452),1.0,Avoid calling average_parameters multiple times per optimizer step (#12452),  scheduled removal of auto_move_data decorator   update CHANGELOG.md   remove unused import   remove test_decorators.py   fix missed merge conflict   Co-authored-by: thomas chaton thomas@grid.ai,1
3708,try GH actions cache (#1558),0.47701883,Moved save_function to accelerator (#6689), added support and checks required for use of datamodule as python dataclass made changes required for dataclass support for LightningDataModule and required tests made the code compliant with future releases edited tests - removed training call. left dataclass decorator to defaults. added tests to check for multilevel inheritence and make sure init isn't called on the parent of defined class modified new to ensure calling of init on LightningDataModule impliciltly added relevant tests for multilevel inheritence cases removed default values from tests  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: tchaton thomas@grid.ai Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
3709,added bug report model (#3901),0.6343926,Fixing critical bugs in newly added hooks and hparams assignment.,,0
3710,Add lightning_app docs (#13656),0.85582215,Update the Lightning App docs (#13537),,1
3711,Use kubectl to get logs from TPU CI instead of gcloud logging. (#2918),0.6181284,- Fixed logging to loggers with multiple eval dataloaders ([#12454](https://github.com/PyTorchLightning/pytorch-lightning/pull/12454)),,0
3712,Let Accelerator inherit from ABC to make sure abstractmethod takes effect (#11521),0.6314115,move specific accelerator code (#3457),,0
3713,[App] Fix bug where previously deleted apps cannot be re-run from the CLI (#16082),0.6053363,Introducing CLI commands for apps (#13602)!,,0
3714,Clarify checkpoint deprecation message (#4640),0.6834472,Resuming from checkpoints (#16167),Co-authored-by: tchaton thomas@grid.ai,0
3715,Changed the model size calculation using ByteCounter (#10123),0.9999999,Changed the model size calculation using ByteCounter (#10123),,1
3716,move base req. to root (#4219),0.9828539,Moved base req. to root (#4219),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Sean Naren sean@grid.ai Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk,1
3717,Add an additional check to ensure pipe checks supported fairscale version (#5853),0.58367914,support for latest test-tube logger optimized for PT 1.2.0.   ,,0
3718,Improved docs for Loggers (#1484),0.73452425,Versioned logs for all loggers.   ,  Avoid wrapping LightningModule in DDP plugins when not fitting   Avoid wrapping LightningModule in DDP plugins when not fitting ,1
3719,Fix doc typo (#2773),0.5514241,Docs,Fixes #8799,0
3720,removed auto val reduce (#2462),0.99027145,Removed auto val reduce (#2462),Co-authored-by: thomas chaton thomas@grid.ai,1
3721,remove deprecated early_stop_callback (#3982),0.9868861,Removed deprecated early_stop_callback (#3982),,1
3722,Maverick registration (#16913),0.5843891,Callback registration through entry points,Co-authored-by: tchaton thomas@grid.ai,0
3723,Example: Simple RL example using DQN/Lightning (#1232),0.69117403,Example using the LightningCLI:,,0
3724,Add the bound instance as method parameter (#8466),0.5349834,"    # Now, the state for this callback gets passed to this new method",,0
3725,Simplify LightningOptimizer (#10224),0.7678026,LightningOptimizer manual optimizer is more flexible and expose toggle_model (#5771),,1
3726,Specify Trainer(benchmark=False) in parity benchmarks (#13182),0.73963755,"Standardized the dataloaders arguments of trainer.{fit,valdiate,test,tune} (#7431)",,1
3727,Removed broken link (#4031),0.6304969,  * Removed `Loop.connect()` ([#16384](https://github.com/Lightning-AI/lightning/pull/16384)),,0
3728,add desc for minimize (#3216),0.62219274,reduced all simplified forward (#3126),,0
3729,Prepare Namespace package (#1543),0.6278305,Implemented ready for components (#16129),,0
3730,Integrate lightning_utilities.core.apply_func (#14537),0.8527224,- Integrate the `lightning_utilities` package (,,1
3731,"Fix log(sync_dist=True, on_epoch=True, on_step=True) not reducing on step (#10227)",0.6887771,Note: you will want to avoid logging with on_epoch=True in case of max_steps=-1.,,0
3732,Fix name,0.6009247,Rename failed -> error in tables (#15608),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
3733,update bug_report model links and notebook (#10665),0.6421143,Fixing critical bugs in newly added hooks and hparams assignment.,,0
3734,Update macos ci (#13794),0.48621973,Updated Multinode Warning (#16091),,0
3735,drop DDP CLI test (#5938),0.63567525,Silenced some warnings. verified ddp refactors (#3483),,0
3736,Fix an import deprecation warning (#8687),0.6870851,Re-Enable Logger's ImportErrors (#1938),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
3737,DataModule compatiblity with Python dataclass (#9039),0.6941425,Compatibility for Python 3.10,,0
3738,Only run TPU standalone tests (#16586),0.6907413,Updated logic for checking TPUs availability (#6767),,0
3739,Adds shortcut for path to log (#4573),0.6678958,Return the default_root_dir as the log_dir when the logger is a LoggerCollection (#8187),,0
3740,Add checkpoint parameter to on_save_checkpoint (#6072),0.74749947,"-def on_save_checkpoint(self, checkpoint):",Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
3741,Rename CollaborativeStrategy to HivemindStrategy (#13388),0.7826923,- Hivemind Strategy,,1
3742,Update lr_finder.rst (#3714),0.7545786,move lr_finder (#3434),,1
3743,Squeeze tensor while logging (#14489),0.8008144,Squeezed tensor values when logging with LightningModule.log (#14489),Co-authored-by: Sean Naren sean@grid.ai Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: thomas chaton thomas@grid.ai,1
3744,Bump docker/setup-buildx-action from 1 to 2 (#13618),0.5793086,Expose RunWorkExecutor to the work and provides default ones for the MultiNode Component (#15561),,0
3745,LiteDataLoader wrapper improvements (#10297),0.69655573,Deprecated LightningDataParallel in favor of new wrapper module LightningParallelModule (#5670),,0
3746,Fix CometML tests (#585),0.6791894,Using .comet.config file for CometLogger (#1913),,0
3747,Remove unnecessary generator (#8470),0.61889035,- Removed `AcceleratorConnector.num_gpus` property ([#12384](https://github.com/PyTorchLightning/pytorch-lightning/pull/12384)),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
3748,Moves hpc auto-resubmit to trainer from test-tube (#207),0.66739947,trainer = pl.Trainer(plugins=SLURMEnvironment(auto_requeue=False)),,0
3749,Add TPUPrecisionPlugin (#10020),0.80340004,Renamed TPUHalfPrecisionPlugin to TPUBf16PrecisionPlugin (#10026),Co-authored-by: tchaton thomas@grid.ai,1
3750,Add catches around fairscale installation,0.54606223,Fixing critical bugs in newly added hooks and hparams assignment.,,0
3751,Link to PyTorch in FSDP docs (#17013),0.67704606,PyTorch,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
3752,Horovod: adjust base LR used by schedulers to scale with the number of workers (#2626),0.60253906,"        scheduler = TanhLRScheduler(optimizer, ...)",,0
3753,Set development version to 1.9.0dev (#15472),0.704703,Set version as today (#13906),,1
3754,updates teardown to account for ddp (#2389),0.70308053,"refactor DDP backend (#3204, #3207, #3208, #3209, #3210)",,1
3755,[Fix] Remove DeepSpeed Plugin FP16 exception (#8462),0.88990855,Removed DeepSpeed FP16 Exception as FP32 is now supported (#8462),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
3756,update mergify (#5608),0.64602417,[0.6.0] - 2022-09-08,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
3757,test for Enable setting property (#15755),0.5736308,Enabling val/test loop disabling (#2692),,0
3758,Remove experiment property from abstract class (#11603),0.5553206,Removed attributes and methods:,Co-authored-by: tchaton thomas@grid.ai Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
3759,Fix incorrect imports in lightning docs (#14678),0.6854539,- Fixed an exception that would occur when creating a `multiprocessing.Pool` after importing Lightning ([#15292](https://github.com/Lightning-AI/lightning/pull/15292)),,0
3760,Update fastapi requirement from <0.83.0 to <0.87.0 in /requirements (#15564),0.5679181,Updated precision attributes in DeepSpeedPlugin (#10164),Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,0
3761,Remove the docs for passing strategy args to accelerator (#15636),0.66763395,The argument distributed_backend has been removed from the Trainer in favor of the new accelerator and strategy arguments (#10017).,Co-authored-by: Jirka jirka.borovec@seznam.cz,0
3762,"Mocking loggers (part 2, neptune)  (#3617)",0.7653691,"Mocking loggers (#3596, #3617, #3851, #3859, #3884, #3853, #3910, #3889, #3926)",Co-authored-by: tchaton thomas@grid.ai,1
3763,Legacy: simple classif training (#8535),0.68609184,class FineTuneLearningRateFinder(LearningRateFinder):,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,0
3764,Add check for verbose attribute of ModelCheckpoint (#6419),0.7488178,Deprecated prefix argument in ModelCheckpoint (#4765),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
3765,feature(cli): login flow fixes and improvements (#16052),0.54371357,- Added support for managing SSH-keys via CLI ([#15291](https://github.com/Lightning-AI/lightning/pull/15291)),Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
3766,Move the _scan_checkpoints utility function (#9312),0.53766114,  * Changed the method signature of `Strategy.save_checkpoint` and `Fabric.load_checkpoint`,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
3767,Tighten the checks for Trainer.terminate_on_nan (#9190),0.72809184,Deprecated Trainer argument terminate_on_nan in favor of detect_anomaly(#9175),,1
3768,ref: remove obscure forward call in eval + CPU backend ___step (#3123),0.98556817,remove obscure forward call in eval + CPU backend ___step (#3123), Remove BasePlugin  Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
3769,outputs in __batch_end hooks (#3966),0.84691405,"Removed output argument from *_batch_end hooks (#3965, #3966)",,1
3770,Setup phased transition away from PyTorch version-specific handling of cuda availability and device counting (#15133),0.60581005,"With DPStrategy, the batch is not explicitly moved to the device (#11780)",,0
3771,Raise exceptions when torch distributed is not available (#10418),0.70686364,- Raised exception in `init_dist_connection()` when torch distributed is not available ([#10418](https://github.com/PyTorchLightning/pytorch-lightning/pull/10418)),,1
3772,fix changelog (#1452),0.743008,Complete changelog,,1
3773,Split GPU/DDP test,0.70414996,DDP is recommended for multi-GPU training,,1
3774,Restore test after #11448 (#11986),0.65050006,Deprecated the TestTubeLogger (#9065),,0
3775,Silenced some warnings. verified ddp refactors (#3483),1.0000001,Silenced some warnings. verified ddp refactors (#3483),,1
3776,Update trainer.py (#7340),0.73808885,Removed pytorch_lightning/trainer/training_loop.py (#7985),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
3777,added single gpu train doc,0.68996084,Train on 2 GPUs in a Jupyter notebook,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
3778,Replace Wandb callback's finalize with no-op (#1193),0.64034164,Removed wandb logger's finalize method (#1193), Fix accumulated_grad_batches typehint,0
3779,Add LRFinder callback (#13802),0.6261942,move lr_finder (#3434),,0
3780,Fix: App comment command execution sequencing  (#15615),0.5744776,clean up hooks in run_evaluation (#3156),,0
3781,Add docs for distributed inference (#15149),0.5465049,Sampler replacement in distributed strategies (#16829),,0
3782,docs: update version matrix (#17041),0.5712408,Docs improvements,Co-authored-by: Knarik Mheryan knarik@codics.am,0
3783,Create CONTRIBUTING.md,0.59567964,Contributors,Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: thomas chaton thomas@grid.ai,0
3784,Fix action_name usage in XLAProfiler (#15886),0.50568134,  * Deprecated the internal `XLADeviceUtils.xla_available` staticmethod,,0
3785,release v0.3.6.3,0.7481444,0.4.0,Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com,1
3786,Support torch.optim.lr_scheduler.ReduceLROnPlateau (#320),0.7378144,        scheduler = torch.optim.lr_scheduler.OneCycleLR(,Co-authored-by: thomas chaton thomas@grid.ai,1
3787,Fix accumulate_grad_batches on init (#9652),0.59550595,      init_args:, Simplify checkpoint connector loading after Checkpoint IO plugins introduction  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
3788,[pre-commit.ci] pre-commit autoupdate (#8388),0.5258014,Full commit list: https://github.com/Lightning-AI/lightning/compare/1.9.0...2.0.0, Add ShardedTensor support in LightningModule  Co-authored-by: Yifu Wang yifuwang@2012@gmail.com Co-authored-by: Sean Naren sean@grid.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com,0
3789,remove AcceleratorConnector.use_ipu (#12110),0.8193695,- Removed `AcceleratorConnector.use_ipu` property ([#12110](https://github.com/PyTorchLightning/pytorch-lightning/pull/12110)),,1
3790,Refactor utilities/imports.py (#5874),0.6864828,from lightning.pytorch.utilities import grad_norm,Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
3791,Azure: local id for e2e (#14432),0.46400514,Refactor cloud dispatch and update to new API (#16456),,0
3792,Classification metrics overhaul: precision & recall (4/n) (#4842),0.81652695,Renaming of precision recall metric (#3308),Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
3793,Fix exception message for invalid custom string plugin (#4979),0.74617225,Updated error message for interactive incompatible plugins (#9896),Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk,1
3794,make extra build for latest (#7593),0.56972355,Setup: added requirement freeze for the next major version (#14480), Remove TrainingTypePlugin.on_save and Accelerator.on_save,0
3795,Fixed the default value of auto_lr_find in docs (#1854),0.62628514,tuner.lr_find(...),,0
3796,Fix ci xos (#647),0.61909455,"| ""64-true""                         | ""64"", 64   |",,0
3797,rename set_random_master_port (#10104),0.6484827,"# if user gave a port number, use that one instead",Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,0
3798,removed mlflow and custom logger  tests (#389),0.75017476,Cleaning up stale logger tests (#3490),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
3799,Docs: move images (#5756),0.5333068,"    pil_image.save(buffered, format=""JPEG"")",Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
3800,Run HPU tests only with yml (#12469) (#12478),0.51708883,Device Stats Monitoring support for HPUs,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
3801,fix typo in Optimization (#5228),0.65362215,Improved error messages for invalid configure_optimizers returns (#3587),,0
3802,update changelog (#15975),0.7345985,Full Changelog, Remove unused imports in WandB logger and corresponding test,1
3803,docs: dont mock imports when running sphinx doctest (#2396),0.6178846,import argparse,Co-authored-by: Yifu Wang yifuwang2012@gmail.com,0
3804,Add ModelSummary.max_depth (#8062),0.8175404,Deprecated mode parameter in ModelSummary in favor of max_depth (#8062),,1
3805,update changelog (#4931),0.74439454,Complete changelog,,1
3806,Update single_gpu_node_16bit_template.py,0.5729955,16-bit can now be used with a single GPU (no DP or DDP in this case). bypasses issue with NVIDIA apex and PT compatibility for DP+16-bit training. ,Co-authored-by: Michele Sanna {ID}+{username}@users.noreply.github.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
3807,Adapt NeptuneLogger to new neptune-client api (#6867),0.8253219,pytorch_lightning.loggers.neptune.NeptuneLogger is now consistent with the new neptune-client API; the old neptune-client API is supported by NeptuneClient from the neptune-contrib repo (#6867),,1
3808,Simplify rank_zero_experiment for torch.compile support (#17216),0.63249314,"Set PyTorch 1.4 as min requirements, also for testing and examples torchvision>=0.5 and torchtext>=0.5 (#5418)",,0
3809,Error messages for removed Trainer mixin methods (#15065),0.78982484,Remove model.trainer call inside of dataloading mixin (#7317), Remove GradInformation module from LightningModule hierarchy,1
3810,Fix support for ModelCheckpoint monitors with dots (#12783),0.79528576,Allow ModelCheckpoint monitor to be None (#3633),,1
3811,Simplify checkpoint connector loading after Checkpoint IO plugin's introduction (#9045),0.7616346,Refactor load in checkpoint connector (#4593),  Add stage 1 support + small doc improvements   Add CHANGELOG.md ,1
3812,Add CheckpointConnector internal commentaries (#4421),0.71980256,CheckpointIO Plugins,,1
3813,Fixed broken link in PR template (#1675),0.5605034,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",  fix batch auto scaling when init_val causes OOM   Update CHANGELOG.md   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
3814,Remove deprecated on_load/save_checkpoint behavior (#14835),0.74897903,Removed deprecation warnings being called for on_{task}_dataloader (#9279),,1
3815,Set the default work start method to spawn on MacOS (#16089),0.87065905,The default start_method for creating Work processes locally on macOS is now 'spawn' (previously 'fork') (#16089), [lightning] Ensure error handling works different trainer entry points,1
3816,doc fixes (#2222),0.6884833,Docs improvements,Co-authored-by: tchaton thomas@grid.ai Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,0
3817,Decouple Tuner from Trainer (#16462),0.7431961,Removed the deprecated TrainerTrainingTricksMixin class (#8679), Replace instances of self.lightning_module.trainer with trainer directly in ddp_spawn and tpu_spawn,1
3818,update NGC docker (#13136),0.6618983,Changed to the NeptuneLogger (#16761):,,0
3819,Fix for PyTorch 1.7 CI (#3768),0.79508126,"Removed PyTorch 1.6 support (#10367, #10738)",Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,1
3820,Add labels to sphinx docs (#2964),0.6705334,Pinned sphinx-autodoc-typehints with <v1.15 (#11400),,0
3821,Fix not running test codes (#13089),0.67379487,- fixed all the .test() calls,,0
3822,fix defecation warnings (#570),0.5572796,Deprecation warning (#3844),Co-authored-by: Yifu Wang yifuwang@2012@gmail.com,0
3823,[loops] Reset reference to dataloader iterator on run end (#9386),0.7536489,Replaced old prefetch iterator with new DataFetcher in training loop (#8953), Add a flavor of training_step that takes dataloader_iter as an argument,1
3824,Add easy access to state_dict in Lite module wrapper (#14629),0.679026,"- In Lightning Lite, state-dict access to the module wrapper now gets passed through to the original module reference ([#14629](https://github.com/Lightning-AI/lightning/pull/14629))",,0
3825,Fix ModelPruning(make_pruning_permanent=True) buffers getting removed when saved during training (#6073),0.6941463,Removed passing a ModelCheckpoint instance to Trainer(checkpoint_callback) (#6166),,0
3826,Update LightningCLI(trainer_defaults=...) doc (#11309),0.76850986,LightningCLI.instantiate_trainer now takes a config and a list of callbacks (#8721),,1
3827,use val_percent_check in validation step (#135),0.76387835,val_percent_check in favour of limit_val_batches,,1
3828,"Updated ""Models defined by data"" (#3433)",0.7125961,model_utils >> model_helpers,,1
3829,Remove the deprecated pl.loops.base module (#16142),0.7350115,Removed the deprecated pytorch_lightning.loops.base module in favor of pytorch_lightning.loops.loop (#16142),,1
3830,Fix skipped tests due to sklearn (#15311),0.59679544,Removed no return warning from val/test step (#6139),detach loaders after run,0
3831,remove redundant iterator call to data fetcher in loops (#9117),0.7134656,Replaced old prefetch iterator with new DataFetcher in training loop (#8953),"Removed an extra quote - """,1
3832,making optimization steps for hooks (#2363),0.99451214,Made optimization steps for hooks (#2363),,1
3833,workers warning not on windows (#1433),0.5377311,warning_utils >> warnings,  Smart handling of EarlyStopping.check_on_train_epoch_end   dummy value   Extra flag ,0
3834,"Reduce speed diff further, lack of GPU saturation is causing regressive times on drone CI",0.65985674,Refactor GPUStatsMonitor to improve training speed (#3257),  Fix swa lrs - needs test   Add test   Update CHANGELOG ,0
3835,Fix pre-commit isort failure on tests/utilities/*.py (#5420),0.70525086,Dropped official support/testing for PyTorch <1.6 (#8288), Remove write_predictions from LightningModule,1
3836,Remove pytest as a requirement to run app (#15449),0.8054053,Remove pytest as a requirement to run app by @manskx in https://github.com/Lightning-AI/lightning/pull/15449,,1
3837,move lightning_app >> lightning/app (#16553),0.7697693,Lightning App,,1
3838,Disable coverage annotations in GitHub (#9077),0.5055543,- Removed the deprecated `BaseProfiler` and `AbstractProfiler` classes ([#14404](https://github.com/Lightning-AI/lightning/pull/14404)),,0
3839,Make ModelCheckpoint(save_top_k=-1) track the best models (#3735),0.77736723,"The ModelCheckpoint callback now saves and restores attributes best_k_models, kth_best_model_path, kth_value, and last_model_path (#10995)",Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
3840,ref: fix & simplify test callback (#4009),0.72162306,- fixed all the .test() calls,,1
3841,Add auto wrapping for DDPFullyShardedNativeStrategy (#14252),0.80536306,- Added support for auto wrapping for `DDPFullyShardedNativeStrategy` ([#14252](https://github.com/Lightning-AI/lightning/pull/14252)),,1
3842,Minimize the number of docker jobs (#10202),0.53754056,"    num_workers=10,",,0
3843,Update evaluation hook tests (#8013),0.7208383,clean up hooks in run_evaluation (#3156),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
3844,Deprecate on_before_accelerator_backend_setup callback hook (#11655),0.78902435,  * `Callback.on_before_accelerator_backend_setup` in favor of `Callback.setup`,,1
3845,docs (#1059),0.73255455,Docs," Deprecate DataModule properties: train_transforms, val_transforms, test_transforms, dims, and size",1
3846,remove dataloader patching on the LightningModule (#9764),0.8349354,"Removed automatic patching of {train,val,test,predict}_dataloader() on the LightningModule (#9764)",Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
3847,default test logger (#1478),0.71762806,Cleaning up stale logger tests (#3490),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
3848,App: change cache location (#17491),0.5320412,Moved save_function to accelerator (#6689),,0
3849,ref: .fit hook clean up (#3198),0.8332329,Trainer.fit hook clean up (#3198),Co-authored-by: thomas chaton thomas@grid.ai,1
3850,Error checking for non-iterables (#17007),0.6131426,  validation_if_necessary(), simple_classif_training fix test pt1.6 automate  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: thomas chaton thomas@grid.ai,0
3851,Use fsspec with OmegaConf saving in saving.py (#3782),0.60813767,Used fsspec instead of gfile for all IO (#3320)," Add tests for utilities/data.py: test_has_iterable_dataset, test_has_len, test_get_len",0
3852,Logger connector re-design _Metadata.reduce_fx fixes. (#7890),0.7664561,Dramatically simplify the LoggerConnector (#7882), Fix truncated backprop through time set on LightningModule and not Trainer,1
3853,fix readme badges (#7354),0.50806725,Removed ProfilerConnector (#7654),,0
3854,move amp checkpoint state management to precision plugin (#7831),0.6344958,Refactor load in checkpoint connector (#4593),,0
3855,clean docs (#966),0.63134295,"docs for all Metrics (#2184, #2209)",,0
3856,[fix] Better support for rank_zero_only setting for SLURM and torchelastic (#6802),0.8522345,Set better defaults for rank_zero_only.rank when training is launched with SLURM and torchelastic (#6802),,1
3857,added warning for distributedsampler in case of evaluation (#11479),0.6782834,- Added a warning when using `DistributedSampler` during validation/testing ([#11479](https://github.com/PyTorchLightning/pytorch-lightning/pull/11479)),,0
3858,Update psutil requirement from <5.9.3 to <5.9.4 in /requirements (#15423),0.63864714,The psutil package is now required for CPU monitoring (#17010),,0
3859,remove unnecesarry gradient freeze/unfreeze for single optimizer setup (#719),0.6711259,Disabled optimizers setup during testing (#3059),  remove deprecated sync step   update chlog ,0
3860,added trainer api docs (#4569),0.71117705,adding Trainer.tune() (#3293),"metrics_to_scalars can return non-float values, such as int or complex, depending on the dtype of the tensor.",1
3861,Support infinite training (#8877),0.81551754,Infinite Training,,1
3862,update for neptune 1.0 (#16761),0.6571999,"    version=""0.0.1"",",  Fix mypy for utilities.xla_device   Add explicit type hint for _XLA_AVAILABLE in utilities.imports ,0
3863,Avoid initializing optimizers during deepspeed evaluation (#14944),0.8505762,Support for manual optimization with DeepSpeed (#7970),,1
3864,Deprecatetruncated_bptt_steps flag on Trainer in favor of same setting on the LightningModule (#7323),0.8643342,Deprecated Trainer.truncated_bptt_steps in favor of LightningModule.truncated_bptt_steps (#7323),No need to use backbone. Calling self proceeds forward.,1
3865,fix key typo (#2374),0.6146053,Key updates,  Fix the majority of mypy issues   Apply @carmocca's suggestion   Handle the exception when nvidia-smi not found   Update get_gpu_memory_map's docstring   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
3866,User-friendly exception when LightningWork.run() method is missing (#14759),0.8914182,Improved the error message when the LightningWork is missing the run method (#14759),,1
3867,Fix docs typo (#6055),0.519546,Docs,,0
3868,Use PyTorch API logging for Lightning Trainer (#6771),0.8052536,package pytorch_lightning.logging,,1
3869,Error on zero length dataloaders (#1280),0.6472168,"Raised MisconfigurationException when total length of dataloader across ranks is zero, and give warning when total length is non-zero, but only local rank length is zero. (#9827)",,0
3870,Distributed sampling parity between Lite and PyTorch (#16101),0.75367093,Restored sampling parity between PyTorch and Fabric dataloaders when using the DistributedSampler (#16101),,1
3871,document exceptions in utilities (#8122),0.56640196,pytorch_lightning.utilities.grads.grad_norm now raises an exception if parameter norm_type <= 0 (#9765),,0
3872,Remove deprecated self.log arguments (#10423),0.71750283,Removed support for self.log()ing a dictionary (#16389),,1
3873,Simplify track grad norm condition (#9992),0.667729,Gradient norm tracking (#16745),,0
3874,Automatic string fixes (#8886),0.5943233,Fixing critical bugs in newly added hooks and hparams assignment.,Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
3875,[ci] tag v1.4.1 for pypa/gh-action-pypi-publish (#4548),0.6462361,PyTorch 1.5  support,,0
3876,Hardware specific parts of Accelerator Refactoring (#5719),0.8144693,Refactored accelerator backends:,,1
3877,"Remove profile(""training_step_and_backward"") (#11222)",0.8043983,"- Removed `profile(""training_step_and_backward"")` in `Closure` class since we already profile calls `training_step` and `backward` ([#11222](https://github.com/PyTorchLightning/pytorch-lightning/pull/11222))",,1
3878,Changelog fixes for Lightning 1.5 release (#10281),0.8107863,Full Changelog: https://github.com/Lightning-AI/lightning/compare/1.8.1...1.8.2,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
3879,continue devel (#1793),0.53236794,"    },",,0
3880,minor doc fix (#4296),0.66254544,Docs improvements,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
3881,release v0.5.1,0.81279945,"    version=""0.0.1"",",Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
3882,update contrib list (#1241),0.5758151,This release has breaking API changes. See #124 for all details. ,,0
3883,release v0.3.5,0.75543964,0.4.0,,1
3884,Update docs about reduce_fx (#13101),0.61749816,remove _evaluate fx (#3197), add mdformat exclude chlog fix ***  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
3885,do not fails all dockers (#2860),0.53821284,clean up hooks in run_evaluation (#3156),,0
3886,fix importing torchtext batch (#6365),0.74993813,import torch,,1
3887,Fix ModelCheckpoint default paths (#413),0.7730069,Deprecated filepath in ModelCheckpoint (#4213),Co-authored-by: tchaton thomas@grid.ai,1
3888,Remove evaluation loop legacy dict returns for *_epoch_end hooks (#6973),0.9509729,Removed evaluation loop legacy returns for *_epoch_end hooks (#6973),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,1
3889,reconfigure mergify (#5499),0.671929,Refactored EpochResultStore (#5522),,0
3890,Remove deprecated DeviceDtypeModuleMixin import (#10442),0.7437873,| Import pl.core.mixins.DeviceDtypeModuleMixin                                                           | 1.10             | No longer supported |, Fix ddp accelerator choice for cpu,1
3891,Fixed #2143 and many more :) (#3855),0.6869253,"At last, lots of bug fixes (see below).",Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
3892,enable ddp as a plugin (#4285),0.75437635,Enabled plugins (#4041),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Justus Schock justus.schock@posteo.de Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
3893,remove .item which causes sync issues (#1254),0.99806154,Remove .item which causes sync issues (#1254),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
3894,Optimizable structural typing (#14994),0.6114649,Refactored setup for typing friendly (#6590),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
3895,Minor Fabric backward refactor (#17433),0.73280305,fabric.backward(loss),,1
3896,release v0.4.6,0.7764323,0.4.0,,1
3897,typing: fix App's core API - App (#16949),0.5763297,- Fixed a bug when launching an app on multiple clusters ([#15226](https://github.com/Lightning-AI/lightning/pull/15226)),,0
3898,pytest default color (#4703),0.61605,- Update `RichProgressBarTheme` styles after detecting light theme on colab ([#10993](https://github.com/PyTorchLightning/pytorch-lightning/pull/10993)), add yesqa fix flake8  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
3899,Update PT to PL conversion doc (#11397),0.55240715,Removed environment variable PL_EXP_VERSION from DDP subprocesses (#7403), update NGC  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
3900,Fix mypy for plugins registry (#7062),0.65441054,Enabled plugins (#4041),,0
3901,Clean up last ModelCheckpoint makedirs call to IOPlugin (#11035),0.7076841,Removed a redundant warning with ModelCheckpoint(monitor=None) callback (#9875),,1
3902,Update model_checkpoint.py (#3801),0.6847525,Removed the deprecated model argument from ModelCheckpoint.save_checkpoint (#8688),,0
3903,2/n Move Precision Plugin into strategy - move optimizer related logics (#10596),0.71650064,- PrecisionPlugin.{post_optimizer_step},,1
3904,Replace NumPy with Torch in examples/fabric/ (#17279),0.6426606,  import torch.nn as nn,,0
3905,Fix propagation of device and dtype properties in Lite modules (#10559),0.5648589,Move lightning module to correct device type when using LightningDistributedWrapper (#6070),Co-authored-by: tchaton thomas@grid.ai Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
3906,added single gpu train test,0.62904286,Train on 2 GPUs in a Jupyter notebook,,0
3907,add 'sanity_checking' to datamodule 'on_after_batch_transfer' docs (#10067),0.59926105,Deprecated the on_sanity_check_start hook in ModelHooks (#598),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
3908,[bug-fix] Metric reduction with Logging (#5150),0.9381196,Metric reduction with Logging (#5150),Co-authored-by: Caleb Robinson calebrob6@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
3909,Fix import error in test_pl_examples.py (#16268),0.6898987,$ PL_FAULT_TOLERANT_TRAINING=MANUAL python script.py,,0
3910,removed opt check,0.59306234,Removed,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
3911,Make LightningModule torch.jit.script-able again (#15947),0.72739416,- Fixed torchscript error with containers of LightningModules ([#14904](https://github.com/Lightning-AI/lightning/pull/14904)),,1
3912,enable any dict and namespace in hparams (#1847),0.641221,hparams can now be anything! (call self.save_hyperparameters() to register anything in the _init_,,0
3913,Fixes #347 (#393),0.60960233,"merge backends (#3476, #3477, #3478, #3480, #3482)",Co-authored-by: thomas chaton thomas@grid.ai,0
3914,Add FileSystem (#16581),0.5649519,Prevent to cd into non-existent folders (#16645),,0
3915,allow user to control optimizer step for every optimizer,0.8360044,        optimizer.step(),"  Add property to delay checkpointing, move loading checkpoint file into the run function to allow deepspeed engine to be loaded   Add a small test   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update pytorch_lightning/accelerators/accelerator.py  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   Address review   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",1
3916,get fullpath before splitting (#2153),0.57482874,Split profilers module (#6261),,0
3917,"Change nb to num in ABCs, comments, and tqdm logging (#613)",0.55282456,Changed resolve_training_type_plugins to allow setting num_nodes and sync_batchnorm from Trainer setting (#7026),,0
3918,"Automatically check DataModule.has_{setup,teardown,prepare_data} [2/2] (#7238)",0.7847446,"Deprecated DataModule properties: has_prepared_data, has_setup_fit, has_setup_validate, has_setup_test, has_setup_predict, has_teardown_fit, has_teardown_validate, has_teardown_test, has_teardown_predict (#7657)",,1
3919,Use fsdp module to initialize precision scalar for fsdp native (#14092),0.7357197,Avoided requiring the FairScale package to use precision with the fsdp native strategy (#14092),,1
3920,simplify changelog (#5135),0.7891858,Complete changelog,  Fix split idx reference   Update CHANGELOG   Add comment ,1
3921,updated docs (#1941),0.65462565,"docs for all Metrics (#2184, #2209)",,0
3922,Fix KFold example (#11230),0.5754801,- Fixed an issue where `HorovodStrategy.teardown()` did not complete gracefully if an exception was thrown during callback setup [#11752](https://github.com/PyTorchLightning/pytorch-lightning/pull/11752), Delete deprecated TrainerLoggingMixin Update CHANGELOG Delete from Trainer,0
3923,Clean up dataloader logic (#926),0.7195315,clean up data reset (#3161),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
3924,Removed duplicated file extension when uploading model checkpoints with NeptuneLogger (#11015),0.8713707,- Removed duplicated file extension when uploading model checkpoints with `NeptuneLogger` ([#11015](https://github.com/PyTorchLightning/pytorch-lightning/pull/11015)),  drop metrics   drop tests   fix imports   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
3925,Remove deprecated LightningIPUModule (#14830),0.89257854,Removed the deprecated LightningDeepSpeedModule (#16041),,1
3926,accelerator refactor - add parallel plugins  (#5714),0.8744788,Refactored Accelerators and Plugins (#5743),  CI: validate JSON   as GHA   PT1.8   32g   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
3927,"Add batch_size, rank_zero_only arguments for log_dict to match log (#8628)",0.6373678,Metric reduction with Logging (#5150), black docs  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
3928,drop packaging (#3105),0.5433686,Implemented ready for components (#16129),"  Change trainer loading behaviour for validate/test/predict   Fix   Fix/add tests   remove   Cleanups   Space   cleanups   Add CHANGELOG.md   Move after setup   Cleanups on logic   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Remve   fix test   feedback   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  Update pytorch_lightning/trainer/properties.py  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   Feedback   Same fix   Same fix   Add test for behaviour, modify based on feedback   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Wording   Apply suggestions from code review   Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   Cleanup docs   Update pytorch_lightning/trainer/trainer.py   Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com   feedback   Fixes to test API   Add carlos description   Move logic further   Move checkpoint connector logic   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com",0
3929,Enable inference mode for evaluation (#12715),0.70102054,Inference mode support,,1
3930,Remove model_connector.py (#10111),0.76399034,Removed the lightning.pytorch.core.saving.ModelIO class interface (#16974),,1
3931,Remove duplicate boring classes (#12951),0.6021329,Add function to remove checkpoint to allow override for extended classes (#16067),  dev + chlog   Add placeholders   Clean previous entry   Add CHANGELOG fix   Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
3932,PL: Test PyTorch 2.0 pre-release on CPU and CUDA (#16764),0.69117594,Dropped official support/testing for PyTorch <1.6 (#8288),  Delete iteration_count and batches_seen   Update CHANGELOG   Protect should accumulate   Update pytorch_lightning/loops/epoch/training_epoch_loop.py ,0
3933,update stale config (#3509),0.6154779,This release fixes that core issue,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
3934,removed old template,0.6858255,- The base Plugin class has been removed. ,  Update Lightning version to v1.4   update notebooks   Update release date in Changelog   docs   Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
3935,Clean code formatting CI job (#8378),0.5191748,- Code coverage (99%),"  amp   amp   docs   add guides   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   amp   amp   docs   add guides   speed guides   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Delete ds.txt   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Update conf.py   Update docs.txt   remove 16 bit   remove finetune from speed guide   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   speed   speed   speed   speed   speed   speed   speed   speed   speed   speed   speed   speed   remove early stopping from speed guide   remove early stopping from speed guide   remove early stopping from speed guide   fix label   fix sync   reviews   Update trainer.rst   Update trainer.rst   Update speed.rst   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   managing data   managing data   amp   amp   docs   sync   sync   amp   amp   add data guide   from review   Apply suggestions from code review   Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci  Apply suggestions from code review  Co-authored-by: ananthsub ananth.subramaniam@gmail.com   from review   from review   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   add data guide   add data guide   add data guide   sync issues   from reviw   Update docs/source/guides/data.rst   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com   add info if import fails   fix cross referencing   Add Datamodule motivation   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   grid docs   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  Update cloud_training.rst  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com",0
3936,docs: listing past versions (#17014),0.5704733,Updated governance docs,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
3937,updated test (#1073),0.66871953,"Refactored training_batch + tests to verify correctness (#2327, #2328)",Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
3938,Transfer learning example (#1564),0.5855335,Learning Rate Finder (#13802),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Kaushik B kaushikbokka@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
3939,lock pytorch nightly version (#4469),0.65828705,package pytorch_lightning.logging,,0
3940,Val idx optional in validation_step (#108),0.7499056,"validation_step, val_dataloader are now optional.   ","  v1.4.0rc2   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   Apply suggestions from code review   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",1
3941,ref: merge backends x/n (#3480),0.8228632,"merge backends (#3476, #3477, #3478, #3480, #3482)",,1
3942,Update pytorch_lightning/trainer/connectors/precision_connector.py,0.80508935,Removed pytorch_lightning.trainer.connectors.OptimizerConnector (#10120),,1
3943,fix GH CI (#1052),0.53453684,This release fixes that core issue,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
3944,Fix local variables being collected into module_arguments dict (#2048),0.6208463,Module imports are now catching ModuleNotFoundError instead of ImportError (#9867),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
3945,update ambiguous info (#3613),0.582989,Updated Multinode Warning (#16091),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
3946,Move generated RST files to subfolder (#1555),0.5951759,move backends back to individual files (#3712),  Initial commit   Update docstrings   Update CHANGELOG.md   Fix mypy   Fixes   Fixes   Update to comments   Fix   mypy   Update on comments   Update   Fix mypy   protected   Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
3947,Fixed tests (#340),0.6886091,- fixed all the .test() calls, Remove undefined name from __all__  Co-authored-by: deepsource-autofix[bot] 62050782+deepsource-autofix[bot]@users.noreply.github.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
3948,fix reduction docstring and clean tests (#2885),0.6354881,Docs improvements,,0
3949,[bugfix] Metric not logged properly in manual optimization (#7228),0.6629313,Improved error messages for invalid configure_optimizers returns (#3587),  Support DataLoaders with missing arguments in replace_sampler   Fix for multiprocessing context   Fixes and test improvements   Fixes and test improvements   Fixes and test improvements   Test any variadic name   Update CHANGELOG   Make sure extra attributes can be present   Skip on old Windows   Update pytorch_lightning/trainer/data_loading.py   Update pytorch_lightning/trainer/data_loading.py   Check is dataloader   Typo ,0
3950,test (#2341),0.6724403,"nb_test_batches to num_test_batches,","  clarify closure usage in gan example   Update docs/source/common/optimizers.rst   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   remove empty line   Update docs/source/common/optimizers.rst   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  do not capitalize if not a sentence  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com",0
3951,Remove the deprecated profiler imports (#16059),0.7013813,Removed ProfilerConnector (#7654),  update docs   update typehints   fix indentation ,1
3952,Update teardown for TPU acc (#7211),0.6681777,Updated logic for checking TPUs availability (#6767),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
3953,[bugfix] Add support for CombinedLoader in validation with ddp (#7102),0.6417216,enabled multiple dataloaders for validation.    ,Co-authored-by: tchaton thomas@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
3954,Add job_name as a staticmethod in SLURMEnvironment class (#10698),0.56539613,- Deprecated the property `Trainer.slurm_job_id` in favor of the new `SLURMEnvironment.job_id()` method ([#10622](https://github.com/PyTorchLightning/pytorch-lightning/pull/10622)), fix CI for PT 1.10 Apply suggestions from code review  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
3955,Add logger flag to save_hyperparameters (#7960),0.6642357,Moved save_hyperparameters to its own function (#7119),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
3956,Bump default E2E tests image version (#17403),0.4692627,The point of this release is more bug fixes ahead of v 1.0.0. We now have CI tests on TPU thanks to @zcain117 from Google! :slightly_smiling_face:,,0
3957,Fix the num_batches value in warning (#10980),0.72037184,Avoid raising the sampler warning if num_replicas=1 (#14097),  Fix deepspeed scheduler logic   Fix tests   Minor changes   Improve tests   inference fix   CHANGELOG   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
3958,Deprecate Trainer.gpus (#12436),0.7510146,Removed the deprecated TrainerLoggingMixin class (#8609),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
3959,Fixed name ref,0.6150938,Rename failed -> error in tables (#15608),,0
3960,cpu backend (#2712),0.6945521,DDP(2) backend (#2796),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
3961,Add link to past versions in the docs header (#17063),0.6787468,A header with the version that generated the config is now included.,,0
3962,Fix accumulate_grad_batches for last batch (#2853),0.6275245,set validation to a fix number of batches,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk Co-authored-by: Ethan Harris ethanwharris@gmail.com,0
3963,Install colossalai==0.1.12 in CI (#16587),0.52487016,Introducing CLI commands for apps (#13602)!,v1.4.0rc1 & chlog,0
3964,Refactor callbacks (#776),0.7381425,Removed deprecated callbacks (#3979),  Maintain Backward compatibility for DeviceDtypeModuleMixin   Update message   Add deprecation test   Update test ,1
3965,update bug template with version (#17226),0.618307,Use correct python version in lightning component template (#13790),  resolve resolution   update changelog   typo   optimize test   update on comments   resolve comments   update ,0
3966,Enable profilers to write to remote files with fsspec (#4162),0.6666767,Moved profilers to their own file (#7822),Co-authored-by: deepsource-autofix[bot] 62050782+deepsource-autofix[bot]@users.noreply.github.com,0
3967,CI: check docker requires (#12677),0.6007552,Early stopping checks on_validation_end (#1458),"  reduce memory leak   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update changelog   Apply suggestions from code review   Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk   resolve flake8   update on comments   resolve bug   update   Undo whitespace changes   remove bug   resolve flake8   revert change   update on comments   delete the ddp wrapper as it hold memory   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   resolve flake8   update on comments   update changelog   resolve test   Update CHANGELOG   Refactor teardown   Fix comment   Do it for non-gpu too   remove ref when the model is not a lightning_module   Fix import error   move down   resolve bug   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   resolve assignement   update   move above   Fix device calls to support tpu training   Updat todo   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: Kaushik B kaushikbokka@gmail.com",0
3968,Classification metrics overhaul: stat scores (3/n) (#4839),0.91580486,Classification metrics overhaul (#4837),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
3969,ref: move specific accelerator code x/n (#3457),0.9467901,move specific accelerator code (#3457),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
3970,Save the loop progress state by default (#10784),0.7391309,"Connect the progress tracking dataclasses to the loops (#8244, #8362)",  resolve deepcopy   update changelog   move private   update on comments   Update CHANGELOG   Set skipped attributes to None   Simplify test   update   update changelog   update   update on comments   typo   update   Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
3971,Pin fairscale version (#10200),0.50280714,FairScale deprecation (in favor of PyTorch's FSDP implementation) (#16353),  Modify deepspeed distributed to support windows   Add weak test   Cleanups   Capture more in tests   Add comment   Cleaner asserts ,0
3972,Safely disable profiler (#11167),0.75042367,Removed ProfilerConnector (#7654),,1
3973,Fix typo in LightningOptimizer (#5736),0.73951674,Improved the error message when the LightningWork is missing the run method (#14759),,1
3974,Abstract Mixin classes (#572),0.6176814,"Base classes (#1326, #1877)", Support devices flag to Trainer,0
3975,changelogs clean (#3082),0.8167601,Complete changelog,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
3976,New metric classes (#1326) (#1877),0.76378703,Sklearn metrics classes (#1327),,1
3977,GPU CI: Increase timeout from 55min to 65min (#13064),0.66631365,Changed min-max GPU memory to be on their own plots (#1358),Co-authored-by: deepsource-autofix[bot] 62050782+deepsource-autofix[bot]@users.noreply.github.com,0
3978,Comet fix (#481),0.71478784,Using .comet.config file for CometLogger (#1913),  i   i   i   i   i   i   i   i   i   i   i   i   i   i   i   i   i   i   i   i   i   i   i   i   i   i   i   i   i   i   i   i   i   i   i   update   update ci   i   i   i   i ,1
3979,passing experiment to wandb (#720),0.57081145,Experimental Features,Co-authored-by: deepsource-autofix[bot] 62050782+deepsource-autofix[bot]@users.noreply.github.com,0
3980,Add ignore param to save_hyperparameters (#6056),1.0,Add ignore param to save_hyperparameters (#6056),Co-authored-by: deepsource-autofix[bot] 62050782+deepsource-autofix[bot]@users.noreply.github.com,1
3981,try adding coverage (#2441),0.5309291,[1.3.7post0] - 2021-06-23,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: tchaton thomas@grid.ai Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: Justus Schock justus.schock@posteo.de Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
3982,Merge pull request #11635 from PyTorchLightning/ci/fix-term,0.6780916,- Fixed bug that forced overriding `configure_optimizers` with the CLI ([#11672](https://github.com/PyTorchLightning/pytorch-lightning/pull/11672)),Co-authored-by: Ramon Emiliani ramon@mbp-de-ana.lan Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk,0
3983,sync,0.63499916,"In previous versions, metrics logged inside epoch-end hooks were forcefully synced. This makes the sync_dist flag irrelevant and causes communication overhead that might be undesired. In this release, we've removed this behaviour and instead warn the user that synchronization might be desired.",These reports can be quite large and involve some processing to produce. It means on larger models there's a noticeable performance hit to produce the cycles/memory reports.,0
3984,Support manual optimization step profiling without a trainer reference (#11883),0.7217739,"            optimizer, max_lr=1e-3, total_steps=self.trainer.estimated_stepping_batches",,1
3985,[bugfix] Fix dataloading for iterable datasets and limit_train_batches (#7306),0.66736823,Replaced old prefetch iterator with new DataFetcher in training loop (#8953),"  [DDP] Remove the outdated limitations of DDP communication hook since 1.9   DDP communication hook can work on multiple backends since 1.9.   SPMD in DDP is completely retired in 1.9, and SPSD is the only option.   Update ddp.py   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
3986,Add Rohit (rohitgr7) to core (#2706),0.5520397,"@ananthsub, @rohitgr7",Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
3987,Remove redundant shebang from source files (#13479),0.6549203,Removed deprecated BaseProfiler.output_filename arg from it and its descendants in favor of dirpath and filename (#9214),  update mergify rules   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
3988,Move _TrainingEpochLoop (#16801),0.6858605,move train outside of setup training (#3297),Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
3989,Attach version_ to checkpoint path only if version is int (#1748),0.5515624,We also added support for setting the checkpoint path statefully:,,0
3990,Update training_loop.py (#913),0.7255372,Removed pytorch_lightning/trainer/evaluation_loop.py (#8056),,1
3991,[App] Wrap LightningClient methods with retry wrapper by default (#16382),0.7587639,Deprecated LightningDataParallel in favor of new wrapper module LightningParallelModule (#5670),,1
3992,Fix to allow custom CheckpointIO with strategy classes (#13785),0.84671926,class CustomCheckpointIO(CheckpointIO):,"  Remove error, add mixed to check   Add test   Remove test   Add changelog   Add test for mixed   Update tests/plugins/test_deepspeed_plugin.py   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  Add special  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",1
3993,replace train_percent_check with limit_train_batches (#2220),0.6343181,(checks val every 100 train epochs),"  move deprecation test to correct 1.6 test file   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  Update tests/deprecated_api/test_remove_1-6.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",0
3994,Update type hint for log_model (#12223),0.6914773,Changed WandbLogger(log_model={True/'all'}) to log models as artifacts (#6231),  regression test   apply fix   simplify test and docs   update changlog ,0
3995,Fix tests for new tensorboard >= 2.6 (#8789),0.7277062,Improved the error message for installing tensorboard or tensorboardx (#17053),Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Justus Schock justus.schock@posteo.de Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
3996,Pre release (#5098),0.7276125,This release includes:,  resolve bug   update   add changelog   Update tests/utilities/test_apply_func.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
3997,TPU gradient clipping.  (#963),0.7465362,Update Gradient Clipping for the TPU Accelerator (#6576),"  resolve issues   update   update   update   add more exceptions   resolve bug   update   update   update changelog   resolve bug   resolve comments   update   update   update changelog   update   update   remove space   update   add progress tracking to loops   validate json   update   convert to dict for better readability   validate reload   update   update   update on comments   remove deadcode   clean changelog   clean changelog   update   update on comments   CHANGELOG   CHANGELOG   Update pytorch_lightning/loops/base.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   whitespace suggestions   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   make fault_tolerant_enabled protected   whitespace fixes around Args   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   typo it's -> its   fix copy-paste typo in progress docstring   Delete classes   Minor change   docs   protected get_loops_state   merge restore_loops with restore_progress   Fix tests after removals   explicit save with trainer.save_checkpoint()   handle optimization restart based on optimizer_idx   update increments   update val batch progress and remove iteration count   update progress tracking for dataloader loops   remove self.dataloader_idx from eval_epoch_loop   add batch progress to predict loop   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   incorporate progress tracking for current_epoch   Fix test   Actually remove it   Remove unused TrainingEpochProgress   Fix optimization progress - missing scheduler   Restarting changes   Scheduler progress   Unused property, reset on epoch   Resolve FIXME   Remove FIXME   fix test_progress (wip)   fix batch_progress.current.reset   Hold off on split progress. Out of scope of this PR   Unnecessary if   fix structure in test_progress   structure   clean up unused variables in test_progress   refactor naming and organization in test_progress   Unnecessary variable   Remove unnecessary diff   Improve comment   Undo typing change to avoid polluting everything with mypy fixes   Fix and improve test_loops.py   Fix and organize test_loop_state_dict   Remove unnecessary checks in test   Update test after disallowing updates on None attributes   Typing   Minor test cleanup   Fix and move loop test   Move test from progress to loops   Reset the scheduler progress   SchedulerProgress fix   Consistent whitespace   Fix final test   Minor test changes   One test to rule them all   Formatting   Rename and clean variables   Shorter names   Shorter scheduler name   Fix optimizer step calculation for stop_batch=2   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Remove empty connects   Update CHANGELOG   Holy shit finally got the formula right   Fix final thing!!!   Do not check state dicts   parametrize multiple_dataloader progress test   Update CHANGELOG.md   Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Justus Schock justus.schock@posteo.de",1
3998,ci: update runner for IPU (#17183),0.60410786,The psutil package is now required for CPU monitoring (#17010),,0
3999,Fix log debug call (#3528),0.661301,Cleaning up stale logger tests (#3490),  Move mixin to core   Move mixin to core ,0
4000,fix dataparallel,0.6716775,"DataModules now avoid duplicate {setup,teardown,prepare_data} calls for the same stage (#7238)",  Debug   Increase SHM size   Debug   Refactor MNIST imports   Undo debugging   Prints ,0
4001,added dist sampler exception,0.67469054,Did not interfere with a default sampler (#1318),,0
4002,Release2 (#2262),0.74381256,This release includes:,  RC0   Update changelog   Co-authored-by: Kaushik B kaushikbokka@gmail.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,1
4003,Use run name for logging with WandbLogger (#12604),0.75948393,"- The `WandbLogger` will now use the run name in the logs folder if it is provided, and otherwise the project name  ([#12604](https://github.com/Lightning-AI/lightning/pull/12604))",  Mark evaluation epoch loops attributes as protected   Fix pre-commit ,1
4004,Check torch.distributed availability before sharded tensor state dict hook registration (#10621),0.69435114,Removed registration of ShardedTensor state dict hooks in LightningModule.__init__ with torch>=2.1 (#16892),,0
4005,Load ckpt path when model provided in validate/test/predict (#8352),0.7634225,"trainer.test(model, ckpt_path=""my_path"") # load path",,1
4006,Add header_style argument to RichModelSummary callback (#16788),0.547806,Deprecated passing weights_summary to the Trainer constructor in favor of adding the ModelSummary callback with max_depth directly to the list of callbacks (#9699),  prepare PT 1.10   dockers   fixes   readme ,0
4007,Load the app before setting LIGHTNING_DISPATCHED (#16071),0.74168015,Lightning App,Co-authored-by: deepsource-autofix[bot] 62050782+deepsource-autofix[bot]@users.noreply.github.com,1
4008,Fix apex scaling with decoupled backward (#2433),0.63953835,                scaled_loss.backward(),,0
4009,added single node distdataparallel,0.57817125,Slightly safer multi node (#15538),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
4010,clean requirements (#2128),0.70022535,"Cleaning (#5948, #5949, #5950)",,1
4011,Support serialized checkpoint loading (#9605),0.6752712,DataModule hooks for loading and saving checkpoints,Co-authored-by: Jirka jirka.borovec@seznam.cz,0
4012,beta release to pypi,0.6928302,PyTorch 1.5  support,,0
4013,Remove Strategy.on_tpu property (#11536),0.765858,- Removed `Strategy.on_tpu` property ([#11536](https://github.com/PyTorchLightning/pytorch-lightning/pull/11536)),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
4014,ci: drop group probot (#14456),0.613179,group connectors (#3472),  expose extract_batch and make public   first pass   early return   add changelog   move to utilities/data.py   add test_data.py   tests are passing   precommit hook   address pep8 failure   Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
4015,Docs/changelog (#2125),0.74629086,Full Changelog,,1
4016,[TPU] Use pull_request_target event (#17377),0.6240528,- Added a try / catch mechanism around request processing to avoid killing the flow ([#15187](https://github.com/Lightning-AI/lightning/pull/15187),  Add log flag to save_hyperparameters   FIx setter   Add test & Update changelog   Address comments   Fix conflicts   Update trainer   Update CHANGELOG.md   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   Fix datamodule hparams fix   Fix datamodule hparams fix   Update test with patch   Update pytorch_lightning/utilities/hparams_mixin.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   Move log_hyperparams to mixin   Update hparams mixin   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
4017,[App] Fix multi-node pytorch example CI (#15753),0.6517626,Native PyTorch implementation,updates: - github.com/PyCQA/isort: 5.9.1 → 5.9.2 Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
4018,fix logger creating directory structure too early in DDP (#6380),0.7278836,DDP + loggers should be fixed,,1
4019,Fix mypy typing errors in pytorch_lightning/callbacks/model_checkpoint.py (#13617),0.7120416,- Fixed bug where `ModelCheckpoint` was overriding the `epoch` and `step` logged values ([#12418](https://github.com/PyTorchLightning/pytorch-lightning/pull/12418)),,1
4020,Run mypy with PyTorch 1.12 (#14044),0.7653286,PyTorch 1.5  support,,1
4021,release v0.6.0,0.79192257,"    version=""0.0.1"",",,1
4022,Update traitlets requirement from <5.2.0 as strict in /requirements (#14666),0.5701837,| Attribute Trainer.tuning                                    | 1.10             | No longer supported         |,,0
4023,[CI] fix horovod tests (#14382),0.68617934,horovod deprecation (#16141),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
4024,"Fix overfit_batches > 0 on distributed_backend = ""ddp"" (#3534)",0.6621759,"refactor DDP backend (#3204, #3207, #3208, #3209, #3210)",,0
4025,Minor formatting fix on model_parallel docs (#16565),0.56946695,Allowing decorate model init with saving hparams inside (#4662),"  resolve issues   update   update   update   add more exceptions   resolve bug   update   update   update changelog   resolve bug   resolve comments   update   update   update changelog   update   update   remove space   update   re-order protected trainer attr   move public method up   add docs to state dict methods   combine __load with load_state_dict   rename shadowed variable   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   move changelog entry to refactor section   refactor loop_progress property for test helper function   update trainer setter docstring   Update CHANGELOG.md   Update pytorch_lightning/loops/base.py   remove trainer check   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com",0
4026,Tests: Fail on FutureWarning (#11541),0.60090196,Removed no return warning from val/test step (#6139),,0
4027,Minor code health fix for Trainer (#8761),0.7249956,Trainer.fit hook clean up (#3198),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: SeanNaren sean@grid.ai,1
4028,Fix mypy errors attributed to pytorch_lightning.core.datamodule (#13693),0.7135484,+ # pytorch_lightning==1.7.0,Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk Co-authored-by: Tilman Krokotsch tilman.krokotsch@iav.de Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Ethan Harris ethanwharris@gmail.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Kaushik B kaushikbokka@gmail.com,1
4029,Extend Lite CPU coverage (#15279),0.5859777,"We are continuing to improve the support for this feature by adding automatic wrapping of layers for use cases where the model fits into CPU memory, but not into GPU memory (#14383).","  add ClusterEnvironment for LSF systems   update init file   add available cluster environments   clean up LSFEnvironment   add ddp_hpc as a distributed backend   clean up SLURMEnvironment   remove extra blank line   init device for DDPHPCAccelerator   We need to do this so we don't send the model to the same device from multiple ranks   committing current state   add additional methods to ClusterEnvironments   add NVIDIA mixin for setting up CUDA envars   remove troubleshooting prints   cleanup SLURMEnvironment   fix docstring   cleanup TorchElasticEnvironment and add documentation   PEP8 puts a cork in it   add set_ranks_to_trainer   remove unused import   move to new location   update LSF environment   remove mixin   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   changelog   reset slurm env   add tests   add licence   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   test node_rank   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   add lsf env to docs   add auto detection for lsf environment   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   fix is_using_lsf() and test   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
4030,Fix regression on default value for find_unused_parameters (#14095),0.75246596,            find_unused_parameters=True,Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
4031,clean loading version (#16591),0.6342664,"Redesigned multi-dataloader support (#16743, #16784, #16939)",  Add nccl env variable docs   Wording   Update docs/source/guides/speed.rst   Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,0
4032,SIGTERM handling is now unrelated to fault tolerance (#16501),0.7766075,"Removed experimental fault-tolerance support (#16516, #16533)",,1
4033,Fix type error when dividing chunk size in colossalai strategy (#16212),0.57236433,Changed the model size calculation using ByteCounter (#10123),"  Add callback to hook tests and add predict test   Fix lambda callback test   Simplify lambda call test   Use LambdaCallback   Dynamically append to called for the model   Remove print   Consistency   Consistency   Prepare args/kwargs testing   yapf doesn't like dict literals   Add arguments for fit no val test   Add arguments for fit no val test   add before_backward_hook   add test   resolve flake8   resolve tests   update changelog   add on_before_backward to LightningModule   update on comments   Test arguments   Datamodule refactor   Fix eval test   remove extra file   resolve bug   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   move to hooks   update   resolve flake8   update on comments   Update full fit + val test   Update test   Remove FIXME   Remove FIXME   Undo change   Fix   Parametrize fit hook test   Comment   Parametrize fit hook test with different precision plugins   Fix tests   Parametrize fit hook test with manual optimization   Unnecessary parenthesis   WIP   Comments   Fix message   Test CI error   Revert ""Test CI error""   This reverts commit 39c4a85a83cf32081b721f939ff83500b93f2dd3.   Add ddp training type teardown   Update CHANGELOG   Adrian's fix   Use destructor   Update CHANGELOG.md   RPC destructor   Update pytorch_lightning/plugins/training_type/ddp.py   Why do you not work :(   Missing condition   Fix deepspeed test   GC collect in conftest   Do not show warnings for special tests   Needs to run on 1.8   To avoid: ""RuntimeError: NCCL error in: /pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp:32, unhandled cuda error, NCCL version 2.4.8""   Run torch 1.8   Skip test due to 'Python bus error'   Debug NCCL   shm size   Disable warnings for special tests   Remove NCCL_DEBUG statement   Try smaller shm size   Revert ""Skip test due to 'Python bus error'""   This reverts commit e0a3e8785d2fecd63667da433a648f958d60ef89.   README and adjust versions   Avoid self.on_gpu call   empty cache cleanup   More garbage collection   Unroll parametrizations   Do not reuse mock   Undo changes   Undo notebooks modification   resolve test   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   delete file   Undo   Fix test   Revert ""WIP""   This reverts commit f5828a8c426ff44275f560aec8d898f56da2cbfe.   Rename   Remove optimizers   Fix bug with LightningOptimizer   Add optimizers   update   update   Update CHANGELOG   On after backward refactor   Do not call super   Fixes   Remove should_accumulate   pre/post backward refactor   Call the LM backward hook   Update tests   Remove dev debug patch   Fix test   Remove optimizer arguments and typing   Docs fixes   Fix comment   Undo changes   Split manual and auto   Undo change   Deepsource   Remove optimizers   Undo changes   Call the hook   Docs   Docs   Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com",0
4034,Update Grid details in README.md (#7139),0.53685415,- Updated `TQDMProgressBar` to run a separate progress bar for each eval dataloader ([#11657](https://github.com/PyTorchLightning/pytorch-lightning/pull/11657)),,0
4035,GH action - label conflicts (#5450),0.46747363,Updated mlflow with using resolve_tags (#6746),,0
4036,Update CODEOWNERS (#6220),0.5601461,Update the Lightning App docs (#13537),,0
4037,release v0.4.0,0.8080354,0.4.0,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
4038,add .deepsource.toml (#8144),0.50904876,- Moved the warning about saving nn.Module in `save_hyperparameters()` to before the deepcopy ([#15132](https://github.com/Lightning-AI/lightning/pull/15132)),,0
4039,Bump actions/setup-python from 2 to 4 (#13952),0.5375681,Moved block_ddp_sync_behaviour out of TrainingBatchLoop to loop utilities (#9192),,0
4040,why copy? (#1579),0.46288967,Avoid using the deprecated LooseVersion (#16162),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
4041,update readme with new notebooks (#3608),0.5172724,This is how you use multi-device in notebooks now:,,0
4042,using lai docs template (#15486),0.59436077,Release LAI docs as stable (#14250),  wip   update   resolve bug   wip   wip   wip   resolved tests   update on comments   update   update   Update pytorch_lightning/utilities/auto_restart.py   Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com   update on comments   Update pytorch_lightning/utilities/auto_restart.py   Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com   resolve bug   update   move properties to top   update docs for fast forward sampler   move public attribute to top   add missing super call   update docs for state_dict   fix merge conflict   add missing super() call   move property to top   update on comments   update   resolve bug   update   update on comments   activate coverage for CaptureIterableDataset   update on comments   Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
4043,"Update torchtext requirement from <=0.12.0,>=0.10.* to >=0.10.0.a,<0.14.0 in /requirements (#13758)",0.7181894,"Set PyTorch 1.4 as min requirements, also for testing and examples torchvision>=0.5 and torchtext>=0.5 (#5418)",,1
4044,Clear env before test cases requiring empty env (#12561),0.5375904,  * `env_parse`,Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
4045,Make tbptt imports Python 3.10 compatible (#13973),0.71245825,Compatibility for Python 3.10,,1
4046,Fix EarlyStopping logic when min_epochs not met (#6705),0.6647614,"min_nb_epochs to min_epochs,",,0
4047,[RFC] Add self.lr_schedulers() to LightningModule for manual optimization (#6567),0.76169026,Add automatic optimization property setter to lightning module (#5169),,1
4048,rename on_expection -> on_exception (#8750),0.76404405,"on_exception = OnExceptionCheckpoint(""."")",,1
4049,Fix an incorrect CHANGELOG link (#7850),0.58723545,Removed logger_connector legacy code (#6733),,0
4050,Deprecate on_keyboard_interrupt callback hook (#9260),0.908526,Deprecated on_keyboard_interrupt callback hook in favor of new on_exception hook (#9260),,1
4051,Merge pull request #1630 from PyTorchLightning/hparams_logger,0.6606938,- Fixed logging to loggers with multiple eval dataloaders ([#12454](https://github.com/PyTorchLightning/pytorch-lightning/pull/12454)),,0
4052,add test for model weights equality when fault-tolerant training (#9481),0.7532936,Fault-tolerant Training,,1
4053,Refactor loop.setup_data with utility functions (#16918),0.67884153,Refactored Loops,"  edit arg to reload_dataloaders_every_n_epoch   init reload_dataloaders_every_n_epoch   edit logic to reload dl   update arg to test datamodule   update arg test dataloader   edit reload dl logic in eval loop   fix var name in reset_train_val_dataloaders   fix error, use current_epoch attribute   edit every_n_epoch to every_n_epochs   edit every_n_epoch to every_n_epochs   edit every_n_epoch to every_n_epochs   edit every_n_epoch to every_n_epochs   edit every_n_epoch to every_n_epochs   edit every_n_epoch to every_n_epochs   assert reload_dataloaders_every_n_epochs positive   assert reload_dataloaders_every_n_epochs positive   add trainer property should reload dl   update should reload dl in train loop   condition on should reload dl in eval loop   pep8   fix update should reload dl in train loop   add test case   replace assertion with misconfig exception   remove unused variable   remove unnecessary checks   replace to BoringModel   remove unrequired comment   deprecate _every_epoch   add deprecated argument to trainer   test case for deprecated arg   remove unrequired assertion in train loop   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  modify misconfig exception for int  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  conv bool to int of depreciated _every_epoch  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  update description of deprecated param  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  update deprecation warning  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   modify argument to int only   fix deprecated test function name   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   merge tests for reload dls   add propery should reload dl   removed and added to trainer property   use property in train loop   remove deprecated test   add deprecated test to new file   test case for exception   update test datamodule every_n_epochs   update trainer docs   update hooks with every_n_epochs   edit format if statement   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   Update CHANGELOG.md   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   typo in exception   pytest check only misconfig exception   remove unnecessary code in test   remove unnecessary code in deprec test   added match in test   typo in comment   revert to prev, keep only req in context manager   Apply suggestions from code review   docs   rebase   Apply suggestions from code review   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   fix import: model_helpers instead of model_utils   fix, add reload_dataloaders_every_n_epochs argument to data connector   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   add required imports   move deprecated log   add missing import rank_zero_warn   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  update varname in should_reload_dl_epoch  suggestion from code review   Fix CHANGELOG. Update deprecation versions   Minor change   change property name, mark protected   update property name   update property name   Remove deprecated *_loop.py files   Rename test func   Update CHANGELOG.md   use rank_zero_deprecation   update deprecation message in trainer api docs   test deprecation with real arg name in message   fix typo in trainer docs   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com",0
4054,Update Readme so that .test will work. (#659),0.60114354,- fixed all the .test() calls,,0
4055,update badges & drop Travis/Appveyor (#826),0.5783529,Changed progress_bar_refresh_rate trainer flag to disable progress bar when setting to 0. (#1108),,0
4056,Keep is_global_zero definitions in sync across strategy and trainer (#11761),0.58993196,Validation is now always run inside the training epoch scope (#7357),  update   add a link   update   remove star   update   Update .github/PULL_REQUEST_TEMPLATE.md   Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk   Update .github/PULL_REQUEST_TEMPLATE.md   update   update   Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk,0
4057,formatting tests: 4/n (#5846),0.55175936,"nb_test_batches to num_test_batches,",,0
4058,Adding non-layer param count to summary (#17005),0.66193867,Override some of the params with new values,,0
4059,drop nb sphinx (#3452),0.5659634,Pinned sphinx-autodoc-typehints with <v1.15 (#11400),,0
4060,[CLI] Add support for ReduceLROnPlateau (#10860),0.7521254,We have also added support for the ReduceLROnPlateau scheduler with shorthand notation:,,1
4061,Raise an exception if check_val_every_n_epoch is not an integer (#6411),0.5294882,"trainer = Trainer(..., val_check_interval=N, check_val_every_n_epoch=None)",,0
4062,Feature: wandb logger (#627),0.7681372,Removed wandb logger's finalize method (#1193),"  Fix mypy for utilities.device_parser   Fix remaining mypy issues + disable ignoring mypy errors   Return one Optional type annotation back   Fix annotation for the parse_tpu_cores method   Remove unused import   Include carmocca's suggestion and fix mypy issue   include carmocca's suggestion   add else statement to parse_gpu_ids to inform mypy gpus is a type of List[int]   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",1
4063,Increase TPU check timeout (#5598),0.9259977,Increased TPU check timeout from 20s to 100s (#5598),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
4064,adjust codeowners for PL (#16602),0.5743866,Changed deprecated enable_pl_optimizer=True (#5244),,0
4065,Remove no return warning from val/test step (#6139),0.98876536,Removed no return warning from val/test step (#6139),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
4066,Fix typo 'wether' (#15710),0.5788007,Changed overwrite to True (#16009),,0
4067,fix get dataloader size (#2375),0.7057772,Reset val_dataloader in tuner/batch_size_scaling (#9857),,1
4068,Fabric: Remove _Connector.is_distributed (#16327),0.60112584,Connect and Disconnect node (#16700),"  Add test for poptorch Options   Hacks to get manual plugin support   Revert changes   Fix tests + ensure logic follow suit   Update pytorch_lightning/plugins/training_type/ipu.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci   Cleaner   Cleaner   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
4069,[CLI] fix ssh listing stopped components (#15810),0.55449444,Porting fixes to autoscaler component (#16249),,0
4070,update notebooks (#6968),0.55050755,Set version as today (#13906),  add fix   toggle test   re-structure   update changelog   update comment   remove debugging assertion ,0
4071,Make plugin compatible for hpu release1.5 (#13338),0.6740565,Enabled plugins (#4041),,0
4072,fix: freeze mypy (#5634),0.5836011,Setup: added requirement freeze for the next major version (#14480),"  optimization docs   example input array docs   step and rank   on_gpu   auto-opt   truncated backprop   fixes   manual backward   backward   all gather   fix configure optimizers html   whitespace   toggle, untoggle   scripting   save hyperparameters   optimizer step   step functions   hparams   model size   summarize   header   rm model summary   Update pytorch_lightning/core/lightning.py   Update pytorch_lightning/core/lightning.py ",0
4073,Optimize CI for PL tests to run only when PL is modified (#13661),0.615721,Changed deprecated enable_pl_optimizer=True (#5244),,0
4074,[Lite] precision_plugin -> precision (#15001),0.79439753,precision plugins (#3504),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
4075,Fix log_dir tracking in case of multiple Trainer instances + DDP (#7403),0.7075565,DDP + loggers should be fixed,,1
4076,Move krshrimali to Alumni (#15568),0.41633523,@irasit @Borda @kaushikb11 ,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
4077,update slack link,0.5982276,- Removed the deprecated `LightningDataModule.on_save/load_checkpoint` hooks ([#14909](https://github.com/Lightning-AI/lightning/pull/14909)),  Fix progress bar updates for Pod Training   Fix progress bar updates for Pod Training   Add _pod_progress_bar_force_stdout ,0
4078,Adds Gradient Clipping to Fabric (#16715),0.72148776,Gradient Clipping Customization,"  update train step   test   x   limits   val   typeo   x   x   step   min gpus   run all loops   x   limit test   profiler   clean up accelerator code   move files   rename   move tests   changelog   reorder callbacks and model hooks   add test description   replace unneccessary method   fix chlog   adjust batch_to_device for DP Plugin   update tests for dataloader idx   unused imports   hook change   switch None   clear memory   change to None   None   None   memory savings   remove redundant todo   hack   cheat   Revert ""cheat""   This reverts commit a8433bd0b4bd35f218993335f7d4ff18977ae423.  Revert ""hack""  This reverts commit 43a6d1edeb62a15ac69ef69ef2352581ba1947a5.   update new epoch loop   remove from old loop code   update chlog   update hook test   changelog   teardown   integrate changes in new eval loop   fix hook calls   add prediction step   bad merge   Revert ""bad merge""   This reverts commit 488080863cf012dcf04446be3b7d973b7340687e.   fix train batch hook test   rm -rf _notebooks   update chlog   release memory   fix type   notebooks mess   debug   Revert ""debug""   This reverts commit eec4ee2f77b5eb39965211a250598ed5d2320e88.   teardown   fix teardown bug   debug   x   debug   Revert ""debug""   This reverts commit a6e61019462b80d09d31b65bed289fa6e4dd15f6. Revert ""debug"" This reverts commit 5ddeaec06911e96730aade1be6ee71d097b46b9a. debug debug Revert ""debug"" This reverts commit 605be746f7daedf265b2c05a1c153ce543394435. Revert ""Revert ""debug"""" This reverts commit a7612d5410409ed886cfb609457349ecf44cbfa8. debug x x x s tol x tol  Fix changelog  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com",1
4079,Update deepspeed requirement support window (#16813),0.6882202,Support for manual optimization with DeepSpeed (#7970),,0
4080,EarlyStopping logging on rank 0 only (#13233),0.7811862,Rank-zero only EarlyStopping messages,  Move result teardown to loops   Update CHANGELOG   Remove teardown from run   Move previous teardown to on_run_end   Add comment   Merge 8250   Remove stage set to None where it shouldnt ,1
4081,Use _PATH in annotations and convert to str if Path (#15560),0.54239506,Add code_dir argument to tracer run (#15771),  add loop restart   update ,0
4082,Merge branch 'master' into new-master,0.38660434,Removed TrainerProperties mixin and moved property definitions directly into Trainer (#9495),,0
4083,Merge branch 'master' into feature/plug,0.5609575,Removed Plugin in base_plugin.py in favor of accessing TrainingTypePlugin and PrecisionPlugin directly instead (#9066),,0
4084,Add test for torch.compile() with Fabric.setup() (#16977),0.6877893,compiled_model = torch.compile(model),  Fixes to ensure ipu options are respected   Better setter   Add test for poptorch Options   Fix test   fix ipu test   Update pytorch_lightning/plugins/training_type/ipu.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
4085,Fix typing for utilities.warnings (#11115),0.6197699,warning_utils >> warnings,"  Skip test due to 'Python bus error'   Debug NCCL   Remove NCCL_DEBUG statement   Revert ""Skip test due to 'Python bus error'""   This reverts commit e0a3e8785d2fecd63667da433a648f958d60ef89.   fix   add test   changelog   yapf   patch os environ   make a special test   destroy pg   debug   revert   revert   problematic test   skip   try the fixture   test   update sensitive test   update changelog   remove comment   update wrong test   update test name   parameterization   Revert ""parameterization""   This reverts commit b0542f43f59c5ce66800883b5e2f0c66a97408cc.   remove conftest   ignore test   teardown   fix merge   deep speed parameterization   uncomment test   update chlog   update changelog   split tests   update test   update test update test update test   update test comments   unroll test   unroll test   unroll test   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   increase shm   sudo   unroll ipu   Revert ""sudo""   This reverts commit 6cc68c1478c151caee86b8a0f8ded16b62a9c8ea.  Revert ""increase shm""  This reverts commit 8c2716348330dbc3c6b3685647f435127b870d79.   x   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   find guilty test   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   POPTORCH_WAIT_FOR_IPU=1   move test   redo parameterize for ipu   de-comment test   move chlog   Update tests/accelerators/test_accelerator_connector.py   Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com  Update tests/accelerators/test_accelerator_connector.py  Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com",0
4086,Support TPU Pod Training (n/n) (#7296),0.78938663,TPU training (#2708),,1
4087,Fix torchmetrics compatibility (#7131),0.7127224,Dropped torchvision dependency in tests and added own MNIST dataset class instead (#986),  Remove methods with unnecessary super delegation.   Update fully_sharded.py   replace init in test   Co-authored-by: deepsource-autofix[bot] 62050782+deepsource-autofix[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Ethan Harris ethanwharris@gmail.com,1
4088,part 5 of #3733 (#3774),0.7005848,[1.5.5] - 2021-12-07,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
4089,Update from_datasets to support arbitrary iterables (#17402),0.75079995,return iterabledataset,Co-authored-by: Jirka jirka.borovec@seznam.cz,1
4090,Remove references to local app admin view (#14306),0.53931296,Remove EpochResultStore and HookResultStore in favor of ResultCollection (#7909), Fix truncated_bptt_steps hiddens detach() Improve truncated_bptt_docs Add missing import Improve documentation wordings pep8 detach typo Update test Implement comments parametrize test Apply suggestions from code review  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Signed-off-by: Guillaume Tauzin guillaumetauzin.ut@gmail.com  Remove import  Signed-off-by: Guillaume Tauzin guillaumetauzin.ut@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
4091,refactored tests (#417),0.7477982,"Refactored training_batch + tests to verify correctness (#2327, #2328)",,1
4092,Bump actions/upload-artifact from 2 to 3 (#14289),0.5122478,Changed warnings and recommendations for dataloaders in ddp_spawn (#6762),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
4093,Add masked language modeling to community examples (#4744),0.5170806,model_utils >> model_helpers,  Update docs for new template   Fixes   Fixes   Drop links ,0
4094,Create memory dynamically (#4938),0.5857711,Resolve memory leak for evaluation (#6326),  Avoid Pillow 8.3.0   Move it to last ,0
4095,Remove examples and loggers from develop dependencies (#15282),0.64998156,Removed logger_connector legacy code (#6733),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
4096,fix examples (#3623),0.66330683,Resolve bug with Finetuning (#5744),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
4097,Be explicit with mypy ignores (#10751),0.5455272,Avoid metadata.entry_points deprecation warning on Python 3.10 (#14052),Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
4098,Add an option to overwrite the existing checkpoint file (#17320),0.6447929,    # you can modify the checkpoint dict directly,Co-authored-by: Ethan Harris ethanwharris@gmail.com,0
4099,Merge branch 'master' of https://github.com/williamFalcon/pytorch-lightning,0.6081593,- Moved ownership of the lightning optimizers from the `Trainer` to the `Strategy` ([#11444](https://github.com/PyTorchLightning/pytorch-lightning/pull/11444)),Co-authored-by: deepsource-autofix[bot] 62050782+deepsource-autofix[bot]@users.noreply.github.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
4100,debugging and gpu guide,0.70118284,DDP Debugging Improvements,,1
4101,Use websockets in e2es (#14138),0.4695983,Initial plugin server (#16523),  Fix double precision casting complex buffers   Update CHANGELOG.md   Fixes   Fixes   Fix   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
4102,Black format pytorch_lightning/core/datamodule.py (#3574),0.74276674,+ # pytorch_lightning==1.7.0,  update github template   Update .github/ISSUE_TEMPLATE/bug_report.md   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update .github/ISSUE_TEMPLATE/bug_report.md  Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com,1
4103,update tests for v2 (#11487),0.6737291,Updated app testing (#16000),,0
4104,docker: run ci only docker related files are changed (#5203),0.53255177,Saved the LightningCLI config on setup and only on the main process (#8017),,0
4105,ci: add current & total count to standalone test output (#17511),0.5625529,    # 2. Add the outputs to the list,,0
4106,LM docs (#4085),0.68943214,"docs for all Metrics (#2184, #2209)",,0
4107,added .md,0.53677213,Meta Module,Co-authored-by: Jirka jirka.borovec@seznam.cz,0
4108,[bugfix] Check LightningOptimizer doesn't delete optimizer hooks (#6305),0.96599686,Check LightningOptimizer doesn't delete optimizer hooks (#6305),,1
4109,Fix saving hyperparameters in a composition where parent is not a LM or LDM (#14151),0.70219904,Add ignore param to save_hyperparameters (#6056),,1
4110,Fix missing arguments when saving hyperparams from parent class only (#9800),0.7191417,Add ignore param to save_hyperparameters (#6056),  add should_raise   update changelog   Update pytorch_lightning/utilities/device_parser.py   Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com   add to tpu_cores parser   add should_raise description   update on comments   update changelog   Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,1
4111,Update PULL_REQUEST_TEMPLATE [skip ci] (#6000),0.55353165,Refactor cloud dispatch and update to new API (#16456),  add how to contribute   docs   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Apply suggestions from code review  Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Roger Shieh sh.rog@protonmail.ch  Update README.md  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Roger Shieh sh.rog@protonmail.ch,0
4112,Quant as optional step (#8464),0.53112507,Changed default value of the max_steps Trainer argument from None to -1 (#9460),  Improvements to progress dataclasses   Update CHANGELOG   Rename function   Undo CODEOWNERS update ,0
4113,Set the last global step saved only when actually saving (#12057),0.5744533,Moved save_hyperparameters to its own function (#7119),,0
4114,CI: HPU support v1.6.0 release (#14794),0.6228858,Moveed HPU broadcast override to the HPU strategy file (#17011),,0
4115,Remove the unused pyDeprecate dependency (#14472),0.7433693,"Removed PyTorch 1.6 support (#10367, #10738)","  EvaluationDataLoaderLoop -> EvaluationLoop   proposed rename files   imports   bad merge   update init files   glue imports together   rename fit_loop.validation_loop to fit_loop.val_loop   move loop   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Group imports   Resolve circular import   Comment   fix test   try to resolve circ import   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com",1
4116,formatting,0.49449357,print(trainer.logger),,0
4117,Create CODE_OF_CONDUCT.md (#96),0.58246124,"From now on, the backend has to be set in the code (#14693):","  Update accelerator_connector.py   Update accelerator_connector.py   Update accelerator_connector.py   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Update accelerator_connector.py   Update accelerator_connector.py   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com",0
4118,Add tests/utilities/test_parsing.py (#4460),0.721168,python script.py test,  add test   add fix   Update CHANGELOG.md ,1
4119,Remove debugging_connector.py (#10113),0.8016459,Removed pytorch_lightning.utilities.debugging.InternalDebugger (#9680),  Fix metric attribute lookup   Update CHANGELOG.md   Split tests ,1
4120,Raise an error if batch_size cannot be inferred from current batch (#10541),0.71333313,- Raised an error if the `batch_size` cannot be inferred from the current batch if it contained a string or was a custom batch object ([#10541](https://github.com/PyTorchLightning/pytorch-lightning/pull/10541)), device ids in barrier  x x s same fix for spawn fix non-nccl  x   add changelog   get nccl backend   get backend   Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,1
4121,lift tensorboard version restriction (#8765),0.69617164,Switch from tensorboard to tensorboardx in TensorBoardLogger (#15728),,0
4122,Fix DeepSpeed lr scheduler logic (#8527),0.7408632,- Fixed the lr-scheduler state not being dumped to checkpoint when using the deepspeed strategy ([#11307](https://github.com/PyTorchLightning/pytorch-lightning/pull/11307)),"  add mechanism to prevent deadlock   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   resolve flake8 + update changelog   update on comments   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   remove space   resolve bugs   overwrite config   update on comments   update on comments   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update   update   update test with comments   Update pytorch_lightning/plugins/training_type/parallel.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  update on comments  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",1
4123,fix(docs): docstring for amp_backend (#2960),0.62631726,enable_pl_optimizer=False by default to temporarily fix AMP issues (#5163),  resolve logging issue   update changelog   remove breakpoint   resolve bugs   remove pass ,0
4124,prune tests (#564),0.6077378,Tested pickling (#1636),[bugfix] Resolve memory not logged when missing metrics,0
4125,"Remove log_text, and log_image from LightningLoggerBase API  (#11857)",0.8780977,- Removed `log_text` and `log_image` from the `LightningLoggerBase` API ([#11857](https://github.com/PyTorchLightning/pytorch-lightning/pull/11857)),  Fix module dict in base finetuning   Update CHANGELOG.md ,1
4126,Add additional else check,0.5972384,        else:,,0
4127,Fix CLI test interaction (#13037),0.631186,- Fixed an issue when using the CLI without arguments ([#14877](https://github.com/Lightning-AI/lightning/pull/14877)),Co-authored-by: deepsource-autofix[bot] 62050782+deepsource-autofix[bot]@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz,0
4128,Add configure_columns method to RichProgressBar (#10288),0.6426772,from pytorch_lightning.callbacks import RichProgressBar,Co-authored-by: deepsource-autofix[bot] 62050782+deepsource-autofix[bot]@users.noreply.github.com,0
4129,fixed import,0.78761053,import argparse,Co-authored-by: deepsource-autofix[bot] 62050782+deepsource-autofix[bot]@users.noreply.github.com,1
4130,[App] Initial plugin server (#16523),0.9413757,Initial plugin server (#16523),Co-authored-by: deepsource-autofix[bot] 62050782+deepsource-autofix[bot]@users.noreply.github.com,1
4131,Fix mypy errors attributed to pytorch_lightning.loggers.mlflow (#13691),0.7354025,Deprecated pytorch_lightning.logging (#767),,1
4132,Misleading exception raised during batch scaling (#2223),0.67748904,"Raised MisconfigurationException when total length of dataloader across ranks is zero, and give warning when total length is non-zero, but only local rank length is zero. (#9827)",  ignore tests   . ,0
4133,add legacy load utility (#9166),0.602525,- Removed legacy argparse utilities ([#16708](https://github.com/Lightning-AI/lightning/pull/16708)),Co-authored-by: deepsource-autofix[bot] 62050782+deepsource-autofix[bot]@users.noreply.github.com,0
4134,Rename LightningLite to Fabric (#16244),0.9238544,"Renamed the class LightningLite to Fabric (#15932, #15938)",,1
4135,Add video tutorials to docs (#3977),0.64824677,"For a full tutorial and running example, visit our docs. TODO: add to docs",Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
4136,[docker] Lock cuda version (#4453),0.53063107,"- Trainer queries the CUDA devices through NVML if available to avoid initializing CUDA before forking, which eliminates the need for the `PL_DISABLE_FORK` environment variable introduced in v1.7.4 ([#14631](https://github.com/Lightning-AI/lightning/pull/14631))",Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
4137,ref: apex plugin (#3502),0.95486224,apex plugin (#3502),,1
4138,fix mypy typing errors in pytorch_lightning/strategies/dp.py (#13564),0.70312417,+ # pytorch_lightning==1.7.0,,1
4139,Clarify what the default values for log are based on hooks (#11611),0.67796165,Return the default_root_dir as the log_dir when the logger is a LoggerCollection (#8187),  Fix notebook links   update   BERT   docs   Update README.md   Apply suggestions from code review   Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
4140,Update tox.ini,0.48014015,moved TPU xxx_step to backend (#3118),,0
4141,Update Trainer flag docs for strategy (#10042),0.698679,"Refactor RunningStage and TrainerState usage (#4945, #7173)",  Add torchelastic check   Add changelog   Address review   fix ,0
4142,Relax lightning app dependency requirements (#13998),0.6872008,Update the Lightning App docs (#13537),,0
4143,Added support and test for custom artifact names in WandbLogger (#16173),0.6730132,"The WandbLogger.name property no longer returns the name of the experiment, and instead returns the project's name (#14145)","  active optimizers   check checkpoint callback   epoch loop properties   epoch loop methods   training_batch_loop   changelog   update chlog   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   unused imports   yapf   backward   fix missing string reference   is_last_batch remains public   remove dead code   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com",0
4144,data transfer model hook (+ refactor) (#1756),0.7242633,refactored dataloader process hook (#3139),"  integrate d180bb2   Minor changes   Refactor loop logic into logger connector   Refactor test   Tighter fx validator   Add back split idx   Typing   update   Conflict   Fix tests   resolve grad_norm   update   move to train loop   Bye grad_norm_dict parameter   Fix sync test   update   Fix bug when validation is run mid epoch   fix grad_norm_dict test   Fix fx_validator test   fix grad_norm_dict test   Fix order bug   Detach tensors in test   resolve some tests   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   remove pdb   resolve flake8   Update test   more tests   Revert last thomas' changes   resolve 1 test   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Refactor context restoration   integrate latest changes from logger connector refactor poc   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   integrate latest changes from logger connector refactor poc   Minor changes   update changelog   Remove unused argument   Update CHANGELOG   Copy call_hook changes   Docs   Fix ref   move to cpu   Bad merge   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   remove pdb   remove pdb   Refactor to   Avoid partial   trigger ci   Bad merge   integrate latest logger connector changes   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   remove grad norm dicts list   Diff   properties first   Bad merge   Reuse metrics_to_scalars   Use active loop   Move to device   resolve test   integrate latest changes from logger connector poc   define union   define union   Update logger connector   Update result   Update imports   Update after rename   Refactor reduce_fx and op   Fix test after rename   mypy   integrate latest logger connector refactor poc changes   Fix test   Refactor test   Deprecate self.log(sync_dist_op) in favor of self.log(reduce_fx)   Undo field   add redundant return   rename   rename files and classes  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci   rename   Replace code   Fix names and imports   Remove metric_attribute   imports   loop hygiene   yapf on loops   protected new loop trigger   rename NEW LOOP guard   integrate latest logger connector changes   integrate latest logger connector changes (eval loop)   resolve todo dataloading reset   re-add notebooks   add missing init   bad merge   remove NEW_LOOP guard   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   flake8   exclude coverage   coverage   integrate #7917, remove teardown from training loop   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  update ""accumulated_batches_reached"" condition  based on if iter count was updated  or not   remove public loop properties   make skip backward protected again   typing base loop   typing fit loop   typing training_batch_loop   typing evaluation loop   typing prediction loop   typing training epoch loop   dataloader_loop   evaluation_dataloader_loop   prediction_dataloader_loop   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   integrate train loop changes from master   integrate eval loop changes from master   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   fix tpipes moving model to cpu and leaving it there.   don't reset fit loop   don't reset fit loop   fix test iteration count <-> batch_idx reset   replace torch.Tensor -> Tensor   fix attribute error to block_ddp_sync_behaviour   fix flake8 and yapf conflict   remove redundant override   add classes   Co-authored-by: Justus Schock justus.schock@rwth-aachen.de Co-authored-by: Justus Schock justus.schock@posteo.de Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com   trainer changes   connect   clean up   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update test renaming   rename evaluation loop to evaluation epoch loop   minor docstring improvements   update chlog   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   try ci fix   update code owners for pl/loops   update mock path   re-order   simplify dataloader reset   simplify get_dataloaders()   save predictions on_run_end()   improve skip condition re-routing   re-order   remove unused type import   check which assert is failing   pig   hobbit   teardown for evaluation   Revert ""hobbit""   This reverts commit e81b0dbee31da813ba6ad58f74d236863c86d18e.  Revert ""pig""  This reverts commit 33d89e0720ce7380af80917b15a79362d9416ae7.  Revert ""check which assert is failing""  This reverts commit b7483b425cab95290eb2cbf354ccb0a77004df83.   free memory in fit loop teardown   update docstring   period   remove dead code   else carlos   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update pytorch_lightning/loops/dataloader/evaluation_dataloader_loop.py  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   update chlog   unused imp   move default construction in run_evaluation   add something for lawyer to read   switch typehint for eval loop trainer property   add missing imports   remove a todo that needs more discussion   combine _get_num_dataloaders with the property   Update pytorch_lightning/loops/dataloader/dataloader_loop.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   black + yapf   avoid coverage on old unused eval loop   empty space in docstring   Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk   resolve todo for args forwarding   weekproxy trainer   fix check for num dataloaders kwargs   clean up num prediction dataloaders property   free memory   rm notebooks folder   rm old file   revert changes to old eval loop   bad merge   undo teardown   setup signature   remove file for notes   free memory   chlog   Revert ""weekproxy trainer""   This reverts commit d4e6969170b80db4c9e6111fa9af507c740cde4a.   connect trainer   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   clean up max batches and dataloaders   max batches handling   no grad handling   unused argument   protected attrs   unused imports   undo unintentional rename   consistent naming   capitalization in docstring   list all args   Update pytorch_lightning/loops/prediction_epoch_loop.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update pytorch_lightning/loops/prediction_epoch_loop.py  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update pytorch_lightning/loops/dataloader/prediction_dataloader_loop.py  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update pytorch_lightning/loops/dataloader/prediction_dataloader_loop.py  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update pytorch_lightning/loops/prediction_epoch_loop.py  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: tchaton thomas@grid.ai Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Justus Schock justus.schock@posteo.de Co-authored-by: Justus Schock justus.schock@rwth-aachen.de Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk",1
4145,CI: skip hanging (#4943),0.5034963,moved hooks around in eval loop (#3195),,0
4146,Deprecate checkpoint_callback from the Trainer constructor in favour of enable_checkpointing (#9754),0.98189956,Deprecated checkpoint_callback from the Trainer constructor in favor of enable_checkpointing (#9754),  Add callback to hook tests and add predict test   Fix lambda callback test   Simplify lambda call test   Use LambdaCallback   Dynamically append to called for the model   Remove print   Consistency   Consistency   Prepare args/kwargs testing   yapf doesn't like dict literals   Add arguments for fit no val test   Add arguments for fit no val test   Test arguments   Datamodule refactor   Fix eval test   Update full fit + val test   Update test   Update resume test   Remove changes   Fix ,1
4147,Add function to remove checkpoint to allow override for extended classes (#16067),1.0000001,Add function to remove checkpoint to allow override for extended classes (#16067),Co-authored-by: Nisheeth Lahoti nisheeth@rephrase.ai,1
4148,Update .run_local_tests.sh,0.5797285,- Added a sanity check that scripts are executed with the `srun` command in SLURM and that environment variables are not conflicting ([#15011](https://github.com/Lightning-AI/lightning/pull/15011)),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
4149,Removed unnecessary _move_optimizer_state method overrides (#10849),0.7277968,  * Removed `optimizer_idx` argument from `PrecisionPlugin.optimizer_step` and all of its overrides in subclasses,,1
4150,Remove leftover clean_logger call in tests (#11080),0.7577381,Cleaning up stale logger tests (#3490),,1
4151,fixing install from src package & CI release (#16027),0.55682266,This release fixes that core issue,Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
4152,Refactor gpu test to use RunIf (#12605),0.6295074,Disabled val and test shuffling (#1600),,0
4153,clarify batch hooks (#2842),0.70848954,clean up hooks in run_evaluation (#3156),Very small typo correction: add forgotten : in finetuning callbacks docs.,1
4154,[App] Moved app.py to root dir for lightning init app <app_name> template (#13853),0.9394753,Unification of app template: moved app.py to root dir for lightning init app <app_name> template (#13853),,1
4155,drop sklearn dependency (#801),0.61715233,Removed dependency on scikit-learn (#801),,0
4156,Init: Models store API (#15811),0.60373694,      init_args:,,0
4157,Add type hints to model checkpoint callback (#3560),0.73048246,Deprecated passing ModelCheckpoint instance to checkpoint_callback Trainer argument (#4336),updates: - github.com/PyCQA/isort: 5.8.0 → 5.9.1 Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
4158,fix Logger tests for Win (#605),0.79008985,Cleaning up stale logger tests (#3490),,1
4159,removed deps,0.7828579,Removed,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
4160,Check parallel_devices passed through strategy is consistent with the accelerator flag (#12105),0.61936533,Ensure we set the eval/train flag correctly on accelerator model (#6877),,0
4161,triger ci only with pull request (#10932),0.5018216,Prevent WandbLogger from dropping values (#5931),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
4162,Fix mypy typing errors in strategies/fully_sharded.py (#13941),0.5845519,$ PL_FAULT_TOLERANT_TRAINING=MANUAL python script.py,,0
4163,Add on_epoch_start to run at the beginning of every loop irrespective of train/val/test (#6498),0.7494551,Changed the behavior of on_epoch_start to run at the beginning of validation & test epoch (#6498),,1
4164,ref: move backends back to individual files (1/5) (ddp_cpu) (#3712),0.8403255,move backends back to individual files (#3712),,1
4165,Single-process multi-node CPU training (#9603),0.7636782,Multiple CPU processes,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
4166,testing map location,0.43866748,"trainer.test(model, ckpt_path=""/path/to/checkpoint.ckpt"")",,0
4167,checkgroup,0.50005686,class CustomCheckpointIO(CheckpointIO):,Loop Refactor 3/N - Evaluation Loop,0
4168,1/n Simplify spawn plugins: Simplify handling of multiprocessing queue (#10034),0.66013545,"Implemented DeepSpeedPlugin._setup_model_and_optimizers (#10009, #10064)",Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
4169,Fix broken test_cpu_amp_precision_context_manager (#9809),0.5939856,Refactored setup_training and remove test_mode (#5388),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
4170,release v0.5.0,0.7823005,"    version=""0.0.1"",",Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
4171,add current_epoch to dumped_params (#3261),0.5629504,Refactored EpochResultStore (#5522),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk,0
4172,Update usage hinting for upgrading checkpoints (#17276),0.69486386,Resuming from checkpoints (#16167),,0
4173,Let DDPSpawnPlugin.spawn return a result from rank 0 (#10162),0.667603,Deprecated DDPPlugin.task_idx in favor of DDPPlugin.local_rank (#8203),  git submodule update --remote   update notebooks in docs   prune   _notebooks   docs   path   path   ignore   head ,0
4174,add wandb autoclass in logger.rst (#2854),0.69889975,Changed automatic casting for LoggerConnector metrics (#5218),,0
4175,"Revert ""import neptune instead of import neptune.new"" (#16898)",0.6288376,import argparse,,0
4176,Fix DataLoader inspection and re-instantiation in Lite (#10334),0.64948046,refactored dataloader process hook (#3139),"  add metric reload   add tests   update changelog   udpate   remove print   remove attribute_name   update   update   resolve test   update on comments   bypass typing bug   update on comments   Update CHANGELOG   Update tests   Update code   Check if TODO persists   Remove unrelated changes   Fixes   Revert ""Check if TODO persists""   This reverts commit 68dac4ae693764b9fca4c8f4cef4fce66ce92122.   Do not serialize dataclasses   Avoid recostructing meta twice   Keep previous sync_fn   Move to device and map_location   Fix bug   Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com",0
4177,improve pickle tests for callbacks (#1717),0.7900169,Tested pickling (#1636),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
4178,Update hook lifecycle (#6538),0.660429,Updated hooks arguments - breaking for setup and teardown (#2850),"  amp   amp   docs   add guides   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   amp   amp   docs   add guides   speed guides   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Delete ds.txt   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Update conf.py   Update docs.txt   remove 16 bit   remove finetune from speed guide   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   speed   speed   speed   speed   speed   speed   speed   speed   speed   speed   speed   speed   remove early stopping from speed guide   remove early stopping from speed guide   remove early stopping from speed guide   fix label   fix sync   reviews   Update trainer.rst   Update trainer.rst   Update speed.rst   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
4179,Skip testing with PyTorch 1.7 and Python 3.9 on Ubuntu (#11217),0.9762251,Skipped testing with PyTorch 1.7 and Python 3.9 on Ubuntu (#11217),  Remove port setting   Drop one of the params to see what happens   Split tests into two   Try using port setting ,1
4180,updated test,0.6751279,"Refactored training_batch + tests to verify correctness (#2327, #2328)",,0
4181,CI: Build new HPU docker image (#13352),0.59762657,Graphcore IPU devices,,0
4182,Prune deprecated metrics for 1.3 (#6161),0.71445155,Removed deprecated metrics (#6161),,1
4183,Include Lezwon in alumni (#11223),0.492849,@SkafteNicki @eladsegal,  add predict examples   update on comments ,0
4184,[App] Add interruptible work (#16399),0.58960295,App,"  resolve manual optimization   resolve manual optimization   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   update changelog   Simplify message   Move from deprecated   Split model parallel/manual model   Use property   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: SeanNaren sean@grid.ai",0
4185,fix connect/ disconnect without arguments (#14877),0.7220808,Connect and Disconnect node (#16700),Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
4186,Changed order of update_learning_rates() and run_training_teardown(). (#1891),0.6587588,"Refactor RunningStage and TrainerState usage (#4945, #7173)",Co-authored-by: Justus Schock justus.schock@rwth-aachen.de Co-authored-by: Justus Schock justus.schock@posteo.de Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,0
4187,Removed test_dataloader call in check_testing_model_configuration (#1670),0.72234297,Removed a redundant warning with ModelCheckpoint(monitor=None) callback (#9875),Co-authored-by: Yifu Wang yifuwang@2012@gmail.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
4188,removed print statements,0.64576894,Removed,Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
4189,Prune deprecated EarlyStopping(mode='auto') (#6167),0.6921557,Deprecated mode='auto' from ModelCheckpoint and EarlyStopping (#4695),,0
4190,do not fails all dockers (#2861),0.5432305,Prevent WandbLogger from dropping values (#5931),,0
4191,Disable tuner with distributed strategies (#12179),0.7589364,- Disabled tuner with distributed strategies ([#12179](https://github.com/PyTorchLightning/pytorch-lightning/pull/12179)),"   Exclude SaveConfigCallback for fast_dev_run=True.    SaveConfigCallback give a clearer message if config file already exists.   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci    Added unit test   Added entry in changelog  Improved save config docstring   Fix log line   Fixes   Fix changelog entry   Update pytorch_lightning/utilities/cli.py   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com  Apply suggestions from code review  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Suggested fixed change  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com",1
4192,Add support for saving with full state-dict in Fabric's FSDP (#17526),0.66417086,Auto-wrapping for FSDP Strategy,Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: tchaton thomas@grid.ai Co-authored-by: Justus Schock justus.schock@rwth-aachen.de Co-authored-by: Justus Schock justus.schock@posteo.de Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,0
4193,Remove deprecated ClustertEnvironment methods (#13458),0.71285665,Cluster creation and deletion now waits by default [#15458,Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
4194,Fix support for dataclasses with ClassVar/InitVar in apply_to_collection (#9702),0.53968096,- Added support for dataclasses in `apply_to_collections` ([#11889](https://github.com/PyTorchLightning/pytorch-lightning/pull/11889)),  Added base docs for IPUs   Fix   Add details around poptorch profiler and model parallelism   more description   Add image   Clearer messaging   Cleanup   Better name   Add note   Add some details around device iterations and model parallelism   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   Add a small install comment   Add clip gradients not supported   Update docs/source/advanced/ipu.rst   Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com  Add note  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,0
4195,Minor logger connector cleanup [1/n] (#7590),0.7382265,Removed logger_connector legacy code (#6733),,1
4196,[bugfix] Resolve LearningRateMonitor + BackboneFinetuning (#7835),0.6898931,Changed computer_vision_fine_tunning example to use BackboneLambdaFinetuningCallback (#5377),"  Update configs to match latest API   Ensure we move the entire model to device before configure optimizer is called   Add missing param   Expose parameters   Update references, drop local rank as it's now infered from the environment variable   Fix ref   Force install deepspeed 0.3.16   Add guard for init   Update pytorch_lightning/plugins/training_type/deepspeed.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   Revert type checking   Install master for CI for testing purposes   Update CI   Fix tests   Add check   Update versions   Set precision   Fix   See if i can force upgrade   Attempt to fix   Drop   Add changelog   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com",0
4197,revert docs,0.6012898,Refactoring,,0
4198,new section + legacy 1.1.6 (#5666),0.69107616,[1.3.6] - 2021-06-15,Co-authored-by: Daniel Dale dan@distributedinsight.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
4199,Fix icon encoding in PL App pretty-print (#14795),0.47332528,Included app templates to the lightning and app packages (#13731),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
4200,fix tests,0.7463666,- fixed all the .test() calls, remove parsing comments \s  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
4201,continues develop (#1419),0.55290043,[1.5.2] - 2021-11-16,Co-authored-by: ehuang68 <> Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
4202,[Blocking CI] Fix pep8 error about unused imports (#9090),0.59687626,Module imports are now catching ModuleNotFoundError instead of ImportError (#9867),  Seed all workers when using DDP   Fix to dataloader seeding   Make argument name explicit   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   Use f-strings when logging   Removed a redundant log message   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
4203,Hang (#2488),0.5962174,(#16002),Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,0
4204,Fix/test pass overrides (#918),0.62153256,Enabling val/test loop disabling (#2692),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
4205,Fix deepspeed scheduler initialization (#12031),0.7409911,"Implemented DeepSpeedPlugin._setup_model_and_optimizers (#10009, #10064)",,1
4206,Remove support for PL_INTER_BATCH_PARALLELISM (#16355),0.6273751,- Removed the experimental `PL_INTER_BATCH_PARALLELISM` environment flag ([#16355](https://github.com/Lightning-AI/lightning/pull/16355)),  update chlog   legacy ,0
4207,Deprecate progress_bar_refresh_rate from Trainer constructor (#9616),0.872081,"Deprecated passing progress_bar_refresh_rate to the Trainer constructor in favor of adding the ProgressBar callback with refresh_rate directly to the list of callbacks, or passing enable_progress_bar=False to disable the progress bar (#9616)",Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
4208,Update rank_zero_only decorator for LSF environments (#12587),0.624889,Set better defaults for rank_zero_only.rank when training is launched with SLURM and torchelastic (#6802),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
4209,docs clean up,0.6662351,Docs improvements,Signed-off-by: Max Ehrlich max.ehr@gmail.com,0
4210,Fix version check in DDP plugin test (#6906),0.6743394,"We had a few (subtle) bugs that affected DDP and a few key things in 0.7.2 so we released 0.7.3 to fix them because they are critical for DDP. sorry about that! still, no API changes, but please do skip straight to  0.7.3 upgrade for those fixes","  Initial changes   Add broken example for now   Fix reference   Fix format   Code runs   Fixes   Clear up files   Add tests, helpers, fixes   Small cleanups   Refactors based on review   Swap to special tests   Add special tests   Add source   Cleanups   Add logic to attach/detach model from devices   Fixes for tests   Fixes for tests   Move earlier   Cleanups   Add check for nvcc   Add tests, cleanups   Fix errors   fix   Try condition   Add missing annotation   Clearer   Clearer message   Fix variable   Cleanups   Add comment   CHANGELOG.md   Add simple selection test   Remove special=True to see what happens   Fix test   Update tests/accelerators/test_ipu.py   Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com   Convert ipu_cores -> ipus   Add typing, fail earlier   simplify precision   Add test, add helper   fix accum   Update pytorch_lightning/plugins/training_type/ipu.py   Co-authored-by: thomas chaton thomas@grid.ai   Use stages   Make sure warning message returned   thorw error   Add more tests, use fs   add comment   Clean   Address feedback, add IPU tests   Fixes   Fix signature   Add types   Remove autoround   Add docstring   ipu_cores -> ipus   Add test, remove unnecessary precision set   Add optimizer test   Add precision back with test   Address code review   Change to probs   Move some of the asserts earlier   Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: thomas chaton thomas@grid.ai",0
4211,Set correct device ids in DDP [wip] (#4297),0.68792367,"If devices > 1, it selects ddp for you","  plugin loading logic   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   integrate loading for test   fix   fix   unused iport   Update pytorch_lightning/trainer/connectors/checkpoint_connector.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com",0
4212,Remove the deprecated code in pl.utilities.data (#16440),0.81793416,- Removed the deprecated code in:,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
4213,fix: update example autoencoder.py to reflect args (#6638),0.60454506,Porting fixes to autoscaler component (#16249),,0
4214,cleaned readme,0.62625825,remember to clear it before continuing,"  removed monitor default value and added depreceation message   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   format change   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   requested changes   added test   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   format changes   typehint change   Update CHANGELOG.md   requested changes   regex   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com",0
4215,fix loading with hparams (#2403),0.6822839,Fixing critical bugs in newly added hooks and hparams assignment.,,0
4216,Early stopping doc (#3193),0.6193014,Early stopping checks on_validation_end (#1458),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
4217,Remove the deprecated Accelerator.setup_environment method (#16436),0.7810186,- Removed the deprecated `Accelerator.setup_environment` method ([#16436](https://github.com/Lightning-AI/lightning/pull/16436)),,1
4218,ci: move Flagships to GH (#16420),0.5142306,"refactored Horovod backend (#3121, #3122)",,0
4219,create new Conda images (#5877),0.44015846,Allow uploading models on W&B (#1339),Co-authored-by: tchaton thomas@grid.ai Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
4220,Rename CORE_CONTRIBUTOR_GUIDELINES to BECOMING_A_CORE_CONTRIBUTOR.md,0.5652127,Renamed and moved core/step_result.py to trainer/connectors/logger_connector/result.py (#7736),,0
4221,Back hook (#424),0.717544,Callback hooks,,1
4222,fixed none bug,0.67945117,tons of bug fixes :wink:,,0
4223,set codecov as informational (#17453),0.57392156,"From now on, the backend has to be set in the code (#14693):",Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
4224,if return -1 from a hook that loop stopps,0.69844675,Set Loop.restarting=False at the end of the first iteration (#8362),,0
4225,deprecate enable_pl_optimizer as it is not restored properly (#5244),0.85597587,Changed deprecated enable_pl_optimizer=True (#5244),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
4226,Fix truncated backprop through time when set on LightningModule and not Trainer (#8804),0.698238,"Trainer(resume_from_checkpoint=...) now restores the model directly after LightningModule.setup(), which is before LightningModule.configure_sharded_model() (#7652)",,0
4227,Small fix in test_cli.py to avoid failure with future version of jsonargparse (#16156),0.593465,Removed duplicated module pytorch_lightning.utilities.arg_parse for loading CLI arguments (#1167),  Fix logs overwriting issue for remote fs   Add test ,0
4228,Remove the BaguaStrategy (#16746),0.6530502,remove _evaluate fx (#3197),,0
4229,Improve progress.py docstrings (#9284),0.6871712,Docs improvements,  New logger connector code   Update CHANGELOG   Update requirements   Fix import path   Add new suffix   Tests   Minor changes   Rename and reorder   Fix import   Formatting   Fix with seed_everything?   Fix test?   Fix test?   Minor change   Minor changes   Minor changes   Force float   Fix minimal bug   Fix minimal bug   Update with latest changes   Fix import   bad merge   update typing   Co-authored-by: tchaton thomas@grid.ai,0
4230,populate some more legacy checkpoints (#5457),0.7008863,Resuming from checkpoints (#16167),  Use apply_to_collection in metrics_to_scalars   Typing   Update CHANGELOG   Update pytorch_lightning/utilities/metrics.py   Whitespace ,1
4231,DOC Minor import fix (#192),0.62773895,Module imports are now catching ModuleNotFoundError instead of ImportError (#9867),"  drop notebooks   add submodule   copy notebooks   docs include ipynb   fix headers   CI   readthedocs   manifest   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   req   workdir   pandoc   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   pandoc   manifest   Apply suggestions from code review   fix versions   checkout   git submodule update --init --recursive --remote   notebooks @docs   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
4232,release v0.5.3.1,0.79184794,0.4.0,  use void   format ,1
4233,CI: Azure publish results (#6514),0.5069201,Resolved a bug where the wrong client was passed to collect cloud logs (#14684),Random fixes for logger connector PoC,0
4234,meta pkg: set version as today (#13906),0.6886424,Set version as today (#13906),,0
4235,using slurm flag to fine node nb,0.59501255,    root_node = os.environ['SLURM_NODELIST'].split(' ')[0],,0
4236,Remove Trainer(multiple_trainloader_mode) in favor of CombinedLoader(mode) (#16800),0.7742586,"To simplify the Trainer interface and with the goal of simpler iterable support inside the Trainer, we removed theTrainer(multiple_trainloader_mode=...) argument. The mode is now agnostic to the trainer stage (""train"" previously) and it's easier to debug and understand for the user as the logic is all encapsulated in the CombinedLoader","  move validation to after aggregation   changelog   add test for training_step_end   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",1
4237,Exclude docs/ from default pytest dirs (#10078),0.62805855,  * Removed the `pytorch_lightning.plugins.ApexMixedPrecisionPlugin` class,  Stricter FxValidator and add hooks   Update CHANGELOG ,0
4238,remove duplicate metric vs step log for train loop (#4173),0.9897566,Removed duplicate metric vs step log for train loop (#4173),,1
4239,Add XLACheckpointIO (#9972),0.5502912,Changed ModelCheckpoint version suffixes to start at 1 (#5008),"  add warning   update changelog   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   logger check   add docstring for test   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com",0
4240,Fix configuration validation error message in Lite CLI (#16334),0.6425717,Configuration Validator (#9779),,0
4241,[TPU] Fix workflow (#17406),0.6396664,Renamed TPUHalfPrecisionPlugin to TPUBf16PrecisionPlugin (#10026),"  add extra test   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   add computation   Update docs/source/common/trainer.rst   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update docs/source/common/trainer.rst  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update tests/trainer/test_dataloaders.py  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   use tmpdir   update on comments   update   Update tests/callbacks/test_progress_bar.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com",0
4242,fix batch auto scaling when init_val causes OOM (#8954),0.6430954,Reset the dataloaders on OOM failure in batch size finder to use the last successful batch size (#14372),"  Add hooks   Add tests for hooks   Add changelog   Test changes, add typing ",0
4243,Preparing for 1.6.0rc1 (#12453),0.578161,Implemented ready for components (#16129),  add test + resolve bug   update changelog   resolve bug   resolve bug   Update pytorch_lightning/callbacks/lr_monitor.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/callbacks/lr_monitor.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   update on comments   resolve comments   update   Update tests/callbacks/test_lr_monitor.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update pytorch_lightning/callbacks/lr_monitor.py  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
4244,fix test for profiler (#800),0.60806787,Removed ProfilerConnector (#7654),,0
4245,Remove Trainer(move_metrics_to_cpu=True) (#16358),0.9402101,Removed the Trainer(move_metrics_to_cpu=True) argument (#16358),,1
4246,Fix rich with uneven refresh rate tracking (#11668),0.70548403,- Fixed `RichProgressBar` progress when refresh rate does not evenly divide the total counter ([#11668](https://github.com/PyTorchLightning/pytorch-lightning/pull/11668)),Co-authored-by: ananthsub ananth.subramaniam@gmail.com,1
4247,Added strict=False for load_from_checkpoint (#2819),0.68611014,Called on_load_checkpoint before loading state_dict (#4057),"  minor fixeS   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
4248,Requirements: try Traitlets >= 5.3.0 (#14679),0.5509044,Implemented ready for components (#16129),,0
4249,fixed new amp bugs (#1593),0.7029742,enable_pl_optimizer=False by default to temporarily fix AMP issues (#5163),,1
4250,[Refactor] Improve auto-encoder example (#9402),0.6112268,Porting fixes to autoscaler component (#16249),,0
4251,Fix optimizers overloads typing annotation (#10069),0.6335021,Resolve bug with Finetuning (#5744),,0
4252,Split progress bar (#449),0.8339405,progress bar,"  Add special tests for IPUs, run nvprof only if cuda available   Add missing min_gpu ",1
4253,Fix docs for 0.8.0 (#2162),0.598918,Docs improvements,  Modify API to ensure hooks defined in the accelerator are called as expected   handle step_end in dp   Add changelog   Update pytorch_lightning/trainer/trainer.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Add todo and explanation  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
4254,Fix TQDMProgressBar reset and update to show correct time estimation (#12889),0.8387464,- Fixed `TQDMProgressBar` reset and update to show correct time estimation (2/2) ([#13962](https://github.com/Lightning-AI/lightning/pull/13962)),,1
4255,Update BECOMING_A_CORE_CONTRIBUTOR.md (#11337),0.68034256,This release fixes that core issue,"  Update training_loop.py   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Update test_training_loop.py   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Update test_training_loop.py   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Update training_loop.py   Update pytorch_lightning/trainer/training_loop.py   Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk   Update test_training_loop.py   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Update training_loop.py   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  Update pytorch_lightning/trainer/training_loop.py  Co-authored-by: ananthsub ananth.subramaniam@gmail.com   Update training_loop.py   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Update test_training_loop.py   Update training_loop.py   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  escape regex  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk Co-authored-by: ananthsub ananth.subramaniam@gmail.com",0
4256,Dp default (#560),0.6134696,Deprecated DDPPlugin.task_idx in favor of DDPPlugin.local_rank (#8203),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com,0
4257,Clarify cluster advanced docs (#16403),0.69599843,Enabled custom clusters (#4048),"  Fixed support for torch Module type hints in LightningCLI    Fix issue with serializing values when type hint is Any.    Run unit test only on newer torchvision versions in which the base class is Module.   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Minor change   Update CHANGELOG.md   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com",0
4258,Use a single instance of rich.console.Console throughout the codebase (#12886),0.976848,Use only a single instance of rich.console.Console throughout codebase (#12886),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
4259,fix case where an IterableDataset doesn't produce a batch for an epoch (#7294),0.6559552,Disabled sampler replacement when using IterableDataset (#11507),"  integrated vulture CI   added vulture in workflows   added vulture in workflows   vulture logs verbose set false   Apply suggestions from code review   ignore name list and args to underscore naming   add ignore names   deadcode whitelist   deadcode whitelist   Apply suggestions from code review   Co-authored-by: Rahul Jha rahul722j@gmail.com  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci   Update whitelist   Sort   Updates   Updates   Apply suggestions from code review   Updates   Co-authored-by: Aniket Maurya aniket.maurya@gdn-commerce.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: chaton thomas@grid.ai Co-authored-by: Rahul Jha rahul722j@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk Co-authored-by: Ethan Harris ethanwharris@gmail.com",0
4260,Prune deprecated checkpoint arguments (#6162),0.6368968,Pruned requirements duplicity (#13739),  Add FSDP docs   Address reviews   Add note about how FSDP can replace pipe parallelism   Add import   Remove sentence ,0
4261,Examples: expose learning rate (#17513),0.73643583,Learning Rate Finder (#13802),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
4262,Update the MockOptimizer set_to_none default to match PyTorch (#17463),0.6925733,Enable PyTorch 1.7 compatibility (#3541),  Initial fix   Initial fix   Initial fix   Updates   Updates   Update typing and docs   Undo accidental refactor   Remove unused imports   Add DDP double precision test   Remove unused variable   Update CHANGELOG.md   Fix test   Update tests   Formatting   Revert bad change   Add back changes   Correct wrapping order   Improve unwrapping   Correct wrapping order   Fix... finally   Respond to comments   Drop ddp test   Simplify ddp spawn test   Simplify ddp spawn test   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
4263,Only track dev debugger events if enabled (#7875),0.60113215,"Disabled checkpointing, earlystopping and logging with fast_dev_run (#5277)",  apply_to_collection improvements and add apply_to_collections   Update CHANGELOG   Minor fix   Minor fix   Remove attr   Swap is first is None   None test   OrderedDict support   flake8   Fix docstring ,0
4264,skip some App tests (#17401),0.77874625,Updated app testing (#16000),,1
4265,Remove Strategy.optimizer_zero_grad (#11246),0.8107183,        optimizer.zero_grad(),,1
4266,ci: replace pip cache with wheels (#16668),0.59157443,Deprecated the use of CheckpointConnector.hpc_load() in favor of CheckpointConnector.restore() (#7652),  update precommit   Update .pre-commit-config.yaml   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Apply suggestions from code review  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
4267,Add compatibility matrix (#16998),0.599418,Configuration Validator (#9779),,0
4268,Update requirements.txt,0.5750625,Show a message when BuildConfig(requirements=[...]) is passed but a requirements.txt file is already present in the Work (#15799),  call time_elapsed   elapsed formatting   format   update test   changelog ,0
4269,Avoid flattening hyperparameters in WandbLogger (#17574),0.8844686,Flattening Wandb Hyperparameters (#2459),  Use typing forward references   Update pytorch_lightning/core/lightning.py ,1
4270,Log gradient norms of any magnitude (#16877),0.7670622,Gradient norm tracking (#16745),  Remove dev debugger metric tracking   Fix tests   Fix test   Import   Clean logging tests   flake8   Docstring ,1
4271,Fix docstrings of on_fit_start/end #12016,0.6588276,| on_pretrain_routine_start  | on_fit_start                 | ,  Some test updates   flake8 ,0
4272,Update contributing templates (#8256),0.6157384,Use correct python version in lightning component template (#13790),  Organize trainer properties   Single quote   Double quote ,0
4273,Update strategy flag in docs (#10000),0.615808,Updated governance docs,  Remove dev debugger metric tracking   Fix tests   Fix test   Import   Fix tests   Fix test   flake8   Fix tests ,0
4274,"Fix flaky test, that is not consistent on some configurations (#12707)",0.6223066,Disabled val and test shuffling (#1600),  Add Test to ensure Training Batch is no longer in GPU memory when running validation   Add Test to ensure Training Batch is no longer in GPU memory when running validation   Add Test to ensure Training Batch is no longer in GPU memory when running validation   Temporary disable other tests   Verbose asserts   Verbose asserts   update tests + revert to original ci   Update test_evaluation_loop.py   Update test_evaluation_loop.py   Update tests/trainer/loops/test_evaluation_loop.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update tests/trainer/loops/test_evaluation_loop.py  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: justusschock justus.schock@psoteo.de Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
4275,(app) Make Logging DEBUG mode lazy (#14464),0.66860795,"Disabled checkpointing, earlystopping and logging with fast_dev_run (#5277)",,0
4276,Fault Tolerant Manual: utilities cleanup (#10703),0.8003293,    * Cleanup some fault tolerant utilities ([#10703](https://github.com/PyTorchLightning/pytorch-lightning/pull/10703)),Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
4277,[Model Parallel] Add configure sharded model hook (#6679),0.6199916,Removed deprecated model hooks (#3980),Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,0
4278,pkg relative imports,0.615875,import argparse,,0
4279,Avoid optional instances in Loops (#10735),0.6304391,- Removed support for loop customization,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
4280,Resolve instantiation problem with init_meta_context (#10493),0.59404373,Support shorthand notation to instantiate datamodules (#10011),,0
4281,Correct function name (#9859),0.5334607,Re-enabled naming metrics in ckpt name (#3060),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
4282,Fix changelog of setting back benchmark=False by default (#13194),0.6047151,bug fix with logging val epoch end + monitor (#3812),,0
4283,Expose DeepSpeed FP16 parameters due to loss instability (#6115),0.8817626,Expose DeepSpeed loss parameters to allow users to fix loss instability (#6115),,1
4284,Parity test (#1284),0.57536936,test_percent_check in favour of limit_test_batches,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
4285,Update pip upgrade command in CI (#17395),0.5696657,pip install rich,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
4286,Metrics load (#228),0.6418955,"docs for all Metrics (#2184, #2209)",  Reset cache weekly   Update ci_test-base.yml   Update docs-checks.yml   Update ci_test-mnodes.yml   Update release-pypi.yml   Remove if latest ,0
4287,update links for collect_env_details.py script (#8436),0.5135138,Deprecates the pytorch_lightning.utilities.enums.AMPType enum, fix: improve UserWarning message when both overfit and training dtaloader shuffling are enabled  fixes issue: #7656   chore: update changelog   Polish userwarning msg in pytorch_lightning/trainer/data_loading.py   Co-authored-by: ananthsub ananth.subramaniam@gmail.com   shuffling typo   Update CHANGELOG.md   Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
4288,remove irrelevant docs in optimizer_step (#4964),0.70748985,# 3. Remove the `optimizer_idx` argument from `training_step`,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
4289,Enable on_before_batch_transfer for DPStrategy and IPUAccelerator (#14023),0.7501221,- Enabled `on_before_batch_transfer` for `DPStrategy` and `IPUAccelerator` ([#14023](https://github.com/Lightning-AI/lightning/pull/14023)),,1
4290,Prune metrics: precision & recall 6/n (#6573),0.7215253,Renaming of precision recall metric (#3308),  Remove on epoch guard from the should stop validation check   Formatting ,1
4291,Fix outdated docs follow up (#1266),0.5862299,Docs improvements,  Remove on epoch guard from the should stop validation check   Formatting ,0
4292,Update datamodules.rst (#3026),0.67748845,Replaced _DataModuleWrapper with __new__ (#7289),  Remove on epoch guard from the should stop validation check   Formatting ,0
4293,Fix mypy errors attributed to pytorch_lightning.demos.boring_classes (#14201),0.7274531,+ # pytorch_lightning==1.7.0,,1
4294,update outdated links in examples (#10127),0.58349776,Updated app URLs to the latest format (#16568),  Increment the total batch idx before the accumulation early exit   Update CHANGELOG ,0
4295,Lite: Don't pop value if they don't exist (#10613),0.50376993,Don't raise a warning when nn.Module is not saved under hparams (#12669),,0
4296,simplify accelerator steps (#5015),0.98228395,Simplify accelerator steps (#5015),,1
4297,Update tests README to point to tests/requirements.txt (#935),0.6281966,Deprecated the TestTubeLogger (#9065),,0
4298,Update dataloader docstrings (#17061),0.6537163,refactored dataloader process hook (#3139),  Fix global step update when the epoch is skipped   Update CHANGELOG   Move test ,0
4299,tasks docs,0.65307903,Docs,  Move parameter validation specific to TPU Training plugins   update docstring ,0
4300,added lightning model docs,0.7353402,LightningCLI Improvements,  Remove ProfilerConnector class   Update trainer.py   Update CHANGELOG.md   Update trainer.py   Update trainer.py   tests ,1
4301,correct trainer.fit production example (#2068),0.8592365,trainer.fit(compiled_model),  Check progress bar existence before printing   Add tests for predict_progres_bar   Add tests for progress_bar printing without training   Update changelog ,1
4302,remove deprecation of gpu string parsing behavior (#8770),0.60531664,- Removed the deprecated code in:,,0
4303,Return only unique names/versions for LoggerCollection (#10976),0.7871196,- `LoggerCollection` returns only unique logger names and versions ([#10976](https://github.com/PyTorchLightning/pytorch-lightning/pull/10976)),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
4304,Move checkpoint io plugins from pl/plugins/io to lite/plugins/io (#14519),0.65268016,CheckpointIO Plugins," Fix some test errors Summary:  Test Plan: Reviewers: Subscribers: Tasks: Tags:   checkpoint consolidation   Update ddp_spawn.py   Update test_metric_result_integration.py   Update test_results.py   Update utils.py   Update utils.py   Update test_all_gather_grad.py   Update test_all_gather_grad.py   Update test_results.py   Revert ""Update test_results.py""   This reverts commit 9d4a2b891d2a4b37e21529a444bda1883d1b5ed1.  Revert ""Merge pull request #1 from shuyingsunshine21/shuyingsunshine21-checkpoint_consolidate""  This reverts commit c5053da789f9d04d2c967a65adf4fb026dc134b8, reversing changes made to 0d23d75bc91e4e0b7805712e394cb093fac22841.  Revert ""Update test_all_gather_grad.py""  This reverts commit 0d23d75bc91e4e0b7805712e394cb093fac22841.  Revert ""Update utils.py""  This reverts commit 70fe5da9c66ceff2fcf4be5b9efdd23a9af8389c.  Revert ""Update utils.py""  This reverts commit a9aae99f6ed6e9388ecf1d8a7bd79966176a65af.  Revert ""Update test_results.py""  This reverts commit ea749068785bbad689a12066544893b1605f20c5.  Revert ""Update test_metric_result_integration.py""  This reverts commit bf70e431b3ce4893de804e0f3b5d59e79346d6d7.  Revert ""Update ddp_spawn.py""  This reverts commit f17210183b84f90c9a62d1ff9b3e05e1fbe5f33b.  Revert ""checkpoint consolidation""  This reverts commit 536c1323b0e6715fb5919196ea48b0fcddddcd66.  Revert ""Revert ""checkpoint consolidation""""  This reverts commit 3a9fde915ad4c69620a6ccc411f5890cb38ba5ac.  Revert ""Revert ""Revert ""checkpoint consolidation""""""  This reverts commit 7a369f47e1a94d701fce48c994cc3f2da266dad0.  Revert ""Revert ""Update ddp_spawn.py""""  This reverts commit 8222dc98ead37d961a52b7366070aa10f66d92d1.  Revert ""Revert ""Update test_metric_result_integration.py""""  This reverts commit 6c095b2370a2afe9d24918a5798ce1ebffed7e0d.  Revert ""Revert ""Update test_results.py""""  This reverts commit 250d0aaaa2e6c6a6a3407bc6c8b83c0fe2479c0b.  Revert ""Revert ""Update utils.py""""  This reverts commit 8651d54d79396eaaba16d7eb1e769a1e91d5702e.  Revert ""Revert ""Update test_all_gather_grad.py""""  This reverts commit dcdcd29731061c919b15ab0b56669259817a81c4.   modify distributed environment to make test pass   fix version for ddp plugin test   fix   fix   changelog   Update CHANGELOG.md   fsdp with full state dict   fix missing import   modify unitest   fix   fix   fix typo   modify test and add changelog   fix   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   limit max_epoch to 1 for testing   test   fix   update   testing remove special for multi gpu   assert gpu   add assertion for gpu   fix   Re-enable special test, use ModelCheckpoint   Fix paths   Fix path passing   test   test   fix test   fix   pre-commit format   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: SeanNaren sean@grid.ai",0
4305,Centralize DDP speedups in docs (#12448),0.699752,Speeding up DDP with find_unused_parameters (#16611)," fix: avoid potential mismatched toggling of optimzier Refs #7405  chore: update CHANGELOG [pre-commit.ci] auto fixes from pre-commit.com hooks for more information, see https://pre-commit.ci fix: resolve a confict chore: update changelog   feat: add a test that fails in master   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  fix typo in tests/trainer/optimization/test_multiple_optimizers.py  Co-authored-by: ananthsub ananth.subramaniam@gmail.com  Polish tests/trainer/optimization/test_multiple_optimizers.py  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Polish tests/trainer/optimization/test_multiple_optimizers.py  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  fix: change placeholder in optimizer_step from positional args to keyword args  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com",0
4306,removed deprecated trainer flags (#3969),0.7681607,Removed the deprecated TrainerTrainingTricksMixin class (#8679),,1
4307,ref: clean up data reset (#3161),0.96424115,clean up data reset (#3161),,1
4308,Deprecate Trainer.devices in favor of Trainer.num_devices and  Trainer.device_ids  (#12151),0.8613978,Trainer.num_devices and Trainer.device_ids,Co-authored-by: ananthsub ananth.subramaniam@gmail.com,1
4309,Support post-localSGD in Lightning DDP plugin (#8967),0.7000939,- Added support for DDP Fork ([#13405](https://github.com/Lightning-AI/lightning/pull/13405)),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com,1
4310,mypy: ignore mypy serve (#15631),0.5317049,def MyCallback(pl.Callback):,Co-authored-by: Yifu Wang yifuwang@2012@gmail.com,0
4311,Ignore dead links due to fabric docs move (#16920),0.5793124,fabric.backward(loss),,0
4312,Docker: fix NCCL building Horovod (#12318),0.6696177,"refactored Horovod backend (#3121, #3122)",  [feat] Support custom filesystems in LightningModule.to_torchscript   Update CHANGELOG.md   Update test_torchscript.py   Update test_torchscript.py   Update CHANGELOG.md   Update test_torchscript.py ,0
4313,Run XLA's dataloader validation per dataloader (#16775),0.720485,enabled multiple dataloaders for validation.    ,,1
4314,[Refactor]  1/2 Move reset_on_restart within the loop reset (#9561),0.8043886,Use completed over processed in reset_on_restart (#9656),,1
4315,Update lightning.py (#3929),0.8138759,Update the Lightning App docs (#13537),  Refactor FxValidator   Fix tests   Fix tests   Class attribute   Fix tests   Better error message   Fix tests   Update pytorch_lightning/trainer/connectors/logger_connector/fx_validator.py ,1
4316,remove torch<1.3.0 warning from tb logger (#3784),0.6970614,Disable torch.inference_mode with torch.compile in PyTorch 2.0 (#17215),  fix method name to flush_logs_every_n_steps in logging doc   apply corrections in comments ,0
4317,Avoid warning when cloning tensor in self.log (#14599),0.6819835,self.log-ing without a Trainer reference now raises a warning instead of an exception (#9733),,0
4318,Raise a warning when nn.Module instance is saved with save_hyperparameters() (#12068),0.78429174,Don't raise a warning when nn.Module is not saved under hparams (#12669),"  Add run_name argument to the MLFlowLogger   Update CHANGELOG   Fix unnecessary line   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Fix style by using yapf   Fix import error when mlflow is not installed   Update CHANGELOG.md   Update tests/loggers/test_mlflow.py   Co-authored-by: akiyuki ishikawa aki.y.ishikwa@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com",1
4319,Deprecate returning extras with grads (#7994),0.8171618,Deprecated automatically detaching returned extras with grads (#7994),,1
4320,Cleanup TPU CI script error management (#14389),0.59790385,Updated logic for checking TPUs availability (#6767),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
4321,"Fix assert wandb Run when mode=""disabled"" (#14112)",0.6213663,WandbLogger does not force wandb reinit arg to True anymore and creates a run only when needed (#4648),Co-authored-by: Yifu Wang yifuwang@2012@gmail.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
4322,added on_test_start() documentation (#7962),0.6936261,Changed the behavior of on_epoch_start to run at the beginning of validation & test epoch (#6498),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
4323,Fix ddp_spawn -> ddp fallback logic when on LSF cluster (#15657),0.6399084,Changed warnings and recommendations for dataloaders in ddp_spawn (#6762),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
4324,Refactor/prune unused EvalModel methods (#5331),0.64513123,Removed deprecated EvalResult (#5633),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
4325,Error messages for the remaining callback hooks (#15064),0.75057435,Callback hooks,  Minor loggger connector cleanup [1/n]   Missing line   Address comments   Rely on validator   Unify current_fx_name and current_hook_fx_name   Fix test ,1
4326,Metric docs fix (#2209),0.83626103,"docs for all Metrics (#2184, #2209)",Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
4327,reset val dataloader for binsearch (#9975),0.8615173,Reset val_dataloader in tuner/batch_size_scaling for binsearch (#9975),"  fixed a small typo   cleaning up   added sub_dir argument to tensorboard and wrote test   sub dir arg exclusively for tensorboard, linted   resolving merge conflict   resolved merge conflict   resolved merge conflict   resolved merge conflict   resolve merge conflict before revert   resolving merge conflict   reverted to pre-lint   added tensorboard sub_dir test   pep8 formatting   removed sub_dir arg from test_all function:   updated feature description   typo in doc description   updated CHANGELOG   Update pytorch_lightning/loggers/tensorboard.py   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com   swapped argument position   added expandvars tests   added expandvars   removed model init   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   fix tests   fix failed test   Revert ""fix failed test""   This reverts commit 50b34c66dabbd8a0b5d84f905280a4ae6b9cdc55.   add env var to test   fix typo in tests   fix tests   for test consistency   fix typo   fix typo 2   Co-authored-by: Ubuntu azureuser@devhenrik.evuifrmjd4lepbj4relcwwu5va.ax.internal.cloudapp.net Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Roger Shieh sh.rog@protonmail.ch",1
4328,Remove lightning_transformers from our docs (#16335),0.77856463,Removed the deprecated LightningDeepSpeedModule (#16041),  docker use $(nproc)   Update typo   Co-authored-by: Roger Shieh sh.rog@protonmail.ch Co-authored-by: Roger Shieh sh.rog@protonmail.ch,1
4329,Prune deprecated hparams setter (#6207),0.6240038,Removed deprecated LightningModule hparams setter (#6207),  [feat] Add stronger validation for checkpoint_callback configuration   chlog   Update callback_connector.py   Update test_model_checkpoint.py   Update pytorch_lightning/trainer/connectors/callback_connector.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   Update pytorch_lightning/trainer/connectors/callback_connector.py   Update tests/checkpointing/test_model_checkpoint.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update CHANGELOG.md  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
4330,Fix strategy type validation in connectors (#16693),0.6843341,Configuration Validator (#9779),updates: - github.com/pre-commit/pre-commit-hooks: v3.4.0 → v4.0.1 Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
4331,Respect the passed dtype with self.log (#10076),0.68547595,"    self.log_dict(grad_norm(self, norm_type=2))",training_step(...) should take self as the first argument. It's a simple but necessary fix.,0
4332,"Fix broken link in ""Build a Model"" section of docs (#16025)",0.61526483,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",  Minor loggger connector cleanup [1/n]   Missing line   Address comments   Rely on validator ,0
4333,Standalone Lite: Connector (#14692),0.68876386,Introduce lightning connect (#14452),,0
4334,added early epoch stopping hook,0.6214187,EarlyStopping now runs at the end of the training epoch by default (#8286),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
4335,"Update neptune-client requirement from <0.16.4,>=0.10.0 to >=0.10.0,<0.16.8 in /requirements (#14582)",0.64768326,"It now supports neptune-client 0.16.16 and neptune >=1.0, and we have replaced the log() method with append() and extend().",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,0
4336,Hyperparameters for datamodule (#3792),0.73545897,Flattening Wandb Hyperparameters (#2459),,1
4337,[HotFix] Resolve TPU Training (#6027),0.8183881,TPU training (#2708),"  Add kubeflow cluster environment   Add KubeflowEnvironment to docs   Add KubeflowEnvironment to the changelog   break up a long line   Add method to detect kubeflow environment   Select Kubeflow environment when available   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   Run pre-commit   task_idx == 0   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com",1
4338,Support returning python scalars in DP (#1935),0.5448886,```python,,0
4339,[1/n] Add LightningFetcher (#8890),0.7802712,Introduce lightning connect (#14452),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
4340,Extend the detection of interactive mode (#15293),0.660154,Inference mode support,,0
4341,summarize total size of model params in bytes (#5590),0.6399143,Changed the model size calculation using ByteCounter (#10123),,0
4342,Support all CombinedLoader modes during evaluation (#17163),0.63871825,The following modes are supported:,,0
4343,new image (#1070),0.57072634,New features,,0
4344,ReduceLROnPlateau bug fix (#1126),0.6740825,Resolve bug with Finetuning (#5744),  replace with kwargs   chlog   fix   add test   fix   device   deepspeed   pep   optional   docs   bc   comments   pep   mypy   pep   Apply suggestions from code review   kwargs   docs   .   .   1.3 -> 1.4   kwargs -> step_kwargs ,0
4345,clean tests imports (#2834),0.6096524,Wrapped imports for traceability (#13924),,0
4346,Hotfix mypy checking (#13653),0.5323335,datamodule = MyLightningDataModule.load_from_checkpoint(,"  refactor results   rename dic -> dict   simplify   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   changelog   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci   fix None check   chlog wording   move process_closure_result to the end   Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
4347,"added set_epoch for distributed sampler, fix for #224 (#225)",0.6794352,The loops now call .set_epoch() also on batch samplers if the dataloader has one wrapped in a distributed sampler (#13396),"  argparse_utils   model_utils   warning_utils   xla_device_utils   chlog   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com",0
4348,disable optimizers setup during testing (#3059),0.9627756,Disabled optimizers setup during testing (#3059),,1
4349,Use XLA utility API to move data to CPU (Single TPU core) (#8078),0.99511456,Used XLA utility API to move data to CPU (Single TPU core) (#8078),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
4350,"Update changelog, increment version (#5148)",0.6933161,Full Changelog,  use_single_gpu   use_horovod   use_ddp2   use_ddp   use_dp   on_gpu   use_tpu   on_tpu   on_cpu   cleaning   chlog   Apply suggestions from code review   Apply suggestions from code review ,0
4351,generalise setup tools (#5617),0.6723933,setup(,  stat_scores_multiple_classes   precision_recall   precision   recall   auc   auroc   multiclass_auroc   iou   clean-up   chlog   flake8   imports   prune ,0
4352,Add missing docs quote (#16797),0.50915664,"Show a message when BuildConfig(dockerfile=""..."") is passed but a Dockerfile file is already present in the Work (#15799)",  accelerator_backend   get_model   clean   chlog   flake8 ,0
4353,prune duplicite test in optim (#6312),0.5278513,Tested pickling (#1636),,0
4354,Remove torch >= 1.6 checks (#8523),0.67289674,Prevent modification of torch.backends.cudnn.benchmark when Trainer(benchmark=...) is not set (#13154),,0
4355,Tighten up mypy config (#5237),0.52825534,class MyLayerSync(pl.plugins.LayerSync):, Clarify logger flag  Clarify behavior of boolean values on the logger flag for Trainer.   Update docs/source/common/trainer.rst   doc   MLFlow now uses env variable as default tracking uri   Solves https://github.com/PyTorchLightning/pytorch-lightning/issues/6894  Update pytorch_lightning/loggers/mlflow.py  Co-authored-by: thomas chaton thomas@grid.ai  changelog  Co-authored-by: SpontaneousDuck kennywitham4@gmail.com Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: jirka jirka.borovec@seznam.cz,0
4356,Add mypy typing to precision plugins. (#6149),0.69908834,Precision Plugins (#5718),Co-authored-by: Sileadim christopher@omnius.com,0
4357,fix formatting - flake8 + isort,0.5479903,Resolve bug with Finetuning (#5744)," Fix some test errors Summary:  Test Plan: Reviewers: Subscribers: Tasks: Tags:   checkpoint consolidation   Update ddp_spawn.py   Update test_metric_result_integration.py   Update test_results.py   Update utils.py   Update utils.py   Update test_all_gather_grad.py   Update test_all_gather_grad.py   Update test_results.py   Revert ""Update test_results.py""   This reverts commit 9d4a2b891d2a4b37e21529a444bda1883d1b5ed1.  Revert ""Merge pull request #1 from shuyingsunshine21/shuyingsunshine21-checkpoint_consolidate""  This reverts commit c5053da789f9d04d2c967a65adf4fb026dc134b8, reversing changes made to 0d23d75bc91e4e0b7805712e394cb093fac22841.  Revert ""Update test_all_gather_grad.py""  This reverts commit 0d23d75bc91e4e0b7805712e394cb093fac22841.  Revert ""Update utils.py""  This reverts commit 70fe5da9c66ceff2fcf4be5b9efdd23a9af8389c.  Revert ""Update utils.py""  This reverts commit a9aae99f6ed6e9388ecf1d8a7bd79966176a65af.  Revert ""Update test_results.py""  This reverts commit ea749068785bbad689a12066544893b1605f20c5.  Revert ""Update test_metric_result_integration.py""  This reverts commit bf70e431b3ce4893de804e0f3b5d59e79346d6d7.  Revert ""Update ddp_spawn.py""  This reverts commit f17210183b84f90c9a62d1ff9b3e05e1fbe5f33b.  Revert ""checkpoint consolidation""  This reverts commit 536c1323b0e6715fb5919196ea48b0fcddddcd66.  Revert ""Revert ""checkpoint consolidation""""  This reverts commit 3a9fde915ad4c69620a6ccc411f5890cb38ba5ac.  Revert ""Revert ""Revert ""checkpoint consolidation""""""  This reverts commit 7a369f47e1a94d701fce48c994cc3f2da266dad0.  Revert ""Revert ""Update ddp_spawn.py""""  This reverts commit 8222dc98ead37d961a52b7366070aa10f66d92d1.  Revert ""Revert ""Update test_metric_result_integration.py""""  This reverts commit 6c095b2370a2afe9d24918a5798ce1ebffed7e0d.  Revert ""Revert ""Update test_results.py""""  This reverts commit 250d0aaaa2e6c6a6a3407bc6c8b83c0fe2479c0b.  Revert ""Revert ""Update utils.py""""  This reverts commit 8651d54d79396eaaba16d7eb1e769a1e91d5702e.  Revert ""Revert ""Update test_all_gather_grad.py""""  This reverts commit dcdcd29731061c919b15ab0b56669259817a81c4.   modify distributed environment to make test pass   modify model state dict to training type plugin   remove changes   add changelog   fixing isort for pre-commit failure   [pre-commit.ci] auto fixes from pre-commit.com hooks   for more information, see https://pre-commit.ci  Address code review  Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: SeanNaren sean@grid.ai",0
4358,Set timeout for DDPSpawnStrategy (#13383),0.6471429,Enforce an epoch scheduler interval when using SWA (#6588),,0
4359,adding check for bandit vulnerabilities 1/n (#17382),0.60883987,Early stopping checks on_validation_end (#1458),"  Update supporters.py   Update apply_func.py   Update supporters.py   Update model_train_dataloaders.py   Update model_train_steps.py   Update test_dataloaders.py   Update CHANGELOG.md   Update model_train_steps.py   Update test_dataloaders.py   Update test_dataloaders.py   Update supporters.py   Update test_supporters.py   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  [pre-commit.ci] auto fixes from pre-commit.com hooks  for more information, see https://pre-commit.ci  Update tests/trainer/test_dataloaders.py  Co-authored-by: Akihiro Nitta nitta@akihironitta.com  Apply suggestions from code review  Co-authored-by: Edgar Riba edgar.riba@gmail.com   Update supporters.py   Update supporters.py   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Edgar Riba edgar.riba@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com",0
4360,Estimate stepping batches with max_steps if max_epochs is not set (#14317),0.7303575,"max_nb_epochs to max_epochs,",,1
4361,Fix distributed types support for CPUs (#8667),0.5851906,Multiple CPU processes,,0
4362,release v0.2.5.1,0.7931757,0.4.0,Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,1
4363,[CLI] Shorthand notation to instantiate datamodules (#10011),0.90109205,Support shorthand notation to instantiate datamodules (#10011),  Log epoch metrics before firing the on_evaluation_end hook (addresses #7166)   test that epoch metrics are logged before on_evaluation_end hook   update CHANGELOG   Shorter test   Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
4364,simplify imports xla / TPU (#4872),0.6306405,from setuptools import setup,"  Automatically check DataModule.has_{setup,teardown,prepare_data}   Use variable   Spacing   Docs   Update CHANGELOG   Remove _DataModuleWrapper   Add test   Update docs/source/extensions/datamodules.rst   Bad merge   add test for invalid name   Remove ValueError   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com",0
4365,add Azure HPU agent (#12258),0.52637845,Refactor cloud dispatch and update to new API (#16456),updates: - github.com/pre-commit/pre-commit-hooks: v2.3.0 → v3.4.0 - github.com/PyCQA/isort: 5.7.0 → 5.8.0 - github.com/pre-commit/mirrors-yapf: v0.30.0 → v0.31.0 Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
4366,Add missing new lines,0.4862284,Fixing critical bugs in newly added hooks and hparams assignment.,,0
4367,Sort Trainer arguments based on importance (#17022),0.5765101,"Standardized the dataloaders arguments of trainer.{fit,valdiate,test,tune} (#7431)",  Update README.md   Update Slack link   Co-authored-by: Akihiro Nitta nitta@akihironitta.com,0
4368,Fix standalone test collection (#13177),0.67935425,- fixed all the .test() calls,,0
4369,Mergify: configuration update (#1200),0.5579635,Refactored EpochResultStore (#5522),  Dont use sphinx 4.0.0   Dont use sphinx 4.0.0   Update comment   Simple    There is no other release between 3.5 and 4.0 Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
4370,Delete checkpoint_connector.has_trained (#8292),0.7356348,Deprecated the use of CheckpointConnector.hpc_load() in favor of CheckpointConnector.restore() (#7652),,1
4371,Track the evaluation loop outputs in the loop (#10928),0.7394805,"Connect the progress tracking dataclasses to the loops (#8244, #8362)",,1
4372,[BugFix] Add on_epoch_end hook after on_test/validation_epoch_end hook (#5986),0.67697495,- Deprecated `Callback.on_epoch_end` hook in favour of `Callback.on_{train/val/test}_epoch_end` ([#11578](https://github.com/PyTorchLightning/pytorch-lightning/pull/11578)),Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: pre-commit-ci[bot] 66853113+pre-commit-ci[bot]@users.noreply.github.com,0
4373,Changes resume_from_checkpoint warning to error (#7075),0.9013488,Changed resume_from_checkpoint warning to an error when the checkpoint file does not exist (#7075),  Add a test   Save and restore current_epoch   Update CHANGELOG   alphabetical order ,1
4374,enables plugins (#4041),0.97399336,Enabled plugins (#4041),  Fix val step logging   Add a type   Fix   Update CHANGELOG.md ,1
4375,fix boolean argparse (#1571),0.6094004,argparse_utils >> argparse,,0
4376,made checkpoint callback optional,0.74680865,Callback hooks for loading and saving checkpoints,  Deprecate TrainerModelHooksMixin   Update CHANGELOG.md   Update model_hooks.py   Update model_hooks.py ,1
4377,Tests: refactor callbacks (#1688),0.6610328,- fixed all the .test() calls,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,0
4378,[TPU] Replace GKE in CI with manual gcloud usage (#17362),0.56434226,Renamed TPUHalfPrecisionPlugin to TPUBf16PrecisionPlugin (#10026),,0
4379,Fix validation progress counter with check_val_every_n_epoch > 1 (#5952),0.72324693,- Fixed main progress bar counter when `val_check_interval=int` and `check_val_every_n_epoch=None` ([#12832](https://github.com/Lightning-AI/lightning/pull/12832),,1
4380,Slightly safer multi node (#15538),1.0000001,Slightly safer multi node (#15538),,1
4381,Fix restoring trainer after lr_find() (#14113),0.6969135,Removed trainer.reset_*_dataloader() methods (#16726),,0
4382,update DALIClassificationLoader to not use deprecated arguments (#4925),0.99254125,Updated DALIClassificationLoader to not use deprecated arguments (#4925),"  deepspeed add train_micro_batch_size_per_gpu argument   Update naming and doc   Modify to use auto naming convention, add test   Add iterable tests   Fix tests, attempt by mocking   Import correct package   Fix comparison   Set as special test   Remove import   Add Changelog   Co-authored-by: SeanNaren sean@grid.ai",1
4383,Remove the deprecated code in pl.utilities.device_parser (#16412),0.75542104,- Removed the deprecated code in:,  chlog + version   readme   . ,1
4384,[docs] Add Mixed Precision detailed docs (#9104),0.644178,Docs improvements,  update versions   chlog   win   str ,0
4385,Add conda env setup (#898),0.5519692,    Connect all procs in the world using the env:// init,  v1.3.0   ci event   chlog   badge   formatting ,0
4386,training amp scaling refactor (#3135),0.9736707,training AMP scaling refactor (#3135),,1
4387,make progress bar match internal epoch counter (#3061),0.8782078,Changed progress bar epoch counting to start from 0 (#3061),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: jirka jirka.borovec@seznam.cz,1
4388,Fix instruction for cherry pick way of fixing mixed branches (#13186),0.57164115,- pickling errors with loggers (txs @awaelchli),,0
4389,Remove call_configure_sharded_model lifecycle property (#9612),0.70050716,Removed call_configure_sharded_model_hook property from Accelerator and TrainingTypePlugin (#9612),"  Added advanced gpu section   Small changes   Better documentation   Address code review   Add warning about using trainer.model, clean up some of the examples   Add section for ddp, remove references and old sequential documentation   Remove Fully Sharded documentation for now   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com   Address code review   Address code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com",1
4390,[docs] Add NCCL environment variable docs (#8345),0.6119044,"adding compute environments (#3837, [#3842)",Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
4391,[App] Format client error ApiExceptions without a traceback (#15130),0.605541,Replace MisconfigurationException with warning in ModelCheckpoint Callback (#4560),  v0.1.3.0rc3   spaces   wip   wip   wip   wip   prune   wip   wip   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
4392,:q,0.60862267,):,  Updating docs and error message to specify that half precission not available on CPU   update messages   Co-authored-by: Martin Kristiansen martinkristiansen@sixgill.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: jirka jirka.borovec@seznam.cz,0
4393,Add persistent flag to Metric.add_state (#4195),0.6547017,Metric states are no longer as default added to state_dict (#4685),  move files   rename ,0
4394,increase release (#4949),0.64974535,This release includes:,  Update configure_optimizers docs   Update pytorch_lightning/core/lightning.py ,0
4395,rename integrations_app for accommodating flagships (#16345),0.56547296,Renamed utils modules (#5199),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
4396,[LAI] Make lite tests safe for combined package (#15204),0.56098247,Release LAI docs as stable (#14250),  Remove outputs from on_train_epoch_end   iterate   Update callback_hook.py   update   early stop?   fix   Update pytorch_lightning/trainer/training_loop.py   Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk   Update trainer.py   update   Update training_loop.py   early stop?   fix   Remove outputs from evaluation epoch end hooks   update   Update test_remove_1-5.py   fix lints   Update base.py   rm-outputs   Update evaluation_loop.py   try-save-more-memory   Update trainer.py   Update trainer.py   cache-at-start   Update evaluation_loop.py   Update training_loop.py   Update training_loop.py   Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk,0
4397,Learning rate log callback (#1498),0.7093316,Learning Rate Finder (#13802),,1
4398,[Bugfix] Detach Loaders after running entrypoint (#8885),0.68282676,Restore original loaders if replaced by entrypoint (#8885),  Remove outputs from on_train_epoch_end   iterate   Update callback_hook.py   update   Update training_loop.py   Update test_training_loop.py   early stop?   fix   update tests   Update test_hooks.py   Update pytorch_lightning/trainer/callback_hook.py   Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk  Update pytorch_lightning/trainer/training_loop.py  Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk   Update trainer.py   Update pytorch_lightning/trainer/trainer.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
4399,CI: hotfix gatekeeper (#13606),0.5696844,Fix frontend hosts when running with multi-process in the cloud (#17324),  add pt 1.9   pull ,0
4400,simplify torch.Tensor (#16190),0.65788907,"Native torch metrics (#1488, #2062)",  update   clarify   update ,0
4401,Deduplicate top level lighting CLI command groups (#15761),0.9960437,Deduplicate top-level lighting CLI command groups (#15761),  enable PT 1.9   fix versions   args   fix ,1
4402,ci: parameterize GPU testing (#16697),0.7685702,GPU training (#2704),,1
4403,Set a less strict pre-commit Python version (#12627),0.62380767,Drop Python 3.6 support,  deprecate-tbptt-trainer   Update CHANGELOG.md   Update lightning.py   test   Update lightning.py   Update training_loop.py   Update training_loop.py   Update lightning.py   Update training_loop.py   Update training_loop.py   update docs   Update accelerator.py   Update accelerator.py   more docs   tweaks   chlog   comments   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
4404,[fix] Ensure we check deepspeed/sharded in multinode DDP (#6297),0.9795776,Ensure we check deepspeed/sharded in multinode DDP (#6297),  wip   XLA   . ,1
4405,Make standalone tests less verbose (#12684),0.5747124,"nb_test_batches to num_test_batches,",,0
4406,Support individual setup of model and optimizer in Lite (#15185),0.9083519,    # Let Lite setup your model and optimizer,  Fix Namespace loading in PyYAML 5.4.x   Remove OmegaConf reference from PyYAML requirements   Max allowed version for pyyaml   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
4407,fix available modules (#11526),0.6463752,Module imports are now catching ModuleNotFoundError instead of ImportError (#9867),,0
4408,Set a fixed stage in the evaluation loops (#17094),0.63915765,        # 4. Perform the optimization in a loop,,0
4409,[DOCS] title clarification in Results page (#2827),0.5690187,"docs for all Metrics (#2184, #2209)",  fix readme badges   Apply suggestions from code review   Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk,0
4410,Fix docstring for LightningWork.has_stopped (#16532),0.78787845,Improved the error message when the LightningWork is missing the run method (#14759),  Pass current_epoch/global_step as monitor candidates   Formatting   Fix deprecated test   Update CHANGELOG ,1
4411,add missing predict docs (#7150),0.58968,"docs for all Metrics (#2184, #2209)",  temp suspend NVIDIA CI build   just skip   todo   if: false ,0
4412,Delete deprecated TrainerTrainingTricksMixin (#8679),0.94023883,Removed the deprecated TrainerTrainingTricksMixin class (#8679),,1
4413,Update Lightning Lite examples (#15599),0.7968632,LightningLite:, CI: move azure pipelines to separate directory  This removes some extra clutter in the top level as we add more pipelines.  rename  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
4414,Fix fork tests failing in environments with CUDA available (#14982),0.67734694,"- Trainer queries the CUDA devices through NVML if available to avoid initializing CUDA before forking, which eliminates the need for the `PL_DISABLE_FORK` environment variable introduced in v1.7.4 ([#14631](https://github.com/Lightning-AI/lightning/pull/14631))",  Fix Dataloader None batch   Fix Dataloader None batch   Update CHANGELOG.md   Fix breaking test   Address comments ,0
4415,ref: inner train loop (intermediate step) 9/n (#3368),0.79530776,"inner train loop (#3359, #3361, #3362, #3363, #3365, #3366, #3367, #3368, #3369, #3370, #3371, #3372, #3373, #3374, #3375, #3376, #3385, #3388, #3397)","  Add test for non-existing mode, the test should fail if something different from power or binsearch is passed.   Add newline.   Apply fix   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   Update tests/tuner/test_scale_batch_size.py   Update pytorch_lightning/tuner/batch_size_scaling.py   Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com",1
4416,Skip a failing Bagua test for manual optimization (#16225),0.6034715,"The Bagua Strategy is a deep learning acceleration framework that supports multiple, advanced distributed training algorithms with state-of-the-art system relaxation techniques. Enabling Bagua, which can be considerably faster than vanilla PyTorch DDP, is as simple as:",,0
4417,Fix isort failures in metrics (#5528),0.65279293,Metric compute() method will no longer automatically call reset() (#5409),  TrainerState refactor   flake8   Update finished check   Test cleanup   Fix tests   Fixes   Reorder   flake8   Update CHANGELOG   Better docs   Better docs   Remove default   Update tests   Bad merge ,0
4418,Rename loops/base.py to loops/loop.py (#13043),0.63895583,Removed the deprecated pytorch_lightning.loops.base module in favor of pytorch_lightning.loops.loop (#16142),"  string gpu input   update docs   deprecation warning   Revert ""update docs""   This reverts commit c5f38934133812280ae98f6489c008a39796dd4d.   deprecation   add changelog   update parser   update warning   implement v1.5 behavior ahead of time   formatting   set accelerator in test to avoid different warning   add warning   remove todo warn   Update pytorch_lightning/utilities/device_parser.py   Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com  resolve flake8  Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: tchaton thomas@grid.ai",0
4419,Support automatic seeding of the LightningCLI (#12822),0.77623904,Removed support for LightningCLI(seed_everything_default=None) (#16131),  feat(wandb): allow custom init args   style: pep8   fix: get dict args   refactor: simplify init args   test: test init args   style: pep8   docs: update CHANGELOG   test: check default resume value   fix: default value of anonymous   fix: respect order of parameters   feat: use look-up table for anonymous   yapf formatting   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
4420,ref: added model connector (#3407),0.7173997,group connectors (#3472)," Update LR schedulers only when their corresponding Optimizer is being used.  In the case when optimizer frequencies are specified, the LR scheduler corresponding to a particular optimizer is updated only when that optimizer is being used in the training loop or epoch.   pep8speak fixes   Fix failing tests   Add docs   PR Feedback   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   formatting fix   PR Feedback - part 2   More PR feedback   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   Add typing imports   Stronger tests and fixes related to that   Add more tests plus PR feedback   Make optimizer_freq_cumsum a cached property   @cached_property is only available after Python 3.8 so had to do it manually.   Fix tests   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   Avoid mutable defaults   Parametrize lr scheduling tests   PR feedback   Apply suggestions from code review   spell   Apply suggestions from code review   flake8   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: chaton thomas@grid.ai Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com",1
4421,Fix potential typo in early stopping monitor keys  (#3213),0.5998821,bug fix with logging val epoch end + monitor (#3812),,0
4422,Fix silent TPU CI failures (#14034),0.6166927,Updated logic for checking TPUs availability (#6767),"This adds an azure-pipelines job so we can verify the runners are connected correctly. Since the IPU branch isn't merged, it won't yet give any actual IPU test coverage.",0
4423,[doc] Update the order of zero_grad and backward (#6478),0.83424044,"Changed the order of backward, step, zero_grad to zero_grad, backward, step (#6147)",  Remove _DataModuleWrapper   Update pytorch_lightning/core/datamodule.py   Update pytorch_lightning/core/datamodule.py   Replace __reduce__ with __getstate__ ,1
4424,Simplify some ddp-spawn tests #10921,0.65061426,DDP Debugging Improvements,"  Fix trainer.plugins type declaration   Don't ClusterEnvironment(Plugin)   fix import error, yapf formatter   Add test   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com",0
4425,Docs: Remove old link (#17514),0.58319163,Removed deprecated EvalResult (#5633), Clarify logger flag  Clarify behavior of boolean values on the logger flag for Trainer.   Update docs/source/common/trainer.rst   doc   Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
4426,Fix import from torch.distributed when distributed not available (#16658),0.7259054,import torch,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
4427,Loop flattening: remove the default .run() implementation (#16427),0.655324,- Removed support for loop customization,^,0
4428,Clarify what's the PyTorch profiler used in docs (#12392),0.66331494,Deprecated PytorchProfiler(profiled_functions) in favor of record_functions (#6349),Co-authored-by: jirka jirka.borovec@seznam.cz,0
4429,drop duplicate metrics (#5014),0.99513507,Drop duplicate metrics (#5014),,1
4430,update chlog after 1.9.1 release (#16696),0.675112,0.4.0,  Update data_loading.py   Update data_loading.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
4431,testing env init,0.5777801,  * `env_prefix`,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
4432,Upgrade CI after the 1.10 release (#10075),0.5764952,The point of this release is more bug fixes ahead of v 1.0.0. We now have CI tests on TPU thanks to @zcain117 from Google! :slightly_smiling_face:,,0
4433,enable tests that were never running (#12585),0.70103717,- fixed all the .test() calls,  Update CODEOWNERS   @carmocca   @borda   Update CODEOWNERS   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: William Falcon waf2107@columbia.edu,1
4434,Update the logic to check for accumulation steps with deepspeed (#9826),1.0000002,Update the logic to check for accumulation steps with deepspeed (#9826),  bugfix-dataloading   rm-logs   Update CHANGELOG.md   Update test_dataloaders.py   Update test_dataloaders.py   Update training_loop.py   Update test_dataloaders.py   Update CHANGELOG.md   Update CHANGELOG.md   Update test_dataloaders.py   Update training_loop.py   Update training_loop.py   comments   address comments   more tests   Update progress.py   Update test_dataloaders.py   Update test_dataloaders.py   Update training_loop.py   Update training_loop.py   test ckpt fix?   update again ,1
4435,update rank_zero condition for logging summary (#9461),0.739693,Removed restrictions in the Trainer that loggers can only log from rank 0; the existing logger behavior has not changed (#8608),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
4436,CI: use concurrency (#11351),0.55752295,Support **DictConfig for hparam serialization (#2519),,0
4437,fix test pkg create (#873),0.6010201,Dropped official support/testing for PyTorch <1.6 (#8288),,0
4438,store: drop requirements_file_path (#16527),0.59184116,Deprecated filepath in ModelCheckpoint (#4213),Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: thomas chaton thomas@grid.ai,0
4439,Add tbptt (#429),0.59738404,Renamed TPUHalfPrecisionPlugin to TPUBf16PrecisionPlugin (#10026),,0
4440,Move Trainer._log_hyperparams to an utility (#16712),0.66002476,Deprecated TrainerLoggingMixin in favor of a separate utilities module for metric handling (#7180),,0
4441,Logger default (#350),0.7677522,Return the default_root_dir as the log_dir when the logger is a LoggerCollection (#8187),  update   update   update apex   update   update   update   remove test.py   update   update   update on comments   update changelog   update   update   typo ,1
4442,Fix typo (#1750),0.61086833,Changed overwrite to True (#16009),  rm-grad-norm-mixin   Update grads.py   Update CHANGELOG.md   Apply suggestions from code review   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   Update docstrings   Update init.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
4443,Content for Lightning with iOS and Android (#14038),0.7574036,Included app templates to the lightning and app packages (#13731),  Update training_loop.py   Update test_dataloaders.py   changelog   delay reload   go back   comments   Update training_loop.py   Update test_dataloaders.py   Update tests/trainer/test_dataloaders.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
4444,mergify: less updates (#5639),0.5968639,[1.3.0] - 2021-05-06,  update   wip   udpate   update   update   update   resolve bug   update on comments   update on comments   update   update   formatting   add comments   update on comments   update   Update pytorch_lightning/callbacks/base.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   update   update   Typing and minor changes   Refactor   Fix deprecated test   Broken commit   Fix broken commit   flake8   Update CHANGELOG   update on comments   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
4445,Docs/improved multigpu doc clarity (#3194),0.65817845,Docs improvements,Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,0
4446,CI: freeze docs requirements [hotfix] (#15865),0.6795683,Setup: added requirement freeze for the next major version (#14480),  add docs   typo   update ,0
4447,Fix loading yaml (#5619),0.5325706,- Fixed a bug where the upload files endpoint would raise an error when running locally ([#14924](https://github.com/Lightning-AI/lightning/pull/14924)),  wip   fix   add test   refactor + test   rm   formatting   update changelog   doc   docstring   remove unused import   Update CHANGELOG.md   Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,0
4448,CI: abstract and make full pkg check (#13460),0.49386302,    # x is 1/nb_gpus of the full batch,  rename-run   fix ,0
4449,Enable back inference mode support with hpu & update links (#15918),0.5546572,"From now on, the backend has to be set in the code (#14693):",  fix   add tests   changelog   fix test ,0
4450,Add Github Action to run TPU tests. (#2376),0.6661168,Updated logic for checking TPUs availability (#6767),  Updated docs to fix typo and update grid status   Update docs/source/starter/new-project.rst   Co-authored-by: ananthsub ananth.subramaniam@gmail.com   Update docs/source/starter/new-project.rst   Update docs/source/starter/new-project.rst   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,0
4451,Loop Refactor 6/N - Remove Old Predict Loop (#8094),0.7055105,Refactored Loops,,1
4452,Updated the structure and applied feedback (#14734),0.65371525,"Refactored evaluation loop interface; added new classes DataLoaderLoop, EvaluationLoop, EvaluationEpochLoop (#7990, #8077)",  deprecate-exp-save-path   Update lightning.py   Update CHANGELOG.md   remove-not-deprecate ,0
4453,fix flag name to flush_logs_every_n_steps in logging doc (#7633),0.6868439,Rename Trainer arguments row_log_interval >> log_every_n_steps and log_save_interval >> flush_logs_every_n_steps (#3748),,0
4454,update contributers list (#1597),0.62309206,Contributors,  rebase   doc   Update training_loop.py   Update CHANGELOG.md   Update CHANGELOG.md   Update CHANGELOG.md   Update CHANGELOG.md   Update CHANGELOG.md ,0
4455,update documentation for callbacks (#4253),0.6595317,Removed deprecated callbacks (#3979),,0
4456,Add a notebook example to reach a quick baseline of ~94% accuracy on CIFAR (#4818),0.79873514,Add a notebook example to reach a quick baseline of ~94% accuracy on CIFAR10 using Resnet in Lightning (#4818),  add a warning   add changelog ,1
4457,Accelerator docs (#4583),0.7017182,Refactored Accelerators and Plugins (#5743),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
4458,Remove deprecated property ModelCheckpoint.period in favor of ModelCheckpoint.every_n_epochs (#9213),0.98482454,Removed deprecated property ModelCheckpoint.period in favor of ModelCheckpoint.every_n_epochs (#9213),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com,1
4459,Integrate Lite launcher into Lightning CLI (#15506),0.74693865,class Lite(LightningLite):,  Fix install bug   Better fix   Fix   Fix   Remove unused import   Update docs conf.py   Updates ,1
4460,docker Conda timeout (#10087),0.49912253,Increased TPU check timeout from 20s to 100s (#5598),  Update trainer.py   cleanup module properties in teardown   Update test_trainer.py   Update lightning.py   Formatting   flake8   Update pytorch_lightning/trainer/trainer.py   Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
4461,Ensure accelerator is valid if running interactively (#5970),0.99999994,Ensure accelerator is valid if running interactively (#5970),  _fit_impl refactor and types   Fix return   Remove return docstring   Fixes   Fixes   Remove trainer.fit return value   Update CHANGELOG   flake8   Undo results change   Fix test   Revert changes for a separate PR   flake8 ,1
4462,Fix a typo in transfer_batch_to_device function's comment (#16659),0.5584507,Raised a MisconfigurationException if batch transfer hooks are overriden with IPUAccelerator (13961),,0
4463,add package info (#359),0.61134195,"    name=""my-package"",",  ci for pre-release   .   drop 3.7 ,0
4464,Support checkpoint save and load with Stochastic Weight Averaging (#9938),0.66218007,DataModule hooks for loading and saving checkpoints,Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: jirka jirka.borovec@seznam.cz,0
4465,Tensorboard logger check if lightning_logs directory exists (#1377),0.7008157,Squeezed tensor values when logging with LightningModule.log (#14489),Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
4466,Add warning when wandb.run already exists (#8714),0.58913994,WandbLogger does not force wandb reinit arg to True anymore and creates a run only when needed (#4648),  Fixed bug: replaced bce_loss_with_logits with bec_loss   Fixed bug: removed sigmoid activation from forward pass   switched names for scores and logits   Co-authored-by: Alexey Misev amisev@fb.com Co-authored-by: Alexey Misev alexey@MacBook-Pro-Natalia.local Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
4467,fix str to_device section in converting.rst (#12243),0.48962817,Avoid relpath bug on Windows (#16164),"   Added cli unit tests for help, print_config and submodules.   Added to cli documentation use of subclass help and print_config, submodules and other minor improvements.  Increased minimum jsonargparse version required for new documented features.   Improvements to lightning_cli.rst   Add check for all trainer parameters in test_lightning_cli_help   Increased minimum jsonargparse version   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com",0
4468,chlog,0.65311027,"    },",  Update fsspec dep and remove un-needed code   Remove unused import ,0
4469,Fix average_precision metric (#2319),0.6768286,Avoid calling average_parameters multiple times per optimizer step (#12452),  add warning   typo   add link ,0
4470,Fix typo (#3174),0.59459734,Changed overwrite to True (#16009),,0
4471,default sched (#6062),0.54542947,Changed fsspec to tuner (#4458),,0
4472,DeepSpeed Infinity Update (#7234),0.7316009,DeepSpeed Stage 1,  wip   update   update   update   update   update   typo   update on comments   update   update   update   update   update changelog   update   Fix merge   Fix merge   move code   resolve test   add extra test   add an extra test   update on comments   add typing   resolve flake8   Refactor and Docs   Fix tests   Fix tests   Fix tests   Duplicate   Fix tests   resolve bug   update   update on comments   Update pytorch_lightning/utilities/imports.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update pytorch_lightning/utilities/device_parser.py  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   update   update   update   update on comments   resolve flkae8   update test   Apply suggestions from code review   update on comments   Update pytorch_lightning/callbacks/prediction_writer.py   Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk  Update pytorch_lightning/callbacks/prediction_writer.py  Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk  Update pytorch_lightning/callbacks/prediction_writer.py  Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk   update on comments   update   update on comment   Apply suggestions from code review   update   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
4473,Make MPSAccelerator platform check expect arm64 (#13992),0.62319434,The psutil package is now required for CPU monitoring (#17010),,0
4474,Handle torch.jit scripted modules in layer summary (#6511),0.66301894,compiled_model = torch.compile(model),  version check   ... ,0
4475,refactor - check F841 (#5202),0.6618303,Silenced some warnings. verified ddp refactors (#3483),  update   update   update   update on comments   update ,0
4476,Docs clean up of results and forward vs training_step confusion (#3584),0.6300951,Move training_output validation to after train_step_end (#7868),  Expose shuffle argument in LightningDataModule.from_datasets   Add test for DataModule initialization with iterable datasets   Add changelog   Remove trailing whitespace   Add more tests for coverage   Fix sequence dataset coverage   Fix Sequence dataset tests   Directly check whether each passed dataset is an IterableDataset   Expose shuffle argument in LightningDataModule.from_datasets   Add test for DataModule initialization with iterable datasets   Add changelog   Remove trailing whitespace   Add more tests for coverage   Fix sequence dataset coverage   Fix Sequence dataset tests   Directly check whether each passed dataset is an IterableDataset   Fix changelog to reflect review direction   Update CHANGELOG.md   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   Fix changelog to reflect review direction (2)   Add suggested braces   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   Reuse isinstance check   Merged tests with parametrize. Use mocks   Co-authored-by: Seongmin Park seongmin.park@actionpower.kr Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
4477,Update community example links (#7087),0.5203132,Updated app URLs to the latest format (#16568),  Update data_connector.py   move-barrier   Update trainer.py   Update ddp.py   changelog   Spacing   Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
4478,Avoid race condition in creating checkpoint directories (#530),0.58297265,Disable saving checkpoints if not trained (#4372),  resolve bug   update changelog   typo   Update tests/trainer/optimization/test_manual_optimization.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Apply suggestions from code review  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
4479,fix test - reduce metric,0.6648023,Drop duplicate metrics (#5014),  wip   update   update   update   update   update   typo   update on comments   update   update   update   update   update changelog   update   Fix merge   Fix merge   move code   resolve test   add extra test   add an extra test   update on comments   add typing   resolve flake8   Refactor and Docs   Fix tests   Fix tests   Fix tests   Duplicate   Fix tests   resolve bug   update   update on comments   update   update changelog   update   update   remove tpu   resolve flake8   update on comments   update on comments   update on comment   resolve flake8   add a cpu test for predict   add None test   update   Update CHANGELOG.md   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  resolve tests  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
4480,extract optimizer loop (#9191),0.7736924,"Refactored TrainingBatchLoop and extracted OptimizerLoop, splitting off automatic optimization into its own loop (#9191)",,1
4481,Do not lose references of trainer in test (#15272),0.72599494,trainer.test(),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,1
4482,Print the e2e app ID as early as possible (#15821),0.5113727,App,,0
4483,Add DeepSpeed Stage 1 + doc improvements for model parallel (#8974),0.7749064,Support for manual optimization with DeepSpeed (#7970),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
4484,CI: clean install & share pkg build (#15986),0.5586233,Enable PyTorch 1.7 compatibility (#3541),  Update model_checkpoint.py   Update CHANGELOG.md   fix-tests   deprecate not remove   Update model_checkpoint.py   Update test_remove_1-5.py ,0
4485,"Update neptune-client requirement from <=0.15.2,>=0.10.0 to >=0.10.0,<0.16.3 in /requirements (#13056)",0.6345711,"It now supports neptune-client 0.16.16 and neptune >=1.0, and we have replaced the log() method with append() and extend().",,0
4486,[App] Resolved root_folder not parsed properly (#16454),0.58433896,Allowed root path to run the app on /path (#14972),,0
4487,Merge pull request #12509 from RobertLaurella/patch-1,0.59414977,"merge backends (#3476, #3477, #3478, #3480, #3482)",,0
4488,Support DDP with LRFinder (#15304),0.65490746,"refactor DDP backend (#3204, #3207, #3208, #3209, #3210)",  make lightning module source of truth for automatic optimization   Update configuration_validator.py   Update model_connector.py   rm-references   Update CHANGELOG.md   Update CHANGELOG.md   Co-authored-by: jirka jirka.borovec@seznam.cz,0
4489,Added boring model as a ipynb so it can be updated (#10521),0.58978,- Deprecated `Trainer.ipus` in favor of `Trainer.num_devices` when IPU is used ([#12386](https://github.com/PyTorchLightning/pytorch-lightning/pull/12386)),  Update Error message for ProfileConnector   Update test ,0
4490,added emb similarity (#3349),0.44879532,Support **DictConfig for hparam serialization (#2519),  deprecate-write-predictions   Update CHANGELOG.md   Update test_remove_1-5.py   Co-authored-by: thomas chaton thomas@grid.ai,0
4491,simplify grad clip tests (#8883),0.56599885,Ensure that clip gradients is only called if the value is greater than 0 (#6330),  simple and boring script   simplify dataloader   replace bug report model ,0
4492,Bug Fix: Remote Logging with Tensorboard (#3236),0.72635925,Changed the default logger to TensorBoardLogger (#609),  rm-trainer-logging   Update CHANGELOG.md   Update metrics.py   Update logging.py   Update metrics.py ,1
4493,task docs harness (#2996),0.6069882,Reset metrics before each task starts (#9410),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
4494,Ignore generated package files (#16605),0.59725,Avoid using the deprecated LooseVersion (#16162),"  Try updating CI to latest fairscale   Update availability of imports.py   Remove some of the fairscale custom ci stuff   Update grad scaler within the new process as reference is incorrect for spawn   Remove fairscale from mocks   Install fairscale 0.3.4 into the base container, remove from extra.txt   Update docs/source/conf.py   Fix import issues   Mock fairscale for docs   Fix DeepSpeed and FairScale to specific versions   Swap back to greater than   extras   Revert ""extras""   This reverts commit 7353479f  ci  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: jirka jirka.borovec@seznam.cz",0
4495,Minor improvement to deepspeed utility comments (#9224),0.70181054,Support for manual optimization with DeepSpeed (#7970),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
4496,Remove the model argument from Lite's optimizer_step via structural typing (#14810),0.7068651,    # Let Lite setup your model and optimizer,  update docs   add datamodule predict   fix docs   typo ,1
4497,Fixed Import in Docs For Multinode Trainer Name Which does Not Exist (#15663),0.60101604,Module imports are now catching ModuleNotFoundError instead of ImportError (#9867),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
4498,Fix CSV logger warning (#4419),0.6205485,bug fix with logging val epoch end + monitor (#3812),,0
4499,Update RequiredTrainerInterface.md,0.62859654,trainer = pl.Trainer(plugins=SLURMEnvironment(auto_requeue=False)),  get_num_classes   tmp   fix one test   fix deprecated tests   fix deprecate   pep8   deprecate 0.3   wip   wip   HaCK   brnch   brnch   format   Apply suggestions from code review   prune   rev   mltilabel   Apply suggestions from code review   master   rev   .   Co-authored-by: Nicki Skafte skaftenicki@gmail.com,0
4500,Standardize positional datamodule and argument names (#7431),0.61555105,Support shorthand notation to instantiate datamodules (#10011),,0
4501,Add warning comment to cloud requirements (#15790),0.6204059,Refactor cloud dispatch and update to new API (#16456),  ban TB 2.5   note   push   Ban tb==2.5.0 and deepspeed==0.3.15   Fix pip command   pull   up   up   Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
4502,Two fixes for handling edge cases in MLflow logging (#16451),0.7120697,"    loggers=[WandbLogger(...), MLFlowLogger(...)]","  Update hooks typing and predict hooks   Update CHANGELOG   Progress   Progress   Add back on_predict_{start,end}   Typing and fix   Update tests/trainer/logging_/test_logger_connector.py   Update tests/callbacks/test_lambda_function.py ",1
4503,Update Fabric introduction (#16672),0.70720696,Fabric,,1
4504,fix s3 download for PT 1.8 (#5840),0.48824134,This release fixes that core issue,,0
4505,Fix *_batch_end typos (#17188),0.6639535,"Removed output argument from *_batch_end hooks (#3965, #3966)",Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: jirka jirka.borovec@seznam.cz,0
4506,Docs12 (#1057),0.68534243,Docs,,0
4507,Fix docs' TensorBoardLogger instantiation (#13038),0.7431404,Fix reset TensorRunningAccum (#5106),  Move on save checkpoint outside of zero check   Remove unnecessary override   Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
4508,Fix SWA with a list of learning rates (#8747),0.722345,Learning Rate Finder (#13802),,1
4509,Enable val/test loop disabling + datamodule tests (#2692),0.90892005,Enabling val/test loop disabling (#2692),  update cluster   update index   update references   update grid docs   update duplicated title   Update docs/source/clouds/cloud_training.rst   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   fix doctest   Remove self-balancing section   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
4510,"Update neptune-client requirement from <0.16.8,>=0.10.0 to >=0.10.0,<0.16.10 in /requirements (#15082)",0.6467341,"It now supports neptune-client 0.16.16 and neptune >=1.0, and we have replaced the log() method with append() and extend().",  Set Apex commit before introduction of new MLP extensions   Refactor install command   Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
4511,release v0.5.3.3,0.772214,0.4.0,,1
4512,Move init_ddp_connection to DDP Plugin (#4407),0.68181145,moves configure ddp to each backend (#3924),  Update model_checkpoint.py   Update model_checkpoint.py   Update model_checkpoint.py ,0
4513,Docs: fix mistakes in New Project docs (#10137),0.6574817,Docs improvements,,0
4514,build docs on master (#3492),0.6287861,"For a full tutorial and running example, visit our docs. TODO: add to docs",  update   Update pytorch_lightning/plugins/precision/double.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update pytorch_lightning/plugins/precision/double.py  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update pytorch_lightning/plugins/precision/double.py  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  resolve tests  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
4515,add nvidia docker image (#5668),0.57476383,nvidia/apex deprecation (#16039),  move hyper_parameters   Update pytorch_lightning/core/lightning.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update pytorch_lightning/utilities/parsing.py  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   resolve flake8   update   resolve tests   Update pytorch_lightning/core/lightning.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
4516,Simplify logger connector access (#8318),0.8465792,Dramatically simplify the LoggerConnector (#7882),,1
4517,remove duplicate tests (#2685),0.6453754,Refactored setup_training and remove test_mode (#5388),Co-authored-by: tchaton thomas@grid.ai Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
4518,ref: move lr_finder (#3434),0.9790567,move lr_finder (#3434),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: thomas chaton thomas@grid.ai,1
4519,[App] Support for headless apps (#15875),0.6274398,App,  update test   Apply suggestions from code review   Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
4520,cleaning up demo file,0.5834149,clean up data reset (#3161),  add test   add changelog   resolve flake8   remove print ,0
4521,less IDE complain about unused args (#6786),0.5772214,argparse_utils >> argparse,,0
4522,Improved docs for pytorch_lightning.core (continued) (#1483),0.7593297,+ # pytorch_lightning==1.7.0,Co-authored-by: tchaton thomas@grid.ai,1
4523,Python logging level docs (#2348),0.60874826,Logging,  update   remove cluster env ,0
4524,refactored ddp backend forward (#3119),0.97928226,refactored DDP backend forward (#3119),  fix   changelog ,1
4525,Support torchtext on a single GPU (#2379),0.6179819,"Unify sLURM/TorchElastic under backend plugin (#4578, #4580, #4581, #4582, #4583)",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
4526,added initial semantic segmentation example (#751),0.7441891,Updated semantic segmentation example with custom u-net and logging (#1371),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com,1
4527,Periodically sync database to the drive (#15441),1.0,Periodically sync database to the drive (#15441),  .   .   Fix link to the section   Fix link to the section   Consistent indent   Update docs   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   Add note for optimizer.optimizer   .   Update hooks   Update closure docstring   Update optimizer methods   Update optimizer   Remove manopt + grad clipping (by @flukeskywalker)   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
4528,[LAI] Bump minimal requirements (#15203),0.53737813,Deprecated max_nb_epochs and min_nb_epochs (#567),  [docs]: add newline to correctly render Example   whitespace   Co-authored-by: Matthew Sarmiento matthewcs@me.com,0
4529,Add predict hook test (#7973),0.61406446,Made optimization steps for hooks (#2363),  Deprecated @auto_move_data in favor of trainer.predict   Update CHANGELOG ,0
4530,Fix training resuming docs (#1265),0.65433675,training forward refactor (#3134),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
4531,forward cache fix (#5895),0.5666164,remove obscure forward call in eval + CPU backend ___step (#3123),,0
4532,Allowed custom BatchSamplers when instantiated in *_dataloader hook (#13640),0.76823866,      for batch in dataloader:,  reset   fix tests   fix tests   Apply suggestions from code review   Co-authored-by: ananthsub ananth.subramaniam@gmail.com   move logic   chglog   pep8   Add test   Improve test   Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk Co-authored-by: ananthsub ananth.subramaniam@gmail.com,1
4533,Move ipu precision flag check to IPUPrecisionPlugin init (#12148),0.6003869,"- When training with `precision=16` on IPU, the cast has been moved off the IPU onto the host, making the copies from host to IPU cheaper ([#13880](https://github.com/Lightning-AI/lightning/pull/13880))",,0
4534,Undo marking tests that don't need it as standalone (#15355),0.65293425,remove _evaluate fx (#3197),,0
4535,"Update typing-extensions requirement from <4.2.1,>=4.0.0 to >=4.0.0,<4.3.1 in /requirements (#13529)",0.5285803,Improved exception message if rich version is less than 10.2.2 (#10839),  Update CI torch-xla version to 1.8   Update minimal to 1.6 ,0
4536,store: adding group-check (#16528),0.561106,group prepare data hook (#3212),  TPU Spawn Rank Error   Update tpu spawn   Fix root device property for tpu spawn   Update changelog ,0
4537,Increase speed diff for drone,0.5694876,"many speed improvements (how we move data, adjusted some flags & PL now adds 300ms overhead per epoch only!)","Version v1 generates a better baseline with higher max_episodes and reward_threshold attained. changed_params --> register(     id='CartPole-v1',     entry_point='gym.envs.classic_control:CartPoleEnv',     max_episode_steps=500,     reward_threshold=475.0, )",0
4538,fix result obj dp auto reduce (#3013),0.9929646,fix result obj DP auto reduce (#3013),  Better approach to register plugins   Add ddp_with_find_unused_parameters_false   Remove unnecessary break   Revert back the ddp commit   Update register override logic   Update register override logic   fix mypy ,1
4539,Docs new section (#2236),0.6272954,Docs improvements,,0
4540,Fix comments for metrics_to_scalars (#8782),0.7114204,Changed metrics_to_scalars to work with any collection or value (#7888),  Update ddp.py   Update CHANGELOG.md ,1
4541,optimizer clean up (#4658),0.81435156,Refactored optimizer (#4658),,1
4542,ref: organize args 2/n (#3442),0.85432553,"organize args (##3435, #3442, #3447, #3448, #3449, #3456)",,1
4543,Rename ttp -> strategy (#11312),0.70221317,"All TrainingTypePlugins have been renamed to Strategy (#11120). Strategy is a more appropriate name because it encompasses more than simply training communcation. This change is now aligned with the changes we implemented in 1.5, which introduced the new strategy and devices flags to the Trainer.",Co-authored-by: Sean Naren sean@grid.ai Co-authored-by: thomas chaton thomas@grid.ai,1
4544,release v0.3.6.5,0.74768215,0.4.0,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com,1
4545,"Update torchmetrics requirement from <0.9.2,>=0.7.0 to >=0.7.0,<0.9.3 in /requirements (#13528)",0.7511114,"Set PyTorch 1.4 as min requirements, also for testing and examples torchvision>=0.5 and torchtext>=0.5 (#5418)",,1
4546,Add code-owners for standalone Lite package (#14694),0.60181653,"- In Lightning Lite, state-dict access to the module wrapper now gets passed through to the original module reference ([#14629](https://github.com/Lightning-AI/lightning/pull/14629))",  Update trainer.py   Update trainer.py   Update trainer.py ,0
4547,moved slurm flag resolution to init,0.5869944,Slurm resubmit with apex + ddp,,0
4548,added debugging util,0.65749276,DDP Debugging Improvements,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
4549,Fix num_sanity_val_steps is clipped to limit_val_batches (#2917),0.6756282,and nb_val_batches to num_val_batches (#567),,0
4550,Added doc strings for Comet logger (#9114),0.76315457,Using .comet.config file for CometLogger (#1913),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
4551,test CLI parsing gpus (#2284),0.5704415,Parsing of GPU Argument,,0
4552,Initialize trainer with None (#4847),0.7072617,adding Trainer.tune() (#3293),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
4553,hotfix: drop bad pyyaml version (#5606),0.6446632,Drop PyTorch 1.9 support (#15347),  rename about   .   .. ,0
4554,Drop torch 1.6 testing (#10390),0.72749376,test selecting the correct backend. temp backends while slurm and TorchElastic are decoupled (#3848),,1
4555,Add ddp_cpu backend for testing ddp without GPUs (#1158),0.78371125,Made DDP the default if no backend specified with multiple GPUs (#1789),Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: ananthsub ananth.subramaniam@gmail.com,1
4556,FIx mypy for init_ddp_connection (#9051),0.6350756,callback system and init DDP (#3836),  Use PickleError base class to detect all pickle errors   Update changelog with #6917   Add pickle test for torch ScriptModule   Co-authored-by: Ken Witham k.witham@kri.neu.edu Co-authored-by: Akihiro Nitta nitta@akihironitta.com,0
4557,Improve typing for logging (#10748),0.69760126,Logging,Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk,0
4558,Change temporary spawn checkpoint name (#10934),0.64873374,The tuner now usees a unique filename to save a temporary checkpoint (#9682),  gating   tests   pep8   changelog ,0
4559,Merge pull request #20 from williamFalcon/test2,0.52690953,refactored dataloader process hook (#3139),,0
4560,Skip sphinx linkcheck on CHANGELOG files (#16259),0.54263526,- Removed deprecated `IndexBatchSamplerWrapper.batch_indices` ([#13565](https://github.com/Lightning-AI/lightning/pull/13565)),Co-authored-by: Daniele Acquaviva danieleacquaviva@Danieles-MacBook-Pro.local,0
4561,Print ragged dict of metrics in EvaluationLoop._print_results properly (#12857),0.61624604,Deprecated Trainer argument print_nan_grads (#1097),,0
4562,ddp flag change,0.6469444,Deprecated flags: (#2213),,0
4563,Decoupled Reinforcement Learning example for Fabric (#17123),0.63177335,"2. Instantiate Fabric directly, without subclassing",Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,0
4564,rtfd: drop building pdf (#8706),0.5474799,remove _evaluate fx (#3197),  fixjoin   fix join on cpu   fix typo   try to undo horovod skip   undo   Try removing skip   Update CHANGELOG   add back skip for test_horovod_multi_optimizer   Add back skip   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
4565,[App] Accelerate Multi Node Startup Time (#15650),0.6230126,Improved LightningTrainerScript start-up time (#15751),  Add SWA warning if not running every epoch   Typo ,0
4566,"Introduce ckpt_path=""hpc"" keyword for checkpoint loading (#14911)",0.83190846,"- Introduce `ckpt_path=""hpc""` keyword for checkpoint loading ([#14911](https://github.,com/Lightning-AI/lightning/pull/14911))",,1
4567,Remove check num_slurm_tasks in Lite (#14761),0.6016277,Removed deprecation warnings being called for on_{task}_dataloader (#9279),Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,0
4568,Fix DeepSpeedPlugin with IterableDataset (#7362),0.71091914,"Implemented DeepSpeedPlugin._setup_model_and_optimizers (#10009, #10064)",Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
4569,drop deprecated result object 1/n (#5005),0.6488888,        return loss0,,0
4570,feat(wandb): let wandb cli handle runs (#4648),0.6656312,WandbLogger does not force wandb reinit arg to True anymore and creates a run only when needed (#4648),"Changing ""defined"" to ""defines"" in keeping with the convention of using present tense in comments.",0
4571,[App] Min replica=0 would break autoscaler component (#16092),0.6579126,Porting fixes to autoscaler component (#16249),Co-Authored-By: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
4572,remove on_perf check hooks (#3178),0.66094303,Removed support for the deprecated on_load_checkpoint signature. The hook now takes a pl_module positional parameter (#8697),  update docker base on PT 1.7   fix path ,0
4573,"Fix for incorrect usage of detach(), cpu(), to() (#6216)",0.55476093,Removed the Trainer(move_metrics_to_cpu=True) argument (#16358),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
4574,ci: flagship apps 1/n (#16304),0.65952516,App,,0
4575,Improve callback documentation for outputs and accumulate_grad_batches (Resolves #15315) (#15327),0.59358716,refactored dataloader process hook (#3139),,0
4576,Swap ordering of imports,0.664342,import argparse,,0
4577,move progress bar test to correct test folder (#5667),0.6643182,progress bar,"  Add single checkpoint capability   Fix checkpointing in test, few cleanups   Add comment   Change restore logic   Move vars around, add better explanation, make todo align with DeepSpeed team   Fix checkpointing   Remove deepspeed from extra, install in Dockerfile   push   pull   Split to two tests to see if it fixes Deepspeed error   Add comment ",0
4578,Remove support for passing strategy name to plugins (#12700),0.78629506,- Removed support for passing strategy names or strategy instances to the plugins Trainer argument ([#12700](https://github.com/Lightning-AI/lightning/pull/12700)),"  Drop libomp to see what happens   Drop openmpi/horovod installs   Revert ""Drop libomp to see what happens""   This reverts commit cdd524f3   Update before install   Skip horovod failing test ",1
4579,Make checkpointing on train epoch end condition dynamic (#15300),0.7000004,"The ModelCheckpoint.save_on_train_epoch_end attribute is now computed dynamically every epoch, accounting for changes to the validation dataloaders (#15300)",Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,1
4580,update optimizer_step example in docs (#10420),0.7465551,        optimizer.step(),Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
4581,leaving lezwon (#6347),0.43395516,@rohitgr7 @tchaton ,,0
4582,update changelog after 1.3.7 (#8075),0.7236857,Complete changelog,  v1.3.0rc1   . ,1
4583,Restore trainer.current_epoch after tuning (#7434),0.7291423,trainer.tune() now returns the tuning result (#7258),,1
4584,add missing logic to new plugins and accelerator (#5734),0.75066864,The accelerator and training type plugin setup hooks no longer have a model argument (#8536), Update model_checkpoint.py,1
4585,release v0.3,0.73713577,"    version=""0.0.1"",",Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
4586,ci/hotfix: if cache/wheels missing (#16769),0.51802117,Porting fixes to autoscaler component (#16249),  Add test for lr_schedulers()   Add lr_schedulers to LightningModule   Update test comment   Update CHANGELOG ,0
4587,Escape percentage symbol in argparse (#499),0.5261749,Model summary: add 1 decimal place (#4745),,0
4588,Remove support for the deprecated torchtext legacy (#14375),0.82667404,- Removed deprecated support for old torchtext versions ([#14375](https://github.com/Lightning-AI/lightning/pull/14375)),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
4589,Add tuner callback docs (#15030),0.7214701,New Tuner Callbacks,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
4590,ModelCheckpoint's save_last now ignores every_n_epochs (#12418),0.7788287,Changed defaults of save_top_k and save_last to None in ModelCheckpoint (#3680),,1
4591,release v0.3.51,0.76516044,"    version=""0.0.1"",","  Ensure we move the model to eval mode before running evaluation   Ensure we set the flag appropriately across all stages   Add test, move hooks logic   Apply same fix to the validate loop   Update pytorch_lightning/trainer/trainer.py   Fix function name   Fix order, add predict   Shorten the name   Fix input dm, drop duplicate on predict start hook call, as it's called in the setup function   Use hook, remove double call ",1
4592,moved badge image,0.4922288,Moved attributes hiddens and split_idx to TrainLoop (#7507),prepare v1.3.0rc,0
4593,CI: settle file names (#16098),0.5177385,  * `save_config_filename`,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
4594,Add custom dataloader support with Lite  (#10279),0.8189493,        # Let Lite setup your dataloader(s),,1
4595,temp disable install app from source (#13318),0.53130144,Improved support for running apps when dependencies aren't installed (#15711)," Update mlflow.py  6745 adds additional info about the run, as in the native API  Update mlflow.py  trying to fix some backward compatibility issues with resolve_tags  wip on backward compatibility  added a default for getattr in case the registry object exists, but has no proper attribute (weird case but who knows...)   fix pep   impoert   fix registry import   try fix failing tests   removed the first if statement, so that resolve_tags would be defined either case  fix formatting  Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz",0
4596,Proper casting for np scalars in hparams logging (#4647),0.68062425,Allow logging of metrics together with hparams (#1630),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
4597,Update to NeMo Documentation (#4328),0.5497135,"It now supports neptune-client 0.16.16 and neptune >=1.0, and we have replaced the log() method with append() and extend().",,0
4598,Fix: passing wrong strings for scheduler interval doesn't throw an error (#5923),0.59348536,Enforce an epoch scheduler interval when using SWA (#6588),,0
4599,ci: fix docs with caches (#17200),0.5262958,Fix frontend hosts when running with multi-process in the cloud (#17324),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
4600,Merge pull request #18 from williamFalcon/tests,0.51403165,"Changed setup() and teardown() stage argument to take any of {fit,validate,test,predict} (#6386)",  Update Changelog for v1.2.7   legacy   Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz,0
4601,Mark outputs as protected in the evaluation loops (#10781),0.6537909,"Marked several methods in EvaluationEpochLoop as protected: on_evaluation_batch_start, evaluation_step, evaluation_step_end (#9516)",,0
4602,[App] Simplify messaging in cloud dispatch (#16160),0.9344856,Simplified messaging in cloud dispatch (#16160)," Fix some test errors Summary:  Test Plan: Reviewers: Subscribers: Tasks: Tags:   checkpoint consolidation   Update ddp_spawn.py   Update test_metric_result_integration.py   Update test_results.py   Update utils.py   Update utils.py   Update test_all_gather_grad.py   Update test_all_gather_grad.py   Update test_results.py   Revert ""Update test_results.py""   This reverts commit 9d4a2b891d2a4b37e21529a444bda1883d1b5ed1.  Revert ""Merge pull request #1 from shuyingsunshine21/shuyingsunshine21-checkpoint_consolidate""  This reverts commit c5053da789f9d04d2c967a65adf4fb026dc134b8, reversing changes made to 0d23d75bc91e4e0b7805712e394cb093fac22841.  Revert ""Update test_all_gather_grad.py""  This reverts commit 0d23d75bc91e4e0b7805712e394cb093fac22841.  Revert ""Update utils.py""  This reverts commit 70fe5da9c66ceff2fcf4be5b9efdd23a9af8389c.  Revert ""Update utils.py""  This reverts commit a9aae99f6ed6e9388ecf1d8a7bd79966176a65af.  Revert ""Update test_results.py""  This reverts commit ea749068785bbad689a12066544893b1605f20c5.  Revert ""Update test_metric_result_integration.py""  This reverts commit bf70e431b3ce4893de804e0f3b5d59e79346d6d7.  Revert ""Update ddp_spawn.py""  This reverts commit f17210183b84f90c9a62d1ff9b3e05e1fbe5f33b.  Revert ""checkpoint consolidation""  This reverts commit 536c1323b0e6715fb5919196ea48b0fcddddcd66.  Revert ""Revert ""checkpoint consolidation""""  This reverts commit 3a9fde915ad4c69620a6ccc411f5890cb38ba5ac.  Revert ""Revert ""Revert ""checkpoint consolidation""""""  This reverts commit 7a369f47e1a94d701fce48c994cc3f2da266dad0.  Revert ""Revert ""Update ddp_spawn.py""""  This reverts commit 8222dc98ead37d961a52b7366070aa10f66d92d1.  Revert ""Revert ""Update test_metric_result_integration.py""""  This reverts commit 6c095b2370a2afe9d24918a5798ce1ebffed7e0d.  Revert ""Revert ""Update test_results.py""""  This reverts commit 250d0aaaa2e6c6a6a3407bc6c8b83c0fe2479c0b.  Revert ""Revert ""Update utils.py""""  This reverts commit 8651d54d79396eaaba16d7eb1e769a1e91d5702e.  Revert ""Revert ""Update test_all_gather_grad.py""""  This reverts commit dcdcd29731061c919b15ab0b56669259817a81c4.   modify distributed environment to make test pass   add DDP communication hook   remove test related setting   remove more test related setting   fix ddp comm hook util import issue   comments   one more fix for test_custom_plugin   fix ddp spwan   fix sgd   address comments and add tests    add is gpu checking 2. modify test a bit 3. formatting    formatting nit   fix conda 3.7 1.7 issue for no torch.distributed.algorithms module   need at least 1.8.0   minor fix   modify changelog   changelog should link to PR number instead of issue number   refine a bit on doc for register_ddp_comm_hook function, like ddp_comm_wrapper explanation and add hyperparameter for power sgd states in example usge   move single device checking before call register_ddp_comm_hook   formatting   comments   typo   pre-commit formatting ",1
4603,refresh LightningModule API docs (#8276),0.8409717,Update the Lightning App docs (#13537),Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
4604,enable fast_dev_run without a validation loop (#1779),0.7190493,Tuner algorithms will be skipped if fast_dev_run=True (#3903),  Update seed.py   Update pytorch_lightning/utilities/seed.py   Co-authored-by: thomas chaton thomas@grid.ai   Update seed.py   Update seed.py   Update seed.py   Co-authored-by: thomas chaton thomas@grid.ai,1
4605,Additional test for logging during validation loop (#3907),0.7420242,Cleaning up stale logger tests (#3490),,1
4606,update default websocket setting (#16446),0.5652683,Initial plugin server (#16523),,0
4607,Update Trainer(precision) docs (#10368),0.73624,"trainer.{logged,progress_bar,callback}_metrics are now updated on-demand (#7882)","  add changelog   add clip by value   fix bug in training tricks.rst   fix bug in trainer.rst   Update trainer.rst   Update trainer.rst   Update CHANGELOG.md   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/plugins/precision/deepspeed_precision.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/utilities/enums.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   yapf formatting   update training tricks   update based on comment   update based on comment   Update pytorch_lightning/trainer/trainer.py   Co-authored-by: ananthsub ananth.subramaniam@gmail.com   update based on comment   pep8   mypy   mypy   Update docs/source/advanced/training_tricks.rst   Co-authored-by: thomas chaton thomas@grid.ai   Update sharded_native_amp.py   Update test_sharded_parity.py   update test codes   Update test_tpu.py   Update pytorch_lightning/trainer/connectors/training_trick_connector.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   Update test_trainer.py   Update enums.py   Update enums.py   add super-class initialization to precision plugins.   add clip_grad horovod cpu test   add clip_grad horovod cpu test   use subprocess check_call   change order of horovod tests   set max_epochs 2 in horovod test   remove clip_grad_val test from horovod-cpu   remove ""type: ignore""   divide clip grad val test in horovod   update based on comments   add super-class initialization to precision plugins.   bugfix   bugfix   revert some changes   revert some changes   Update tests/models/test_horovod.py   merge master   Delete signature test   No point in testing a signature Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz",1
4608,Update notebooks submodule and add tutorial view to docs (#9420),0.642921,"For a full tutorial and running example, visit our docs. TODO: add to docs",Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
4609,Improve epoch_result_store code quality (#4875),0.5981424,Refactored EpochResultStore (#5522),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
4610,Correct false error message relating to Trainer(precision) (#14828),0.7976304,Trainer is now raising a MisconfigurationException instead of a warning if Trainer.{validate/test} is missing required methods (#10016),There seem to be 3 arguments missing in the lr_find call in the tunining.py file.,1
4611,fix broken opt link,0.57287276,Updated error message for interactive incompatible plugins (#9896),  Add test for symlink support and initial fix   Respond to comment and add docstring   Update CHANGELOG.md   Simplify   Update pytorch_lightning/utilities/cloud_io.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Make LightningLocalFileSystem protected  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
4612,[bugfix] DeepSpeed with no schedulers (#8580),0.7060267,"Implemented DeepSpeedPlugin._setup_model_and_optimizers (#10009, #10064)",,1
4613,Add unit tests for pl.utilities.grads (#9765),0.69448817,Tip: This feature is also very useful for unit testing!, Fix DPP + SyncBN   Ensure that model is already on correct GPU before applying SyncBN conversion  Fix order of SyncBN for ddp_spawn,0
4614,[Metrics] R2Score (#5241),0.6642667,Regression metrics (#2221),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
4615,fix worker warning (#2504),0.6150497,warning_utils >> warnings,,0
4616,Updated iterable dataset with len warning message (#6972),0.671085,Disabled sampler replacement when using IterableDataset (#11507),  sanitize none params during pruning   amend ,0
4617,fix/enable - check F401 (#5201),0.61804175,Changed fsspec to tuner (#4458),  fix iterable dataset len check   update predict and validate   add validate to test   add changelog   add predict ,0
4618,update Docs req. (#824),0.6113016,Docs improvements,,0
4619,added auto port find,0.6028151,Porting fixes to autoscaler component (#16249),  Update training_type_plugin.py   Update accelerator.py   Update pytorch_lightning/plugins/training_type/training_type_plugin.py   Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com,0
4620,fix(wandb): prevent WandbLogger from dropping values (#5931),0.923023,Prevent WandbLogger from dropping values (#5931),Fixes  #6800,1
4621,typing: fix App's core API - Work (#16946),0.5649923,- Fixed a bug when launching an app on multiple clusters ([#15226](https://github.com/Lightning-AI/lightning/pull/15226)),Co-authored-by: rohitgr7 rohitgr1998@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
4622,Do not force sync_dist=True on epoch end (#13364),0.70518947,Do not force metric synchronization on epoch end,  less IDE complain about unused args   ... ,1
4623,Fix schedule reset logic in pytorch profiler (#10837),0.847595,"Resolved schedule step bug for PyTorch Profiler (#6674, #6681)",,1
4624,[CLI] change cluster creation cost savings mode default (#14132),0.6945195,Enabled custom clusters (#4048),  Update logic for checking TPUs availability   fix flake8   add fix ,0
4625,1/n Add FaultTolerantMode (#10645),0.7209298,- Fault Tolerant Manual,,1
4626,rename accelerator_backend -> accelerator (#6034),0.8624681,Renamed all backends to Accelerator (#4066),  Add 1.2.6 sections to CHANGELOG   Update CHANGELOG.md   legacy   Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz,1
4627,added comet testing dep,0.65102005,Using .comet.config file for CometLogger (#1913),,0
4628,"Update gcsfs requirement from <2022.6.0,>=2021.5.0 to >=2021.5.0,<2022.8.0 in /requirements (#14079)",0.5410857,Generalized Optimizer validation to accommodate both FSDP 1.x and 2.x (#16733),  Added base docs   Add more information   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
4629,Add warning if multiple batch_sizes are found from ambiguous batch (#10247),0.64795643,"    batch_size=32,","  Add context to call hook to handle all modules defined within the hook   Expose some additional parameters   Added docs, exposed parameters   Make sure we only configure if necessary   Setup activation checkpointing regardless, saves the user having to do it manually   Add some tests that fail currently   update   update   update   add tests   change docstring   resolve accumulate_grad_batches   resolve flake8   Update DeepSpeed to use latest version, add some comments   add metrics   update   Small formatting fixes, clean up some code   Few cleanups   No need for default state   Fix tests, add some boilerplate that should move eventually   Add hook removal   Add a context manager to handle hook   Small naming cleanup   wip   move save_checkpoint responsability to accelerator   resolve flake8   add BC   Change recommended scale to 16   resolve flake8   update test   update install   update   update test   update   update   update test   resolve flake8   update   update   update on comments   Push   pull   Update pytorch_lightning/plugins/training_type/deepspeed.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update pytorch_lightning/plugins/training_type/deepspeed.py  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   update   Apply suggestions from code review   Swap to using world size defined by plugin   update   update todo   Remove deepspeed from extra, keep it in the base cuda docker install   Push   pull   update   update   update   update   Minor changes   duplicate   format   format2   Co-authored-by: SeanNaren sean@grid.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz",0
4630,Rename profiler to profilers (#12308),0.70622385,Moved profilers to their own file (#7822),  Update Bolts link   Update Bolts link   formt   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
4631,Fix restoring training state during trainer.fit only (#9413),0.76851,Disabled training when limit_train_batches=0 (#4371),  fix_hydra   update changelog   Co-authored-by: Your Name you@example.com,1
4632,Deprecate Trainer.should_rank_save_checkpoint property (#11068),0.9149569,Removed should_rank_save_checkpoint property from Trainer (#9433),  update chlog v1.2.5   legacy ,1
4633,Fix resetting internal bars in RichProgressBar after each trainer stage (#15377),0.69031656,Changed progress_bar_refresh_rate trainer flag to disable progress bar when setting to 0. (#1108),,0
4634,Add 'app' parameter into the command example (#14055),0.59660304,Add --app_args support from the CLI (#13625),,0
4635,add docs (#8952),0.63778335,Docs,"  Add base hook for model parallel   fix callback signature   Simplify hook   Add hook logic   add tests   add property setter   add logic for being called once   Update changelog   Fix   fix return type   fix lambda callback test   Fix tests   Apply code suggestions   add logic for setup_optimizers_predispatch   add common dummy model   Swap call order   Remove test that isn't needed anymore   Update tests   Add a bit more doc   Few code review fixes   Update pytorch_lightning/accelerators/accelerator.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   Change hook name   Fix test   Test setup hook, refactor names   Swap call order of callbacks and model initialization   Change name of context manager   Co-authored-by: SeanNaren sean@grid.ai Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com",0
4636,"Remove ""Optional"" hint from non-None arguments (#12214)",0.6041511,# 2. Remove the `hiddens` argument,  move save_checkpoint responsability to accelerator   update ,0
4637,Minor refactors to init_dist_connection (#11733),0.6634711,moves init apex from LM to apex connector (#3923),  update_logic   update   Update tests/utilities/test_xla_device_utils.py   Update pytorch_lightning/utilities/xla_device.py   Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com  Update pytorch_lightning/utilities/xla_device.py  Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com   update test   Update tests/utilities/test_xla_device_utils.py   update   Apply fix   Docstring   flake8   update   Co-authored-by: Your Name you@example.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
4638,fix/test quant (#6040),0.6147524,Deprecated the TestTubeLogger (#9065),  support python 3.9   update CI   onnxruntime   .   .   onnxruntime   t 55   t 75   add script   use   onnx   onnx   onnx   whl   np   find   21   Apply suggestions from code review   Apply suggestions from code review   onnx   CI   req   ~ dockers   min   .   drop horovod   drop horovod   drop horovod   fix   fix   . ,0
4639,"Update matplotlib requirement from <3.5.3,>3.1 to >3.1,<3.6.2 in /requirements (#15422)",0.5019614,Dropped torchvision dependency in tests and added own MNIST dataset class instead (#986),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
4640,Fix typo in combined_loader.py documentation (#17256),0.58907235,Removed duplicated module pytorch_lightning.utilities.arg_parse for loading CLI arguments (#1167),  use latest   remake   examples ,0
4641,Add note for PyCharm users on configuration for RichProgressBar (#10307),0.7056936,from pytorch_lightning.callbacks import RichProgressBar,,1
4642,adding missing changelogs (#5019),0.71537435,Full Changelog,  add warning non reduced   add test   update test   update changelog   Update pytorch_lightning/trainer/connectors/logger_connector/epoch_result_store.py   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com  update  Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,1
4643,[feat] Add better support for predict + ddp 2/3 (#7215),0.62877184,Enable mixed precision in DDPFullyShardedStrategy when precision=16 (#12965),,0
4644,Let TorchCollective works on the torch.distributed WORLD process group by default (#16995),1.0,Let TorchCollective works on the torch.distributed WORLD process group by default (#16995),,1
4645,add forgotten test in #7240 (#7283),0.56746775,Enabling val/test loop disabling (#2692),,0
4646,Save and load hparams from checkpoints,0.7969278,checkpoints now store hparams,  Add artifcact_location arg to MLFlow logger   Add CHANGELOG URL   Update test ,1
4647,Add Trainer.state to spawn queue #10937,0.669677,Trainer.request_dataloader now takes a RunningStage enum instance (#8858),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
4648,cleaning (#2449),0.91325897,"Cleaning (#5948, #5949, #5950)",  use external deprecate   simplify   simplify   simplify   flake8   .   others   . ,1
4649,Only import PostLocalSGD related modules when it's needed (#10359),0.6816354,Module imports are now catching ModuleNotFoundError instead of ImportError (#9867),  update docs   add hook and update docs   update tests   chlog   Update CHANGELOG.md   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  chlog  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
4650,Lightning docker image based on base-cuda (#3637),0.63488555,Updated LightningTemplateModel to look more like Colab example (#1577),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: chaton thomas@grid.ai,0
4651,Remove non-existent Lite checks (#15325),0.5909079,Silenced some warnings. verified ddp refactors (#3483),,0
4652,"Update wandb requirement from <0.12.17,>=0.8.21 to >=0.8.21,<0.12.19 in /requirements (#13308)",0.569378,Removed wandb logger's finalize method (#1193),  Fix checkpoint callback issue for TPUs   update changelog   add barrier   apply code suggestions   update trainer test   remove spaces   fix tpu tests   Apply suggestions from code review   add comment   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
4653,Fix support for torch Module type hints in LightningCLI (#7807),0.75311065,- Fixed issue where the CLI fails with certain torch objects ([#13153](https://github.com/Lightning-AI/lightning/pull/13153)),,1
4654,ref: result 1/n (make monitor default to checkpoint_on to simplify re… (#3571),0.8865161,Result - make monitor default to checkpoint_on to simplify (#3571),  Metrics holder cleanup and better error message   Update pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py   _VALUE -> _METRIC_TYPE ,1
4655,Mark CheckpointConnector as protected (#11550),0.7260034,- Marked `trainer.checkpoint_connector` as protected ([#11550](https://github.com/PyTorchLightning/pytorch-lightning/pull/11550)),,1
4656,Reuse code in demos.BoringModel (#16242),0.56957763,"Refactored evaluation loop interface; added new classes DataLoaderLoop, EvaluationLoop, EvaluationEpochLoop (#7990, #8077)",,0
4657,cleaning as standalone==mirror (#16601),0.6971507,"Cleaning (#5948, #5949, #5950)",  Remove E231 from ignore list   Follow E231   Update pytorch_lightning/trainer/data_loading.py ,0
4658,remove backward from training batch loop (#9265),0.735788,Refactored training loop (#2336),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,1
4659,fix flake8 for new plugins (#5951),0.6921827,Updated error message for interactive incompatible plugins (#9896),,0
4660,Rename replace_sampler_ddp|replace_sampler to use_distributed_sampler (#16829),0.70994425,Allow setting replace_sampler_ddp=True with a distributed sampler already added (#4273),  fix: update example autoencoder.py to reflect args   Update pl_examples/basic_examples/autoencoder.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
4661,Merge pull request #52 from alok/ptl-pl,0.49565074,@ptl.data_loader,  classif   grad_img   nlp   ssl   format ,0
4662,rtfd: building on PRs only (#17086),0.5440345,Refactor Model backward (#2276),,0
4663,Update examples with multiple optimizers (#16710),0.8497547,Working with multiple optimizers (#16539),,1
4664,"Docs for Pruning, Quantization, and SWA (#6041)",0.5726026,"docs for all Metrics (#2184, #2209)",  update coverage config   parallel   parallel   Apply suggestions from code review   Apply suggestions from code review   paralel   paralel   paralel   combine   combine   .   ..   ..   ..   rev   cb   cb   drop   drop   .   ..   ...   ...   ...   . ,0
4665,LightningCLI support for argument links applied on instantiation (#7895),0.71445876,| Argument LightningCLI(auto_registry=...)                                                                   | 1.9             | Not necessary anymore                           |,  Refactor profilers   Update PassThrough   WIP - This is broken and will change   Update pytorch_lightning/profiler/pytorch.py   Co-authored-by: thomas chaton thomas@grid.ai   resolve tests   resolve tests   find output   try something   update   add support for test and predict   update   update   use getattr   test   test   update   tests   update   update   update   update   update   remove file   update   update   update   update   update   test   update#   update   update tests   update   add suport for 1.8   rename records   add support for 1.8   update   resolve flake8   resolve test   Refactor basic profilers   Fixes   Unused import   Introduce setup   Profile on all ranks. Print to stdout on 0   Introduce dirpath + filename   CHANGELOG   Add tests. Address comments   add on_run_stage_setup   add on_run_stage_setup function   update   add test for RegisterRecordFunction   update lightnng flow direction   move variable to private   remove trace   Undo code that should be in 3/4   Multi-stage multi-rank   2/5 changes   Pass stage in del   Remove TODOs   Describe on_evaluation_end. Add tests   Typo   Address comments   deepcopy tests   Advanced teardown   Fix teardown test   Fix tests   Minor change   Update CHANGELOG.md   Fix test   Quick fixes   Fix 6522   resolve ddp tests   resolve tests   resolve some tests   update tests   resolve tests   update   resolve tests   resolve some tests   Missed fixes from 3/5   Fixes   resolve some tests   resolve test for 1.7.1   Broken refactor   Missed stage   Minor changes   resolve tests   Update CHANGELOG   resolve bug   remove print   Typo   Cleanup   resolve ddp test   remove barrier   update profiler   update   Smaller model   update   resolve tests   update   Minor changes. CHANGELOG   Minimize diff   update to 1.8.1   RunIf. Extra code. Check segfault   resolve tests   Typo. Bad merge   Fixing a bad merge   replace for kineto   Update pytorch_lightning/profiler/pytorch.py   Co-authored-by: ananthsub ananth.subramaniam@gmail.com  Update pytorch_lightning/profiler/pytorch.py  Co-authored-by: ananthsub ananth.subramaniam@gmail.com   Minor changes   Bad merge   Use lists for flexibility   Use sets   predict_step   Ananth's suggestion   update   Docs   Update pl_examples/basic_examples/profiler_example.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   update example   update example   Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
4666,Fix tpu cleanup (#3056),0.64393187,clean up data reset (#3161),Co-authored-by: thomas chaton thomas@grid.ai,0
4667,Remove RC candidate install (#8322),0.5551766,"Removed PyTorch 1.6 support (#10367, #10738)",,0
4668,Trainer: auto default (#16847),0.72074014,trainer = pl.Trainer(plugins=SLURMEnvironment(auto_requeue=False)),  add predict_step   Update predict_loop.py   Update trainer.py   Update trainer.py   resolve bugs   update   update   update   resolve bug   resolve some failing tests   udpate tests   update   resolve tests   add a test   remove typo   add a test for attachement   update   changed to on_train_dataloader   remove flash_special_attr   resolve tests   update   update   update   update on comments   Update pytorch_lightning/trainer/data_loading.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
4669,remove source-lit docs 2 (#15527),0.60762274,Removed the deprecated LightningDeepSpeedModule (#16041),,0
4670,"Revert ""Misleading exception raised during batch scaling (#1973)"" (#2219)",0.62318003,"Raised MisconfigurationException when total length of dataloader across ranks is zero, and give warning when total length is non-zero, but only local rank length is zero. (#9827)",Co-authored-by: tchaton thomas@grid.ai,0
4671,fix codecov reports (#1867),0.5318424,Updated mlflow with using resolve_tags (#6746),  psnr   r2score   ssim   chlog ,0
4672,clean up dead code,0.6120249,- Removed the deprecated code in:,  explained_variance   tests   mean_absolute_error   mean_squared_error   mean_relative_error   mean_squared_log_error   chlog ,0
4673,Add configure_gradient_clipping hook in LightningModule (#9584),0.7848115,1. Override LightningModule hook,  fix comparing versions   chlog   .   ...   datasets ,1
4674,CI: adjust examples for lightning.pytorch (#15457),0.763884,+ # pytorch_lightning==1.7.0,  add setup   update   updates on comment   Minor changes   Extra import   Docs   Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
4675,FSDP (native) support for LightningLite (#14967),0.74452394,- Added Native FSDP Strategy ([#12447](https://github.com/Lightning-AI/lightning/pull/12447)),  mock examples   drop from GA ,1
4676,Remove dead code in TrainingEpochLoop (#10750),0.7358958,Removed the deprecated TrainerLoggingMixin class (#8609),  refactoring setup   .   docs   flake8 ,1
4677,simplify imports Omegaconf (#4873),0.57403046,"Separated utils: imports & enums (#5256, #5874)",Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com,0
4678,Callback docs with autosummary (#3908),0.6105472,Read more about callback entry points in our docs.,"  Allow training_type_plugin to delay optimizer configure   Add missing references to trainer, add a CPU accelerator based test ",0
4679,set 1.2.0dev (#5132),0.6145073,"    version=""0.0.1"",",,0
4680,Move s3fs to cloud extras (#15729),0.62184227,Swaped torch.load for fsspec load in cloud_io loading (#3692),,0
4681,Few typo correction (#2011),0.48905665,"    },",,0
4682,Avoid zeros in dice and iou (#2567),0.5378915,        return 7  # lucky number 7,,0
4683,[Metrics] [Docs] Add section about device placement (#5280),0.5993136,device_stats = DeviceStatsMonitor(),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
4684,[App] Serve datatypes with better client code (#16018),0.55014265,"for data: val_dataloader, test_dataloader, train_dataloader",  Add Tests for val and test-steps   Add native AMP   pep8 tests   pep8 plugin   changelog ,0
4685,Prepare v1.5.0rc0 (#9893),0.6250733,0.4.0,  try Azure   -e   path ,0
4686,Set find unused parameters to True by default to fix breaking compatibility (#6438),0.63957095,Changed the default of find_unused_parameters back to True in DDP and DDP Spawn (#6438),  confusion_matrix   iou   f_beta   hamming_distance   stat_scores   tests   flake8   chlog ,0
4687,Unpin PyYAML<=5.4.1 (#8329),0.7025671,Drop PyTorch 1.9 support (#15347),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Roger Shieh sh.rog@protonmail.ch Co-authored-by: Kaushik Bokka kaushikbokka@gmail.com,1
4688,Support torch 1.10.1 (#11095),0.70378876,Removed dependency on torchvision (#797),  add NVIDIA flows   push   pull   ...   extras   ci prune   fix   tag   .   list ,1
4689,Ref: Pull duplicate data interface definition up into DataHooks class (#3344),0.9821098,duplicate data interface definition up into DataHooks class (#3344),,1
4690,Remove untested NVIDIA Dali example (#16306),0.6463828,nvidia/apex deprecation (#16039),,0
4691,Faster callback configuration validator checks (#11785),0.71614075,Configuration Validator (#9779),"  Move connection setup into the setup function. Call setup hook after we set up the accelerator   Added CHANGELOG.md   fix setup order in callback test   fix input arguments in test   Mock distributed function, remove protection to turn into training type hook   Remove import   Add missing mock, ensure custom plugin does not create children process   Skip test on windows   Update deepspeed to init connection in setup   Do not initialize distributed module   Move DeepSpeed tests to special tests since dist communication is being set up   Special the test to see if this fixes CI   Delete accelerator connector test to see if its causing build to fail   Delete deepspeed test   Revert ""Delete accelerator connector test to see if its causing build to fail""   This reverts commit edde60b8  Revert ""Delete deepspeed test""  This reverts commit 9d317429   Reverse hook   Reverse setup hooks to debug again   Add todo so i know where i left off   For single device move in pre_dispatch after setup function   Add additional model to device hook if any additional parameters have been set   See if we can enable deepspeed tests   Revert ""See if we can enable deepspeed tests""   This reverts commit b5450def   See if this hook approach works   Introduce new granular hooks   Remove import, fix tpu spawn by moving the function to setup   Added missing special test   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com",1
4692,hotfix for GHA tpu (#5762),0.5528345,Updated logic for checking TPUs availability (#6767),  Update changelog for v1.2.4   lagacy v1.2.4   prune duplicates from changelog   Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz,0
4693,Merge pull request #11388 from PyTorchLightning/ci/mergify-team,0.6352854,- Removed `configure_sync_batchnorm` from `ParallelStrategy` and all other strategies that inherit from it ([#11754](https://github.com/PyTorchLightning/pytorch-lightning/pull/11754)),  avg precision   precision   recall   curve   tests   chlog   isort   fix ,0
4694,Fix docs typo: train_batch => val_batch (#4659),0.56123114,Fixing critical bugs in newly added hooks and hparams assignment.,  update doc   update example ,0
4695,Docs (#1024),0.80701476,Docs,  class: AUC AUROC   func: auc auroc   format   tests ,1
4696,Patch for device placement (Reduce host and device syncs) (#17334),0.6136817,Made parallel devices optional across all plugins (#6051),  prune accuracy   chlog   flake8   Apply suggestions from code review   Co-authored-by: Nicki Skafte skaftenicki@gmail.com   wrap   test   test   fix   Co-authored-by: Nicki Skafte skaftenicki@gmail.com,0
4697,(app) Run the flow only if the state has updated 1/2 (#14076),0.8226584,Run the flow only if the state has changed from the previous execution (#14076),  fix deprecation wrapper & tests   flake8 ,1
4698,Fix the gradient_clip_algorithm has no effect issue. (#6928),0.7807557,    gradient_clip_algorithm,  add trick to doc   update   update path   Update docs/source/benchmarking/performance.rst   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
4699,Avoid using the deprecated LooseVersion (#16162),0.9999999,Avoid using the deprecated LooseVersion (#16162),  add outputs param for on_val/test_epoch_end hooks   update changelog   fix warning message   add custom call hook   cache logged metrics   add args to docstrings   use warning cache   add utility method for param in sig check   Update CHANGELOG.md   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   update docstring   add test for eval epoch end hook   add types and replace model ref   add deprecation test   fix test fx name   add model hooks warning   add old signature model to tests   add clear warning cache   sopport args param   update tests   add tests for model hooks   code suggestions   add signature utils   fix pep8 issues   fix pep8 issues   fix outputs issue   fix tests   code fixes   fix validate test   test   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
4700,Fix prepare_data implementation in BoringDataModule (#10915),0.67269266,"DataModules now avoid duplicate {setup,teardown,prepare_data} calls for the same stage (#7238)",  docs   wrapper   test   count   flake8 ,0
4701,Add docstring example to use dataloader_idx in transfer_batch_to_device (#8982),0.6050535,dataloader = fabric.setup_dataloaders(dataloader),  _basic_input_validation   _check_shape_and_type_consistency   _check_num_classes_binary   _check_num_classes_mc   _check_num_classes_ml   _check_top_k   _check_classification_inputs   _input_format_classification   _reduce_stat_scores   DataType   rest   flake8   chlog ,0
4702,Update LightningDataModule docs (#17238),0.92442113,Update the Lightning App docs (#13537),  change tests   fix   test   _defaults_from_env_vars   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
4703,Add missing import statements in lightning_module.rst (#11946),0.6896672,Module imports are now catching ModuleNotFoundError instead of ImportError (#9867),  return from plugin   dont return for tpu ,0
4704,reverted achange from testcode:: to code-block:: python (#3453),0.57514113,Removed pytorch_lightning/trainer/evaluation_loop.py (#8056),  base class   extensions   chlog   _stable_1d_sort   _check_same_shape   _input_format_classification_one_hot   utils   to_onehot   select_topk   to_categorical   get_num_classes   reduce   class_reduce   tests ,0
4705,fixed multi-gpu tests,0.6998874,- Full tests that run multiple models in different configs,  Update hook lifecycle   Update docs/source/common/lightning_module.rst ,0
4706,Protect functions not to be accessed by user (#4305),0.67800635,Restricted public access to several internal functions (#8024),,0
4707,Fix broken trainer flags nb (#4159),0.6599344,Trainer.fit hook clean up (#3198),  Clean up docs and add some explicitness around stages   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
4708,fixes #4,0.6875726,"At last, lots of bug fixes (see below).",  init test: test_lr_find_with_bs_scale   Update test_lr_finder.py   remove gpu req   try boring model   custom boring model   pep8   fix typo   Update test_lr_finder.py   typo   typo ,0
4709,added warning when changing monitor and using results obj (#3014),0.6134523,Deprecated default value of monitor argument in EarlyStopping callback to enforce monitor as a required argument (#7907),  deprecate metrics   examples   req   docs   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com  pep8  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com,0
4710,prepare RC0 (#8399),0.51411265,Implemented ready for components (#16129),,0
4711,ENG-1524: Change retry mechansim default in LightningClient (#15412),0.7492076,Changed the default LightningClient(retry=False) to retry=True (#16382),"  init information retrieval metrics   changed retrieval metrics names, expanded arguments and fixed typo   added 'Retrieval' prefix to metrics and fixed conflict with already-present 'average_precision' file   improved code formatting   pep8 code compatibility   features/implemented new Mean Average Precision metrics for Information Retrieval + doc   fixed pep8 compatibility   removed threshold parameter and fixed typo on types in RetrievalMAP and improved doc   improved doc, put first class-specific args in RetrievalMetric and transformed RetrievalMetric in abstract class   implemented tests for functional and class metric. fixed typo when input tensors are empty or when all targets are False   fixed typos in doc and changed torch.true_divide to torch.div   fixed typos pep8 compatibility   fixed types in long division in ir_average_precision and example in mean_average_precision   RetrievalMetric states are not lists and _metric method accepts predictions and targets for easier extension   updated CHANGELOG file   added '# noqa: F401' flag to not used imports   added double space before '# noqa: F401' flag   Update CHANGELOG.md   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   change get_mini_groups in get_group_indexes   added checks on target inputs   minor refactoring for code cleanness   split tests over exception raising in separate function && refactored test code into multiple functions   fixed pep8 compatibility   implemented suggestions of @SkafteNicki   fixed imports for isort and added types annontations to functions in test_map.py   isort on test_map and fixed typing   isort on retrieval and on init.py and utils.py in metrics package   fixed typo in pytorch_lightning/metrics/init.py regarding code style   fixed yapf compatibility   fixed yapf compatibility   fixed typo in doc   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com",1
4712,Fix: Repeated .fit() calls ignore max_steps iteration bound  (#5936),0.58565605,"- Removed the `fit_loop.{min,max}_steps` setters ([#16803](https://github.com/Lightning-AI/lightning/pull/16803))",  document exceptions for metrics/functional   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Apply suggestions from code review  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com,0
4713,[2 / 3] improvements to saving and loading callback state (#7187),0.6803887,Callback hooks for loading and saving checkpoints,  testing on python 3.8   req ,0
4714,Update changelog for development post 1.8 (#15444),0.6678904,Full Changelog: https://github.com/Lightning-AI/lightning/compare/1.8.0...1.8.0.post1,,0
4715,Fix apt repo issue for docker (#3823),0.5536901,Resolved a bug where the install command was not installing the latest version of an app/component by default (#14181),  resolve bug   update   update changelog   update PR   Update pytorch_lightning/trainer/connectors/logger_connector/epoch_result_store.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   add todo   resolve issues   resolve flake8   update   add coverage for reduce   wip   restore back to brodbact   remove test.py   resolve flake8   update   check world size   resolve test   update   use pytorch version when defined   update on comments   update on comments   flake8   resolve bugs   Update CHANGELOG.md   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   update   update   update   update   remove test   update   resolve flake8   update   update   update   proxy   update   update   resolve typo   prune   update parallel   update   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
4716,Deprecate tuning enum and trainer properties (#15100),0.7196391,| Enum TrainerFn.TUNING                                    | 1.10             | No longer supported         |,  add docs and minor updates   docs   fraction ,1
4717,"App: Fix AppState, streamlit example (#17452)",0.62912464,App,,0
4718,added test model to do also,0.7387979,- Full tests that run multiple models in different configs,  Fix zero_grad in docs   Fix zero_grad in docs ,1
4719,Add document to showcase scaleout on hpu (#14357),0.5260439,Allowing decorate model init with saving hparams inside (#4662),  Remove unused mixing attributes   Missing import ,0
4720,Call DataModule hooks implicitly in trainer (#2755),0.7141366,train_dataset = trainer.train_dataloader.loaders[0].dataset,,1
4721,Merge pull request #17 from williamFalcon/tests,0.53045547,"merge backends (#3476, #3477, #3478, #3480, #3482)",  Update model_checkpoint.py   add tests   Update model_checkpoint.py   Update test_model_checkpoint.py   fix tests   every_n_batches   Update test_model_checkpoint.py   defaults   rm tests   Update model_checkpoint.py   Update test_model_checkpoint.py   Prune deprecated metrics for 1.3 (#6161)   prune deprecated metrics for 1.3   isort / yapf   Update model_checkpoint.py   add tests   defaults   Update CHANGELOG.md   pre-commit   Update model_checkpoint.py   update defaults   Update test_remove_1-5.py   Update model_checkpoint.py   Update model_checkpoint.py   Update model_checkpoint.py   Update model_checkpoint.py   Update model_checkpoint.py   Update model_checkpoint.py   fix tests   Update test_model_checkpoint.py   Update model_checkpoint.py   Update model_checkpoint.py   Update model_checkpoint.py   Update test_model_checkpoint.py   ckpt-callback   Update test_model_checkpoint.py   Update model_checkpoint.py   Update model_checkpoint.py   validation-end   Update model_checkpoint.py   Update test_model_checkpoint.py   Update test_model_checkpoint.py   Update test_model_checkpoint.py   Update test_model_checkpoint.py   clarify-names   Make names explicit as to which hooks they apply to   Use step instead of batch for consistency with global step   Update model_checkpoint.py   Update model_checkpoint.py   Update model_checkpoint.py   Update model_checkpoint.py   Update model_checkpoint.py   mutual-exclusive   Make every_n_train_steps and every_n_val_epochs mutually exclusive   fix-default-0   Update CHANGELOG.md   formatting   make-private   make attributes private to the class  rebase  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
4722,Make StreamLit UI less flaky (#17598),0.5591404,Simpler interface,,0
4723,update warning in docs regarding support of tuner features in DDP (#9011),0.70080566,Silenced some warnings. verified ddp refactors (#3483),  add exceptions and test   hook   fix   clean up   clean up   regex   regex   docs   rev   comment and docs   chlog   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Apply suggestions from code review  Co-authored-by: chaton thomas@grid.ai   Monkey-patch device count   docs   pep   api_change   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: chaton thomas@grid.ai,1
4724,Add Trainer.validate(…) method to run one validation epoch (#4948),0.8091657,Validation is now always run inside the training epoch scope (#7357)," argparse: Add inplace option  Replicate in GAN model   datamodule: Deduplicate logic w/ argparser utilities   Update pl_examples/domain_templates/generative_adversarial_net.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Apply suggestions from code review  Co-authored-by: Akihiro Nitta nitta@akihironitta.com   Keep docstrings   Correct name   Whitespace   Consistency   fix weird type stuff   try alt - use_argument_group   fix syntax + lint   fix ci errs   fix ci   change examples... still failing w/ ""unrecognized arguments: --batch_size""   address review   mnist_datamodule: add some docstrings   argparse: check cls or cls.init for param   didn't capture issue, but meh   fix lint   fix no-doc edge case   address review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com",1
4725,[Feat] Adding PruningCallback (#5618),0.66835093,adding Trainer.tune() (#3293),,0
4726,adding CI for e2e on Azure (#14282),0.49074224,Refactor cloud dispatch and update to new API (#16456),,0
4727,remove todos (#11804),0.741781,Removed deprecated: (#2760),  cleaning SWA (#6259)   rename   if   test   chlog   Remove opt from manual_backward in docs (#6267)   switch agents pool (#6270)   Allow user to disable the automatic formatting of checkpoint file names.   Added changelog entry.   Made flake8 happy.   Applied review suggestion: quotes for special characters in docstring   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   Fixed example in docstring.   Fixed syntax error in docstring.   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: thomas chaton thomas@grid.ai Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
4728,Use torch.nn.utils.clip_grad_norm_  and add clip_grad_by_value support for TPU (#7025),0.79892766,Changed clip_grad_norm to use torch.nn.utils.clip_grad_norm_ (#7025),Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: chaton thomas@grid.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
4729,Add torchelastic check when sanitizing GPUs (#8095),0.7109729,GPU training (#2704),,1
4730,ref: organize args 3/n (#3449),0.87974715,"organize args (##3435, #3442, #3447, #3448, #3449, #3456)",  Ensure we set the default device before initializing deepspeed   Add CHANGELOG.md   Update pytorch_lightning/plugins/training_type/deepspeed.py   Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,1
4731,Verify Fabric.launch() was called (#17570),0.64886874,fabric.launch(),  add test   update changelog   update   rename function ,0
4732,Drop gatekeeper CI checks (#14717),0.63720626,Early stopping checks on_validation_end (#1458),"  Set find unused parameters to True by default to fix breaking models, add suggestion to re-enable   Add changelog ",0
4733,Fix gradient accumulation for ShardedDataParallel (#9122),0.665414,"    gradient_clip_val,",  raise an exception if check_val_every_n_epoch is not an integer   remove unused object   add type hints   add return type   update exception message   update exception message ,0
4734,ref: merge backends x/n (#3477),0.84140646,"merge backends (#3476, #3477, #3478, #3480, #3482)","  fix dummy logger   docs   update docs   add changelog   add none return annotation   return empty string for name, version ",1
4735,Ignore _notebooks when running flake8 (#13990),0.5950257,Avoid using the deprecated LooseVersion (#16162),  update changelog   legacy 1.2.3   Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz,0
4736,specify cache matrix (#1725),0.4866203,Enabled custom clusters (#4048),  typing   yapf   typing ,0
4737,"Pass {fit,validate,test,predict} to setup() and teardown() (#6386)",0.85640407,"Changed setup() and teardown() stage argument to take any of {fit,validate,test,predict} (#6386)",  fix   add simple test   fix imports   add changelog   tighter test with on_fit_start hook closer to the dispatch call   move class inside test f unction   add a comment ,1
4738,fix trainer distributed attributes (#5303),0.70442426,Trainer(distributed_backend='ddp2')  ,,1
4739,Re-design call_hook interface (#10575),0.6642882,Callback hooks,  fixed bug where tuner would not tune lr if also tuning batch_size   added a '+1' to computing the smoothed loss. This maintains the behavior for the smoothed loss as before the bug fix   pep8 fix   add changelog   Co-authored-by: chaton thomas@grid.ai Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
4740,fix 16 bit for TPU (#1020),0.6844462,Support 8-core TPU on Kaggle,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
4741,Fix trainer not resetting lightning_optimizers (#6372),0.7912655,Check LightningOptimizer doesn't delete optimizer hooks (#6305),  fix   update   fix   move the class outside ,1
4742,Add prefix argument in loggers (#4557),0.6654492,Properly pass some Logger's parent's arguments to super().__init__() (#12609),,0
4743,Add support for torch.set_detect_anomaly (#9848),0.67692393,"Lightning makes it easier to debug your code, so we've added support for torch.set_detect_anomaly. With this, PyTorch detects numerical anomalies like NaN or inf during forward and backward. Read more about anomaly detection here",,0
4744,"rename trainer modules, drop _mixin (#571)",0.7098849,Removed the deprecated TrainerTrainingTricksMixin class (#8679), Fix bug  Fix AttributeError: 'NoneType' object has no attribute 'finalize'   Update CHANGELOG.md   deleted a period   Update CHANGELOG.md   Co-authored-by: Akihiro Nitta nitta@akihironitta.com   Update CHANGELOG.md   Update pytorch_lightning/plugins/training_type/tpu_spawn.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
4745,Fix scripting causing false positive deprecation warnings (#10555),0.631215,warning_utils >> warnings,,0
4746,Mistake in parameters' grad norm tracking (#2012),0.7554338,Gradient norm tracking (#16745),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
4747,Fix import,0.76144165,import argparse,  Update tensorboard.py   Update logging.rst   pep8   Update logging.rst   Update logging.rst   Apply suggestions from code review   add code sample   Update logging.rst   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
4748,Direct support for compiled models (#15922),1.0000001,Direct support for compiled models (#15922),Co-authored-by: ananthsub ananth.subramaniam@gmail.com,1
4749,[App] Introduce Lightning Storage Commands (#16606),0.8189647,Resolved Lightning App with remote storage (#17426),  Fix manual optimization docs   Fix typo. Thanks @import-antigravity ,1
4750,Remove train_transforms in LightningDataModule (#12662),0.84893596,- Removed the deprecated `train_transforms` argument from the `LightningDataModule` constructor([#12662](https://github.com/Lightning-AI/lightning/pull/12662)),  improve doc to describe how to combine batches of multiple test and val dataloaders simultaneously   fix typo   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  use paramref  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
4751,Fix SWA LR scheduler not being stepped (#12446),0.66648406,lr_scheduler now activated after epoch    ,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: chaton thomas@grid.ai,0
4752,Minor grammatical updates to the Pull Request Template (#12719),0.5588307,- Added support for custom parameters in subclasses of `SaveConfigCallback` ([#14998](https://github.com/Lightning-AI/lightning/pull/14998)),,0
4753,Docs: fix failing make (#5988),0.58880144,Fixing critical bugs in newly added hooks and hparams assignment.,  Fix automatic_optimization   Fix automatic_optimization   Uncomment fairscale ,0
4754,track batch size (#2950),0.70940775,Batch Size Finder (#11089),  remove warning   auto_opt   chlog   auto_opt   no_warning_call   rm old code   add warning for predict   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
4755,remove InternalDebugger.track_load_dataloader_call (#9675),0.7310287,Removed deprecation warnings being called for on_{task}_dataloader (#9279),"  Use f-""""""-string   Add r   Use Trainer.   r -> noqa: W605 ",1
4756,[pre-commit.ci] pre-commit autoupdate (#8067),0.52486855,Deprecated mode='auto' from ModelCheckpoint and EarlyStopping (#4695),  adjust versions   release   manifest   pep8   CI   fix   build ,0
4757,Fix ResultCollection._get_cache with multielement tensors (#9582),0.6136298,Auto convert tensors to contiguous format when gather_all (#4907)," Update code  Co-authored-by: EliaCereda   More property updates   Move properties. Introduce trainer._fitting   Use trainer.fitting   Fix reset dataloaders   Unused code   RunningStage.SANITY_CHECKING   Use setters   Fix bugs   Fix bugs   TrainerState.{FITTING,VALIDATING,TESTING,PREDICTING,TUNING}   Fix bugs   Fix bugs   Fix tests   Update CHANGELOG. Add deprecation warning. Fix tests   Unused imports   Optional trainer   More deprecation. More refactoring   Correct version   Use properties   Address comments   flake8   Missed renamings   Typo   is -> ==   It is recommended to use  for Enums since they are singletons, however, since the LightningEnum subclasses str, it's not a good idea in case a user sets the state/stage with a str   Also for tests   Typo   Address @tchaton's comments   PEP8   Correct property   Update CHANGELOG   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update pytorch_lightning/trainer/trainer.py  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Remove called sanity check  Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com",0
4758,Fix import order,0.6361327,import argparse,,0
4759,fix tmpdir (#1012),0.64463043,Renamed TPUHalfPrecisionPlugin to TPUBf16PrecisionPlugin (#10026),  copy torchtext batch   update   rev   rev ,0
4760,Typo in tuner/lr_finder.py (#13453),0.75435805,tuner.lr_find(...),  resolve bug   update changelog   Update tests/trainer/test_trainer.py   Update pytorch_lightning/profiler/profilers.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   resolve comments   resolve flake8   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
4761,skip CircleCI config on master (#2732),0.5191878,Removed ProfilerConnector (#7654),  patch download   CI   isort   extra ,0
4762,"Error handling for accelerator=""mps"" and ddp strategy pairing (#16153)",0.6756112,Fix hanging in DDP HPC accelerators (#5157),  update changelog for v1.2.2   ckpr 1.2.2   Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz,0
4763,LightningCLI changes for jsonargparse>=4.0.0 (#10426),0.8168169,- `LightningCLI`'s shorthand notation changed to use jsonargparse native feature ([#12614](https://github.com/Lightning-AI/lightning/pull/12614)),  resolve bug   resolve flake8   revert name ,1
4764,fix overfit_batch sampler replacement logic (#10486),0.6933367,overfit_pct in favour of overfit_batches,"  handle distributed_sampler_kwargs   move emptying cache to accelertor   fix a few tests   restoring the result from subprocess   fix queue.get() order for results   add missing ""block_backward_sync"" context manager   add missing ""block_backward_sync"" context manager   fix sync_batchnorm   fix supported gpu-ids for tuple   fix clip gradients and inf recursion   accelerator selection: added cluster_environment plugin   fix torchelastic test   fix reduce early stopping decision for DDP   fix tests: callbacks, conversion to lightning optimizer   fix lightning optimizer does not pickle   fix setting benchmark and deterministic option   fix slurm amp test   fix prepare_data test and determine node_rank   fix retrieving last path when testing   remove obsolete plugin argument   fix test: test_trainer_config   fix torchscript tests   fix trainer.model access   move properties   fix test_transfer_batch_hook   fix auto_select_gpus   fix omegaconf test   fix test that needs to simulate slurm ddp   add horovod plugin   fix test with named arguments   clean up whitespace   fix datamodules test   remove old accelerators   fix naming   move old plugins   move to plugins   create precision subpackage   create training_type subpackage   fix all new import errors   fix wrong arguments order passed to test   fix LR finder   Added sharded training type and amp plugin   Move clip grad to precision plugin   Added sharded spawn, select accelerators based on distributed_backend + enable custom fp16 plugin automatically   Fix import issue, attempting to fix tests   Fix initial test   Reflect hook logic from master, should wrap model after move to device   Optional state consolidation, since master has optimizers not wrapped   change attribute for instance test   reset optimizers   optimizers are not used in main process, so state would be wrong.   legacy   imports in accel   legacy2   trainer imports   fix import errors after rebase   move hook to new setup location   provide unwrapping logic   fix trainer callback system   added ddp2 implementation   fix imports .legacy   move plugins   restore legacy   drop test.py from root   add tpu accelerator and plugins   fixes   fix lightning optimizer merge   reset bugreportmodel   unwrapping   step routing forward   model access   unwrap   opt   integrate distrib_type   sync changes   sync   fixes   add forgotten generators   add missing logic   update   import   missed imports   import fixes   isort   mv f   changelog   format   move helper to parallel plugin   d   add world size   clean up   duplicate   activate ddp_sharded and tpu   set nvidia flags   remove unused colab var   use_tpu <-> on_tpu attrs   make some ddp_cpu and clusterplugin tests pass   Ref/accelerator connector (#5742)   final cleanup   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  connector cleanup  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  trainer cleanup  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  accelerator cleanup + missing logic in accelerator connector  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  add missing changes to callbacks  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  reflect accelerator changes to lightning module  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  clean cluster envs  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  cleanup plugins  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  add broadcasting  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   yapf   remove plugin connector   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   plugins   manual optimization   update optimizer routing   add rank to torchelastic   fix memory mixed precision   setstate on trainer for pickling in ddp spawn   add predict method   add back commented accelerator code   adapt test for sync_batch_norm to new plugin   fix deprecated tests   fix ddp cpu choice when no num_processes are given   yapf format   skip a memory test that cannot pass anymore   fix pickle error in spawn plugin   x   avoid   x   fix cyclic import in docs build   add support for sharded   update typing   add sharded and sharded_spawn to distributed types   make unwrap model default   refactor LightningShardedDataParallel similar to LightningDistributedDataParallel   update sharded spawn to reflect changes   update sharded to reflect changes   Merge 1.1.5 changes   fix merge   fix merge   yapf isort   fix merge   yapf isort   fix indentation in test   copy over reinit scheduler implementation from dev1.2   fix apex tracking calls with dev_debugger   reduce diff to dev1.2, clean up   fix trainer config test  when gpus>0 and num_processes >0 and ddp_cpu   sort plugin tests legacy/new   fix error handling for amp on cpu   fix merge   fix merge fix merge   [Feat] Resolve manual_backward (#5837)   resolve manual_backward   resolve flake8   update   resolve for ddp_spawn   resolve flake8   resolve flake8   resolve flake8   Co-authored-by: Ubuntu ubuntu@ip-172-31-88-60.ec2.internal   fix tests/accelerator tests on cpu   [BugFix] Resolve manual optimization (#5852)   resolve manual_optimization   update   update   Co-authored-by: Ubuntu ubuntu@ip-172-31-88-60.ec2.internal   Remove copy trainer parameters to happen earlier within the loop and add safe guard to get ref model (#5856)   resovle a bug   Accelerator refactor sharded rpc (#5854)   rpc branch   merge   update handling of rpc   make devices etc. Optional in RPC   set devices etc. later if necessary   remove devices from sequential   make devices optional in rpc   fix import   uncomment everything   fix cluster selection   Co-authored-by: Ubuntu ubuntu@ip-172-31-88-60.ec2.internal   resolve bug   fix assert in rpc test   resolve a test   fix docs compilation   accelerator refactor - fix for sharded parity test (#5866)   fix memory issue with ddp_spawn   x   x x x x x x x x   x   Remove DDP2 as this does not apply   Add missing pre optimizer hook to ensure lambda closure is called   fix apex docstring   [accelerator][BugFix] Resolve some test for 1 gpu (#5863)   update   revert init   resolve a bug   update   resolve flake8   update   update   update   revert init   resolve a bug   update   resolve flake8   update   update   update   update   update   revert init   resolve a bug   update   resolve flake8   update   update   update   revert init   update   resolve flake8   update   update   update   update   update   all_gather   update   make plugins work, add misconfig for RPC   update   update   remove breaking test   resolve some tests   resolve flake8   revert to ddp_spawn   Co-authored-by: root root@ip-172-31-88-60.ec2.internal Co-authored-by: Ubuntu ubuntu@ip-172-31-88-60.ec2.internal Co-authored-by: Justus Schock justus.schock@rwth-aachen.de   yapf isort   resolve flake8   fix apex doctests   fix apex doctests 2   resolve docs   update drone   clean env   update   update   update   update   merge   Fix RPC related tests, clean out old API, update for new accelerator API [skip ci] (#5881)   Fix RPC related tests, clean out old API, update for new accelerator API   Move tests out of legacy folder, update paths and names   Update test_remove_1-4.py   Expose properties for tpu cores/gpus/num_gpus   Add root GPU property   Move properties to properties.py   move tests that were previously in drone   Fix root GPU property (#5908)   Move root GPU to property, remove horovod set as this is handled in horovod plugin, ensure we mock correctly to set GPU accelerator   Add missing tests back   fix best model path transfer when no checkpoint callback available   Fix setup hook order [wip] (#5858)   Call trainer setup hook before accelerator setup   Add test case   add new test   typo   fix callback order in test   Co-authored-by: tchaton thomas@grid.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   rename ddp sequential -> rpc sequential for special test   revert   fix stupid merge problem   abstract the cluster plugins   default plugin   integrate default environment   fix property   adapt tests   adjust test   fix world size access   base cluster env   revert rebase errors   revert rebase errors   missing import   revert unrelated change   remove unused cluster local rank   remove unrelated changes   fix unrelated changes   fix pep8   remove unused var   reset permissions   ypaf   test default environment   test torchelastic environment   world  size as int   tests for slurm environment   changelog   test comments   remove unintended change   keep master port fixed after it is generated   test random master port   yapf   add missing default environment   move helper function   rename default environment   rename   rename   yapf   Update pytorch_lightning/plugins/environments/lightning_environment.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update CHANGELOG.md  Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com  spawn -> create  Co-authored-by: justusschock justus.schock@posteo.de Co-authored-by: SeanNaren sean@grid.ai Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz Co-authored-by: Justus Schock justus.schock@rwth-aachen.de Co-authored-by: chaton thomas@grid.ai Co-authored-by: Ubuntu ubuntu@ip-172-31-88-60.ec2.internal Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: root root@ip-172-31-88-60.ec2.internal Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com",0
4765,docs: update broken links & latest/stable (#16994),0.6356919,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",  fix   update   update   add changelog   Update CHANGELOG.md   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update tests/accelerators/test_dp.py  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  update changelog  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
4766,"Remove convert_to_half, suggest using model.half (#7974)",0.6093204,Refactor Model backward (#2276),  docstring changes in accelerators   docstrings moved   whitespaces removed   PEP8 correction[1] ,0
4767,"Update tensorboard requirement from <2.10.0,>=2.9.1 to >=2.9.1,<2.11.0 in /requirements (#14200)",0.705722,Improved the error message for installing tensorboard or tensorboardx (#17053),  update   resolve bug ,1
4768,Fix typo in Loop.replace docstring (#13452),0.50887537,Replaced _DataModuleWrapper with __new__ (#7289),  Call clip gradients if clip val greater than 0   format   Format   Move to top of file ,0
4769,Add code_dir argument to tracer run (#15771),1.0,Add code_dir argument to tracer run (#15771),  document exception for metrics/classification   minor formatting fixes   fix trailing whitespaces   document exception for metrics   Apply suggestions from code review   Co-authored-by: Nicki Skafte skaftenicki@gmail.com  Apply suggestions from code review  Co-authored-by: Nicki Skafte skaftenicki@gmail.com  Apply suggestions from code review  Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com,1
4770,drop sklearn dependency (#4912),0.6142313,Drop PyTorch 1.9 support (#15347),  default_root_dir=tmpdir   miss ,0
4771,Avoid wrapping prediction dataloader twice on TPU (#16571),0.7010475,Ensured process_dataloader is called when tpu_cores > 1 to use Parallel DataLoader (#6015),  add to docs   update docs   Apply suggestions from code review   Update pytorch_lightning/core/hooks.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   nested loaders   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   shorten text length   Update pytorch_lightning/core/hooks.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
4772,Update strategy import statements (#11231),0.647187,import argparse,  Fix when _stable_1d_sort to work when n >= N   Apply suggestions   Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
4773,try remove pr (#2543),0.6033472,Removed deprecated: (#2760),  add ignore param to save_hyperparameters   add docstring for ignore   add type for frame object   Update pytorch_lightning/core/lightning.py   Co-authored-by: Nicki Skafte skaftenicki@gmail.com  Update pytorch_lightning/core/lightning.py  Co-authored-by: Nicki Skafte skaftenicki@gmail.com   fix whitespace   Update pytorch_lightning/core/lightning.py   Co-authored-by: Nicki Skafte skaftenicki@gmail.com   Parametrize tests   Update pytorch_lightning/core/lightning.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update pytorch_lightning/core/lightning.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   seq   fix docs   Update lightning.py   Update lightning.py   fix docs errors   add example keyword   update docstring   Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
4774,Generalize Optimizer validation to accommodate both FSDP 1.x and 2.x (#16733),0.9945682,Generalized Optimizer validation to accommodate both FSDP 1.x and 2.x (#16733),"  Create branch tests/4400_parsing   Rename test file for parsing.py   Fix lightning_hasattr   Fix lightning_hasattr   Fix lightning_setattr   Add empty lines and remove rubbish spaces   Raise AttributeError not ValueError   Use getattr in hasattr   Remove rubbish spaces   Fix getattr   Fix by flake8   Add tests for str_to_bool_or_str   Fix by flake8   Add tests for str_to_bool   Add tests for is_picklable   Add tests for clean_namespace   Fix typo   Fix lightning_getattr   Add tests for AttributeDict   Add tests for flatten_dict   Fix by flake8   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   Apply isort   Revert ""Apply suggestions from code review""   Define unpicklable_function outside   Add comment to test_clean_namespace   Add tests for parse_class_init_keys   Add tests for get_init_args and collect_init_args   Share objects across the tests   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk",1
4775,reduce parity test,0.6640575,test_percent_check in favour of limit_test_batches,,0
4776,fix boolean check on iterable dataset when len not defined (#6828),0.6127428,return iterabledataset,  Rely on training type plugin when saving   Add better typing to training type plugin ,0
4777,Rename GPUAccelerator to CUDAAccelerator,0.73430526,"- `accelerator=""gpu""` now automatically selects an available GPU backend (CUDA and MPS currently) ([#13642](https://github.com/Lightning-AI/lightning/pull/13642))",  ci: azure reinstall torchtext   move   todos   0.6.0   skip examples   formatter   skip   todo   Apply suggestions from code review ,1
4778,Remove use of jsonargparse internals (#12918),0.5663383,Removed deprecated API (#2073),  drop unused pl model in ckpt   irelevant   on_evaluation_batch_start   evaluation_epoch_end   attach_datamodule ,0
4779,Fix DataLoader re-instantiation when attribute is array (#15409),0.8008652,- Fixed an issue with DataLoader re-instantiation when the attribute is an array and the default value of the corresponding argument changed ([#15409](https://github.com/Lightning-AI/lightning/pull/15409)),  resolve an issue with TPU   update   add changelog ,1
4780,move pl/utilities/xla_device.py to lite/utilities/xla_device.py (#14514),0.62028515,xla_device_utils >> xla_device,Co-authored-by: chaton thomas@grid.ai,0
4781,remove early stopping tracking from internal debugger (#9327),0.6492338,Early stopping checks on_validation_end (#1458),  AMP   fuse   yapf ,0
4782,Fix loggers and update docs (#964),0.68399024,Removed LoggerStages (#5673),,0
4783,Skip replacing dataloader sampler if it's already a distributed sampler (#4273),0.7546662,Allow setting replace_sampler_ddp=True with a distributed sampler already added (#4273),  update   resolve flake8   update   update   update changelog   update   resolve flake8   Co-authored-by: Your Name you@example.com,1
4784,Update documentation for the basic skills tutorial level 2 on how to validate and test a model (#14874),0.72534776,"Deprecated model steps training_end, validation_end and test_end (#1051, #1056)",  add fairscale & windows to skipif   add deepspeed to runif   fairscale   deepspeed   flake8   Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz,1
4785,Remove hpc_save (#11101),0.58465385,Removed deprecated: (#2760),  special   rpc ,0
4786,remove extra whitespace (#1060),0.6265045,Remove unnecessary intermediate layers in Dockerfiles (#5697),  TPU   horovod   extra   fix   Apply suggestions from code review   Co-authored-by: Nicki Skafte skaftenicki@gmail.com  doc  Co-authored-by: Nicki Skafte skaftenicki@gmail.com,0
4787,Checkpoint migration for loop's internal state (#15500),0.74184203,Save the loops state with the checkpoint (opt-in) (#8362),  try to fix imports   legacy 1.2.1 ,1
4788,Freeze requirements for CI (#14007),0.73488855,Setup: added requirement freeze for the next major version (#14480),,1
4789,added test seeds (#306),0.596867,Check environ before selecting a seed to prevent warning message (#4743),,0
4790,Update tensorboard.py (#3920),0.7300241,Improved the error message for installing tensorboard or tensorboardx (#17053),"  Ensure we check deepspeed/sharded in multinode   Add CHANGELOG.md   Add CHANGELOG.md   Drop mock, use actual multi-gpu node ",1
4791,Update collect env details and issue template (#14017),0.56013966,Only check versions / env when not in the cloud (#15504),  args   native   apex   isort ,0
4792,Fix srun detection causing permission error on non-SLURM platforms (#15485),0.96189284,Fix an issue with the SLURM srun detection causing permission errors (#15485),Co-authored-by: chaton thomas@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
4793,Add DDP Spawn being default for Multi GPUs (#6292),0.87469363,Made DDP the default if no backend specified with multiple GPUs (#1789),  win   isort   flake8 ,1
4794,Fix issue with no-init dataclass fields in move_to_device (#9963),0.6185592,move prepare_data to data connector (#3307),  Improved early stopping documentation   Changed to 120 column format   doc   doc   doc   Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz,0
4795,suspend PR greeting,0.45019323,Resolve TPU miss rendezvous (#6781),  ngpus   gpu   isort   pt   flake8 ,0
4796,Update hpu mixed precision link (#14974),0.70603,Mixed precision overhaul (#16783),,1
4797,Improved EarlyStopping.patience documentation (#6278),0.66649085,    --trainer.callbacks.patience=5 \,  Change default for CPU offload to false for best throughput/memory efficiency   Add changelog   default   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
4798,change name (#1519),0.63257813,Rename failed -> error in tables (#15608),  docstring changes in tuner   added full stop ,0
4799,Update DeepSpeed version requirement in Dockerfile (#7326),0.728443,Updated precision attributes in DeepSpeedPlugin (#10164),,1
4800,formatting 4/n: Trainer (#5720),0.6862309,"Refactor RunningStage and TrainerState usage (#4945, #7173)",,0
4801,[bug] Fix Pytorch profiler with emit_nvtx (#6260),0.7155521,"Resolved schedule step bug for PyTorch Profiler (#6674, #6681)",  rename   if   test   chlog ,1
4802,Add progress tracking on Loops - 2/n (#8362),0.8673744,"Connect the progress tracking dataclasses to the loops (#8244, #8362)",,1
4803,"Find parameters which are specified in the LightningDataModule, only (#4347)",0.7172224,Updated metrics to use LightningEnum (#5689),"  Fix for incorrect detach/cpu calls (#6214)   Fix incorrect use of detach(), to(), and cpu(), #6214   Fix incorrect use of detach() and cpu(), #6214   update pr   add typing   chlog   more...   revert on module   update on comments   revert changes on model   Co-authored-by: tchaton thomas@grid.ai Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz",1
4804,Lite Docs and Example Improvements (#10303),0.65831697,There were two different ways of importing Lite in <= 1.9.0,Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
4805,Freeze Tensorboard to 2.2.0 (#3039),0.6644079,Switch from tensorboard to tensorboardx in TensorBoardLogger (#15728),  docstring changes in profilers   minor changes in profilers.py ,0
4806,Call on_before_zero_grad model hook (#1493),0.6256199,Deprecated the on_sanity_check_start hook in ModelHooks (#598),,0
4807,Add individuals to Metrics in CODEOWNERS (#4413),0.6023973,Allow metrics logged together with hparams (#1630),  skipif + yapf + isort   tests   docs   pp ,0
4808,cleaning SWA (#6259),0.8325604,"Cleaning (#5948, #5949, #5950)",Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Prajakta Phadke pphadke@iu.edu Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
4809,Move add_to_queue/get_from_queue to DDPSpawnPlugin (#9118),0.76920986,"Deprecated add_to_queue, get_from_queue from LightningModule in favor of corresponding methods in the DDPSpawnPlugin (#9118)",,1
4810,Fix issue_703: backward compatibility with python3.6 (#715),0.74418294,Compatibility for Python 3.10,Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: chaton thomas@grid.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
4811,[docs] Include all components in the API reference (#15805),0.56615305,Implemented ready for components (#16129)," Update apply_func.py  The name Batch is no longer located under torchtext.data --Error message-- File ""/home/daniel/py38/lib/python3.8/site-packages/pytorch_lightning/utilities/apply_func.py"", line 25, in       from torchtext.data import Batch                                                 ImportError: cannot import name 'Batch' from 'torchtext.data' (/home/daniel/py38/lib/p ython3.8/site-packages/torchtext/data/init.py) You can fix this by changing line line 28 to:     from torchtext.legacy.data import Batch   Update apply_func.py   Update apply_func.py   Update apply_func.py   Update apply_func.py   Update apply_func.py ",0
4812,[CLI] Add unit test with a model that has a parameter with lazy_instance default. (#11509),0.6116853,cli.trainer.fit(cli.model),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz Co-authored-by: Akihiro Nitta nitta@akihironitta.com,0
4813,[App] Rename failed -> error in tables (#15608),0.89541864,Rename failed -> error in tables (#15608),,1
4814,Trigger automatic rebase on issue comment (#1695),0.605187,Deprecated the on_sanity_check_start hook in ModelHooks (#598),,0
4815,ci: allow dispatch for flagship integrations (#16612),0.587896,Implemented ready for components (#16129),  Document exceptions in loggers   minor formatting   docstring changed in comet.py   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
4816,Fix rich progress bar metric render on epoch end (#11689),0.76081336,Changed progress bar epoch counting to start from 0 (#3061),Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,1
4817,Fix race condition in Fabric test (#17002),0.5480365,Fabric,"  Fix for multiple callbacks   Add CHANGELOG.md   Remove old params   Skip tests on windows using ddp   Change name of the variable to not clash with should stop, which is separate   Apply suggestions from code review   Fix params   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",0
4818,Remove deprecated BaseProfiler and AbstractProfiler (#14404),0.7526696,- Removed the deprecated `BaseProfiler` and `AbstractProfiler` classes ([#14404](https://github.com/Lightning-AI/lightning/pull/14404)),,1
4819,Create Dockerfile (#1569),0.59725386,Create single file in TensorBoardLogger (#777),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Kaushik Bokka kaushikbokka@gmail.com,0
4820,finished data parallel,0.77943426,Data Parallelism,,1
4821,Create LICENSE,0.4419956,Pruned requirements duplicity (#13739),  add issue config   remove question template   update URL   Update README.md   Update README.md   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update .github/ISSUE_TEMPLATE/config.yml  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
4822,Train End Error Handling Fix (#6864),0.6205157,Deprecated outputs in both LightningModule.on_train_epoch_end and Callback.on_train_epoch_end hooks (#7339),,0
4823,clean and organize fit (#3938),0.6403709,Trainer.fit hook clean up (#3198),Co-authored-by: Roger Shieh sh.rog@protonmail.ch Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
4824,[CLI] Adding opportunity to see basic cluster logs (#14334),0.7331662,Enabled custom clusters (#4048),  prune prefix   prune mode=auto   chlog ,1
4825,Fix on_train_batch_end signature and call in ProgressBarBase example (#8836),0.6570996,Pass batch outputs to on_train_batch_end instead of epoch_end outputs (#4369),  fix bug   fix tests   changelog   fix pep8   fix tests   fix and add some tests   add test for rlop   chlog   Update CHANGELOG.md   Co-authored-by: rohitgr7 rohitgr1998@gmail.com,0
4826,Tensorboard Docu about Hyperparams saving (#5158),0.6747663,Flattening Wandb Hyperparameters (#2459),  prune deprecated metrics for 1.3   isort / yapf ,0
4827,upgrade min deps (#4934),0.56700253,"refactor DDP backend (#3204, #3207, #3208, #3209, #3210)",  prune enable_pl_optimizer   prune automatic_optimization ,0
4828,added multi-node proc 0 ip reading,0.6005214,Changed default setting for communication of multi-node training using DDPShardedPlugin (#6937),  prune profiler   chlog ,0
4829,Run Command from App Comments (#15577),0.5730821,Introducing CLI commands for apps (#13602)!,,0
4830,Merge pull request #11 from cinjon/modulefix,0.51341474,  * Removed `opt_idx` argument from `BaseFinetuning.finetune_function` callback method,"  fixing tested values   .   tests   yapf   softmax   hvd   rename   lr   duplicate   drop   classif   rm EvalModel   Revert ""rm EvalModel""   This reverts commit 6c3fb39ebe0c4bfb52357bccfd050438f2c0f31c.   update tests   fix   azure   azure   self   cpu   Apply suggestions from code review   Co-authored-by: rohitgr7 rohitgr1998@gmail.com",0
4831,Pin test requirements to their current latest versions (#15157),0.5270835,Updated logic for checking TPUs availability (#6767),Co-authored-by: chaton thomas@grid.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
4832,Update multi_node_cluster_template.py,0.56788486,Slightly safer multi node (#15538),  Be more specific with DeepSpeed compatibility   Better wording ,0
4833,Prune metrics: others 11/DoNe (#6659),0.56302834,Allow logging of metrics together with hparams (#1630),"  running stage   circular import   running stage cleanup   fix unused import   fix running stage access   add return type   Revert ""add return type""   This reverts commit 65b0fe269c6547213e34b6a88b97bee31cdfe8c7.  try fix typing",0
4834,Remove deprecated TestTubeLogger (#12859),0.9031079,Deprecated the TestTubeLogger (#9065),  Trainer.test should return only test metrics (#5214)   resolve bug   merge tests   Fix metric state reset (#5273)   Fix metric state reset   Fix test   Improve formatting   Co-authored-by: Ananya Harsh Jha ananya@pytorchlightning.ai   print() method added to ProgressBar   printing alongside progress bar added to LightningModule.print()   LightningModule.print() method documentation updated   ProgressBarBase.print() stub added   stub   add progress bar tests   fix isort   Progress Callback fixes   test_metric.py duplicate DummyList removed   PEP and isort fixes   CHANGELOG updated   test_progress_bar_print win linesep fix   test_progress_bar.py remove whitespaces   Update CHANGELOG.md   Co-authored-by: chaton thomas@grid.ai Co-authored-by: Tadej Svetina tadej.svetina@gmail.com Co-authored-by: Ananya Harsh Jha ananya@pytorchlightning.ai Co-authored-by: Alexander Snorkin Alexander.Snorkin@acronis.com Co-authored-by: rohitgr7 rohitgr1998@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
4835,make loggers pickleable (#2518),0.6894138,- pickling errors with loggers (txs @awaelchli),,0
4836,[App] Enable help without running application (#15196),0.7177714,Improved support for running apps when dependencies aren't installed (#15711),  Fix wrong render   Improve classification metrics docs   Improve other domain metrics docs   Change the structure level in the docs ,1
4837,Remove the deprecated get_progress_bar_dict (#12839),0.75766706,Deprecated ProgressBar callback in favor of TQDMProgressBar (#10134),  Update Contributing Guide   update docs ,1
4838,Update tests/plugins/*.py to use devices instead of gpus or ipus (#11872),0.6641809,Dropped official support/testing for PyTorch <1.6 (#8288),  fix weird test   fix apex plugin test   fix raise   cpu test   fix type   add changelog ,0
4839,[BugOnFeat] Resolve bug with Finetuning (#5744),0.930485,Resolve bug with Finetuning (#5744),,1
4840,Change lightning module params to dict when loading (#1639),0.719,Move lightning module to correct device type when using LightningDistributedWrapper (#6070),"  Expose deepspeed config parameters to init function due to instability in parameters   See if tests can run on normal CI, without special tests   Add changelog   Update pytorch_lightning/plugins/training_type/deepspeed.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com",1
4841,continue 0.8.x (#2264),0.6146541,[0.6.0] - 2022-09-08,"  Enable ZeRO optimization, and make sure that the lightning module hook is called when we move to half precision   Added test, update to function ",0
4842,Fix mypy errors attributed to pytorch_lightning.loggers.comet (#13689),0.7193311,Deprecated pytorch_lightning.logging (#767),"  Give priority to plugins to set distributed mode, and then accelerator   Add CHANGELOG.md   Update CHANGELOG.md   Remove very scary line   Ensure we set cluster environment after slurm configured if necessary   Simplify the fix with a reset   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com",1
4843,rename about (#7002),0.62730956,Renames model steps (#1051),  reduction docs   docs for abstract base method   make mean the default   add preliminary chlog   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
4844,[tests/loggers] refactor with BoringModel (#5440),0.671082,Cleaning up stale logger tests (#3490),,0
4845,"Update pandoc requirement from <=2.2,>=1.0 to >=1.0,<=2.3 in /requirements (#16960)",0.5053698,"Set PyTorch 1.4 as min requirements, also for testing and examples torchvision>=0.5 and torchtext>=0.5 (#5418)",  precision fixes   add amp test model   fix test   revert   move assert to training step   fix test   fix test   remove unrelated changes   add changelog   remove unused import ,0
4846,Fix isort a few failures (#5504),0.6483518,Improved error messages for invalid configure_optimizers returns (#3587),,0
4847,ref: move train outside of setup training (#3297),0.9759717,move train outside of setup training (#3297),  pypi azure badges - tags   pep8   id ,1
4848,dataloaders with fast_dev_run (#1787),0.65131474,"Working with multiple dataloaders (#16800, #16753)",  add Azure tags trigger   fix   mnodes ,0
4849,Shared Fabric._wrap_and_launch code (#17473),0.7162773,fabric.launch(),  v1.2.0   docs ,1
4850,[App] Auto-upgrade / detect environment mis-match from the CLI (#15434),0.97226423,Auto-upgrade / detect environment mis-match from the CLI (#15434),,1
4851,Update version for rc1 release (#13910),0.6537712,Set version as today (#13906),  Empty commit   Raise AttributeError instead of ValueError   Make functions private   Update tests   Add match string   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  lightning to Lightning  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
4852,CI: Add remote fetch (#16001),0.57296526,"Accessing dataloaders (#16726, #16800)",  fix docs   update on comments   Apply suggestions from code review   Co-authored-by: Nicki Skafte skaftenicki@gmail.com  Apply suggestions from code review  Co-authored-by: Nicki Skafte skaftenicki@gmail.com  Apply suggestions from code review  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   rm comment   Update docs/source/common/lightning_module.rst   Co-authored-by: chaton thomas@grid.ai Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: chaton thomas@grid.ai,0
4853,[App] Raise error when launching app on multiple clusters (#15484),0.7801161,- Fixed a bug when launching an app on multiple clusters ([#15226](https://github.com/Lightning-AI/lightning/pull/15226)),  v1.2.0rc2   chlogs   chlogs   format   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
4854,Remove the unused utilities.finite_checks (#16682),0.6617181,- Removed the unused `lightning.pytorch.utilities.finite_checks.print_nan_gradients` function ([#16682](https://github.com/Lightning-AI/lightning/pull/16682)),"  Add warnings to hooks   Add default idx to prevent signature change in the future   Nothing to see here   Add default val to transfer_batch_to_device hook   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Revert ""Add default val to transfer_batch_to_device hook""  This reverts commit 5c6a68f2 Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",0
4855,Metrics docs  (#2184),0.92300415,"docs for all Metrics (#2184, #2209)",,1
4856,[tests/checkpointing] refactor with BoringModel (#4661),0.62689114,Removed ModelSummary validation from train loop on_trainer_init (#6610),  flake8   fix cyclic import   isort ,0
4857,Clean up Argparse interface with trainer (#1606),0.77127206,Improved Argparse usability with Trainer,  rename accelerator backend   rename new additions from master   add proper deprecation   pep8   warning match   add missing warning type ,1
4858,Refactor: Runif for TPU and Horovod 5/n (#6301),0.7182402,"refactored Horovod backend (#3121, #3122)",  rename get_model -> lightning_module   update references to get_model   pep8   add proper deprecation   remove outdated _get_reference_model   fix cyclic import ,1
4859,Remove support for LightningCLI(seed_everything_default=None) (#16137),0.98664707,Removed support for LightningCLI(seed_everything_default=None) (#16131),Co-authored-by: chaton thomas@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Sean Naren sean.narenthiran@gmail.com,1
4860,Support Slurm Autorequeue for Array Jobs (#15040),0.74250674,- Added support for requeueing slurm array jobs ([#15040](https://github.com/Lightning-AI/lightning/pull/15040)),Put .test() in  code blocks,1
4861,Bugfix/4449 dict attribute error (#4480),0.5744612,- Fixed an attribute error when running the tuner together with the `StochasticWeightAveraging` callback ([#14836](https://github.com/Lightning-AI/lightning/pull/14836)),,0
4862,Prune EvalModelTemplate (1/n) (#10969),0.5744342,Refactored EpochResultStore (#5522),  Make parallel devices optional across all plugins so that they can be instantiated   Add any to types to capture vars passed in ,0
4863,Resolve minor formatting issue (#14706),0.6097064,Resolve bug with Finetuning (#5744),  add hooks   comment   docs   add tests   make it private   fix tests   docs   chlog   testcode   codefactor   fix doctest   fix doctest   suggestions   is always overriden   pep and BoringModel   BoringModel   docs   docs   docs   fix   rebase   rebase   suggestions   docs   suggestions   try fix docs   docs   update name   yapf   docs   rebase   yapf ,0
4864,Remove duplicated test classes (#14122),0.63863605,Refactored setup_training and remove test_mode (#5388),  Add descriptions to accelerator broadcast function/clean up all_gather   Remove todo ,0
4865,Docs: Fix broken get-started link (#5960),0.6527343,- Fixed a bug where the upload files endpoint would raise an error when running locally ([#14924](https://github.com/Lightning-AI/lightning/pull/14924)),  fix/test quant   ...    ,0
4866,[TPU] Call auto_device_count for is_available (#17509),0.6552917,Updated logic for checking TPUs availability (#6767),,0
4867,clean avail. imports & enums (#5256),0.7549398,"Separated utils: imports & enums (#5256, #5874)",  et al.   Apply suggestions from code review   Apply suggestions from code review   Co-authored-by: chaton thomas@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: chaton thomas@grid.ai,1
4868,Parse all lines in app file looking for shebangs to run commands. (#15714),0.9963525,Parse all lines in app file looking for shebangs to run commands (#15714),  Fix: Allow hashing of metrics with lists in their state   Add test case and modify semantics of Metric hash in order to be compatible with structural equality checks   Fix pep8 style issue   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,1
4869,Fixes to the K-fold loop example (#15225),0.6531707,Refactored Loops,  Remove TrialMNISTDataModule   Allow using TrialMNIST in the MNISTDataModule   Update tests/helpers/datasets.py ,0
4870,Fix (weights only) checkpoints loading without pl (#3287),0.7119387,Refactor load in checkpoint connector (#4593),  added on_post_move_to_device   added tests   docs and refactors   Update tests/backends/test_tpu_backend.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update docs/source/tpu.rst  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update docs/source/tpu.rst  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/core/decorators.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/core/decorators.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update docs/source/tpu.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update pytorch_lightning/core/decorators.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update pytorch_lightning/core/decorators.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update pytorch_lightning/core/decorators.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update pytorch_lightning/core/decorators.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update pytorch_lightning/core/hooks.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  moved weight sharing module back to test  updated tpu available   add count to warning   fix doctest   import trainer in doctest   import trainer in doctest   do not test code as no TPU device   param count to layer count   formatting   update docs   update import   update   resolve tests   remove legacy accelerator   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: tchaton thomas@grid.ai Co-authored-by: Your Name you@example.com,1
4871,Typing fixes (#17000),0.62529075,Refactored setup for typing friendly (#6590),  ro1   ro2 ,0
4872,Remove Trainer._device_type (#11992),0.7897545,Removed the deprecated TrainerTrainingTricksMixin class (#8679),"  Move to CUDA image   Remove deepspeed install as deepspeed now in the cuda image   Remove path setting, as ninja should be in the container now ",1
4873,Refactor: clean trainer device & distrib setters (#5297),0.8991318,Refactor: clean trainer device & distributed getters (#5300),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
4874,ref: remove on_eval_start hook (#3176),0.69065225,The *_epoch_end hooks were removed (#16520),,0
4875,Serve component (#15609),0.7144747,Implemented ready for components (#16129),"  Trainer only references accelerator where it can   Move teardown to the trainer, as it is reponsible for the accelerator ",1
4876,Reroute LightningModule gradient clipping to Fabric (#16941),0.75323147,        # Lightning will handle the gradient clipping,"  Add initial deepspeed changes   Address code review   Move static method outside of function   Fixes   Add missing annotation   Remove seed setting   Doc changes   Doc changes, add address reviews   Fix docs   Try fixing issue by moving to torch adam   Clean up check   Changes, better APIs!   Add wrapper, swap to git install revision   Add special test   Add warning   Address review   Add better disclaimer   Turn off ZeRO for testing due to compilation   Add description on modifying parameters via the plugin   Doc strings clear   Small doc fixes   Fix hash, reduce test   Added CI change   Move to azure pipeline   Fix test name   Add missing flag   Remove sudo...   Try conda instead   Swap to conda base   Try suggested install   Apply suggestions from code review   Apply suggestions from code review   Revert ""Apply suggestions from code review""   This reverts commit 41cca05a  Revert ""Apply suggestions from code review""  This reverts commit e06ec29e   Remove setter   Address most review   Move out function, remove DeepSpeed from requirements   Install deepspeed/mpi4py within container   Use special tests, move to master commit for deepspeed   Export path   Force compile to happen first   Remove!   Debugging ninja   Fix error in optimizer step logic   Attempt to fix symbolic link   Reverse to aid debugging   Export path again   Clean up mess   var   Revert ""var""   This reverts commit 3450eaca   Address review, add todo   Add note about unsupported functionality   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: tchaton thomas@grid.ai Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz",1
4877,[App] Introduce Multi Node Component (#15524),0.714576,Slightly safer multi node (#15538),,1
4878,Fix LRScheduler import for PyTorch 2.0 (#15940),0.7399795,from pytorch_lightning.plugins.environments import SLURMEnvironment,  update changelog   apply untoggle_optimizer when result is None   update tests   still return loss sometimes   Update CHANGELOG.md   Co-authored-by: deng-cy dcy1996@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
4879,Fix callback instantiation with CLI subcommands (#9203),0.60334766,"Anywhere in your program, you can now call the CLI directly:",  Update properties.py   pep8 ,0
4880,release 1.1.0 (#5048),0.8063259,"    version=""0.0.1"",",  add padding   fix   fix   Update pytorch_lightning/callbacks/progress.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   updated based on suggestion   changelog   add test   fix pep8   resolve test   fix code format   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: tchaton thomas@grid.ai,1
4881,Prepare 1.1.3 release (#5365),0.6782035,"    version=""0.0.1"",",,0
4882,Fix pre-commit isort failure on tests/metrics/*.py (#5424),0.6641518,Dropped official support/testing for PyTorch <1.6 (#8288),  move accelerator connector   rename BackendConnector -> AcceleratorConnector ,0
4883,fix(docs/app): broken links in the intermediate/web-ui section (#15691),0.6457585,- Fixed a bug where the upload files endpoint would raise an error when running locally ([#14924](https://github.com/Lightning-AI/lightning/pull/14924)),,0
4884,move TPU cleaning to GH actions (#5991),0.66668427,"Cleaning (#5948, #5949, #5950)",  add hook   update changelog   resolve tests ,0
4885,Remove deprecated LoggerCollection (#14283),0.84081113,Removed LoggerStages (#5673),  fix tpus   update   add back reduction in val_loss   resolve some bugs with TPUs   update changelog   update on comments   forgot status   Fix train_bn arg   resolve comments   update on comments   Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,1
4886,Rename DDPFullyShardedPlugin to DDPFullyShardedStrategy (#11143),0.8207843,Merged the implementation of DDPSpawnStrategy into DDPStrategy and removed DDPSpawnStrategy (#14952),  add backend support   resolve flake8   update changelog   update   Apply suggestions from code review   Update docs/source/advanced/multi_gpu.rst   add patch as context manager   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
4887,continue towards 1.3 (#6069),0.67107916,[1.3.6] - 2021-06-15,  Add openmpi to our base container for DeepSpeed MPI support   conda   Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz,0
4888,Fabric docs typo correction (#16635),0.5483287,Changed overwrite to True (#16009),  ref lr_finder a bit   chlog   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,0
4889,Remove unused auto_collect_arguments class method (#14015),0.6405071,"  * Removed class methods from Trainer: `default_attributes()`, `from_argparse_args()`, `parse_argparser()`, `match_env_arguments()`, `add_argparse_args()`",,0
4890,add test for trainer.test() (#1858),0.848313,trainer.test(),  Add hydra fix that was missing from master   Remove error commas   Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,1
4891,Deprecate the HorovodStrategy (#16141),0.8551842,horovod deprecation (#16141),  Add dim to PSNR   Update CHANGELOG.md   Update CHANGELOG.md   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   Add reduction tests   Recover warnings on reduction and add tests   Add copyright texts   Refactor PSNR   Change warnings   Update pytorch_lightning/metrics/functional/psnr.py   Change functional.psnr dim doc Co-authored-by: Nicki Skafte skaftenicki@gmail.com   Change PSNR dim docs   Apply suggestions from code review   tests   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz,1
4892,update codeowners (#14718),0.58101916,Update the Lightning App docs (#13537),  Add deprecation warning when logging val_loss with no monitor   EOF   Update CHANGELOG   Clear warning cache before testing   pep8   Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz,0
4893,update contributors in README (#974),0.540385,Contributors,Signed-off-by: smajumdar titu1994@gmail.com,0
4894,Fix PyTorchProfiler prefix typo (#8308),0.6770991,Improved PyTorchProfiler chrome traces names (#8009),,0
4895,Deprecate amp_level from Trainer (#13898),0.7612515,Deprecated the Trainer(amp_level=...) argument,,1
4896,cutout examples,0.58836174,Gradient Clipping Customization,"Normalized spelling to ""license"" as in the URL ""https://github.com/PytorchLightning/pytorch-lightning/blob/master/LICENSE""",0
4897,Update old PL links (#13349),0.5847091,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",  Remove torch<=1.4.0 checks   Update pytorch_lightning/utilities/data.py   Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com,0
4898,Update path type annotation for load_from_checkpoint (#15540),0.6378503,Changed Checkpoint path parameter from filepath to dirpath (#1016),"  integrate distrib_type   sync changes   sync   fixes   add forgotten generators   add missing logic   update   import   missed imports   import fixes   isort   mv f   changelog   format   move helper to parallel plugin   d   add world size   clean up   duplicate   activate ddp_sharded and tpu   set nvidia flags   remove unused colab var   use_tpu <-> on_tpu attrs   make some ddp_cpu and clusterplugin tests pass   Ref/accelerator connector (#5742)   final cleanup   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  connector cleanup  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  trainer cleanup  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  accelerator cleanup + missing logic in accelerator connector  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  add missing changes to callbacks  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  reflect accelerator changes to lightning module  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  clean cluster envs  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  cleanup plugins  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  add broadcasting  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   yapf   remove plugin connector   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   plugins   add predict_loop   manual optimization   clean predictloop   update optimizer routing   add predict loop on new accelerator   resolve a bug   add rank to torchelastic   add predict_loop   add predict loop on new accelerator   resolve a bug   fix memory mixed precision   update   setstate on trainer for pickling in ddp spawn   add predict_loop   clean predictloop   add predict loop on new accelerator   resolve a bug   add predict_loop   add predict loop on new accelerator   resolve a bug   add predict_loop   add predict loop on new accelerator   resolve a bug   add predict_loop   add predict loop on new accelerator   resolve a bug   add predict_loop   clean predictloop   add predict loop on new accelerator   resolve a bug   add predict_loop   add predict loop on new accelerator   resolve a bug   resolve tests   add predict method   add back commented accelerator code   adapt test for sync_batch_norm to new plugin   fix deprecated tests   fix ddp cpu choice when no num_processes are given   yapf format   skip a memory test that cannot pass anymore   remove sanetize   rename train to run_train   remove useless hooks   add misconfigurationException   remove wrong naming   resolve some legacy   udpate docstring   fix pickle error in spawn plugin   x   avoid   x   fix cyclic import in docs build   add support for sharded   update typing   add sharded and sharded_spawn to distributed types   make unwrap model default   refactor LightningShardedDataParallel similar to LightningDistributedDataParallel   update sharded spawn to reflect changes   update sharded to reflect changes   Merge 1.1.5 changes   fix merge   fix merge   yapf isort   fix merge   yapf isort   fix indentation in test   copy over reinit scheduler implementation from dev1.2   fix apex tracking calls with dev_debugger   reduce diff to dev1.2, clean up   fix trainer config test  when gpus>0 and num_processes >0 and ddp_cpu   sort plugin tests legacy/new   fix error handling for amp on cpu   fix merge   fix merge fix merge   [Feat] Resolve manual_backward (#5837)   resolve manual_backward   resolve flake8   update   resolve for ddp_spawn   resolve flake8   resolve flake8   resolve flake8   Co-authored-by: Ubuntu ubuntu@ip-172-31-88-60.ec2.internal   fix tests/accelerator tests on cpu   [BugFix] Resolve manual optimization (#5852)   resolve manual_optimization   update   update   Co-authored-by: Ubuntu ubuntu@ip-172-31-88-60.ec2.internal   Remove copy trainer parameters to happen earlier within the loop and add safe guard to get ref model (#5856)   resovle a bug   Accelerator refactor sharded rpc (#5854)   rpc branch   merge   update handling of rpc   make devices etc. Optional in RPC   set devices etc. later if necessary   remove devices from sequential   make devices optional in rpc   fix import   uncomment everything   fix cluster selection   Co-authored-by: Ubuntu ubuntu@ip-172-31-88-60.ec2.internal   resolve bug   fix assert in rpc test   resolve a test   fix docs compilation   accelerator refactor - fix for sharded parity test (#5866)   fix memory issue with ddp_spawn   x   x x x x x x x x   x   Remove DDP2 as this does not apply   Add missing pre optimizer hook to ensure lambda closure is called   fix apex docstring   [accelerator][BugFix] Resolve some test for 1 gpu (#5863)   update   revert init   resolve a bug   update   resolve flake8   update   update   update   revert init   resolve a bug   update   resolve flake8   update   update   update   update   update   revert init   resolve a bug   update   resolve flake8   update   update   update   revert init   update   resolve flake8   update   update   update   update   update   all_gather   update   make plugins work, add misconfig for RPC   update   update   remove breaking test   resolve some tests   resolve flake8   revert to ddp_spawn   Co-authored-by: root root@ip-172-31-88-60.ec2.internal Co-authored-by: Ubuntu ubuntu@ip-172-31-88-60.ec2.internal Co-authored-by: Justus Schock justus.schock@rwth-aachen.de   yapf isort   resolve flake8   fix apex doctests   fix apex doctests 2   resolve docs   update drone   clean env   update   update   update   update   merge   Fix RPC related tests, clean out old API, update for new accelerator API [skip ci] (#5881)   Fix RPC related tests, clean out old API, update for new accelerator API   Move tests out of legacy folder, update paths and names   Update test_remove_1-4.py   Expose properties for tpu cores/gpus/num_gpus   Add root GPU property   Move properties to properties.py   move tests that were previously in drone   Fix root GPU property (#5908)   Move root GPU to property, remove horovod set as this is handled in horovod plugin, ensure we mock correctly to set GPU accelerator   Add missing tests back   fix best model path transfer when no checkpoint callback available   Fix setup hook order [wip] (#5858)   Call trainer setup hook before accelerator setup   Add test case   add new test   typo   fix callback order in test   Co-authored-by: tchaton thomas@grid.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   rename ddp sequential -> rpc sequential for special test   revert   fix stupid merge problem   Use property in connector for sampler (#5913)   merge the import conflicts   fix spawning of processes in slurm   [wip] Fix some bugs for TPU [skip ci] (#5878)   fixed for single tpu   fixed spawn   fixed spawn   update   update   wip   resolve bugs   resolve bug   update on comment   removed decorator   resolve comments   set to 4   update   update   need cleaning   update   update   update   resolve flake8   resolve bugs   exclude broadcast   resolve bugs   change test   update   update   skip if meet fails   properly raise trace   update   add catch   wrap test   resolve typo   update   typo   Co-authored-by: Lezwon Castelino lezwon@gmail.com Co-authored-by: Your Name you@example.com   resolve some tests   update   fix imports   update   resolve flake8   update azure pipeline   skip a sharded test on cpu that requires a gpu   resolve tpus   resolve bug   resolve flake8   update   updat utils   revert permission change on files   suggestions from carlos   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   remove unrelated formatting changes   remove incomplete comment   Update pytorch_lightning/accelerators/init.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   remove unrelated formatting change   add types   warn 1.7 ddp manual backward only if ddp kwarg unset   yapf + isort   pep8 unused imports   fix cyclic import in docs   Apply suggestions from code review   typer in accelerator.py   typo   resolve flake8   update code   update   Update pytorch_lightning/trainer/predict_loop.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update pytorch_lightning/trainer/predict_loop.py  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   fix merge   fix merge   reset legacy accelerator   add missing rename dispatch   rename post traning   update code   resolved comments   typo   typo   add flow description   resolve comments   update on comments   update flow   add backticks   resolve tpu   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: justusschock justus.schock@posteo.de Co-authored-by: Justus Schock justus.schock@rwth-aachen.de Co-authored-by: Ubuntu ubuntu@ip-172-31-88-60.ec2.internal Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: SeanNaren sean@grid.ai Co-authored-by: root root@ip-172-31-88-60.ec2.internal Co-authored-by: Lezwon Castelino lezwon@gmail.com Co-authored-by: Your Name you@example.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com",0
4899,Updated docs to fix typo and update grid status (#7270),0.5870577,Update the Lightning App docs (#13537),  hotfix for tpu   update changelog   Update CHANGELOG.md   Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com Co-authored-by: Sean Naren sean.narenthiran@gmail.com,0
4900,update ref to 1.5 as stable (#10311),0.62834835,Release LAI docs as stable (#14250),  Propagate to_cpu flag down the recursion chain   Refactor   Add test   Update CHANGELOG   Update tests/utilities/test_memory.py   Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
4901,Do not describe when there's no summary (#6681),0.5260605,Overview,,0
4902,Update building docker images (#12837),0.48998535,Update the Lightning App docs (#13537),  correct docs   fix levels ,0
4903,Add LightningCLI(save_config_overwrite=False|True) (#8059),0.8232868,"- `SaveConfigCallback` instances should only save the config once to allow having the `overwrite=False` safeguard when using `LightningCLI(..., run=False)` ([#14927](https://github.com/Lightning-AI/lightning/pull/14927))",  update the script to use DataModule   add message at for the frozen parameters   add message about trainable parameters   resolve flake8 ,1
4904,"for #330, use tqdm.auto in trainer (#752)",0.68701386,"Moved the default tqdm_dict definition from Trainer to LightningModule, so it can be overridden by the user (#749)","  fix trainer.model access   move properties   fix test_transfer_batch_hook   fix auto_select_gpus   fix omegaconf test   fix test that needs to simulate slurm ddp   add horovod plugin   fix test with named arguments   clean up whitespace   fix datamodules test   remove old accelerators   fix naming   move old plugins   move to plugins   create precision subpackage   create training_type subpackage   fix all new import errors   fix wrong arguments order passed to test   fix LR finder   Added sharded training type and amp plugin   Move clip grad to precision plugin   Added sharded spawn, select accelerators based on distributed_backend + enable custom fp16 plugin automatically   Fix import issue, attempting to fix tests   Fix initial test   Reflect hook logic from master, should wrap model after move to device   Optional state consolidation, since master has optimizers not wrapped   change attribute for instance test   reset optimizers   optimizers are not used in main process, so state would be wrong.   legacy   imports in accel   legacy2   trainer imports   fix import errors after rebase   move hook to new setup location   provide unwrapping logic   fix trainer callback system   added ddp2 implementation   fix imports .legacy   move plugins   restore legacy   drop test.py from root   add tpu accelerator and plugins   fixes   fix lightning optimizer merge   reset bugreportmodel   unwrapping   step routing forward   model access   unwrap   opt   integrate distrib_type   sync changes   sync   fixes   add forgotten generators   add missing logic   update   import   missed imports   import fixes   isort   mv f   changelog   format   move helper to parallel plugin   d   add world size   clean up   duplicate   activate ddp_sharded and tpu   set nvidia flags   remove unused colab var   use_tpu <-> on_tpu attrs   make some ddp_cpu and clusterplugin tests pass   Ref/accelerator connector (#5742)   final cleanup   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  connector cleanup  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  trainer cleanup  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  accelerator cleanup + missing logic in accelerator connector  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  add missing changes to callbacks  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  reflect accelerator changes to lightning module  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  clean cluster envs  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  cleanup plugins  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  add broadcasting  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   yapf   remove plugin connector   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   plugins   manual optimization   update optimizer routing   add rank to torchelastic   fix memory mixed precision   setstate on trainer for pickling in ddp spawn   add predict method   add back commented accelerator code   adapt test for sync_batch_norm to new plugin   fix deprecated tests   fix ddp cpu choice when no num_processes are given   yapf format   skip a memory test that cannot pass anymore   update on comments   fix pickle error in spawn plugin   x   avoid   x   fix cyclic import in docs build   add support for sharded   update typing   add sharded and sharded_spawn to distributed types   make unwrap model default   refactor LightningShardedDataParallel similar to LightningDistributedDataParallel   update sharded spawn to reflect changes   update sharded to reflect changes   Merge 1.1.5 changes   fix merge   fix merge   yapf isort   fix merge   yapf isort   fix indentation in test   copy over reinit scheduler implementation from dev1.2   fix apex tracking calls with dev_debugger   reduce diff to dev1.2, clean up   fix trainer config test  when gpus>0 and num_processes >0 and ddp_cpu   sort plugin tests legacy/new   fix error handling for amp on cpu   fix merge   fix merge fix merge   [Feat] Resolve manual_backward (#5837)   resolve manual_backward   resolve flake8   update   resolve for ddp_spawn   resolve flake8   resolve flake8   resolve flake8   Co-authored-by: Ubuntu ubuntu@ip-172-31-88-60.ec2.internal   fix tests/accelerator tests on cpu   [BugFix] Resolve manual optimization (#5852)   resolve manual_optimization   update   update   Co-authored-by: Ubuntu ubuntu@ip-172-31-88-60.ec2.internal   Remove copy trainer parameters to happen earlier within the loop and add safe guard to get ref model (#5856)   resovle a bug   Accelerator refactor sharded rpc (#5854)   rpc branch   merge   update handling of rpc   make devices etc. Optional in RPC   set devices etc. later if necessary   remove devices from sequential   make devices optional in rpc   fix import   uncomment everything   fix cluster selection   Co-authored-by: Ubuntu ubuntu@ip-172-31-88-60.ec2.internal   resolve bug   fix assert in rpc test   resolve a test   fix docs compilation   accelerator refactor - fix for sharded parity test (#5866)   fix memory issue with ddp_spawn   x   x x x x x x x x   x   Remove DDP2 as this does not apply   Add missing pre optimizer hook to ensure lambda closure is called   fix apex docstring   [accelerator][BugFix] Resolve some test for 1 gpu (#5863)   update   revert init   resolve a bug   update   resolve flake8   update   update   update   revert init   resolve a bug   update   resolve flake8   update   update   update   update   update   revert init   resolve a bug   update   resolve flake8   update   update   update   revert init   update   resolve flake8   update   update   update   update   update   all_gather   update   make plugins work, add misconfig for RPC   update   update   remove breaking test   resolve some tests   resolve flake8   revert to ddp_spawn   Co-authored-by: root root@ip-172-31-88-60.ec2.internal Co-authored-by: Ubuntu ubuntu@ip-172-31-88-60.ec2.internal Co-authored-by: Justus Schock justus.schock@rwth-aachen.de   yapf isort   resolve flake8   fix apex doctests   fix apex doctests 2   resolve docs   update drone   clean env   update   update   update   update   merge   Fix RPC related tests, clean out old API, update for new accelerator API [skip ci] (#5881)   Fix RPC related tests, clean out old API, update for new accelerator API   Move tests out of legacy folder, update paths and names   Update test_remove_1-4.py   Expose properties for tpu cores/gpus/num_gpus   Add root GPU property   Move properties to properties.py   move tests that were previously in drone   Fix root GPU property (#5908)   Move root GPU to property, remove horovod set as this is handled in horovod plugin, ensure we mock correctly to set GPU accelerator   Add missing tests back   fix best model path transfer when no checkpoint callback available   Fix setup hook order [wip] (#5858)   Call trainer setup hook before accelerator setup   Add test case   add new test   typo   fix callback order in test   Co-authored-by: tchaton thomas@grid.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   rename ddp sequential -> rpc sequential for special test   revert   fix stupid merge problem   Use property in connector for sampler (#5913)   merge the import conflicts   fix spawning of processes in slurm   [wip] Fix some bugs for TPU [skip ci] (#5878)   fixed for single tpu   fixed spawn   fixed spawn   update   update   wip   resolve bugs   resolve bug   update on comment   removed decorator   resolve comments   set to 4   update   update   need cleaning   update   update   update   resolve flake8   resolve bugs   exclude broadcast   resolve bugs   change test   update   update   skip if meet fails   properly raise trace   update   add catch   wrap test   resolve typo   update   typo   Co-authored-by: Lezwon Castelino lezwon@gmail.com Co-authored-by: Your Name you@example.com   resolve some tests   update   fix imports   update   resolve flake8   update azure pipeline   skip a sharded test on cpu that requires a gpu   resolve tpus   resolve bug   resolve flake8   update   updat utils   revert permission change on files   suggestions from carlos   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   remove unrelated formatting changes   remove incomplete comment   Update pytorch_lightning/accelerators/init.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   remove unrelated formatting change   add types   warn 1.7 ddp manual backward only if ddp kwarg unset   yapf + isort   pep8 unused imports   fix cyclic import in docs   Apply suggestions from code review   typer in accelerator.py   typo   Apply suggestions from code review   formatting   update on comments   update typo   Update pytorch_lightning/trainer/properties.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   update   update on comments   resolve some comments   update on comments   resolve test   add toggle_model   update   update on comments   update doc   typo   update   typo   remove space   update   update on comments   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: justusschock justus.schock@posteo.de Co-authored-by: SeanNaren sean@grid.ai Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz Co-authored-by: Justus Schock justus.schock@rwth-aachen.de Co-authored-by: Ubuntu ubuntu@ip-172-31-88-60.ec2.internal Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: root root@ip-172-31-88-60.ec2.internal Co-authored-by: Lezwon Castelino lezwon@gmail.com Co-authored-by: Your Name you@example.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com",0
4905,Added missing parameters (#237),0.62098396,Improved error messages in replace_sampler when the DataLoader attributes are not included in the signature or the signature is missing optional arguments (#8519),  boring   boring ,0
4906,Add should_rank_save_checkpoint property to Training Plugins (#7684),0.82416177,Removed should_rank_save_checkpoint property from Trainer (#9433),Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,1
4907,Initial commit,0.38379985,Contributors," doc: Add hint towards using ArgumentParser.add_argument_group  Since pl adds many arguments, it is nice to distinguish these arguments  fixup! address review  Co-authored-by: chaton thomas@grid.ai",0
4908,Remove Trainer's track_grad_norm argument (#16745),0.7758022,trainer = L.Trainer(track_grad_norm=2),  try to extend fairscale available   1.2 ,1
4909,Enable quick start e2e test again and run on cloud without installing dependencies 😎  (#15546),0.5781806,Only check versions / env when not in the cloud (#15504),"  Move pl_bolts assert to actually do something   Define val, test steps, use _DATASETS_PATH   Use DATASETS_PATH in DALI classifier   Fix incorrect paths and style in example READMEs   Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com",0
4910,Fix Deepspeed and lightning calling scheduler (#9788),0.7002984,Improved LightningTrainerScript start-up time (#15751),  remove legacy plugins   imports   formatting   fix docs references   fix cluster environment inheritance   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
4911,release 1.3.0 (#7404),0.7714212,0.4.0,Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
4912,Add refresh_rate to RichProgressBar (#10497),0.7486789,- Fixed `RichProgressBar` progress when refresh rate does not evenly divide the total counter ([#11668](https://github.com/PyTorchLightning/pytorch-lightning/pull/11668)),  clean AMP logic   cleaning   ...   ...   Even apex ,1
4913,Move to alumni (#15522),0.3866533,[1.2.1] - 2021-02-23,,0
4914,Deprecate Trainer.training_type_plugin in favor of trainer.strategy (#11141),0.79514176,trainer = Trainer(plugins=[SLURMEnvironment(requeue_signal=signal.SIGHUP)]),  fix nightly releases   readme   cuda   doxker   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  revert  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
4915,fixing tests (#372),0.6459576,"Refactored training_batch + tests to verify correctness (#2327, #2328)",,0
4916,Add guards to cluster deletion from cli (#16053),0.68552005,Cluster creation and deletion now waits by default [#15458,  move TPU cleaning to GH actions   test   . ,0
4917,Finish Allow on_save_checkpoint... (#3688),0.69568706,Save the loops state with the checkpoint (opt-in) (#8362),  try random TPU config   random   Apply suggestions from code review   Co-authored-by: chaton thomas@grid.ai Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,0
4918,fix batch typo,0.56655574,data_batch -> batch    ,Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,0
4919,Add enable_progress_bar to Trainer constructor (#9664),0.7301142, - Progress bar: `Trainer(enable_progress_bar=True)`,Co-authored-by: chaton thomas@grid.ai,1
4920,hotfix: skip unsupported metric compostions (#5664),0.5552287,Removed deprecated metrics (#8586),,0
4921,Fix Mixing hparams and arguments in LightningModule (#1505),0.6930469,Split LightningCLI.add_core_arguments_to_parser into LightningCLI.add_default_arguments_to_parser + LightningCLI.add_core_arguments_to_parser (#8721),  add typing   clean up   isort   fix typing in log_dir ,0
4922,Bump actions/checkout from 2 to 3 (#14540),0.5752344,- Fixed main progress bar counter when `val_check_interval=int` and `check_val_every_n_epoch=None` ([#12832](https://github.com/Lightning-AI/lightning/pull/12832),  on train end   switch order ,0
4923,Update changelog after 1.5.2 release (#10590),0.720508,Full Changelog,,1
4924,v1.3.0rc1 (#6925),0.638806,"    version=""0.0.1"",",  clean up sampler unused logic   undo cached   imports ,0
4925,[feat] Support iteration-based checkpointing in model checkpoint callback (#6146),0.782063,Deprecated passing ModelCheckpoint instance to checkpoint_callback Trainer argument (#4336),CONTRIBUTING: Add concrete example for running single test,1
4926,Rename test file from log_dir to test_log_dir (#9105),0.6202384,Deprecated the TestTubeLogger (#9065),,0
4927,Fix to_torchscript() causing false positive deprecation warnings (#10470),0.64929855,Avoided false positive warning about using sync_dist when using torchmetrics (#14143),  enable testing DDP examples   args   ddp_spawn   ddp as extra script   path   Conflicts: .drone.yml   install   -u   q ,0
4928,fix(docs/app): setup muse card (#15513),0.56186235,- Fixed and issue that prevented setting a custom `CheckpointIO` plugin with strategies ([#13785](https://github.com/Lightning-AI/lightning/pull/13785)),,0
4929,drop contribution badge (#17471),0.55073524,Deprecated flags: (#2213),  Add Raises: section to docstring   Add Raises section to the docs   Add raises section to the docs   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   fix   Remove unnecessary instance check   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
4930,release vusing pytorch summarywriter now,0.6600146,PyTorch 1.5  support,,0
4931,add pytorch profiler codeowner (#9479),0.724883,| Import pytorch_lightning.profiler                                                                          | 1.9             | pytorch_lightning.profilers                   |,  remove legacy accelerators   update imports   formatting   Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz,1
4932,"Update comet-ml requirement from <3.31.6,>=3.1.12 to >=3.1.12,<3.31.8 in /requirements (#13874)",0.6503606,Using .comet.config file for CometLogger (#1913),Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,0
4933,Introduce ServableModuleValidator Callback (#13614),0.8313719,ServableModule and its Servable Module Validator Callback,,1
4934,Support logging for MetricCollection with compute groups (#15580),0.7360497,Allow logging of metrics together with hparams (#1630),,1
4935,Move _POPTORCH_AVAILABLE and _IPU_AVAILABLE (#16509),0.65225863,- No longer set a `DistributedSampler` to the `poptorch.DataLoader` when IPUs are used ([#12114](https://github.com/PyTorchLightning/pytorch-lightning/pull/12114)),,0
4936,Merge branch 'extend-CI' of https://github.com/Borda/pytorch-lightning into extend-CI,0.60280025,- Added `LightningCLI.configure_optimizers` to override the `configure_optimizers` return value ([#10860](https://github.com/PyTorchLightning/pytorch-lightning/pull/10860)),,0
4937,Prepare 1.1.6 release (#5661),0.65516984,"    version=""0.0.1"",",,0
4938,update type (#10163),0.6515107,Set version as today (#13906),,0
4939,clean up tests/test_profiler.py (#867),0.6587212,Cleaning up stale logger tests (#3490),,0
4940,Remove unused gcsfs dependency (#14962),0.6166152,Used fsspec instead of gfile for all IO (#3320),,0
4941,Properly terminate MultiProcessing Runtime (#16623),0.78149354,Removed the SingleProcessRuntime (#15933),,1
4942,use docker for conda CI (#3841),0.5099398,    default_port = 12910,,0
4943,Disable the IPU check group (#16886),0.59733176,Stopped optimizer_zero_grad from being called after IPU execution (#12913),,0
4944,chlog after App 0.5.6 & 0.5.7 (#14352),0.60522544,0.4.0,  v1.2.0rc0   chlog   chlog   chlog   chlog   Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,0
4945,Update cloud docs (#8569),0.73685074,Refactor cloud dispatch and update to new API (#16456),  fix repeated fit calls ignoring max_steps   fix fast dev progress bar ,1
4946,refactor (#3851),0.740573,Refactoring,Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: chaton thomas@grid.ai Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,1
4947,Refactor fetching function (#11516),0.6698469,refactored dataloader process hook (#3139),"  restoring the result from subprocess   fix queue.get() order for results   add missing ""block_backward_sync"" context manager   add missing ""block_backward_sync"" context manager   fix sync_batchnorm   fix supported gpu-ids for tuple   fix clip gradients and inf recursion   accelerator selection: added cluster_environment plugin   fix torchelastic test   fix reduce early stopping decision for DDP   fix tests: callbacks, conversion to lightning optimizer   fix lightning optimizer does not pickle   fix setting benchmark and deterministic option   fix slurm amp test   fix prepare_data test and determine node_rank   fix retrieving last path when testing   remove obsolete plugin argument   fix test: test_trainer_config   fix torchscript tests   fix trainer.model access   move properties   fix test_transfer_batch_hook   fix auto_select_gpus   fix omegaconf test   fix test that needs to simulate slurm ddp   add horovod plugin   fix test with named arguments   clean up whitespace   fix datamodules test   remove old accelerators   fix naming   move old plugins   move to plugins   create precision subpackage   create training_type subpackage   fix all new import errors   fix wrong arguments order passed to test   fix LR finder   Added sharded training type and amp plugin   Move clip grad to precision plugin   Added sharded spawn, select accelerators based on distributed_backend + enable custom fp16 plugin automatically   Fix import issue, attempting to fix tests   Fix initial test   Reflect hook logic from master, should wrap model after move to device   Optional state consolidation, since master has optimizers not wrapped   change attribute for instance test   reset optimizers   optimizers are not used in main process, so state would be wrong.   legacy   imports in accel   legacy2   trainer imports   fix import errors after rebase   move hook to new setup location   provide unwrapping logic   fix trainer callback system   added ddp2 implementation   fix imports .legacy   move plugins   restore legacy   drop test.py from root   add tpu accelerator and plugins   fixes   fix lightning optimizer merge   reset bugreportmodel   unwrapping   step routing forward   model access   unwrap   opt   integrate distrib_type   sync changes   sync   fixes   add forgotten generators   add missing logic   update   import   missed imports   import fixes   isort   mv f   changelog   format   move helper to parallel plugin   d   add world size   clean up   duplicate   activate ddp_sharded and tpu   set nvidia flags   remove unused colab var   use_tpu <-> on_tpu attrs   make some ddp_cpu and clusterplugin tests pass   Ref/accelerator connector (#5742)   final cleanup   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  connector cleanup  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  trainer cleanup  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  accelerator cleanup + missing logic in accelerator connector  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  add missing changes to callbacks  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  reflect accelerator changes to lightning module  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  clean cluster envs  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  cleanup plugins  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  add broadcasting  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   yapf   remove plugin connector   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   plugins   manual optimization   update optimizer routing   add rank to torchelastic   fix memory mixed precision   setstate on trainer for pickling in ddp spawn   add predict method   add back commented accelerator code   adapt test for sync_batch_norm to new plugin   fix deprecated tests   fix ddp cpu choice when no num_processes are given   yapf format   skip a memory test that cannot pass anymore   fix pickle error in spawn plugin   x   avoid   x   fix cyclic import in docs build   add support for sharded   update typing   add sharded and sharded_spawn to distributed types   make unwrap model default   refactor LightningShardedDataParallel similar to LightningDistributedDataParallel   update sharded spawn to reflect changes   update sharded to reflect changes   Merge 1.1.5 changes   fix merge   fix merge   yapf isort   fix merge   yapf isort   fix indentation in test   copy over reinit scheduler implementation from dev1.2   fix apex tracking calls with dev_debugger   reduce diff to dev1.2, clean up   fix trainer config test  when gpus>0 and num_processes >0 and ddp_cpu   sort plugin tests legacy/new   fix error handling for amp on cpu   fix merge   fix merge fix merge   [Feat] Resolve manual_backward (#5837)   resolve manual_backward   resolve flake8   update   resolve for ddp_spawn   resolve flake8   resolve flake8   resolve flake8   Co-authored-by: Ubuntu ubuntu@ip-172-31-88-60.ec2.internal   fix tests/accelerator tests on cpu   [BugFix] Resolve manual optimization (#5852)   resolve manual_optimization   update   update   Co-authored-by: Ubuntu ubuntu@ip-172-31-88-60.ec2.internal   Remove copy trainer parameters to happen earlier within the loop and add safe guard to get ref model (#5856)   resovle a bug   Accelerator refactor sharded rpc (#5854)   rpc branch   merge   update handling of rpc   make devices etc. Optional in RPC   set devices etc. later if necessary   remove devices from sequential   make devices optional in rpc   fix import   uncomment everything   fix cluster selection   Co-authored-by: Ubuntu ubuntu@ip-172-31-88-60.ec2.internal   resolve bug   fix assert in rpc test   resolve a test   fix docs compilation   accelerator refactor - fix for sharded parity test (#5866)   fix memory issue with ddp_spawn   x   x x x x x x x x   x   Remove DDP2 as this does not apply   Add missing pre optimizer hook to ensure lambda closure is called   fix apex docstring   [accelerator][BugFix] Resolve some test for 1 gpu (#5863)   update   revert init   resolve a bug   update   resolve flake8   update   update   update   revert init   resolve a bug   update   resolve flake8   update   update   update   update   update   revert init   resolve a bug   update   resolve flake8   update   update   update   revert init   update   resolve flake8   update   update   update   update   update   all_gather   update   make plugins work, add misconfig for RPC   update   update   remove breaking test   resolve some tests   resolve flake8   revert to ddp_spawn   Co-authored-by: root root@ip-172-31-88-60.ec2.internal Co-authored-by: Ubuntu ubuntu@ip-172-31-88-60.ec2.internal Co-authored-by: Justus Schock justus.schock@rwth-aachen.de   yapf isort   resolve flake8   fix apex doctests   fix apex doctests 2   resolve docs   update drone   clean env   update   update   update   update   merge   Fix RPC related tests, clean out old API, update for new accelerator API [skip ci] (#5881)   Fix RPC related tests, clean out old API, update for new accelerator API   Move tests out of legacy folder, update paths and names   Update test_remove_1-4.py   Expose properties for tpu cores/gpus/num_gpus   Add root GPU property   Move properties to properties.py   move tests that were previously in drone   Fix root GPU property (#5908)   Move root GPU to property, remove horovod set as this is handled in horovod plugin, ensure we mock correctly to set GPU accelerator   Add missing tests back   fix best model path transfer when no checkpoint callback available   Fix setup hook order [wip] (#5858)   Call trainer setup hook before accelerator setup   Add test case   add new test   typo   fix callback order in test   Co-authored-by: tchaton thomas@grid.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   rename ddp sequential -> rpc sequential for special test   revert   fix stupid merge problem   Use property in connector for sampler (#5913)   merge the import conflicts   fix spawning of processes in slurm   [wip] Fix some bugs for TPU [skip ci] (#5878)   fixed for single tpu   fixed spawn   fixed spawn   update   update   wip   resolve bugs   resolve bug   update on comment   removed decorator   resolve comments   set to 4   update   update   need cleaning   update   update   update   resolve flake8   resolve bugs   exclude broadcast   resolve bugs   change test   update   update   skip if meet fails   properly raise trace   update   add catch   wrap test   resolve typo   update   typo   Co-authored-by: Lezwon Castelino lezwon@gmail.com Co-authored-by: Your Name you@example.com   resolve some tests   update   fix imports   update   resolve flake8   update azure pipeline   skip a sharded test on cpu that requires a gpu   resolve tpus   resolve bug   resolve flake8   update   updat utils   revert permission change on files   suggestions from carlos   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   remove unrelated formatting changes   remove incomplete comment   Update pytorch_lightning/accelerators/init.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   remove unrelated formatting change   add types   warn 1.7 ddp manual backward only if ddp kwarg unset   yapf + isort   pep8 unused imports   fix cyclic import in docs   Apply suggestions from code review   typer in accelerator.py   typo   Apply suggestions from code review   formatting   update on comments   update typo   Update pytorch_lightning/trainer/properties.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   update   suggestion from code review   suggestion from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: SeanNaren sean@grid.ai Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz Co-authored-by: chaton thomas@grid.ai Co-authored-by: Ubuntu ubuntu@ip-172-31-88-60.ec2.internal Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: root root@ip-172-31-88-60.ec2.internal Co-authored-by: Lezwon Castelino lezwon@gmail.com Co-authored-by: Your Name you@example.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com",0
4948,clean v2 docs (#691),0.5775435,Docs improvements,  Raise if scheduler interval not 'step' or 'epoch'   Add test for unknown 'interval' value in scheduler   Use BoringModel instead of EvalModelTemplate   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   Fix import order   Apply yapf in test_datamodules   Add missing imports to test_datamodules   Fix too long comment   Update pytorch_lightning/trainer/optimizers.py   Fix unused imports and exception message   Fix failing test   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
4949,CI: drop nightly (#15480),0.53692555,(#16002),  tests: Remove usage of --flake8 flag   Remove commented line   Co-authored-by: Carlos Mocholi carlossmocholi@gmail.com,0
4950,Improvements for rich progress (#9579),0.8116636,New Rich Progress Bar,  fix tests   =   Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz,1
4951,integrate with CircleCI (#2486),0.5750413,"adding compute environments (#3837, [#3842)", 644,0
4952,Remove the deprecated code in pl.utilities.seed (#16422),0.7285479,- Removed the deprecated code in:,  Update test_datamodules.py   fix code format issue   fix test restore   fix code format issue ,1
4953,Fix an issue to avoid the impact of sanity check on reload_dataloaders_every_n_epochs for validation (#13964),0.6811931,"Removed the on_epoch guard from the ""should stop"" validation check (#7701)", typing for callback base,0
4954,Disable train dataloader shuffle when overfit_batches is active. (#3501),0.60094285,def train_dataloader(...):,Co-authored-by: chaton thomas@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
4955,docs: temp drop S3 from index (#15099),0.54823864,Updated SSIM metric (#4566)(#4656),  add class method   add tests   docstring   pep   Add type annotations   Co-authored-by: Nicki Skafte skaftenicki@gmail.com   pep   fix import   remove num_workers inference   Update pytorch_lightning/core/datamodule.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update pytorch_lightning/core/datamodule.py  Co-authored-by: Nicki Skafte skaftenicki@gmail.com  Update pytorch_lightning/core/datamodule.py  Co-authored-by: Nicki Skafte skaftenicki@gmail.com   fix syntax   typing fix   list -> sequence   list -> sequence   missing import   fix test   Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
4956,updated dist sampler,0.640468,Automatic distributed samplers,,0
4957,Enable precision autocast for LightningModule step methods in Fabric (#17439),1.0000002,Enable precision autocast for LightningModule step methods in Fabric (#17439),,1
4958,CI: split tests-examples (#990),0.5463799,test_percent_check in favour of limit_test_batches,,0
4959,reduce test warnings (#2202),0.65427196,Do not override PYTHONWARNINGS (#4700),,0
4960,set min PT version for legacy (#7358),0.60004854,Set version as today (#13906),,0
4961,remove exception line,0.5711299,except Exception:,  update with BoringModel   update with BoringModel   step   try TPU   TPU   update tests   update tpu tests   self   fix   dp   update tests   ref   update tests   fix tpu tests   fix dp and run_prediction   dp   only dp   Apply suggestions from code review   Apply suggestions from code review   Apply suggestions from code review   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
4962,remove InternalDebugger (#9680),0.7203622,Removed pytorch_lightning.utilities.debugging.InternalDebugger (#9680),  empty   sq   obs   int   ts   helpers   chlog   yapf   avg   dupl   Apply suggestions from code review   Co-authored-by: Nicki Skafte skaftenicki@gmail.com  Apply suggestions from code review  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   fixes   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   fixes   note   warn   45   link   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Apply suggestions from code review  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   yapf   flake8   Apply suggestions from code review   Apply suggestions from code review   Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
4963,ref: inner train loop (intermediate step) 14/n (#3373),0.7821032,"inner train loop (#3359, #3361, #3362, #3363, #3365, #3366, #3367, #3368, #3369, #3370, #3371, #3372, #3373, #3374, #3375, #3376, #3385, #3388, #3397)",  wip   ..   ...   Apply suggestions from code review   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,1
4964,Use new GitHub labels (#10552),0.51556456,- Added support for adding descriptions to commands either through a docstring or the `DESCRIPTION` attribute ([#15193](https://github.com/Lightning-AI/lightning/pull/15193),  MetricsHolder(to_float=True)   Update CHANGELOG   Update tests/callbacks/test_progress_bar.py   flake8   Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz,0
4965,is_overridden improvements (#7918),0.6632503,Deprecated is_overridden(model=...) in favor of is_overridden(instance=...) (#7918),  add swa callback   switch back to 1.6.0   remove optimizer_step   move super   update   forgot update_parameters   update on comments   works for ddp   resolve flake8   remove set_model   resolve flake8   resolve cpu   resolve flake8   resolve flake8   update   update on comments ,0
4966,added test for model loading and predicting,0.7128748,- Full tests that run multiple models in different configs,Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz Co-authored-by: Nicki Skafte skaftenicki@gmail.com,1
4967,run TPU tests with multiple versions (#3024),0.7373698,Updated logic for checking TPUs availability (#6767),  add azure timeout   rework ,1
4968,removed dead code in grads,0.6690098,- Removed the deprecated code in:,"  Remove pruning check because it was added in 1.4.0 and that is our minimal torch version   Fixing many bugs   Fix misconfig test   Fix tests   Improve error message   Reduce whitespace   WIP   TODOs   _MODULE_CONTAINERS   Add LTH test   Allow resampling   Iterative pruning   Log pruning percentage   Properly make pruning permanent   Fix docstring   Minor changes   Test loading non-permanent model   corrent bugs   Revert ""corrent bugs""   This reverts commit ffb8d4754710ce7473f7e14542af8f1594962b5d.   Add beta warning   Fix docs   2 verbosity levels   OCD   Co-authored-by: Your Name you@example.com",0
4969,Save the ResultCollection in the loops state dict (#8641),0.68488526,Save the loops state with the checkpoint (opt-in) (#8362),,0
4970,doc reqs,0.75627315,Docs,  add base Azure pipeline   skip ,1
4971,Fixed missing arguments in lr_find call (#6784),0.67631197,Removed non-finite values from loss in LRFinder (#1862),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com Co-authored-by: Kaushik Bokka kaushikbokka@gmail.com,0
4972,Fix pickling error with CSVLogger (#10388),0.6505572,Dropped the LightningCLI ArgumentParser when pickling (#8017),  update test/boring model   reorder ,0
4973,Bump docker/build-push-action from 1.1.0 to 3 (#14651),0.5694625,Refactored Accelerators and Plugins (#5743),  change default   .   p   0.21.2   .   fix   . ,0
4974,add minimum codeowners for lite (#10298),0.56030613,        # Let Lite setup your dataloader(s),  create new Conda images   .   . ,0
4975,Improve DummyLogger (#6398),0.6441954,Deprecated the TestTubeLogger (#9065),  fix imorts   . ,0
4976,[2/2] Remove training loop force calling early stopping callback (#7069),0.8093996,Deprecate early_stop_callback Trainer argument (#3845),  ci   ver   list   pt   nk   ch   4.9 ,1
4977,CI: validate JSON & fix benchmark (#8567),0.5853919,"Validation DataLoader 0:  38%|███      | 12/32 [00:12<00:20,  1.01s/it]",,0
4978,Tests: refactor trainer dataloaders (#1690),0.78234327,"Standardized the dataloaders arguments of trainer.{fit,valdiate,test,tune} (#7431)",  yapf core   Update pytorch_lightning/core/lightning.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
4979,fix for pyTorch 1.1 (#552),0.818382,"Removed PyTorch 1.6 support (#10367, #10738)",  cb   log   prof   tune   flake8 ,1
4980,LiteDataLoader code improvements and docs (#10625),0.61679924,Make the _LiteDataLoader an iterator and add supports for custom dataloader (#10279),,0
4981,[App] Application logs in CLI (#13634),0.783331,Introducing CLI commands for apps (#13602)!,Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com (cherry picked from commit 0a50bb406fa41dfa6a0e2be52f531a9c81c87d00),1
4982,Improve typing for plugins (#10742),0.71408844,Enabled plugins (#4041),  Seperate epoch validaton from step validation   update system   test   baked logic in callbacks   unbake logic in callbacks   fix the call for scheduler   use property   pep   correct rebase   gitignore   ref   add tests   fix   add early stopping test   trigger   chlog   rev   1.3   log   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   Update pytorch_lightning/trainer/training_loop.py   Update CHANGELOG.md   Apply suggestions from code review   Co-authored-by: chaton thomas@grid.ai Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com (cherry picked from commit e429f97b670846bf0e0595074c4d3c4f152dcf27),1
4983,Add example of getting DataLoader from within LightningModule (#10903),0.7276825,LightningDataModule.load_from_checkpoint,(cherry picked from commit 3b7afb932b838e51b3bf99bc4213053c63c277cb),1
4984,"clean docs, enable grad clip in manual mode (#4078)",0.58161545,Changed clip_grad_norm to use torch.nn.utils.clip_grad_norm_ (#7025),  fix   batch size ,0
4985,fix setup call while testing (#2624),0.67523897,Refactored setup_training and remove test_mode (#5388),Co-authored-by: Roman Tezikov roman.tezikov@lamoda.ru,0
4986,Remove deprecated property is_slurm_managing_tasks from accelerator connector (#10353),0.8271004,- Removed deprecated property `is_slurm_managing_tasks` from AcceleratorConnector ([#10353](https://github.com/PyTorchLightning/pytorch-lightning/pull/10353)),  add new   restructure   yapf   move   fix ,1
4987,[CI SKIP] Sequential data & TPU support docs fix (#3956),0.6723352,Updated logic for checking TPUs availability (#6767),,0
4988,Handle edge case in Fabric.setup() when model has no parameters (#17441),0.5509604,The Fabric.run() method is no longer abstract (#14992),,0
4989,fix state extraction from batch when fault-tolerant training (#9281),0.7191852,Fault-tolerant Training,,1
4990,Clip before step (#10248),0.5480112,[1.1.8] - 2021-02-08,,0
4991,Early stopping when validation is disabled (#1235),0.8734988,Early stopping checks on_validation_end (#1458),,1
4992,load_from_checkpoint returns the expected type (#15496),0.6462432,Called on_load_checkpoint before loading state_dict (#4057),  utils   tuner   base ,0
4993,reset master,0.6185863,clean up data reset (#3161),  cb   acc   plug   . ,0
4994,[App] Show logs command to be standalone and re-usable (#15343),0.6796808,"Finally, loggers are also now configurable with shorthand:",  models   ckpt   core   log ,0
4995,Add Database Component (#14995),0.6918562,- Added an Database Component ([#14995](https://github.com/Lightning-AI/lightning/pull/14995), fix s3 download for PT 1.8,0
4996,Test fx (#390),0.63973635,test_end >> test_epoch_end,  formatting   isort   make   yapf   isort ,0
4997,Prefix seed_everything log messages with rank info (#14031),0.8305837,- Added prefix to log message in `seed_everything` with rank info ([#14031](https://github.com/Lightning-AI/lightning/pull/14031)),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com (cherry picked from commit e8c17551291e61972297c12f2491d7b4e87bf511),1
4998,Refactor tests for TPU Accelerator (#9718),0.7411165,Update Gradient Clipping for the TPU Accelerator (#6576),,1
4999,Do not warn when the name key is used in the lr_scheduler dict (#5057),1.0,Do not warn when the name key is used in the lr_scheduler dict (#5057),resolve doc boring commit docs torchvision tpu Update dockers/tpu-tests/tpu_test_cases.jsonnet Update dockers/tpu-tests/tpu_test_cases.jsonnet,1
5000,Add OutputResult [1/2] (#9437),0.7418254,    # 2. Add the outputs to the list,  Update lightning.py   update changelog   add a 3 optimizer test   resolve flake8   remove extra code   typo   resolve typo   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: tchaton thomas@grid.ai Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,1
5001,Simplify fetching's loader types (#13111),0.6967964,Simplified data loader.,  Add docs for non-slurm cluster setup   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update docs/source/cluster.rst  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update docs/source/cluster.rst  Co-authored-by: Alexander alexander@reshytko.com Co-authored-by: chaton thomas@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
5002,Add process launchers (#11643),0.58685476,Introducing CLI commands for apps (#13602)!,,0
5003,Accelerator API docs (#6936),0.72379863,Refactored Accelerators and Plugins (#5743),  -y   t   .   t ,1
5004,[docs] Added description of saving using ddp (#4660),0.57866883,  * `save_config_multifile`,Co-authored-by: chaton thomas@grid.ai,0
5005,ref: refactored horovod backend (#3121),0.9732298,"refactored Horovod backend (#3121, #3122)",  disable training when zero num_train_batches with limit_train_batches   refactor train skip condition   fix formatting issues   fix formatting issues   ref: test error msg   fix tests for data loader calls   fix train dataloader condition   update limit_train_batches upper range in test comment   remove model state check test   Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,1
5006,More tests for TPU accelerator in Lite (#14960),0.6707507,Update Gradient Clipping for the TPU Accelerator (#6576),  fix and update tests   update with ModelCheckpoint   chlog   wip wandb fix   all fixed   Co-authored-by: chaton thomas@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,0
5007,Fix batch size extraction when set by the user in LightningModule.log (#10408),0.7024635,Deprecated LightningModule.model_size (#8343),,1
5008,Do not modify MANIFEST.in on install (#15549),0.6252004,Avoid using the deprecated LooseVersion (#16162),,0
5009,Add opt_idx to scheduler config if not assigned by user (#11247),0.645072,- Added a `MisconfigurationException` if user provided `opt_idx` in scheduler config doesn't match with actual optimizer index of its respective optimizer ([#11247](https://github.com/PyTorchLightning/pytorch-lightning/pull/11247)),  add make docs   docs ,0
5010,fast_dev_run can be int (#4629),0.7530797,Updated fast_dev_run to accept integer representing num_batches (#4629),  started to write failing test. just getting into the framework...   started to write failing test. just getting into the framework...   added failing test for misconfiguration of lr finder   made test startup quickly. making sure without the fix it also fails slowly   improved test   fixed for linter   fixed for linter   yet another fix for the linter   yet another fix for the linter   fixed comment by @carmocca   fixed comment by @carmocca   Fix test   chlog   Apply suggestions from code review   Fix test   Update pytorch_lightning/tuner/lr_finder.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update pytorch_lightning/tuner/lr_finder.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   Update tests/trainer/test_lr_finder.py   Update pytorch_lightning/tuner/lr_finder.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update pytorch_lightning/tuner/lr_finder.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   Update pytorch_lightning/tuner/lr_finder.py   Update tests/trainer/test_lr_finder.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
5011,Fix: Revert  lightning_lite.utilities.rank_zero_only to preserve backward compatibility (#15536),0.7041091,- Deprecated `pytorch_lightning.utilities.distributed.rank_zero_only` in favor of `pytorch_lightning.utilities.rank_zero.rank_zero_only` ([#11747](https://github.com/PyTorchLightning/pytorch-lightning/pull/11747)),  Fix docs   typo   import   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: chaton thomas@grid.ai Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,1
5012,Deprecate terminate_on_nan Trainer argument in favor of detect_anomaly (#9175),0.9750296,Deprecated Trainer argument terminate_on_nan in favor of detect_anomaly(#9175),"  Fix mypy when prepending $PYTHONPATH to sys.path   attempt mypy fix   Revert ""attempt mypy fix""   This reverts commit fb7ed827d924452218ec23ddb2fd7484952a1294.  fix mypy  Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz",1
5013,added model save load test,0.6683667,- Added model configuration checking before it runs,,0
5014,release App 0.6.0 RC (#14370),0.56773186,"    version=""0.0.1"",",  add contrib questions   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Apply suggestions from code review  Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Roger Shieh sh.rog@protonmail.ch Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Roger Shieh sh.rog@protonmail.ch,0
5015,Cast hparams to dict when not using omegaconf (#4770),0.5565137,Support **DictConfig for hparam serialization (#2519),,0
5016,Remove devel.txt requirements file (#17466),0.5995815,Avoid using the deprecated LooseVersion (#16162),,0
5017,ci: hotfix name groupcheck (#17152),0.5249698,Dropped name column from cluster list (#15721),"Not entirely sure this is the ""right"" solution to this problem, but currently when model fitting is finished the TensorBoardLogger attribute _experiment (a SummaryWriter) is left with an open file handle. This causes issues in particular on Windows systems (and probably others), and also makes the files un-syncable on cloud-synced devices like OneDrive. This PR adds a close() to finalize to make sure this handle is closed upon fit completion. Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com",0
5018,[App] Change overwrite to True (#16009),0.8439481,Changed overwrite to True (#16009),  [docker][base-conda] Combine ENV+COPY instructions   [docker][base-cuda] Combine ENV+COPY instructions   [docker][base-xla] Combine ENV+COPY instructions   [docker][base-cuda] Fix COPY instruction   [docker][base-xla] Fix quote in ENV   [docker][base-xla] Fix $PATH in ENV   [docker][base-conda] Fix COPY instruction   chlog   Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz,1
5019,[App] Fix bug when using structures with works (#15911),0.59204733,Fixing critical bugs in newly added hooks and hparams assignment.,  Update CODEOWNERS   Update CODEOWNERS   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Apply suggestions from code review  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com,0
5020,Use checkpoint_connector.hpc_save in SLURM (#4217),0.98599243,Used checkpoint_connector.hpc_save in SLURM (#4217),,1
5021,Change the default prog_bar=False to True in LightningModule.log_grad_norm (#11472),0.8284313,- Set the `prog_bar` flag to False in `LightningModule.log_grad_norm` ([#11472](https://github.com/PyTorchLightning/pytorch-lightning/pull/11472)),,1
5022,Bump peter-evans/create-pull-request from 4 to 5 (#17313),0.56918895,- Added a try / catch mechanism around request processing to avoid killing the flow ([#15187](https://github.com/Lightning-AI/lightning/pull/15187),  filtering unsqueeze warning directly in DP   add changelog   constrain to module   Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
5023,Some test updates (#7761),0.66809464,"Refactored training_batch + tests to verify correctness (#2327, #2328)",resolve failing test,0
5024,Standardize Lite's filenames (#15058),0.6479565,Deprecated Profiler(output_filename) in favor of dirpath and filename (#6621),Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com,0
5025,Remove deprecated on_save_checkpoint argument (#8688),0.8016306,Removed the deprecated model argument from ModelCheckpoint.save_checkpoint (#8688),  fix f1 metric   Apply suggestions from code review   chlog   Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz,1
5026,update usage of deprecated profiler (#5010),0.66994715,Moved profilers to their own file (#7822),,0
5027,try fix: Docker with Conda & PT 1.8 (#5842),0.55549514,Fix hanging in DDP HPC accelerators (#5157),,0
5028,removing legacy profiler arg (#9178),0.70196027,- Removed legacy argparse utilities ([#16708](https://github.com/Lightning-AI/lightning/pull/16708)),  resolve custom dataloader   update changelog   fix tests   update on comments   resolve comments   add support for custom batch_sampler   Update tests/trainer/test_data_loading.py   resolve test   resolve flake8   resolve yapf   Co-authored-by: Ubuntu ubuntu@ip-172-31-88-60.ec2.internal Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
5029,Allow self-reviewing to rerun gatekeeper (#12672),0.5765663,"Removed the on_epoch guard from the ""should stop"" validation check (#7701)",,0
5030,Simplify codeowners (#15585),0.6978762,Simplify optimization Logic (#4984),,0
5031,make Trainer.resume_from_checkpoint a read-only property (#7857),0.7049549,Deprecated checkpoint_callback from the Trainer constructor in favor of enable_checkpointing (#9754),  Parepre 1.1.6 release   Remove headers   Apply suggestions from code review   docs: Apply docs suggestions for release   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Kaushik B 45285388+kaushikb11@users.noreply.github.com (cherry picked from commit c462b274ee193a40ab0172931e0c5102a7be51bc),1
5032,Update test_models.py,0.6531706,Skipped testing with PyTorch 1.7 and Python 3.9 on Ubuntu (#11217),  Append current work dir (hydra dir) to ensure all processes reference the same directory   Add changelog   Add different job name for DDP child processes to log to   Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com (cherry picked from commit ca68cac57ad8eefc9b477ee126eb42a483f27a39),0
5033,ref: part a of #3733 (#3766),0.66390854,[1.3.8] - 2021-07-01,  passing batch outputs to on_train_batch_end   styling   updating epoch end logic   also condition on on_train_epoch_end hooks   more readable   pep8   pep8   readability suggestion accepted   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   adding test_training_epoch_end_metrics_collection_on_override test   fix formatting   fix formatting   Co-authored-by: Swetha Mandava smandava@nvidia.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: Roger Shieh sh.rog@protonmail.ch (cherry picked from commit 5fcca4e43b243cd9fdb08050b285fb052856f13b),0
5034,docs: rename source-app (#16863),0.56194216,Update the Lightning App docs (#13537),  Rebase onto master   indent fix   Remove duplicated logic   Use single return   Remove extra else   add __contains__ to TestHparamsNamespace to fix tests   Fix lightning_setattr to set all valid attributes   update doc   better names   fix holder order preference   tests for new behavior   Comment about using the last holder   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Sean Naren sean.narenthiran@gmail.com (cherry picked from commit eee3b1a284470b3eca6ebd1815c0ddd7f550c667),0
5035,Pretty test results with pprint (#1176),0.53755635,- Implemented a new native and rich format in `_print_results` method of the `EvaluationLoop` ([#11332](https://github.com/PyTorchLightning/pytorch-lightning/pull/11332)),  fix toggle_optimizer   update doc   resolve bug   update   Update pytorch_lightning/core/lightning.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   update on comments   update on comments   update   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com (cherry picked from commit c76cc23b3dc44cadd718b8b85469ace4b8cfb445),0
5036,Fixing logic (#1734),0.6589321,"Simplified ""should run validation"" logic (#7682)",  Fix Metric.state_dict   Update CHANGELOG.md   Update CHANGELOG.md   Detach tensors in a list if needed   Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com (cherry picked from commit e87424adfb0a4d1ddd496d97b6974f65172c3191),0
5037,[Fix]:  Improve documentation (#4670),0.6479738,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. "," Ignore step param in Neptune logger's log_metric method  The step parameter is ignored because Neptune requires strictly increasing step values, a condition which is sometimes violated in Lighting e.g. when fit() and test() are called one after another on some models. step could be enabled again once Lightning guarantees that step values are always strictly increasing. Also a minor bugfix: the log_text() method should use Neptune's log_text() method.   Update neptune.py   Update test_neptune.py   Update test_all.py   fix neptune tests   add chlog   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com (cherry picked from commit 5d76b31881398188556c186e967166d21eea9746)",0
5038,Remove deprecated loaded_optimizer_states_dict property (#10346),0.70651424,Deprecated LightningModule.loaded_optimizer_states_dict (#8229),  warn about duplicate metrics   update changelog   suggestions from rohit   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   multiple values in message   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,1
5039,Remove deprecated test_tube dependency from environment.yml (#14617),0.70162374,Deprecated the TestTubeLogger (#9065),  Fix showing test results for tensors   Fix docs   Update pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py   Co-authored-by: Nicki Skafte skaftenicki@gmail.com  Fix lint issues  Co-authored-by: Nicki Skafte skaftenicki@gmail.com,1
5040,Sai prasanna master (#219),0.49040446,"@ananthsub, @rohitgr7",  mergify: less updates   Drop label ,0
5041,Fix the number of training batches used in the training loop (#653),0.7600781,Refactored training loop (#2336),Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,1
5042,fix typo in forward return (#2301),0.63295007,The PrecisionPlugin.backward hooks no longer returns a value (#8328),  change timeout to 100   add to CHANGELOG.md   update test   updates   reduce TPU_TIMEOUT_CONSTANT during test   Update tests/utilities/test_xla_device_utils.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   patch TPU_TIMEOUT_CONSTANT   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,0
5043,nan detection and intervention  (#1097),0.636149,Deprecated Trainer argument terminate_on_nan in favor of detect_anomaly(#9175),  DP device fix   potential fix   fix merge   update tests   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,0
5044,CI: Let dependabot check GHA updates weekly (#14274),0.5288886,CPU stats monitoring,,0
5045,Update learning rate on each backward pass instead of each forward pass. (#1477),0.7154274,Changed lr schedule step interval behavior to update every backwards pass instead of every forwards pass (#1477),  try fix update and review   less spam ,1
5046,Update docs with new Lightning Lite usage 1/n (#15600),0.80127925,Update the Lightning App docs (#13537),  Update help steps   Update README.md   Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com   Update README.md   Update README.md   Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com,1
5047,update coverage config (#6524),0.528565,This release fixes that core issue,  drop potetionaly bad version   * ,0
5048,Add check for callable with datamodule len (#10003),0.6183816,Support shorthand notation to instantiate datamodules (#10011),,0
5049,CI: black docs (#8566),0.573031,"docs for all Metrics (#2184, #2209)", add a multi-nodesworkflow  Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz,0
5050,drop deprecated reorder from AUC (#5004),0.7707949,Removed reorder parameter of the auc metric (#5004),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
5051,to_torchscript method for LightningModule (#3258),0.6722218,from pytorch_lightning import LightningModule,,0
5052,Readme changes (#3078),0.5051899,Replaced _DataModuleWrapper with __new__ (#7289),  resolve bug + add doc   Update pytorch_lightning/callbacks/finetuning.py   resolve bug   start adding more test   add more tests for finetuning callback functions   rename to flatten_modules   resolve doc   Update pytorch_lightning/callbacks/finetuning.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   resolve comments   remove update on BoringModel   update on comments   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
5053,copyright Lightning AI team (#16647),0.66706586,"  [#14537](https://github.com/Lightning-AI/lightning/pull/14537),",,0
5054,Quantisation (#5706),0.6098305,and nb_val_batches to num_val_batches (#567),,0
5055,Update changelog after 1.5.3 release (#10744),0.7270998,Full Changelog,,1
5056,clean up docs (#4095),0.66529226,"Cleaning (#5948, #5949, #5950)",  drop deprecated docs   notes   . ,0
5057,Rename optimization loops (#16598),0.75088525,        # 4. Perform the optimization in a loop,resolve wrong merge tpu yapf,1
5058,Fix ModelCheckpoint race condition in file existence check (#5155),0.6626927,Fix saved filename in ModelCheckpoint if it already exists (#4861),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
5059,bump version 1.9.0 (#16356),0.59971964,"    version=""0.0.1"",",Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
5060,Fix typo (#1027),0.606428,Changed overwrite to True (#16009),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: chaton thomas@grid.ai,0
5061,deprecated: epoch indexing from 1 (#2206),0.9124801,Changed epoch indexing from 1 instead of 0 (#2206),Co-authored-by: chaton thomas@grid.ai,1
5062,add docker badge (#2980),0.5485575,adding Trainer.tune() (#3293),  fix wrong argument in argparse   remove wrong default arg in argparser   disable add help argparse ,0
5063,Integrate progress tracking into the progress bar (#11213),0.8856124,progress bar,  Start with the failing test   Then fix the failing test   Update CHANGELOG ,1
5064,Improved docs for LightningModule (#1389),0.7853322,Update the Lightning App docs (#13537),  fix val_check_interval with fast_dev_run   chlog ,1
5065,CI: merge two install steps (#15776),0.46628666,Split profilers module (#6261),  Update ddp_cpu_hpc_accelerator.py   Update CHANGELOG.md ,0
5066,is-instance check to determine the type of a plugin for teardown decision (#8741),0.5899839,"Changed setup() and teardown() stage argument to take any of {fit,validate,test,predict} (#6386)",  update test   syntax   fix   update test   scheduler   only apex   fix   rev drone   chlog ,0
5067,[ModelPruning] Add missing attribute with use_global_unstructured=False and verbose (#6045),0.6429291,Deprecated prefix argument in ModelCheckpoint (#4765),  Add documentation to tensorboard   Remove unnecessary whitespaces   Update pytorch_lightning/loggers/tensorboard.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   Add metrics to tensorboard logger   Whitespace removed   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
5068,Add ddp_find_unused_parameters_false to Registry (#7224),0.85240614,Changed the default of find_unused_parameters to False in DDP (#5185),  Update accelerator_connector.py   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
5069,Added changeable extension variable for model checkpoints (#4977),0.66700196,A base Checkpoint class for extra customization,  configure mergify   drop gha   drop commented section   Co-authored-by: github-actions[bot] 41898282+github-actions[bot]@users.noreply.github.com Co-authored-by: Roger Shieh sh.rog@protonmail.ch,0
5070,Fixes #3993 (#3996),0.6012416,Resolve bug with Finetuning (#5744),  reset   fix reset   changelog   update chlog   typing   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: github-actions[bot] 41898282+github-actions[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
5071,[BUGFIX] AMP + Precision unscale grad (#4441),0.6957172,Apex mixed precision gets replaced with AMP (#16149),  add section   test legacy   Co-authored-by: github-actions[bot] 41898282+github-actions[bot]@users.noreply.github.com,0
5072,Remove deprecated support for passing the warning category positionally (#14470),0.7348685,- Removed deprecated support for passing the `rank_zero_warn` warning category positionally ([#14470](https://github.com/Lightning-AI/lightning/pull/14470)),"  ref and fix call for on_pretrained_routine   avoid failing tests   unnecessary_call   unnecessary call in accelerators   tmpdir   rm test_mode   pep   updates   more ref   Revert ""more ref""   This reverts commit 5d9e95f87343a4d9853eb30ca883d1dbfba369c6.  more refac  Co-authored-by: github-actions[bot] 41898282+github-actions[bot]@users.noreply.github.com",1
5073,Fix mypy errors attributed to pytorch_lightning.loggers.logger.py (#13541),0.7806253,Deprecated pytorch_lightning.logging (#767),  pipeline release CI   trigger   trigger   .   t1   t2   t1   t2 ,1
5074,Update Torch Elastic documentation (#8248),0.6736616,PyTorch 2.0 and torch.compile,Co-authored-by: github-actions[bot] 41898282+github-actions[bot]@users.noreply.github.com,0
5075,Mark stage argument in hooks as required (#14064),0.60798055,"Removed output argument from *_batch_end hooks (#3965, #3966)",  fix on_after_backward docs   doc fix ,0
5076,FIX-5311: Cast to string _flatten_dict (#5354),0.5432286,"    img_str = base64.b64encode(buffered.getvalue()).decode(""UTF-8"")",  update tests with new auto_opt api   Apply suggestions from code review   Co-authored-by: github-actions[bot] 41898282+github-actions[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
5077,Initialize trainer with None in DDPAccelerator (#4915),0.69607866,Trainer only references accelerator (#6039),,0
5078,Updates theme Sphinx configuration (#893),0.60911846,Pinned sphinx-autodoc-typehints with <v1.15 (#11400),  move images   logo ,0
5079,readme badges (#17114),0.47578925,[1.4.6] - 2021-09-10,  yapf trainer   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   .   fix   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
5080,"Add src, docs and other important folders",0.53261757,ls: List files from your Cloud Platform Filesystem,  Update pyproject.toml   Update setup.cfg   Update test.txt   Update CONTRIBUTING.md   Update requirements/test.txt ,0
5081,Add test_dataloaders to test method (#1434),0.68962675,"for data: val_dataloader, test_dataloader, train_dataloader",,0
5082,refactor - check E501 (#5200),0.6473266,Refactored EpochResultStore (#5522)," Add DataType, AverageMethod and MDMCAverageMethod",0
5083,Add support for returning callback from LightningModule.configure_callbacks (#11060),0.83202094,- Added support for returning a single Callback from `LightningModule.configure_callbacks` without wrapping it into a list ([#11060](https://github.com/PyTorchLightning/pytorch-lightning/pull/11060)),  add missing logic   missed imports   import fixes   isort   mv f   changelog   format   move helper to parallel plugin   d ,1
5084,Fix spawn plugins not deleting temp checkpoint (#10935),0.6326189,apex plugin (#3502),  rank access   tests for property   weekref   logger   changelog   torchscript   changelog   chlog   .   amp   yapf   flake8   Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz,0
5085,Raise exception for strategy=ddp_cpu|tpu_spawn (#10185),0.57768863,Updated logic for checking TPUs availability (#6767),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz,0
5086,pep8,0.47700763,"    devices=8, ",  add basic accelerator class. Co-Authored with @awaelchi   pep8   Co-authored-by: @awaelchi  add cpu accelerator  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  add gpu accelerator  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  add tpu accelerator  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  add accelerator connector  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  add single device training  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  add single tpu  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  add tpu spawn  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   make on_colab_kaggle utility func   add basic accelerator class. Co-Authored with @awaelchi   pep8   Co-authored-by: @awaelchi  add cpu accelerator  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  add gpu accelerator  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  add tpu accelerator  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  add accelerator connector  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  add single device training  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  add single tpu  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  add tpu spawn  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   make on_colab_kaggle utility func   fixes   move   yapf   .   .   .   flake8   sync accelerator connector changes from dev1.2   changelog   fix tpu handling   tpu   aval   yapf   Update pytorch_lightning/plugins/training_type/tpu_spawn.py   Co-authored-by: chaton thomas@grid.ai  Update pytorch_lightning/accelerators/accelerator_connector.py  Co-authored-by: chaton thomas@grid.ai  Update pytorch_lightning/plugins/training_type/tpu_spawn.py  Co-authored-by: chaton thomas@grid.ai   Update tpu_spawn.py   Update pytorch_lightning/accelerators/accelerator_connector.py   Co-authored-by: chaton thomas@grid.ai  indentation  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz Co-authored-by: chaton thomas@grid.ai,0
5087,fix: improve UserWarning message (#7685),0.67082644,warning_utils >> warnings,  yapf metrics   op ,0
5088,Add HPU Accelerator column to the precision doc (#12499),0.69432366,"The Accelerator and PrecisionPlugin have moved into Strategy. All strategies now take an optional parameter accelerator and precision_plugin (#11022, #10570).",  add basic accelerator class. Co-Authored with @awaelchi   add basic trainign type plugin. Co-Authored with @awaelchi   pep8   Co-authored-by: @awaelchi  update copyright  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  add apex_amp  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  add mixed base class  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  add native amp  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  add native amp sharded  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  add tpu bfloat  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  add inits  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update precision_plugin.py  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
5089,[pre-commit.ci] pre-commit suggestions (#16224),0.52347577,The preemption/termination signal is now configurable (#14626):,Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,0
5090,Moving lightning_api_access out of base requirements (#15844),0.7001862,"Add support for Lightning API through the configure_api hook on the LightningFlow and the Post, Get, Delete, Put with HttpMethods (#13945)",  module   fix model access   scalar conversion   refactor   kwargs   auto unsqueeze   refactor code duplication   clean up   docs   update dp docs   changelog   generalize test   test   rename   warning cache   isort   unsqueezing test   device   device   scalar test   device   device   include coverage of overrides   clear   add deprecation test   docs   improve coverage   increase coverage   fix merge   extend test   rename base class   mention the predict method in docs   combine iteration over collection   remove override   move   line   Apply suggestions from code review   fix running stage   f401   fix cyclic import   Co-authored-by: github-actions[bot] 41898282+github-actions[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
5091,Add KFold Loop example (#9965),0.6379459,Refactored Loops,  add basic accelerator class. Co-Authored with @awaelchi   Add base plugin class. Co-authored with @awaelchi   add basic trainign type plugin. Co-Authored with @awaelchi   add basic precision plugin. Co-Authored with @awaelchi   Add missing inits. Co-authored with @awaelchi   pep8   Co-authored-by: @awaelchi   ignore  flake8   coverage omit   imports in init   lost   imports   flake8   .   .   chlog   Update pytorch_lightning/plugins/training_type/training_type_plugin.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/plugins/training_type/training_type_plugin.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/plugins/training_type/training_type_plugin.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/plugins/training_type/training_type_plugin.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/plugins/training_type/training_type_plugin.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/plugins/training_type/training_type_plugin.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/plugins/training_type/training_type_plugin.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
5092,Integration tests for Precision in Lite (#14815),0.6543838,precision plugins (#3504),  drop LoggerStages   chlog ,0
5093,change Checkpoint callback's save_best_only to save_top_k (#128),0.78437304,"Removed the save_best_only argument from ModelCheckpoint, use save_top_k=1 instead (#128)",  yapf pl base   over   dist   utils   Apply suggestions from code review   flake8   neew way ,1
5094,move batch to device before sending it to hooks (#7378),0.66527706,"With DPStrategy, the batch is not explicitly moved to the device (#11780)",,0
5095,Move src/pytorch_lightning/lite to src/lightning_lite (#14735),0.7144046,from pytorch_lighting.lite import LightningLite,Co-authored-by: github-actions[bot] 41898282+github-actions[bot]@users.noreply.github.com,1
5096,"Drop ""full"" suffix in CI (#15320)",0.43890265,Increased DeepDiff's verbose level to properly handle dict changes (#13960),  doc updates   typo suggestions by rohit   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Apply suggestions from code review  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  update based on suggestions  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
5097,WIP: Moved grad_norm tracking code to __run_tng_batch  (#278),0.6381236,"- Gradient norm tracking with `track_grad_norm` no longer rounds the norms to 4 digits, but instead logs them at full resolution ([#16877](https://github.com/Lightning-AI/lightning/pull/16877))",,0
5098,"Improve LigtningEnum, etc. (#12750)",0.5772078,Refactored EpochResultStore (#5522),  define YAPF   add check   add check   add temp ignore   apply yapf   ex ,0
5099,"Revert ""ci(Mergify): configuration update (#12599)"" (#12642)",0.60180026,Refactored EpochResultStore (#5522),  start adding predict   add predict   resolve test   add predict   remove limit_predict   update   add test for predict   typo   update on comments   remove predict_step   update ddp_shareded   check ddp_sharded   resolve on comments   resolve isort   update dp   add test dp 1 gpu   made default forward   resolve path   resolve bug   update on comments   resolve doc   resolve bug   update   resolve bug   update on comments   resolve pep8   update test doc   update on comments   solve special tests   resolve bug   resolve flake8   Update pytorch_lightning/callbacks/progress.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update pytorch_lightning/trainer/trainer.py  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   add predict to LightningModule   missing predict   typo   rename is_prediction to _predicting   add   update   update   update doc   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,0
5100,Refactor: skipif for multi - gpus 1/n (#6266),0.6349399,Used fsspec instead of gfile for all IO (#3320),  base files   auc done   init files   auc class interface   fixing auc   more fixes   working auroc   update auc   add docs   remove leftovers from merge   suggestions   fix f-string   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   add deprecated tests   make logic clearer   Update pytorch_lightning/metrics/classification/auroc.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   fix   fix   fix docs   fix isort   fix deprecated test   fix tests   fix tests   fix isort   Apply suggestions from code review   add enum   deprecate old impl   update from suggestions   chlog   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz,0
5101,Update ci_dockers.yml (#3935),0.5444009,refactored dataloader process hook (#3139),Remove from skipped module in pyproject.toml and fix failures on: - pytorch_lightning/core/*.py,0
5102,update logo 48px (#7530),0.48294353,Renamed TPUHalfPrecisionPlugin to TPUBf16PrecisionPlugin (#10026),  wip   add pruning callback   add condition for duplicated weights   update on comments   update on comments   update on comments   add more tests   resolve flake8   resolve on comments   update changelog   update on comments   update on comments   change order   remove ddp_spawn skip   update   typo   Update pytorch_lightning/callbacks/pruning.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/callbacks/pruning.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   update on comments   forgot platform   update on comments   remove     @rank_zero_only   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,0
5103,fixing TPU tests (#2632),0.7661602,Updated logic for checking TPUs availability (#6767),  tests: legacy   legacy: accel   legacy: plug   fix imports   mypy   flake8 ,1
5104,Error messages for unsupported Trainer attributes (#15059),0.79068124,Trainer is now raising a MisconfigurationException instead of a warning if Trainer.{validate/test} is missing required methods (#10016),  Rename original filepath to v0   Clean-up   Suggestions from code review   Revert renaming. Start version number at 1   Add ModelCheckpoint.STARTING_VERSION   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   Add note about class attributes   Update CHANGELOG   Fix doc   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
5105,make device property always return a device with index (#4851),0.6631206,"            device_ids=device_ids,",  folders   common / advanced / extensions   paths   flake8   isort   Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,0
5106,Track CPU stats with DeviceStatsMonitor (#11795),0.8276596,- Added CPU metric tracking to `DeviceStatsMonitor` ([#11795](https://github.com/Lightning-AI/lightning/pull/11795)),  skip PT unsupported compositions   flake8 ,1
5107,CI: Replace _ of in GHA workflow filenames with - (#13917),0.5121507,Deprecated tags_csv in favor of hparams_file (#1271),  implement compositional metrics   implement composition functions for metrics   test compositions   docs   pytest   pep8   fix argument resolution   return all kwargs if filtering not possible   fix typo   implement hashing   Update pytorch_lightning/metrics/compositional.py   Co-authored-by: Nicki Skafte skaftenicki@gmail.com  Update docs/source/metrics.rst  Co-authored-by: Nicki Skafte skaftenicki@gmail.com   add representation   Apply suggestions from code review   Update docs/source/metrics.rst   Co-authored-by: Nicki Skafte skaftenicki@gmail.com   chlog   flake8   Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz,0
5108,Fix pre-commit isort failure on tests/backends/*.py (#5430),0.70656645,Dropped official support/testing for PyTorch <1.6 (#8288),,1
5109,Fix LightningCLI tests with invalid subclass arguments order. (#12570),0.70497406,- Fixed ``LightningCLI`` signature parameter resolving for some lightning classes ([#13283](https://github.com/Lightning-AI/lightning/pull/13283)),,1
5110,Fix visual progress bar bug / properly reset progress bar (#4579),0.77326584,Better progress bar (#16695),,1
5111,Check CI_PULL_REQUEST and set GITHUB_REF accordingly. (#2741),0.5999856,"GitHubComponent(api_token=os.environ[""API_TOKEN""])",(cherry picked from commit 652df1886abc63820d956aef7a468ff4cf1bea7f),0
5112,[Metrics] AUC/AUROC class interface (#5479),0.7008223,Deprecated reorder parameter of the auc metric (#4237),,1
5113,fix ci: release (#5037),0.6559913,This release fixes that core issue,,0
5114,Check for dataloader_idx presence in the hooks (#16837),0.7072691,"- The `dataloader_idx` argument is now optional for the `on_{validation,test,predict}_batch_{start,end}` hooks. Remove it or default it to 0 if you don't use multiple dataloaders ([#16753](https://github.com/Lightning-AI/lightning/pull/16753))",,1
5115,Remove nivida/apex (#16149),0.67537135,- `nvidia/apex` removal ([#16149](https://github.com/Lightning-AI/lightning/pull/16149)),  ci: update recurent events   split events   .   .   Co-authored-by: github-actions[bot] 41898282+github-actions[bot]@users.noreply.github.com (cherry picked from commit c00d5709c4fb40259ad05fe827e24ad421ce2da2),0
5116,update no_warning_call utility in tests (#11557),0.61905336,"Changed rank_zero_warn to NotImplementedError in the {train, val, test, predict}_dataloader hooks that Lightning(Data)Module uses (#9161)",  update used Twine   .   .   install   install   .   .   .   .   .   .   Co-authored-by: github-actions[bot] 41898282+github-actions[bot]@users.noreply.github.com (cherry picked from commit 9611a7f8976657ae36b7c2af61178f5b80f5ce81),0
5117,removed old files,0.6823469,rm: Delete files from your Cloud Platform Filesystem,,0
5118,Explain configure_sharded_model in ColossalAI docs (#16872),0.5786028,Direct support for compiled models (#15922),  resolve bug   Apply suggestions from code review   resolve package import   resolve import   update on comments   update on comments   hacky fix   update   exit   update   to_container   typo   resolve import   update   resolve pep8   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Sean Naren sean.narenthiran@gmail.com (cherry picked from commit bb5031b3bf7a9b5afac0d2918b7476f3f887ee35),0
5119,update tutorials (#11402),0.5771455,Integrated TrainingEpochLoop.total_batch_idx (#8598),,0
5120,disable step logging in epoch hooks (#10409),0.73829764,Removed LoggerStages (#5673),"  update   clean test   still in progress   udpdate test   update   update   resolve flake   add test for zero_grad   update   works without accumulated_grad   update   update   resolve amp   revert back to True   update   clean tests   cleaned out   typo   update test   git repare bug   remove print   udpate   Fix formatting/optimizer imports   Refactor the test for cleanliness   Add vanilla model to the test, better var names   Fixed var names, let's clean up these mock tests   repare test   update test   resolve flake8   add manual_optimization   update tests   resolve flake8   add random accumulate_grad_batches   improve test   Update tests/trainer/optimization/test_parity_automatic_optimization.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update tests/trainer/optimization/test_parity_automatic_optimization.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   update   clean tests   correct bug   Apply suggestions from code review   format   adress comments   update on comments   wip   typo   depreceate enable_pl_optimizer   resolve latest bugs   update   resolve merge   add comment   Update pytorch_lightning/core/lightning.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update tests/deprecated_api/test_remove_1-3.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/trainer/connectors/optimizer_connector.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/trainer/trainer.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/trainer/trainer.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update tests/trainer/optimization/test_parity_automatic_optimization.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   update on comments   update restore   add a property   remove setstate as not needed anymore   update test   provide optimizer to on_before_zero_grad   update on comments   update on comments   Update pytorch_lightning/trainer/trainer.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update tests/trainer/optimization/test_parity_automatic_optimization.py  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update tests/trainer/optimization/test_parity_automatic_optimization.py  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update tests/trainer/optimization/test_parity_automatic_optimization.py  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   mofidy import   update changelog   resolve flake8   update   update   clean doc   Co-authored-by: SeanNaren sean@grid.ai Co-authored-by: Ubuntu ubuntu@ip-172-31-62-109.ec2.internal Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Sean Naren sean.narenthiran@gmail.com (cherry picked from commit f2e99d617f05ec65fded81ccc6d0d59807c47573)",1
5121,[sharded plugin] Fix check for fp16 precision (#7825),0.75251406,precision plugins (#3504),  Update test_manual_optimization.py   Update governance.rst   Update test_manual_optimization.py   Update test_manual_optimization.py   (cherry picked from commit d30e316a35c0246296a4f62cf964f131759e851f),1
5122,Update the documentation of configure_optimizers() (#2071),0.74358666,Refactored optimizer (#4658),"  Check environment var independently to selecting a seed to prevent unnecessary warning message   Add if statement to check if PL_GLOBAL_SEED has been set   Added seed test to ensure that the seed stays the same, in case   if   Delete global seed after test has finished   Fix code, add tests   Ensure seed does not exist before tests start   Refactor test based on review, add log call   Ensure we clear the os environ in patched dict   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: chaton thomas@grid.ai (cherry picked from commit 635df27880521c29d2d8d094e5f790a6658ad5a3)",1
5123,Use standard Comet env variable names in docstring (#816),0.572799,Using .comet.config file for CometLogger (#1913),  populate some more legacy checkpoints   .   pt freeze   .   skip   Co-authored-by: chaton thomas@grid.ai (cherry picked from commit f065ea65bf4a577e2fe050049bbdbcb0cad5effc),0
5124,Update CI to pull torch 2.0 stable (#17107),0.6528879,Disable torch.inference_mode with torch.compile in PyTorch 2.0 (#17215),  add automatic optimization property setter to lightning module   Update test_manual_optimization.py   Co-authored-by: chaton thomas@grid.ai (cherry picked from commit 87482935a3d03cab3ffff336c88e0ae977a2beee),0
5125,Added Horovod distributed backend (#1529),0.83329225,"refactored Horovod backend (#3121, #3122)",  GH action - auto-update PRs   .   (cherry picked from commit 92bbf2fdd6509a273795a9bda08d95cd952c7791),1
5126,Add check to ensure 1.6,0.614442,Early stopping checks on_validation_end (#1458),  fixed docs in lightning.py   few more   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com (cherry picked from commit 499d5031e87e8a4ba54059ced69337f53b94e66f),0
5127,TrainerState refactor [5/5] (#7173),0.87714076,"Refactor RunningStage and TrainerState usage (#4945, #7173)",  GH action - label conflicts   .   trigger   trigger   .   (cherry picked from commit f1e28d1e436b852a92d6d2dc5ae7c080b006c748),1
5128,Introduce CheckpointIO Plugin (#8743),0.9140711,CheckpointIO Plugins,  resolve bug   resolve tests   update   Update tests/loggers/test_tensorboard.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com (cherry picked from commit a053d758d03558d2aa5a328b2f6befbc133a0ebc),1
5129,Support compiling a module after it was set up by Fabric (#17529),0.68450457,Implemented ready for components (#16129),  hacking out   update   remove useless on_before_forward   update   remove overriden   iremove os   use on_before_forward   resolve flake8   add test   update   add single_process_per_device   resolve flake8   update   resolve   update   update   update   add comment   resolve bug with sharded   update   remove property   update   resolve test   resolve bug   update on comments   update doc   Update pytorch_lightning/core/hooks.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   update on comments   Update pytorch_lightning/plugins/ddp_plugin.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update pytorch_lightning/plugins/ddp_plugin.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   resolve pep8   add device_ids to pipe   update on comments   update   resolve   update   update   update   Co-authored-by: Ubuntu ubuntu@ip-172-31-62-109.ec2.internal Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Sean Naren sean.narenthiran@gmail.com (cherry picked from commit d510707bc99d43dd2cfd877428d9cc16af8b4074),0
5130,Fix ModelCheckpoints name formatting (#3163),0.8101279,Changed ModelCheckpoint version suffixes to start at 1 (#5008),(cherry picked from commit 019e4ff8cddadfde4d3bd48b4e8b8d294950ca2a),1
5131,default to O1 (#334),0.7235589,Changed default apex level to 'O2' (#2362),  wip   generate   clean   tests   copy   download   download   download   download   download   download   download   download   download   download   download   flake8   extend   aws   extension   pull   pull   pull   pull   pull   pull   pull   try   try   try   got it   Apply suggestions from code review   (cherry picked from commit 72525f0a8396ae6dce5cf78ddf71e75fbba2dbfc),1
5132,update conda packages (#2593),0.50168544,Parsed local package versions (#13933),(cherry picked from commit 4c6f36e6e14a5e3bace1fe32505ae0fe6f8bc682),0
5133,fixed ddp flag crash (#3927),0.6967951,"refactor DDP backend (#3204, #3207, #3208, #3209, #3210)",  only run ci on docker related files   docker related files changed!   install pytorch along with cudatoolkit   build docker only on SUN   conda exit status has been fixed   reverts back to old conda version   add more docker related files   conda env update --name   create env and install pytorch again   create env and install pytorch again   ${PYTORCH_CHANNEL}   dont update pytorch with conda env update   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   Update dockers/base-conda/Dockerfile   Apply suggestions from code review   remove checks in cron job   Apply suggestions from code review   readd #   readd #   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Roger Shieh sh.rog@protonmail.ch (cherry picked from commit cc624358c8e396e966f9c51b3010f6a986047fc6),0
5134,hotfix import torch (#15849),0.80102265,import torch,(cherry picked from commit ee8373110aa89f1049d7ac53c5d491e7eba68cf1),1
5135,update core contributors (#3224),0.6382897,This release fixes that core issue,  add profiler   add profiler   update   resolve flake8   update doc   update changelog   clean doc   delete prof file   merge pr codebase   update   update doc   update doc   update doc   update on comments   update docstring   update docstring   try   update test   Update pytorch_lightning/profiler/init.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/profiler/init.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   update on comments   remove old code   add support for ddp   resolve flake8   Update pytorch_lightning/profiler/init.py   Co-authored-by: Sean Naren sean.narenthiran@gmail.com   resolve tests   resolve flake8   Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Sean Naren sean.narenthiran@gmail.com,0
5136,Remove deprecated rank zero utilities (#14471),0.750625,- Removed deprecated `utilities.distributed.rank_zero_{warn/deprecation}` ([#10451](https://github.com/PyTorchLightning/pytorch-lightning/pull/10451)),  testcode - python   revert   simple   testcode @rst   pl   fix   pip   update   conf   conf   nn.   typo ,1
5137,lower timeouts for inactive issues (#1250),0.62210965,Increased TPU check timeout from 20s to 100s (#5598),  prune EvalResult   drop tests   drop usage   drop class   prune ,0
5138,Fixed skipped horovod tests (#2514),0.66062975,"refactored Horovod backend (#3121, #3122)",,0
5139,Tut (#1000),0.5356295,(#16002),  generalize setup tools   drop unused   imports   ci ,0
5140,Missing import,0.7218622,import argparse,,1
5141,CI: use native oldest for package (#15094),0.5162674,Avoid using the deprecated LooseVersion (#16162),,0
5142,added small eps to dice and iou to avoid NaN (#2545),0.5592829,Remove nan loss in manual optimization (#5121),,0
5143,Bug fix hparam logging with metrics (#1647),0.7943027,Allow logging of metrics together with hparams (#1630),  fix yaml   chlog ,1
5144,add title and description to ServeGradio (#15639),0.5258969,- Added support for adding descriptions to commands either through a docstring or the `DESCRIPTION` attribute ([#15193](https://github.com/Lightning-AI/lightning/pull/15193),  Add and fix the docs of BackboneLambdaFinetuningCallback   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Roger Shieh sh.rog@protonmail.ch Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,0
5145,Revert part of #10279 (#10376),0.67123234,Refactor Model backward (#2276),  simplified model size calc   fix spaces   fix newlines   minor refactor   Update pytorch_lightning/core/memory.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   make model size property   fix doctest   Update pytorch_lightning/core/memory.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   remove explicit doctest from file   better docs   model precalculate size 1.0 mbs   better comment   Update tests/core/test_memory.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update tests/core/test_memory.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   merge _model_size into model_size property itself   minor comment fix   add feature to changelog   added precision test   isort   minor def name typo   remove monkeypath set env as boringmodel wont need any torch hub cache   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,0
5146,Set default strategy to ddp_fork in interactive environments (#13746),0.63418055,DDP custom implementation support (override these hooks):,  update mypy for tests   freeze ,0
5147,Update DeepSpeed docs (#6528),0.745093,Updated precision attributes in DeepSpeedPlugin (#10164),  docs(wandb): add details to args   feat(wandb): no sync between trainer and W&B steps   style: pep8   tests(wandb): test sync_step   docs(wandb): add references   docs(wandb): fix typo   feat(wandb): more explicit warning   feat(wandb): order of args   style: Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  style: long line  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Sean Naren sean.narenthiran@gmail.com,1
5148,[App] Fix VSCode IDE debugger (#15747),0.5558382,bug fix with logging val epoch end + monitor (#3812),  upgrade docs packages   cmd   -cmd ,0
5149,Disable package parametrizations until they are fixed (#15273),0.62540144,Avoid using the deprecated LooseVersion (#16162),  add possibility for nested loaders   pep8: newline ,0
5150,update tests for v2 (#11486),0.68126374,Updated app testing (#16000),  update Trainer is_ attributes   tests   more   isort   split   rename   check   fix ,0
5151,formatting 6/n: metrics (#5722),0.6380349,Classification metrics overhaul (#4837),,0
5152,Reduce title length (#8709),0.60174865,Truncated long version numbers in progress bar (#2594),  refresh   add tests   docs   chlog   chlog   Update pytorch_lightning/trainer/trainer.py   Co-authored-by: Nicki Skafte skaftenicki@gmail.com  update docstring  Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: chaton thomas@grid.ai,0
5153,Fix pkg version issue while compiling docs (#14914),0.64209175,Pkg: fix parsing versions by @Borda in https://github.com/Lightning-AI/lightning/pull/15401,Note that assert are being removed in optimized with compiling to optimised byte code (python -o producing *.pyo files).,0
5154,"Revert ""Use base version when comparing torch versions"" (#17019)",0.7681885,- Fixed an issue with comparing torch versions when using a version of torch built from source ([#17030](https://github.com/Lightning-AI/lightning/pull/17030)),Remove from skipped module in pyproject.toml and fix failures on: - pytorch_lightning/trainer/*.py,1
5155,Fix: Update sphinx-autodoc-typehints minimal version (#12468),0.81703717,Pinned sphinx-autodoc-typehints with <v1.15 (#11400),Remove from skipped module in pyproject.toml and fix failures on: - pytorch_lightning/metrics/*.py,1
5156,Change AWS credentials to Lightning ones (#13703),0.6610625,"No longer need to set master port, Lightning does it for you using the job id.",  Add stuff   Change metrics documentation layout   Add stuff   Add stat scores   Change testing utils   Replace len(.shape) with .ndim   More descriptive error message for input formatting   Replace movedim with permute   PEP 8 compliance   WIP   Add reduce_scores function   Temporarily add back legacy class_reduce   Division with float   PEP 8 compliance   Remove precision recall   Replace movedim with permute   Add back tests   Add empty newlines   Add precision recall back   Add empty line   Fix permute   Fix some issues with old versions of PyTorch   Style changes in error messages   More error message style improvements   Fix typo in docs   Add more descriptive variable names in utils   Change internal var names   Revert unwanted changes   Revert unwanted changes pt 2   Update metrics interface   Add top_k parameter   Add back reduce function   Add stuff   PEP3   Add depreciation   PEP8   Deprecate param   PEP8   Fix and simplify testing for older PT versions   Update Changelog   Remove redundant import   Add tests to increase coverage   Remove zero_division   fix zero_division   Add zero_div + edge case tests   Reorder cls metric args   Add back quotes for is_multiclass   Add precision_recall and tests   PEP8   Fix docs   Fix docs   Update   Change precision_recall output   PEP8/isort   Add method _get_final_stats   Fix depr test   Add comment to deprecation tests   isort   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   Add typing to test   Add matc str to pytest.raises   Co-authored-by: chaton thomas@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
5157,Fix an import deprecation warning (#3110),0.6721811,Re-Enable Logger's ImportErrors (#1938),Remove from skipped module in pyproject.toml and fix failures on: - pytorch_lightning/accelerators/*.py,0
5158,Bump codecov/codecov-action from 1 to 3 (#13620),0.55266273,Add code_dir argument to tracer run (#15771),  simple tests restructure   logging_process   typo ,0
5159,Deprecate on_colab_kaggle func (#14247),0.6748624,- Deprecated the `on_colab_kaggle` function ([#14247](https://github.com/Lightning-AI/lightning/pull/14247)),Remove from skipped module in pyproject.toml and fix failures on: - pytorch_lightning/callbacks/.py - pytorch_lightning/cluster_environments/.py - pytorch_lightning/profiler/.py - pytorch_lightning/tuner/.py Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz,0
5160,Add hint in docs for how to use shared memory (#6036),0.5913116,    # You should be aware of the implications on memory usage,Remove from skipped module in pyproject.toml and fix failures on: - pytorch_lightning/utilities/*.py,0
5161,prune SimpleModel (#5862),0.5524773,Sanitize None params during pruning (#6836),Remove from skipped module in pyproject.toml and fix failures on: - pytorch_lightning/loggers/*.py,0
5162,[CLI] drop name column from cluster list (#15721),0.912344,Dropped name column from cluster list (#15721),  Remove tests.base from skipped module in pyproject.toml   Fix pre-commit isort failure on tests/base/*.py ,1
5163,Add a method signature check for setup (#12960),0.6684337,Configuration Validator (#9779),  Fix pre-commit isort failure on tests/backends/*.py   Remove tests.backends from skipped module in pyproject.toml ,0
5164,[App] Add CloudMultiProcessBackend to run an children App within the Flow in the cloud (#15800),0.5998677,- Added support to start lightning app on cloud without needing to install dependencies locally ([#15019](https://github.com/Lightning-AI/lightning/pull/15019),Co-authored-by: chaton thomas@grid.ai,0
5165,log/save_interval based on global step (#3667),0.76324856,row_log_interval and log_save_interval are now based on training loop's global_step instead of epoch-internal batch index (#3667),  Remove tests.loggers from skipped module in pyproject.toml   Fix pre-commit isort failure on tests/loggers/*.py ,1
5166,release v0.3.4.1,0.8374983,0.4.0,  Remove tests.callbacks from skipped module in pyproject.toml   Fix pre-commit isort failure on tests/callbacks/*.py ,1
5167,Efficient gradient accumulation in LightningLite (#14966),0.731429,        # Lightning will handle the gradient clipping,,1
5168,Add torch.cuda rng state to seed save/load (#14384),0.6372123,- Added an argument `include_cuda` in `pytorch_lightning.utilities.seed.isolate_rng` to disable managing `torch.cuda`'s rng ([#16423](https://github.com/Lightning-AI/lightning/pull/16423)),  Remove tests.models from skipped module in pyproject.toml   Fix pre-commit isort failure on tests/models/*.py ,0
5169,update GH templates' labels (#9295),0.53346074,Changed clip_grad_norm to use torch.nn.utils.clip_grad_norm_ (#7025),  Remove tests.trainer from skipped module in pyproject.toml   Fix pre-commit isort failure on tests/trainer/*.py ,0
5170,Avoid optional Tracker attributes and enable mypy (#9320),0.82910925,Avoid optional Tracker attributes (#9320),  Remove tests.metrics from skipped module in pyproject.toml   Fix pre-commit isort failure on tests/metrics/*.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
5171,[Metrics] Disable default reset after compute (#5409),0.77266914,Metric compute() method will no longer automatically call reset() (#5409),  Fix pre-commit isort failure on tests/plugins/*.py   Remove tests.plugins from skipped module in pyproject.toml   Update pyproject.toml   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
5172,CI: Force docs warnings to be raised as errors (+ fix all) (#1191),0.60831547,warning_utils >> warnings,"  add wrapper   add squeeze   replace LightningDistributedDP   update import   module access   inputs   refactor warning   update   resolve flake8   remove old class   set find unused params to False   update docstrings   update docs   update docs   add changelog   deprecation   rename wrapper -> module   rename pl_module   add unit tests   Revert ""add changelog""   This reverts commit 02ec0a6864f4ba2ace3bb6fc6ebc364e1a80ffd7.  Revert ""set find unused params to False""  This reverts commit 8e451515e6ba3227d00f4a5cb63f332cfedb7b30. Co-authored-by: Ubuntu thomas@grid.ai",0
5173,Ci: fix install pkg name (#15259),0.6057842,"Removed PyTorch 1.6 support (#10367, #10738)",  set find unused params to False   add changelog   fix changelog   fix test   update docs   update changelog   Co-authored-by: chaton thomas@grid.ai,0
5174,update changelog after 1.8.4.post (#16008),0.7188696,Full Changelog: https://github.com/Lightning-AI/lightning/compare/1.8.0...1.8.0.post1,  Add LambdaCallback   docs   add pr link   Conflicts: CHANGELOG.md   convention   Fix Callback Typo   Update pytorch_lightning/callbacks/lambda_cb.py   Co-authored-by: Nicki Skafte skaftenicki@gmail.com  Update pytorch_lightning/callbacks/lambda_cb.py  Co-authored-by: Nicki Skafte skaftenicki@gmail.com  Update pytorch_lightning/callbacks/lambda_cb.py  Co-authored-by: Nicki Skafte skaftenicki@gmail.com   use Misconfigureation   update docs   sort export   use inspect   string fill   use fast dev run   isort   remove unused import   sort   hilightning   highlighting   highlighting   remove debug log   eq   res   results   add misconfig exception test   use pytest raises   fix   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update pytorch_lightning/callbacks/lambda_cb.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   hc   rm pt   fix   try fix   whitespace   new hook   add raise   fix   remove unused   rename   Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: chaton thomas@grid.ai,1
5175,add section & add testing ckpt 1.1.4 (#5495),0.69434345,"trainer.test(model, ckpt_path=""/path/to/checkpoint.ckpt"")",  reset   self._cache -> cache (make cache local variable so it is not overwritten)   pep8   fix metric result integration   rm print statements   better comment   changelog   Update docs/source/metrics.rst   Co-authored-by: Roger Shieh sh.rog@protonmail.ch Co-authored-by: Teddy Koker teddy.koker@gmail.com Co-authored-by: Roger Shieh sh.rog@protonmail.ch,0
5176,[App/Feature] HTTP Queues (#14978),0.6116477,- Added an HTTPQueue as an optional replacement for the default redis queue ([#14978](https://github.com/Lightning-AI/lightning/pull/14978),  update with BoringModel and introduce BoringDataModule   isort   fix   rm random_split   fix test   fix test   update   update test_results   val_step   update tests   rebase   rebase ,0
5177,ngc (#8242),0.59115547,(#16002),  add missing val/test hooks   chlog   None   Co-authored-by: Roger Shieh sh.rog@protonmail.ch,0
5178,Mark SLURM detection methods in AcceleratorConnector as protected (#10101),0.7964188,Deprecated access to the AcceleratorConnector.configure_slurm_ddp method and marked it as protected (#10101),  simplify training phase as Enum   tests   .   .   rename   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Apply suggestions from code review  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   rename   flake8   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
5179,Increase timeout in PyTorch CI jobs due to long grpcio installation (#15411),0.6481658,"Resolved schedule step bug for PyTorch Profiler (#6674, #6681)",  set minimal req. PT 1.4   chlog ,0
5180,Deprecate on_hpc_{save/load} hooks (#10911),0.65996146,refactored dataloader process hook (#3139),  unify LightningEnum   hash   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update states.py  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
5181,Bugfix: Scheduler monitor for manual optimization (#7643),0.6984802,"        scheduler = TanhLRScheduler(optimizer, ...)",  warnings   .   .   flake8   .   .   .   use_tpu   use_dp   .   use_ddp   .   use_horovod   .   .   . ,0
5182,Delete tests.helpers.TrialMNISTDataModule (#5999),0.68694437,Deprecated the TestTubeLogger (#9065),  Remove tests.checkpointing from skipped module in pyproject.toml   Fix pre-commit isort failure on tests/checkpointing/*.py ,0
5183,[Bug Fix] Allow logger to support indexing (#4595),0.6278244,- Fixed logging to loggers with multiple eval dataloaders ([#12454](https://github.com/PyTorchLightning/pytorch-lightning/pull/12454)),  Throw MisconfigurationError on unknown mode   Add tests   Add match condition for deprecation message ,0
5184,Merge pull request #1635 from PyTorchLightning/pkl,0.64793754,- Removed `configure_sync_batchnorm` from `ParallelStrategy` and all other strategies that inherit from it ([#11754](https://github.com/PyTorchLightning/pytorch-lightning/pull/11754)),  prune check on Trainer fit result   flake8   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  .  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
5185,Validate the model input of trainer methods (#13892),0.81122303,trainer.test(model),  Remove tests.utilities from skipped module in pyproject.toml   Fix pre-commit isort failure on tests/utilities/*.py ,1
5186,fix requirements for package (#15285),0.675065,Implemented ready for components (#16129),  header   update docs   punctuation   adding another note   some more notes   Update docs/source/tpu.rst   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  punctuation  Co-authored-by: Lezwon Castelino lezwon@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: chaton thomas@grid.ai,0
5187,Fix yapf-isort conflict (#7500),0.61973155,Resolve bug with Finetuning (#5744),  fix   Update CHANGELOG   add test   fix   pep   docs   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
5188,disable_logger (#9837),0.7393461,Removed LoggerStages (#5673),  use BoringModel   use BoringModel   use BoringModel   trigger   limit_batches   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
5189,Replace meta_tags.csv with hparams.yaml (#1271),0.9041793,Replace mata_tags.csv with hparams.yaml (#1271),"  add ppo rl lightning template   flake   import gym without try as in qnet example   fix import format   remove torch.optim import, not required   fix import format isort   add trainer argparse   change name of trajectory collection method   add repo in references   fix typo in comments   use isinstance to verify actionspace type   use fstring   deduplication of logic code   rename unused forloop variable   use pl.seed_everything instead   remove unused numpy import   format string printed on error   fix typo in comments   Co-authored-by: chaton thomas@grid.ai",1
5190,use automethod for LightningModule method (#4025),0.78679115,Enable precision autocast for LightningModule step methods in Fabric (#17439),"  resolve bug   add tests   add tests   resolve flake8   update   update   remove globals   typo   Update pytorch_lightning/utilities/distributed.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   update   update   add suport int, float   update   resolve pep8   Update pytorch_lightning/core/lightning.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update tests/utilities/test_all_gather_grad.py  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   update doc   add bool and np.ndarray   resolve conflicts   resolve conflicts   resolve pep8   add changelog   Update pytorch_lightning/core/lightning.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Ubuntu ubuntu@ip-172-31-62-109.ec2.internal Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com",1
5191,Workaround for actions/checkout in conda CI jobs (#12758),0.5298861,System customization syncing for jobs run (#16932),  Feat: Add BackboneLambdaFinetunningCallback   update changelog   resolve pep8 and update changelog   add finetunning example   resolve example   iremove milestones from model   iupdate   update   Update pytorch_lightning/callbacks/init.py   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com  Update pytorch_lightning/callbacks/init.py  Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com   update   add comments   resolve test   Update pytorch_lightning/callbacks/finetuning.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update tests/trainer/logging/test_logger_connector.py  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   update on comments   resolve merge   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
5192,update NGC (#8652),0.5880761,Changed to the NeptuneLogger (#16761):,"Previously pre-commit was using any version of isort found on developer machine. Now, used isort is from the official repository and version is set to 5.7.0 (which is nowadays the latest release). Co-authored-by: Akihiro Nitta nitta@akihironitta.com Co-authored-by: Akihiro Nitta nitta@akihironitta.com",0
5193,"[docs] Clear up default logging, showing you don't need to pass a logger (#9408)",0.7648942,for logger in loggers:,  added Iou   Create iou.py   Update iou.py   Update iou.py   Update CHANGELOG.md   Update metrics.rst   Update iou.py   Update iou.py   Update init.py   Update iou.py   Update iou.py   Update classification.py   Update classification.py   Update classification.py   Update init.py   Update init.py   Update iou.py   Update classification.py   Update metrics.rst   Update CHANGELOG.md   Update CHANGELOG.md   add iou   add test   add test   removed iou   add iou   add iou test   add float   reformat test_iou   removed test_iou   updated format   updated format   Update CHANGELOG.md   updated format   Update metrics.rst   Apply suggestions from code review   merge suggestions Co-authored-by: Nicki Skafte skaftenicki@gmail.com   added equations   reformat init   change format   change format   deprecate iou and test for this   fix changelog   delete iou test in test_classification   format change   format change   format   format   format   delete white space   delete white space   fix tests   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Apply suggestions from code review  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   better deprecation   fix docs   Apply suggestions from code review   fix todo   Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
5194,Speed up gradient clipping and allow parameters on multiple devices. (#2767),0.69572794,Gradient Clipping Customization, docs + precision + recall + f_beta + refactor  Co-authored-by: Teddy Koker teddy.koker@gmail.com  rebase  Co-authored-by: Teddy Koker teddy.koker@gmail.com  fixes  Co-authored-by: Teddy Koker teddy.koker@gmail.com   added missing file   docs   docs   extra import   add metric collection   add docs + integration with log_dict   add test   update   update   more test   more test   pep8   fix doctest   pep8   add clone method   add clone method   merge-2   changelog   kwargs filtering and tests   pep8   fix test   update docs   Update docs/source/metrics.rst   Co-authored-by: Roger Shieh sh.rog@protonmail.ch   fix docs   fix tests   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   fix docs   fix doctest   fix doctest   fix doctest   fix doctest   Co-authored-by: ananyahjha93 ananya@pytorchlightning.ai Co-authored-by: Teddy Koker teddy.koker@gmail.com Co-authored-by: Nicki Skafte nugginea@gmail.com Co-authored-by: Roger Shieh sh.rog@protonmail.ch Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
5195,CI: clean building docs (#14216),0.5846636,"Cleaning (#5948, #5949, #5950)",  added iamge-gradients (#4763)   fixed tests code format   made recommended fixes   removed explicit device flags   tried to fix doctest failure   pep8 and doctest fixes   added to docs/metrics   updated CHANGELOG   added the noqa flag   added suggested modification to changelog   Co-authored-by: Nicki Skafte skaftenicki@gmail.com  recommended update to docstring  Co-authored-by: Nicki Skafte skaftenicki@gmail.com  removed device from docstring  Co-authored-by: Nicki Skafte skaftenicki@gmail.com   made recommended fixes   Update CHANGELOG.md   Apply suggestions from code review   added 1-line docstrings   Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Roger Shieh sh.rog@protonmail.ch,0
5196,Deprecate nvidia/apex (#16039),0.91397125,nvidia/apex deprecation (#16039),  [bug-fix] Metric reduction with Logging (#5150)   add test   resolve bug   udpate test   wrongly copy / paste   update test   resolve a second bug   Co-authored-by: Ubuntu ubuntu@ip-172-31-62-109.ec2.internal   iupdate   resolve bugs   add test back   correct flake8   resolve flake8   update on comments   update tests   add a test   add test   update to Callable   Co-authored-by: Ubuntu ubuntu@ip-172-31-62-109.ec2.internal,1
5197,Update CHANGELOG after the v1.9.4 release (#16906),0.71864897,Here is a selection of important changes that are not backward compatible with versions < 1.5. The full list of changes and removals are listed in the changelog at the bottom.,  fix docs typos   Apply suggestions from code review   Co-authored-by: Wansoo Kim rladhkstn8@gmail.com  flake8  Co-authored-by: Wansoo Kim rladhkstn8@gmail.com,1
5198,[App] Revert raising error from LightningClient on 500 (#16723),0.7497343,Improved the error message when the root LightningFlow passed to LightningApp is missing the run method (#14760),"  test_cpu refactoring - BoringModel and checkpoints; test_gpu refactoring - BoringModelboring_model refactoring - validation, testing; Fix - run_prediction as dispatcher for testing BoringModel   Removed EvalModelTemplate import from test_cpu and test_gpu   Reverting unintended changes   Issues with checkpointing   Fixed tests for logging and checkpointing   Fix for dispatcher   test_cpu refactoring - BoringModel and checkpoints; test_gpu refactoring - BoringModelboring_model refactoring - validation, testing; Fix - run_prediction as dispatcher for testing BoringModel   Removed EvalModelTemplate import from test_cpu and test_gpu   Reverting unintended changes   Issues with checkpointing   Fixed tests for logging and checkpointing   Fix for dispatcher   Fixed acc check for stocasticity of seeds   Fixed according to @borda suggestions   Hparams for boring_model   Deprecated RuntimeParamChagneModelAssing (functionality is tested in RuntimeParamChangeModelSaving)   Reduced boring_model parameters to just in and out features, test_cpu modelsinherit BoringModel to specify additional parameters (e.g., optimizer)   Fix PEP8   Update tests/base/develop_pipelines.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update tests/base/boring_model.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update tests/base/develop_pipelines.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update tests/models/test_cpu.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update tests/models/test_cpu.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   Merged test_early_stopping with all_features; added TODO for self.log   Fixed test_all_features trainer options   Ready for review!   Update tests/models/test_cpu.py   Thank you! :) Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update tests/models/test_cpu.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update tests/models/test_cpu.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update tests/models/test_cpu.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update tests/models/test_cpu.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   added optimizer_name, lr, and batch_size as hparams for save_hparameters()   Fixes for reducing PR size   Reverse test_hparams (removed DEPRECATED test for hparams direct assignment)   Changes for in_features   Fixed hparams   Fixed parameters for boring_model   Update tests/models/test_cpu.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update tests/models/test_cpu.py  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update tests/models/test_cpu.py  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   fix for pep8   Fixed run_predction and TODO   fix min acc for darwin/windows without pl_opt   eval as DEFAULT run_prediction strategy   Updated val_dataloader for running_test_no_val   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: chaton thomas@grid.ai Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com",1
5199,docs: updated broken links (#16191),0.66326106,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",  fix num_workers for Windows example   chlog   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  warn  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
5200,simplify docs (#3267),0.69839257,reduced all simplified forward (#3126),  drop duplicated metric helper   .   fix tests   Co-authored-by: Nicki Skafte skaftenicki@gmail.com,0
5201,Do not modify PACKAGE_NAME on install (#15493),0.50402665,Avoid using the deprecated LooseVersion (#16162),,0
5202,Switch to PyTorch 1.6 in Drone CI (#4393),0.70537466,Enable PyTorch 1.7 compatibility (#3541),  Prepare 1.1.3 release   Fix flake8 error   suppress   Remove 1.1.4 section   Add missing commits to CHANGELOG   Update PR template   Add missing commit   fix   Update CHANGELOG.md   Apply suggestions from code review   Apply suggestions from code review   Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz Co-authored-by: Jirka Borovec Borda@users.noreply.github.com (cherry picked from commit 4d9db866a11f3b4b9b923bca811911ac79dad914),1
5203,Remove return_result argument from DDPSpawnPlugin.spawn() (#10867),0.7721443,- Removed argument `return_result` from the `DDPSpawnPlugin.spawn()` method ([#10867](https://github.com/PyTorchLightning/pytorch-lightning/pull/10867)),  fix   params   add test   add another types   chlog   Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz Co-authored-by: Jirka Borovec Borda@users.noreply.github.com (cherry picked from commit 6536ea42fa88eda63f17dae60c51d9669b409b78),1
5204,Update ci_pr-gatekeeper.yml (#12661),0.52684987,- Marked `trainer.checkpoint_connector` as protected ([#11550](https://github.com/PyTorchLightning/pytorch-lightning/pull/11550)),  refactor imports of logger dependencies   fix   fix   fix   name   fix   mocks   fix tests   fix mlflow   fix test tube   fix wandb import check   whitespace   name   name   hack   hack   rev   fix   update mlflow import check   try without installing conda dep   .   .   .   .   .   .   .   .   .   Co-authored-by: Adrian Wälchli adrian.waelchli@inf.unibe.ch Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com (cherry picked from commit ec0fb7a3ec709699243c76dae04ee1e4ce2406a0),0
5205,Prune metrics: other classification 7/n (#6584),0.6210731,Pruned deprecated classif. metrics from pytorch_lightning.metrics.functional.classification (#7499),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com (cherry picked from commit 410d67fbe866ee20069e88db1be729d27ae0af48),0
5206,Merge pull request #39 from Borda/cutout-examples,0.58690226,"merge backends (#3476, #3477, #3478, #3480, #3482)","There was a typo in Documentation of Code of the compute() function of Recall metric at line 210. It said ""Computes accuracy over state."" which should have been ""Computes recall over state."" (cherry picked from commit d568533b6ba0768ac7a6d028b3a25c5970a65e80)",0
5207,removed bad hook call,0.7864634,We removed some Callback hooks that were ambiguous to use Removed deprecated callback hooks (#14834):,  Change the classifier input from 2048 to 1000.   Update docs for Imagenet example   Thanks @rohitgr7  Apply suggestions from code review  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com (cherry picked from commit a40e3a325e71640786717094a67c41805ab30593),1
5208,ref: enable self.log for eval loop metrics (#3715),0.82001793,Integrated metrics API with self.log (#3961),  resolve bug   update code   add set -e   Update pytorch_lightning/callbacks/model_checkpoint.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   update test   Update tests/checkpointing/test_trainer_checkpoint.py   Co-authored-by: Sean Naren sean.narenthiran@gmail.com  Update tests/checkpointing/test_trainer_checkpoint.py  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   update on comments   resolve test   convert to set   update   add error triggering   update   update on comments   update   resolve import   update   update   Update pytorch_lightning/plugins/rpc_plugin.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  update  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Ubuntu ubuntu@ip-172-31-62-109.ec2.internal Co-authored-by: Jirka Borovec Borda@users.noreply.github.com (cherry picked from commit d5b367871fa3924090ec74bf903bd172bd3e2343),1
5209,Remove ci tpu test from github workflows (#8965),0.6041651,- Removed `Strategy.on_tpu` property ([#11536](https://github.com/PyTorchLightning/pytorch-lightning/pull/11536)),  Fix weights_summary   use mode   fix   optional   what was I thinking   (cherry picked from commit 062800aa99cff6eb82838b355374305d9433507e),0
5210,Fix merge issue,0.5984308,"merge backends (#3476, #3477, #3478, #3480, #3482)",  fix   fix   chlog   no momentum warning   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  ref  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com (cherry picked from commit 371daea594f1f9b6b1f3a7071688ba61fe0b335a),0
5211,Feature: auto scale batch size (#1638),0.73205745,Tune batch size,  add a check for scheduler and optimizer   pep   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com (cherry picked from commit c7d0f4c3a29bd5524e0b66f9196f123b64d1587a),1
5212,continue (#2416),0.66381025,[1.1.6] - 2021-01-26,"  Disable checkpointing, earlystopping and logger with fast_dev_run   docs   chlog   disable callbacks and enable DummyLogger   add log   use dummy logger method   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: chaton thomas@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com (cherry picked from commit f7402455218e5087e61d8255a4a87a8db58a7194)",0
5213,Checkpoint migration for ModelCheckpoint state-key changes (#15606),0.6695597,Forced ModelCheckpoint callbacks to run after all others to guarantee all states are saved to the checkpoint (#5731),"  Add empty resume_from_checkpoint acceptance #4366   Fix general error catch with focused file check   Add fsspec HTTP extras   Add fsspec's HTTPFileSystem  support through http extras. pl has supported remote http file (e.g. #2925), so this commit do not add new functionality.   Fix potential too much logging in DDP   Add PR changelog   Add well-written argument explanation   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Fix DDP-compatible restore logging  Notify from where the states are restored. This feature temporally deleted as a result of PR review. With succeeding review, added with DDP compatibility.   Fix utility import pathes   Refactor load step commentaries   Refactor hpc ckpt suffix acquisition   Refactor restore/hpc_load match   Refactor hpc load trial   Refactor checkpoint dir check   Refactor unneeded function nest   Refactor nested If   Refactor duplicated cache clear   Refactor attempt flow with if/elif   Fix pip8   Refactor hook commentary   Co-authored-by: chaton thomas@grid.ai   Fix pep8   Refactor hpc load checkpoint path acquisition   Fix pip8   Fix typo   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Fix typo  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Fix doc  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   Refactor None Union type with Optional   Fix build-doc CI failure debuged in #5329   Fix fsspec import during build-doc #5329   Fix test epoch   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   Fix test with latest test models   .   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: chaton thomas@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: Roger Shieh sh.rog@protonmail.ch (cherry picked from commit b0051e8c036fa3312ad4d37aa7141bea64ac6148)",0
5214,ref: merge backends x/n (#3478),0.8366363,"merge backends (#3476, #3477, #3478, #3480, #3482)",  update docs and add pathlib support   fix   (cherry picked from commit dd442b6d335a5553961e7904cd9454b738cb72cd),1
5215,Fix formatting issue in Trainer docs (#12777),0.61151934,Removed the deprecated TrainerLoggingMixin class (#8609),  fix: logits -> probs in accuracy metrics documentation   Update metrics.rst   Update metrics.rst   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com (cherry picked from commit 15a400b95faf45124f28fd8ca4ef7520d47df88a),0
5216,Clean up environment access in plugins (#6941),0.62255764,Enabled plugins (#4041),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com (cherry picked from commit 0e593fb6a8b6abadb61f7cb6754b79b2117d7f0f),0
5217,Selects random port for lightning app when running multiple apps locally (#15819),0.6451734,Lightning App,  black formatting and migrated to self.log logging   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  migrated to accuracy in the metrics package  migrated to accuracy in the metrics package   removed trailing whitespace   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com (cherry picked from commit 17a0784c5e9789ec06e720f5f4a06a80756e73fa),0
5218,Resolve FitLoop setter TODOs (#16803),0.6193628,class MyCustomLoop(pl.loops.FitLoop):,(cherry picked from commit 51af3957fc7f18a4ce101c6da2a8177a36217d74),0
5219,fix bugs in semantic segmentation example (#1824),0.69833577,Updated semantic segmentation example with custom u-net and logging (#1371),  update isort config   apply   (cherry picked from commit 724f1051f0cef154999dbeb7c1ce468ff13a8da5),0
5220,Update stale-bot message (#15410),0.5891083,- Removed deadlock detection / process reconciliation (`PL_RECONCILE_PROCESS=1`) ([#16204](https://github.com/Lightning-AI/lightning/pull/16204)),(cherry picked from commit d20fd8e5ab1a52747fee2cd53290a679d8b726d0),0
5221,Apply import formatting to files in the 2nd top level (#4717),0.55671275,from setuptools import setup,  refactor python in GH actions   .  .  (cherry picked from commit ab7512d7ba0b522114f97db58c8d0b9555d7b75a),0
5222,Update for M1 Mac installations (#14350),0.5781222,"Defaults to ""mps"" when run on M1 or M2 Apple machines",  docs   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  ref  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com (cherry picked from commit 64163c2662e0aae420388f84ca1525f25bd23e24),0
5223,release v0.4.2,0.81794703,0.4.0,"There were four instances where optimizer_idx was the argument for optimizer_step() under Step optimizers at arbitrary intervals. However, instead of using it what was being used inside (erroneously) was optimizer_i. (cherry picked from commit dd98a60e901ccd511136bd955e0964eaa5b4e8dd)",1
5224,Remove old platform docs (#16499),0.627503,Remove MetricsHolder (#7909),  Fix metric state reset   Fix test   Improve formatting   Co-authored-by: Ananya Harsh Jha ananya@pytorchlightning.ai (cherry picked from commit 4913cbb987a0516f8b33c016134b19c0588d107a),0
5225,Don't raise DeprecationWarning for LoggerConnector.gpus_metrics (#9959),0.7024185,Changed automatic casting for LoggerConnector metrics (#5218),  update docs   Update docs/source/metrics.rst   Co-authored-by: Shreeyak shreeyak.sajjan@gmail.com  Update docs/source/metrics.rst  Co-authored-by: Shreeyak shreeyak.sajjan@gmail.com  Update docs/source/metrics.rst  Co-authored-by: Shreeyak shreeyak.sajjan@gmail.com   Update docs/source/metrics.rst   Update docs/source/metrics.rst   Update docs/source/metrics.rst   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update docs/source/metrics.rst  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   Update docs/source/metrics.rst   Update docs/source/metrics.rst   try fix failing doc test   Co-authored-by: Roger Shieh sh.rog@protonmail.ch Co-authored-by: Shreeyak shreeyak.sajjan@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com (cherry picked from commit dabfeca92e0702e55f09ac53e9412672cd258cd3),1
5226,resurface lost ddp info message (#8111),0.7003286,"refactor DDP backend (#3204, #3207, #3208, #3209, #3210)",  Remove examples from isort ignore list   Apply isort   (cherry picked from commit 0c7c9e85404ce4be33cc65f95a029b6bc03d84e4),1
5227,feat: save checkpoint before deleting old ones (#1453),0.7372613,Disable saving checkpoints if not trained (#4372),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com (cherry picked from commit eb1d61caaf49e6b87c0edd50472eb3474da17376),1
5228,[bugfix] Perform reduction for dict in training_step and DP (#6324),0.6926387,Removed automatic reduction of outputs in training_step when using DataParallel,  resolve bug   merge tests   (cherry picked from commit 9ebbfece5e2c56bb5300cfffafb129e399492469),0
5229,Bugfix/5487 auto lr ordering (#5638),0.61696416,Porting fixes to autoscaler component (#16249),(cherry picked from commit d1e97a4f114a285349e31e330c7bf8937bc1ee04),0
5230,Allow user to select individual TPU core to train on (#1729),1.0,Allow user to select individual TPU core to train on (#1729),  minor doc fix   minor doc fix   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  suggestions  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com (cherry picked from commit 8d8098c04e716c8b9ccf5bc9208dd4f23b2b8e14),1
5231,Add typing for utilities/memory.py (#11545),0.5790765,- Modified `supporters.py` so that in the accumulator element (for loss) is created directly on the device ([#12430](https://github.com/PyTorchLightning/pytorch-lightning/pull/12430)),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com (cherry picked from commit 90c1c0f68b4983c685e9d009482890e578800439),0
5232,fixed correct module on hpc save,0.5805051,  * `save_config_multifile`,  Add TPU example   add badge   add badge   add badge   bullets   name   trigger   add dataset name   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: chaton thomas@grid.ai (cherry picked from commit b930b5f2220f800c9f22eb74b19b3bcf8478c735),0
5233,"Fix: Start Lightning App on Cloud if Repo Begins With Name ""Lightning"" (#14025)",0.7090875,- Added support to start lightning app on cloud without needing to install dependencies locally ([#15019](https://github.com/Lightning-AI/lightning/pull/15019),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com (cherry picked from commit 1d533074b3b84729bd76c29456b750ba8f151d81),1
5234,Loop Refactor 5/N - Prediction Loop (#7700),0.7158891,Refactored Loops,Co-authored-by: Jirka Borovec Borda@users.noreply.github.com (cherry picked from commit 5651c9cc1ab37efa48332a35b7d55c33da1bab7d),1
5235,[Metrics] Confusion matrix class interface (#4348),0.53240275,Sklearn metrics classes (#1327),  skip some description from pypi   flake8   (cherry picked from commit 9b3c6a3e843837996d890c9e02823889a10b1d77),0
5236,Fix compare version for packages (#10762),0.66242677,Parsed local package versions (#13933),Co-authored-by: Rohit Gupta rohitgr1998@gmail.com (cherry picked from commit b22b1c2df25156f6d93eb3d8b3c85cf7b072e57e),0
5237,Improvements for rich progress bar (#9559),0.8761337,New Rich Progress Bar,Co-authored-by: Rohit Gupta rohitgr1998@gmail.com (cherry picked from commit c479351a938240fbda6774a404494ee399ff361a),1
5238,split tests for deprecated api (#5071),0.5806501,Removed deprecated API (#2073),  refactor   memory   show   clean   clean   try   device   reset   fix   fix   mean   hook   format   add todo   Co-authored-by: chaton thomas@grid.ai Co-authored-by: chaton thomas@grid.ai (cherry picked from commit 6adc1b32bdeddbc34282abe0fd9f654c0cba570b),0
5239,Fix typo and code rendering in docs (#5940),0.53385377,Resolve bug with Finetuning (#5744),  [bugfix] Group defaults to WORLD if None   fix no_grad   Update pytorch_lightning/utilities/distributed.py   Update pytorch_lightning/utilities/distributed.py   Co-authored-by: Gregor Koporec gregork@unicorn.gorenje.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Sean Naren sean.narenthiran@gmail.com (cherry picked from commit 176735097ab5be9ee21d3e7a3dedc174f3e0dd3f),0
5240,Move newly added Trainer methods to be with other methods (#11335),0.7610793,Removed TrainerProperties mixin and moved property definitions directly into Trainer (#9495),  update chlog for future 1.1.3rc   prune   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com (cherry picked from commit 27f3f973d6c55aeeba7895f1b6111ce743e16725),1
5241,Some fixes to the trainer docstring (#9227),0.73807096,Removed the deprecated TrainerTrainingTricksMixin class (#8679),,1
5242,CI: enable testing of react e2e (#14364),0.49400762,test_end >> test_epoch_end,,0
5243,Fix Train result in val and test steps doc (#3440),0.6824944,Move training_output validation to after train_step_end (#7868),,0
5244,add RobertLaurella as docs owner (#12973),0.4537675,Updated governance docs,,0
5245,Fix docs typo (#1355),0.513628,Docs,,0
5246,Performance docs (#2191),0.6721741,"docs for all Metrics (#2184, #2209)",,0
5247,"Improve argument validation for validate(), test(), and predict() (#7605)",0.76762515,  validation_if_necessary(),  draft   fix   drop folder   Co-authored-by: chaton thomas@grid.ai,1
5248,speed-up testing (#504),0.63253266,We have upgrade Continues Integration to speed up the automatic testing. ,Co-authored-by: Roger Shieh sh.rog@protonmail.ch,0
5249,[CLI] Add option to enable/disable config save to preserve multiple files structure,0.760587,  * `save_config_multifile`,,1
5250,feat: option to add custom meta tags to the UI container (#14915),0.6057105,Meta Module,  skip test   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com,0
5251,Support passing storage_options in trainer.save_checkpoint() API (#11891),0.66796637,- Added optional `storage_options` argument to `Trainer.save_checkpoint()` to pass to custom `CheckpointIO` implementations ([#11891](https://github.com/PyTorchLightning/pytorch-lightning/pull/11891)),  Remove Sourcerer   trigger   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
5252,Bugfix/cuda oom detection and handling (#6934),0.57242894,tons of bug fixes :wink:,  reduce verbosity level in drone   verbosity ,0
5253,Update trainer.py (#3834),0.73378503,Removed pytorch_lightning/trainer/training_loop.py (#7985),  feat(wandb): offset logging step when resuming   feat(wandb): output warnings   fix(wandb): allow step to be None   test(wandb): update tests   feat(wandb): display warning only once   style: fix PEP issues   tests(wandb): fix tests   tests(wandb): improve test   style: fix whitespace   feat: improve warning   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  feat(wandb): use variable from class instance  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   tests(wandb): check warnings   feat(wandb): use WarningCache   tests(wandb): fix tests   style: fix formatting   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
5254,Lightning Release v1.4 (#8579),0.80148494,Lightning 2.0 is the official release for Lightning Fabric :tada:,Co-authored-by: Gregor Koporec gregork@unicorn.gorenje.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Sean Naren sean.narenthiran@gmail.com,1
5255,Update docs for limit_predict_batches (#6507),0.61742413,"docs for all Metrics (#2184, #2209)",  Fix deprecation call   fix   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
5256,Resolve some codefactor issues (#756),0.65334845,Resolve bug with Finetuning (#5744),  update DALIClassificationLoader to not use deprecated arguments   fix line length   dali version check added and changed args accordingly   versions   checking version using disutils.version.LooseVersion now   .   ver   import   Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
5257,Fix number of references to LightningModule (#12897),0.7196538,"add .log to lightning module (#3686, #3699, #3701, #3704, #3715)",Fixed a small bug with the WandbLogger docs. Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Roger Shieh sh.rog@protonmail.ch,1
5258,add possibility for nested loaders (#5404),0.64299697,# pass loaders as list. This will create batches like this:,"  update CHANGELOG.md, increment for RC   Add missing changelog update   Added a few more   Move to added   Address code review   Missing space   Remove unreleased   Remove lines   Update CHANGELOG.md   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com",0
5259,(1/n) tests: Use strategy flag instead of accelerator for training strategies (#9931),0.68738246,trainer.test(),  docs   script   dump   desc   import   import   if   norm   t   finished   isort   typing   Co-authored-by: Nicki Skafte skaftenicki@gmail.com   xlabel   pandas   time   Co-authored-by: Nicki Skafte skaftenicki@gmail.com,0
5260,fixed memory leak from opt return (#1528),0.7876234,Resolve memory leak for evaluation (#6326),  define tests   fix basic   fix gans   unet   test   drop   format   fix   revert   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,1
5261,Add PyTorch 1.8 Profiler 5/5 (#6618),0.7810461,PyTorch 1.5  support,  draft   wip   CI   drop pl geometry   copy   logo ,1
5262,Fix ModelCheckpoint not being fixable (#1632),0.7613858,Replace MisconfigurationException with warning in ModelCheckpoint Callback (#4560),,1
5263,[typing] Add typehint for broadcast in training type plugin (#6777),0.6567432,Automatically set sync_batchnorm for training_type_plugin (#6536),  drop install FairScale for TPU   typo   Co-authored-by: Roger Shieh sh.rog@protonmail.ch,0
5264,Fix num_nodes not set for FSDPStrategy (#17438),0.71007216,Deprecated num_nodes and sync_batchnorm arguments in DDPPlugin and DDPSpawnPlugin (#7026),  Disable pl optimizer temporarily to fix AMP issues   Add todo and enable pl optimizer in the test ,1
5265,quick fix (#4697),0.6198023,Resolve bug with Finetuning (#5744),  add test   resolve bug   udpate test   wrongly copy / paste   update test   resolve a second bug   Co-authored-by: Ubuntu ubuntu@ip-172-31-62-109.ec2.internal,0
5266,Move profiler tests (#6619),0.6262052,move lr_finder (#3434),  remove nan loss whe missing   Update pytorch_lightning/core/lightning.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Apply suggestions from code review  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
5267,Docs for Trainer init method need double quotes added (#16498),0.55645,"trainer = Trainer(callbacks=GradientAccumulationScheduler({""1"": 5, ""10"": 3}))",  resolve bug   clean code   resolve comments   Update tests/trainer/optimization/test_multiple_optimizers.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   resolve another bug   add comments   use abs to find diff   update   resolve flake8   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
5268,remove parameterize from TPU tests (#2561),0.66413575,  * Removed the `Trainer(tpu_cores=...)` argument,  support number   add two tests   wip   add ddp in special test   remove a test   move device to bottom   simplify test   update test   Update pytorch_lightning/core/step_result.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  resolve sync_ddp  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
5269,Add test to verify that lowering gpus on restart works with sharded spawn (#15317),0.6292548,Disabled val and test shuffling (#1600),  Prune CHANGELOG.md   Update CHANGELOG.md   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update CHANGELOG.md  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update CHANGELOG.md  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update CHANGELOG.md  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update CHANGELOG.md  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
5270,update chlong after 1.0.8 (#4845),0.6524616,0.4.0,,0
5271,Add support for len(datamodule) (#9895),0.634318,Support **DictConfig for hparam serialization (#2519), Fix hang in DDP HPC accelerators  init_device was never called  Update CHANGELOG.md,0
5272,Device updates for TPU Pod (#7243),0.64433527,Updated logic for checking TPUs availability (#6767),  Fix reset TensorRunningAccum   add test for TensorRunningAccum's reset method   fix CI failed due to PEP8   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
5273,[App] Handling s3 rate limiting in framework (#14411),0.5666095,Avoided requiring the FairScale package to use precision with the fsdp native strategy (#14092),  Update isort config   Apply isort with new config   Fix typo in isort config   fix rebase   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
5274,pkg: append PL to lightning (#16921),0.71225506,+ # pytorch_lightning==1.7.0,"  disable version if not required   disable version if not required   pep   chlog   improve test   improve test   parametrize test and update del_list   Update pytorch_lightning/callbacks/model_checkpoint.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   try appending version to already saved ckpt_file   Revert ""try appending version to already saved ckpt_file""   This reverts commit 710e05e01f738d982aabf1f36c09fa59293e5c0c.   add more assertions   use BoringModel   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: chaton thomas@grid.ai Co-authored-by: Roger Shieh sh.rog@protonmail.ch",1
5275,[warning] Add a warning with missing callback with resume_from_checkpoint (#7254),0.8295396,Changed resume_from_checkpoint warning to an error when the checkpoint file does not exist (#7075),,1
5276,Organize docstring (#4906),0.6280831,"docs for all Metrics (#2184, #2209)",,0
5277,ref: inner train loop (intermediate step) 1/n (#3359),0.78197694,"inner train loop (#3359, #3361, #3362, #3363, #3365, #3366, #3367, #3368, #3369, #3370, #3371, #3372, #3373, #3374, #3375, #3376, #3385, #3388, #3397)", Add colab badges to notebook  Add colab badges to notebook to notebooks 4 & 5  Add colab badges  Co-authored-by: chaton thomas@grid.ai,1
5278,Fix: gather_all_tensors cross GPUs  in DDP (#3319),0.668635,Made DDP the default if no backend specified with multiple GPUs (#1789),  add unimplemented methods   test   test   flake8 ,0
5279,[bugfix] Group defaults to WORLD if None (#5125),0.77277595,Distributed group defaults to WORLD if None (#5125),"  add support for wrong dtype in apply_func   apply loader resetting to possible collection of loaders   add combined loader iter class   integrate combined loader iter to training loop   fix imports   fix imports   finish supporters   add tests for supporters   add test for model with multiple loaders   fix trainer integration   fix instance check   Train loaders (#4032)   patch for issues discussed in #1959, encapsulating underlying datastructures returned from train_dataloader   update data_loading.py to it uses patch discussed in #1959   rename class   Separate CombinedLoaderIterator into two classes, and update related tests. (#4606)   Fix the bugs after rebasing.   Add custom get_len for apply_to_collection   Refactor MultiIterator to be as CombinedLoaderIterator   To get the right num_training_batches. Call the wrapper for multi trainloader in data_loading.py, instead of training_loop.py   Reload _loader_iters when calling iter   Don't transform DataLoader to CombinedLoaderIterator when it's along   Updates test_fit_multiple_train_loaders for testing num_training_batches   Seperate CombinedLoaderIterator into CombinedLoaderIterator and CombinedDataLoader. Add CombinedDataset for unified DataLoader format.   Initialize CombinedDataLoader before calculating num_training_batches. Also updating self._worker_check for multiple loaders   Update tests for supporters   Update tests for multiple trainloaders. Add tests about few_workers for multiple loaders.   Fix pep8 issues   Add tests for train_loader_patch.py   Add descriptions to multiple_trainloader_mode   Remove unused variables   Add docstrings and typing   Add more tests for better converage   Remove unused commented codes   Add sampler property   Remove extract_dataset   Update typing   pep8   Update train_loader_patch.py   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/trainer/supporters.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   reviewer comments   fix stupid import   add docs   add back line separator   fix line sep   pep8   Apply suggestions from code review   fix   fix   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Apply suggestions from code review  Co-authored-by: Nicki Skafte skaftenicki@gmail.com  flake8  Co-authored-by: Justus Schock justusschock@justuss-mbp.fritz.box Co-authored-by: Christofer Fransson christofer_fransson@yahoo.com Co-authored-by: YI-LIN SUNG r06942076@ntu.edu.tw Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz",1
5280,DOC Adds reference to test-tube (#205),0.6151572,Deprecated the TestTubeLogger (#9065),  naive replace   simplify   clean   .   fix   .   fix   fix ,0
5281,Make the SLURM Preemption/Timeout Signal Configurable (#14626),0.7461276,The preemption/termination signal is now configurable (#14626):,  mark todo exceptions   .   .   .   .   .   .   .   .   try   . ,1
5282,Base classes for accelerator refactoring (#5715),0.7837906,Refactored into accelerator module:,  drop TrainResult   .   .   .   .   .   . ,1
5283,new chlog template (#3963),0.57212245,Allowing decorate model init with saving hparams inside (#4662),  model valid   model train   model test   model opt ,0
5284,replace Hparams by init args (#1896),0.6575544,Allowing decorate model init with saving hparams inside (#4662),  drop deprecated fbeta metrics   flake8   imports   chlog ,0
5285,Share the training step output data via ClosureResult (#9349),0.653722,Integrated TrainingEpochLoop.total_batch_idx (#8598),  drop deprecated checkpoint filepath   tests ,0
5286,move pl/utilities/apply_func.py to pl/utilities/apply_func.py (#14516),0.5963354,"- Deprecated the functions in `pytorch_lightning.utilities.apply_func` in favor of `lightning_utilities.core.apply_func` ([#14516](https://github.com/Lightning-AI/lightning/pull/14516), [#14537](https://github.com/Lightning-AI/lightning/pull/14537))",  add r2metric   change init   add test   add docs   add math   Apply suggestions from code review   Co-authored-by: Teddy Koker teddy.koker@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   changelog   adjusted parameter   add more test   pep8   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  add warnings for adjusted score  Co-authored-by: Teddy Koker teddy.koker@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
5287,Simplify hanging queue test (#10591),0.5865825,Simplify optimization Logic (#4984),  fix trainer distributed attributes   .   fix ,0
5288,Introduce StrategyRegistry (#11233),0.67553043,"    strategy=HivemindStrategy(target_batch_size=8192), ","  Add stuff   Change metrics documentation layout   Add stuff   Add stat scores   Change testing utils   Replace len(.shape) with .ndim   More descriptive error message for input formatting   Replace movedim with permute   PEP 8 compliance   WIP   Add reduce_scores function   Temporarily add back legacy class_reduce   Division with float   PEP 8 compliance   Remove precision recall   Replace movedim with permute   Add back tests   Add empty newlines   Add empty line   Fix permute   Fix some issues with old versions of PyTorch   Style changes in error messages   More error message style improvements   Fix typo in docs   Add more descriptive variable names in utils   Change internal var names   Break down error checking for inputs into separate functions   Remove the (N, ..., C) option in MD-MC   Simplify select_topk   Remove detach for inputs   Fix typos   Update pytorch_lightning/metrics/classification/utils.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update docs/source/metrics.rst  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   Minor error message changes   Update pytorch_lightning/metrics/utils.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   Reuse case from validation in formatting   Refactor code in _input_format_classification   Small improvements   PEP 8   Update pytorch_lightning/metrics/classification/utils.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update pytorch_lightning/metrics/classification/utils.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source/metrics.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update pytorch_lightning/metrics/classification/utils.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Apply suggestions from code review  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   Alphabetical reordering of regression metrics   Change default value of top_k and add error checking   Extract basic validation into separate function   Update to new top_k default   Update desciption of parameters in input formatting   Apply suggestions from code review   Co-authored-by: Nicki Skafte skaftenicki@gmail.com   Check that probabilities in preds sum to 1 (for MC)   Fix coverage   Split accuracy and hamming loss   Remove old redundant accuracy   Minor changes   Fix imports   Improve docstring descriptions   Fix imports   Fix edge case and simplify testing   Fix docs   PEP8   Reorder imports   Add top_k parameter   Update changelog   Update docstring   Update docstring   Reverse formatting changes for tests   Change parameter order   Remove formatting changes 2/2   Remove formatting 3/3   .   Improve description of top_k parameter   Apply suggestions from code review   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   Remove unneeded assert   Update pytorch_lightning/metrics/functional/accuracy.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   Remove unneeded assert   Explicit checking of parameter values   Apply suggestions from code review   Co-authored-by: Nicki Skafte skaftenicki@gmail.com   Apply suggestions from code review   Fix top_k checking   PEP8   Don't check dist_sync in test   add back check_dist_sync_on_step   Make sure half-precision inputs are transformed (#5013)   Fix typo   Rename hamming loss to hamming distance   Fix tests for half precision   Fix docs underline length   Fix doc undeline length   Replace mdmc_accuracy parameter with subset_accuracy   Update changelog   Fix unwanted accuracy change   Enable top_k for ML prob inputs   Test that default threshold is 0.5   Fix typo   Update top_k description in helpers   updates   Update styling and add back tests   Remove excess spaces   fix torch.where for old versions   fix linting   Update docstring   Fix docstring   Apply suggestions from code review (mostly docs)   Default threshold to None, accept only (0,1)   Change wrong threshold message   Improve documentation and add tests   Add back ddp tests   Change stat reduce method and default   Remove DDP tests and fix doctests   Fix doctest   Update changelog   Refactoring   Fix typo   Refactor   Increase coverage   Fix linting   Consistent use of backticks   Fix too long line in docs   Apply suggestions from code review   Fix deprecation test   Fix deprecation test   Default threshold back to 0.5   Minor documentation fixes   Add types to tests   Co-authored-by: Teddy Koker teddy.koker@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: chaton thomas@grid.ai Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com",0
5289,Update onnxruntime requirement from <1.13.0 to <1.14.0 in /requirements (#15672),0.55410606,Setup: added requirement freeze for the next major version (#14480),"  Implement partial auroc metric   Add pycodestyle changes   Added tests for max_fpr   changelog   version for tests   fix imports   fix tests   fix tests   Added more thresholds in (0,1] to test max_fpr   Removed deprecated 'reorder' param from auroc   changelog   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   remove old structure   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  fix test error  Co-authored-by: Nicki Skafte nugginea@gmail.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com",0
5290,Update obsolete URL in HPU docs (#15112),0.5694335,Updated app URLs to the latest format (#16568),  clean avail. imports   enums   fix missing ,0
5291,annotat unused vars (#5017),0.57481474,Removed deprecated EvalResult (#5633),  add tests for Trainer attributes   drop empty ,0
5292,Fixes app CLI tests by checking for the dynamically assigned port (#16283),0.5769237,Introducing CLI commands for apps (#13602)!,  CI fix nigtly releases   format   fix unrelated flake8 ,0
5293,ignore test module model,0.6044654,- Full tests that run multiple models in different configs,  warnings   argparse   mutils   xla device   deprecated   tests   simple   flake8   fix   flake8   1.4 ,0
5294,[App] Fixed Multi Node and add examples (#15557),0.67043304,Expose RunWorkExecutor to the work and provides default ones for the MultiNode Component (#15561),"  Add stuff   Change metrics documentation layout   Add stuff   Change testing utils   Replace len(.shape) with .ndim   More descriptive error message for input formatting   Replace movedim with permute   PEP 8 compliance   Division with float   Style changes in error messages   More error message style improvements   Fix typo in docs   Add more descriptive variable names in utils   Change internal var names   Break down error checking for inputs into separate functions   Remove the (N, ..., C) option in MD-MC   Simplify select_topk   Remove detach for inputs   Fix typos   Update pytorch_lightning/metrics/classification/utils.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update docs/source/metrics.rst  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   Minor error message changes   Update pytorch_lightning/metrics/utils.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   Reuse case from validation in formatting   Refactor code in _input_format_classification   Small improvements   PEP 8   Update pytorch_lightning/metrics/classification/utils.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update pytorch_lightning/metrics/classification/utils.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source/metrics.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update pytorch_lightning/metrics/classification/utils.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Apply suggestions from code review  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   Alphabetical reordering of regression metrics   Change default value of top_k and add error checking   Extract basic validation into separate function   Update to new top_k default   Update desciption of parameters in input formatting   Apply suggestions from code review   Co-authored-by: Nicki Skafte skaftenicki@gmail.com   Check that probabilities in preds sum to 1 (for MC)   Fix coverage   Split accuracy and hamming loss   Remove old redundant accuracy   Minor changes   Fix imports   Improve docstring descriptions   Fix edge case and simplify testing   Fix docs   PEP8   Reorder imports   Update changelog   Update docstring   Update docstring   Reverse formatting changes for tests   Change parameter order   Remove formatting changes 2/2   Remove formatting 3/3   .   Improve description of top_k parameter   Apply suggestions from code review   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   Remove unneeded assert   Update pytorch_lightning/metrics/functional/accuracy.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   Remove unneeded assert   Explicit checking of parameter values   Apply suggestions from code review   Co-authored-by: Nicki Skafte skaftenicki@gmail.com   Apply suggestions from code review   Fix top_k checking   PEP8   Don't check dist_sync in test   add back check_dist_sync_on_step   Make sure half-precision inputs are transformed (#5013)   Fix typo   Rename hamming loss to hamming distance   Fix tests for half precision   Fix docs underline length   Fix doc undeline length   Replace mdmc_accuracy parameter with subset_accuracy   Update changelog   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   Suggestions from code review   Fix number in docs   Update pytorch_lightning/metrics/classification/accuracy.py   Replace topk by argsort in select_topk   Fix changelog   Add test for wrong params   Add Google Colab badges (#5111)   Add colab badges to notebook   Add colab badges to notebook to notebooks 4 & 5  Add colab badges  Co-authored-by: chaton thomas@grid.ai   Fix hanging metrics tests (#5134)   Use torch.topk again as ddp hanging tests fixed in #5134   Fix unwanted notebooks change   Fix too long line in hamming_distance   Apply suggestions from code review   Apply suggestions from code review   protect   Update CHANGELOG.md   Co-authored-by: Teddy Koker teddy.koker@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: chaton thomas@grid.ai Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Roger Shieh sh.rog@protonmail.ch Co-authored-by: Shachar Mirkin shacharmirkin@gmail.com",0
5295,Allow access to ckpt_path within context of fit() (#11696),0.6482181,"    'path/to/checkpoint.ckpt',",  refactor - check F401   missed   fix ,0
5296,fix optimizer docs (#4084),0.75284255,Refactored optimizer (#4658),,1
5297,Fix references for ResultCollection.extra and improve str and repr (#8622),0.6355957,Improved string conversion for ResultCollection (#8622),,0
5298,"Update gym[classic_control] requirement from <0.25.2,>=0.17.0 to >=0.17.0,<0.26.3 in /requirements (#15136)",0.64629865,"Deprecated setting Trainer(max_steps=None); To turn off the limit, set Trainer(max_steps=-1) (default) (#9460)",  annotate all unused vars   rank_zero_warn   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  f1 fixed  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,0
5299,fix gpu template (#2255),0.58828324,- Deprecated `Trainer.num_gpus` in favor of `Trainer.num_devices` when GPU is used ([#12384](https://github.com/PyTorchLightning/pytorch-lightning/pull/12384)),  enable to use self.log in callbacks   update   revert back to assert ,0
5300,remove logging from callback (#3939),0.75092703,Removed LoggerStages (#5673),,1
5301,Minor typo in the description of Adam's beta 2 (#4715),0.56230325,"Epoch 8:  53%|█████    | 17/32 [5.13/s, v_num=2]",  sett xxx_AVAILABLE as protected   docs ,0
5302,Remove deprecated method ClusterEnvironment.creates_children (#10339),0.80323637,Deprecated ClusterEnvironment.creates_children() in favor of ClusterEnvironment.creates_processes_externally (property) (#10106),  Do not warn when the name key is used   Missing line   Consistency   Update pytorch_lightning/callbacks/lr_monitor.py   Update docs   Update pytorch_lightning/core/lightning.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update CHANGELOG  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
5303,updated tests and docs,0.7156262,Docs improvements,  Refactor load step commentaries   Refactor hpc ckpt suffix acquisition   Refactor restore/hpc_load match   Refactor hpc load trial   Refactor checkpoint dir check   Refactor unneeded function nest   Refactor nested If   Refactor duplicated cache clear   Refactor attempt flow with if/elif   Fix pip8   Refactor hook commentary   Co-authored-by: chaton thomas@grid.ai   Fix pep8   Refactor hpc load checkpoint path acquisition   Fix pip8   Fix doc   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Refactor None Union type with Optional  Co-authored-by: chaton thomas@grid.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Roger Shieh sh.rog@protonmail.ch,1
5304,Fix trainer.predict(return_predictions=False) does not track batch_indices (#13629),0.75456285,- Fixed `Trainer.predict(return_predictions=False)` to track prediction's batch_indices ([#13629](https://github.com/Lightning-AI/lightning/pull/13629)),  Improve some tests   Add TrainerState asserts   Co-authored-by: Roger Shieh sh.rog@protonmail.ch,1
5305,undo test changes,0.64769626,Refactored setup_training and remove test_mode (#5388),Co-authored-by: chaton thomas@grid.ai Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
5306,drop fairscale for PT <= 1.4 (#4910),0.64504474,FairScale deprecation (in favor of PyTorch's FSDP implementation) (#16353),  imports   imports   flake8   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
5307,ci: separate integrations (#17170),0.573821,Split profilers module (#6261),  branch merge   sample   update with valid input tensors   pep   pathlib   Updated with BoringModel and added more input types   try fix   pep   skip test with torch < 1.4   fix test   Apply suggestions from code review   update tests   Allow any input in to_onnx and to_torchscript   Update tests/models/test_torchscript.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   no_grad   try fix random failing test   rm example_input_array   rm example_input_array   Co-authored-by: chaton thomas@grid.ai Co-authored-by: Jeff Yang ydcjeff@outlook.com Co-authored-by: Roger Shieh sh.rog@protonmail.ch Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com,0
5308,Create mkdocs.yml,0.4770318,"class ProductionReadyModel(LitModule, ServableModule):",  remove beta from F1   remove from functional   Co-authored-by: Teddy Koker teddy.koker@gmail.com,0
5309,Call on_load_checkpoint before loading state_dict (#4057),0.9884365,Called on_load_checkpoint before loading state_dict (#4057),  fix functional f1 fbeta formatting   Update f_beta.py   remove line breaks   Update f_beta.py   add line breaks and pad  pad linea breaks with 2 spaces instead of tab,1
5310,Squeeze the early stopping monitor (#10461),0.6909846,Squeeze the early stopping monitor to remove empty tensor dimensions (#10461),Co-authored-by: Roger Shieh sh.rog@protonmail.ch,0
5311,releasing RC2 (#15436),0.55958366,This release includes:,Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
5312,Add functions to collate deepspeed zero 3 checkpoints (#8701),0.65651083,Update the logic to check for accumulation steps with deepspeed (#9826),  CI: upload report only on failer   Apply suggestions from code review   Co-authored-by: chaton thomas@grid.ai  Apply suggestions from code review  Co-authored-by: chaton thomas@grid.ai Co-authored-by: Roger Shieh sh.rog@protonmail.ch Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
5313,Remove DeepSpeed version restriction from Lite (#13967),0.723963,Updated compatibility for LightningLite to run with the latest DeepSpeed 0.7.0 (13967),  add back compatibility for deprecated metrics   fix   imports   imports ,1
5314,Simplify TPUSpawn rank management (#11163),0.70029366,"- Removed `TPUSpawnStrategy.{tpu_local_core_rank,tpu_global_core_rank}` attributes in favor of `TPUSpawnStrategy.{local_rank,global_rank}` ([#11163](https://github.com/Lightning-AI/lightning/pull/11163))",  add back compatibility for metrics   tests   Add deprecated metric utility functions back to functional (#5062)   add back deprecated metric utility functions to functional   pep   pep   suggestions   move   Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz   more   fix   import   docs   tests   fix   Co-authored-by: Teddy Koker teddy.koker@gmail.com,1
5315,testing master_Addr flag,0.51708156,"- The Trainer's signal handlers are now registered for `trainer.{validate,test,predict}` ([#17017](https://github.com/Lightning-AI/lightning/pull/17017))",  add rc release   update changelog   Update CHANGELOG.md   Update CHANGELOG.md   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
5316,Fixed use of LightningCLI in computer_vision_fine_tuning.py example (#9934),0.6892983,- Added a `PrecisionPlugin.teardown` method ([#10990](https://github.com/PyTorchLightning/pytorch-lightning/pull/10990)),  resolve urgent bug   update pr   update doc   update   remove typo   add defaults   Update pytorch_lightning/init.py   Update setup.py   update doc   Update docs/source/optimizers.rst   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   update   resolve doc   debug test   update test   Update docs/source/optimizers.rst   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update docs/source/optimizers.rst  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update docs/source/optimizers.rst  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   remove useless import   Update docs/source/optimizers.rst   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
5317,Remove deprecated LightningModule.on_post_move_to_device (#13548),0.8939822,- Removed deprecated `LightningModule.on_post_move_to_device` ([#13548](https://github.com/Lightning-AI/lightning/pull/13548)),  drop duplicate metrics   keep   fix ,1
5318,Rename training plugin test files & names to strategy (#11303),0.7032614,"All TrainingTypePlugins have been renamed to Strategy (#11120). Strategy is a more appropriate name because it encompasses more than simply training communcation. This change is now aligned with the changes we implemented in 1.5, which introduced the new strategy and devices flags to the Trainer.",  check if optimizer support closure   cleanup test   resolve tests   resolve flake   update test due to patch limit   update   update dep   Update tests/core/test_lightning_optimizer.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update tests/core/test_lightning_optimizer.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   resolve bug   update test   resolve tests   Update requirements/extra.txt   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   remove bolts dep   remove bolts   add missing bolts dep for tests   remove need for bolts   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
5319,Update colossalai version in Dockerfile (#16766),0.56684417,Refactored EpochResultStore (#5522),,0
5320,Fix log_dir property (#5537),0.77721107,Return the default_root_dir as the log_dir when the logger is a LoggerCollection (#8187),  Fix flake8 error to fix CI   Correct weights-loading to use correct callbacks   Fix dangling links   Co-authored-by: chaton thomas@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
5321,Add tags to the rendezvous calls for TPU.  (#921),0.66428196,Resolve TPU miss rendezvous (#6781),  Initialize trainer with None   add typing to all accelerators   resolve imports   update   add typing   removed typo   update   Fix formatting and imports in accelerator   Co-authored-by: maxjeblick maxjeblick@users.noreply.github.com Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: SeanNaren sean@grid.ai Co-authored-by: Roger Shieh sh.rog@protonmail.ch,0
5322,[Bug] Add SharedCycleIteratorState (#8889),0.56000286,refactored dataloader process hook (#3139),  simplify accelerator steps   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
5323,Add docs for Trainer.state (#12663),0.72435004,adding Trainer.tune() (#3293),  Add a notebook example to reach a quick baseline of ~94% accuracy on CIFAR10 using Resnet in Lightning   Remove outputs   PR Feedback   some changes   some more changes   Co-authored-by: chaton thomas@grid.ai Co-authored-by: rohitgr7 rohitgr1998@gmail.com,1
5324,Deprecate moved warning functions (#8085),0.7278788,Removed deprecation warnings being called for on_{task}_dataloader (#9279),  drop deprecated usage automatic_optimization   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Apply suggestions from code review  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
5325,CI: Add PR labeler (#13475),0.56643474,Re-enabled naming metrics in ckpt name (#3060),  drop deprecated profiler   lut   Co-authored-by: Roger Shieh sh.rog@protonmail.ch,0
5326,Update uvicorn requirement from <0.19.1 to <0.21.2 in /requirements (#17314),0.5333129,Only check versions / env when not in the cloud (#15504),  release 1.1.0   pep8 ,0
5327,Fix disabling progress bar on non-zero ranks using Horovod backend (#1709),0.7064965,Changed progress bar epoch counting to start from 0 (#3061),  drop usage of deprecated checkpoint_callback   fix   fix ,1
5328,Lightning Lite Examples (#9987),0.86943537,LightningLite,  add intermediate setters   show inputs   fix options   move   fix   less talk   fix   talk less   str   cases   rename   Co-authored-by: chaton thomas@grid.ai,1
5329,updated early stopping docs (#1410),0.65148145,Early stopping checks on_validation_end (#1458),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
5330,Remove AcceleratorConnector.has_tpu  (#12109),0.8447739,- Removed `AcceleratorConnector.has_tpu` property ([#12109](https://github.com/PyTorchLightning/pytorch-lightning/pull/12109)),,1
5331,DM docs (#2713),0.7129023,"docs for all Metrics (#2184, #2209)",  drop deprecated reorder from AUC   chlog   fix   fix   simple   fix   fix   fix   Co-authored-by: Roger Shieh sh.rog@protonmail.ch,1
5332,Fix TPU CI (#16613),0.70801276,Resolve TPU miss rendezvous (#6781),"  add pp doc   udpate doc   update doc   update doc   Update docs   update doc   udpate   update doc   update doc   Formatting, update sharded zip link   Update docs/source/multi_gpu.rst   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   Apply suggestions from code review   Reference directly to section   Co-authored-by: SeanNaren sean@grid.ai Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",1
5333,update load_from_checkpoint docstrings (#11467),0.6190571,datamodule = MyLightningDataModule.load_from_checkpoint('path/to/checkpoint.ckpt'),Co-authored-by: Roger Shieh sh.rog@protonmail.ch Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
5334,"Update deepdiff requirement from <=5.8.1,>=5.7.0 to >=5.7.0,<6.2.3 in /requirements (#16006)",0.58439636,- Moved the warning about saving nn.Module in `save_hyperparameters()` to before the deepcopy ([#15132](https://github.com/Lightning-AI/lightning/pull/15132)),  fix GH release badges   rtd ,0
5335,Fixed bug: replaced bce_loss_with_logits with bce_loss (#7096),0.5294562,Replace MisconfigurationException with warning in ModelCheckpoint Callback (#4560),"  Added changes for RPC plugin   Add missing kwargs   Fix code format   Loading refactors by introducing is_distributed var, fix optimizer step flow   Add rpc guard   Added docstrings and typing   resolve comments   Add additional rpc hook, refactor name of exit process hook for clarity   remove annotation   Modify behaviour to allow optional return, add test for rpc plugin   resolve tests   rename is_ddp_based   update   update for windows   update   resolve test   code smell   Added sequential plugin   resolve bug   update   cleanup   add Exception   resolve docs   Remove ddp support   Revert distributed -> ddp   Update pl_examples/basic_examples/conv_sequential_example.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pl_examples/basic_examples/conv_sequential_example.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/plugins/ddp_sequential_plugin.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   Address code review points   Update pytorch_lightning/plugins/ddp_sequential_plugin.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/plugins/ddp_sequential_plugin.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   Add missing return   Fix formatting, add datamodule args   add small comment   resolve comments   resolve comments   update source for fairscale   update extras   remove staticmethod   resolve flake8   Skip tests that are failing due to bug upstream with multiple optimizers and shard   update   update on comments   clean test   latest comments   remove old comments   add todo   Update version   update   resolve bugs   resolve bugs   update test   remove hanging test   Update pytorch_lightning/plugins/ddp_sequential_plugin.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   resolve on comments   Update pytorch_lightning/plugins/ddp_sequential_plugin.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   resolve on comments   Update pytorch_lightning/plugins/ddp_sequential_plugin.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update pytorch_lightning/plugins/ddp_sequential_plugin.py  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update pytorch_lightning/plugins/ddp_sequential_plugin.py  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  Update pytorch_lightning/plugins/ddp_sequential_plugin.py  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  remove ImportError  Co-authored-by: SeanNaren sean@grid.ai Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com",0
5336,Add warning for few workers (#1378),0.6178906,Do not override PYTHONWARNINGS (#4700),  adding missing changelogs   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Apply suggestions from code review  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
5337,Fix deadlinks in docs (#10739),0.5457356,Remove .item which causes sync issues (#1254),,0
5338,Use the local batch_idx to update the progress bar (#16760),0.7569392,Run main progress bar updates independent of val progress bar updates in TQDMProgressBar (#12563),  fix images   not sleep   a0   path   assets   assets   bitecode   rls   rls   badges   fix   org   drop   clean   codecov   fix   clean ,1
5339,CI: Azure clear workspace (#14460),0.45679227,Cleaner API to accommodate the various research use cases   ,Co-authored-by: chaton thomas@grid.ai Co-authored-by: Roger Shieh sh.rog@protonmail.ch,0
5340,code-owners for App (#13497),0.6705245,App,  drop pyright & add mypy   detail   name   fix   flake8   ver   Co-authored-by: Sean Naren sean.narenthiran@gmail.com,0
5341,set tt version,0.55368245,Set version as today (#13906),"  all_gather   ddp   horovod   grad tests   fixed ddp   ddp fixed, removed tpu, horovod for now   changelog   windows fix   windows fix   removed batch from ctx   all_gather   ddp   horovod   grad tests   fixed ddp   ddp fixed, removed tpu, horovod for now   changelog   windows fix   windows fix   removed batch from ctx   removed code duplication   merge   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",0
5342,Fix failure when DataLoader(batch_size=None) is passed (#10345),0.6196177,Reset the dataloaders on OOM failure in batch size finder to use the last successful batch size (#14372),"  Added changes for RPC plugin   Add missing kwargs   Fix code format   Loading refactors by introducing is_distributed var, fix optimizer step flow   Add rpc guard   Added docstrings and typing   resolve comments   Add additional rpc hook, refactor name of exit process hook for clarity   remove annotation   Modify behaviour to allow optional return, add test for rpc plugin   resolve tests   rename is_ddp_based   update   update for windows   update   resolve test   code smell   Revert back to init_ddp_connection for backwards compat   Swap to explicit name for property   Add missing speed parity increase for CI variability, fix call counts for child process   Co-authored-by: tchaton thomas@grid.ai",0
5343,Add log_grad_norm hook to LightningModule (#7873),0.82355887,"add .log to lightning module (#3686, #3699, #3701, #3704, #3715)",,1
5344,Rename restore_checkpoint_after_pre_dispatch to restore_checkpoint_after_setup (#11166),0.660342,Deprecated the use of CheckpointConnector.hpc_load() in favor of CheckpointConnector.restore() (#7652),  fast_dev_run can be int   pep   chlog   add check and update docs   logging with fdr   update docs   suggestions   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   fdr flush logs   update trainer.fast_dev_run   codefactor and pre-commit isort   tmp   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Roger Shieh sh.rog@protonmail.ch Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com,0
5345,Update governance.rst (#8956),0.7613034,Updated governance docs,Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: chaton thomas@grid.ai Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com,1
5346,Fix weights_save_path when logger is used + simplify path handling + better docs (#2681),0.7732687,- Changed checkpoints save path in the case of multiple loggers and user-provided weights_save_path from `weights_save_path/name1_name2/version1_version2/checkpoints` to `weights_save_path/checkpoints` ([#12372](https://github.com/Lightning-AI/lightning/pull/12372)),Co-authored-by: chaton thomas@grid.ai Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
5347,get help from docstring (#4344),0.717602,Docs,Co-authored-by: chaton thomas@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
5348,Batch MLFlowLogger requests (#15915),0.7224084,MLFlowLogger now logs hyperparameters and metrics in batched API calls (#15915),"  Rely on ddp plugin for blocking sync behaviour, and skip if we're using manual optimization   debug   Revert ""debug""   This reverts commit ccca6b6b   Expose manual reduce for automatic optimization   Add input arguments   Enable parity test   clean imports   Expose hook after to ensure we reset   Fix naming   add   fix test   resolve on comments   typo   Update tests/trainer/optimization/test_manual_optimization.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update tests/trainer/optimization/test_manual_optimization.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   update on comments   resolve comments   Co-authored-by: SeanNaren sean@grid.ai Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",1
5349,added trainer docs,0.6960794,  trainer:,"  Rewording   Update fairscale install link to include bucket fix, add benchmark results   Added percentage gain   Address codereview   Update docs/source/multi_gpu.rst   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update multi_gpu.rst  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",0
5350,Update warnings for available accelerators not being used (#11909),0.6880291,- Avoided fallback on CPU if no devices are provided for other accelerators ([#12410](https://github.com/PyTorchLightning/pytorch-lightning/pull/12410)),"  Add stuff   Change metrics documentation layout   Change testing utils   Replace len(.shape) with .ndim   More descriptive error message for input formatting   Replace movedim with permute   Style changes in error messages   More error message style improvements   Fix typo in docs   Add more descriptive variable names in utils   Change internal var names   Break down error checking for inputs into separate functions   Remove the (N, ..., C) option in MD-MC   Simplify select_topk   Remove detach for inputs   Fix typos   Update pytorch_lightning/metrics/classification/utils.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update docs/source/metrics.rst  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   Minor error message changes   Update pytorch_lightning/metrics/utils.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   Reuse case from validation in formatting   Refactor code in _input_format_classification   Small improvements   PEP 8   Update pytorch_lightning/metrics/classification/utils.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update pytorch_lightning/metrics/classification/utils.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update docs/source/metrics.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update pytorch_lightning/metrics/classification/utils.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Apply suggestions from code review  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   Alphabetical reordering of regression metrics   Change default value of top_k and add error checking   Extract basic validation into separate function   Update desciption of parameters in input formatting   Apply suggestions from code review   Co-authored-by: Nicki Skafte skaftenicki@gmail.com   Check that probabilities in preds sum to 1 (for MC)   Fix coverage   Minor changes   Fix edge case and simplify testing   Co-authored-by: Teddy Koker teddy.koker@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: chaton thomas@grid.ai Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com",0
5351,Link to arxiv summary instead of pdf in pruning docs (#10282),0.5637897,Remove unnecessary intermediate layers in Dockerfiles (#5697),"  Rely on ddp plugin for blocking sync behaviour, and skip if we're using manual optimization   debug   Revert ""debug""   This reverts commit ccca6b6b   Expose manual reduce for automatic optimization   Add input arguments   Enable parity test   clean imports   Expose hook after to ensure we reset   Fix naming   add   fix test   uniformize optimizer logic   resolve test   resovle flake8   resolve amp bug   update tests   remove bug   remove optimizer_step in accelerators   typo   update lightning optimizer   set doesn't work with ddp_spawn   resolve flake8   update threshold   ignore pyright   correct codeFactor   remove useless if   remove zer_grad function   simplify step   remove typo   resolve bug   Apply suggestions from code review   update on comments   resolve bugs   remove tests   Update pytorch_lightning/trainer/configuration_validator.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   simplify testing   add more tests   Co-authored-by: SeanNaren sean@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com",0
5352,Fix pickling issues with rich progress bar (#15319),0.7619196,- Fixed a pickling error when using `RichProgressBar` together with checkpointing ([#15319](https://github.com/Lightning-AI/lightning/pull/15319)),  simplify CI horovod   reorder ,1
5353,Fix misleading ModelCheckpoint documentation on every_n_epochs parameter (#10421),0.7719918,Deprecated ModelCheckpoint(every_n_val_epochs) in favor of ModelCheckpoint(every_n_epochs) (#8383),  Added changeable extension variable for model checkpoints   Removed whitespace   Removed the last bit of whitespace   Wrote tests for FILE_EXTENSION   Fixed formatting issues   More formatting issues   Simplify test by just using defaults   Formatting to PEP8   Added dummy class that inherits ModelCheckpoint; run only one batch instead of epoch for integration test   Fixed too much whitespace formatting   some changes   Co-authored-by: rohitgr7 rohitgr1998@gmail.com,1
5354,Removed the deprecated datamodule_checkpointhooks (#14909),0.73211026,Removed deprecation warnings being called for on_{task}_dataloader (#9279),  refactor   solve pyright   remove logging in batch_start functions   update docs   update doc   resolve bug   update   correct script   resolve on comments ,1
5355,update docs for progress bat values (#1253),0.62756705,Run main progress bar updates independent of val progress bar updates in TQDMProgressBar (#12563),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Sean Naren sean.narenthiran@gmail.com,0
5356,Fix main progress bar counter when val_check_interval=int and check_val_every_n_epoch=None (#12832),0.81292355,- Fixed main progress bar counter when `val_check_interval=int` and `check_val_every_n_epoch=None` ([#12832](https://github.com/Lightning-AI/lightning/pull/12832),  Fix exception error from generator to list of valid names   Update pytorch_lightning/plugins/plugin_connector.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
5357,Add tests for GCS filesystem (#7946),0.5206518,Using gfile to support remote directories (#2164), [Bug Fix] Allow logger to support indexing  This should fix #4540   Adding test for indexes for DummyLogger   Apply suggestions from code review   Co-authored-by: chaton thomas@grid.ai   pep8   added test for dummyexperiment   Co-authored-by: chaton thomas@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Sean Naren sean.narenthiran@gmail.com,0
5358,neptune online (#1499),0.48664656,(#16002),  Fixed PYTHONPATH for ddp test model   Removed debug calls   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: chaton thomas@grid.ai Co-authored-by: Sean Naren sean.narenthiran@gmail.com,0
5359,Refactor result handling in training loop (#7506),0.98416173,Refactored result handling in training loop (#7506),  docs logo   use png   cleaning   vectorial   icon   icon   use png   cleaning   aync   Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: chaton thomas@grid.ai,1
5360,Fix broken test_is_picklable with PT1.10 (#9810),0.5924088,- fixed all the .test() calls,  convert memory format   changelog   formatting   suggestions   retrigger tests   Co-authored-by: Nicki Skafte nugginea@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: chaton thomas@grid.ai,0
5361,fix tpu tests,0.736511,Updated logic for checking TPUs availability (#6767),  Improve code quality   black -l 120 -S   Fix pyright error   Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: chaton thomas@grid.ai,1
5362,[bugfix] Resolve PyTorch Profiling for Manual Optimization (#9316),0.7442875,"Resolved schedule step bug for PyTorch Profiler (#6674, #6681)","  add option to step result to do aggregation on a specific device   in dp: do aggregation on root gpu   Update CHANGELOG.md   pep8   trailing whitespace   uncomment DP   more cases   tmpdir   test   note   move to root   move result stupid result object revert to master undo import add ""to"" method to result generalize to try a test try a test Revert ""try a test"" This reverts commit 22e3c1001e6c5774ea18ad925830304c245bf145. Revert ""try a test"" This reverts commit 4d2d8fb2a52d552894809a0cbe51af126d78f070. new test max epochs super epoch end  log in test hanging test undo test initial test that fails on master step end pass step end step end epoch end print step check dev clean up test sanity check wtf is go ing on frustration debugging test test test test test test test test test unused import   dist backend -> accelerator   remove todo   Co-authored-by: Justus Schock justus.schock@rwth-aachen.de Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com",1
5363,ref: moving train loop to own object 2/n (intermediate steps) (#3313),0.83849126,"train loop refactor - moving train loop to own object (#3310, #3312, #3313, #3314)",  initial changes   remove old   init files   add average precision   add precision_recall_curve   add roc   cleaning   docs   pep8   docs   pep8   changelog   examples prune duplicate roc   format   imports   fix   format   flake8   duplicate   fix   flake8   docs   docs   Co-authored-by: Teddy Koker teddy.koker@gmail.com Co-authored-by: Nicki Skafte nugginea@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz,1
5364,[Refactor] Remove _run_evaluation + 3 EvaluationLoop (#8065),0.75233257,remove _evaluate fx (#3197),"  add option to step result to do aggregation on a specific device   in dp: do aggregation on root gpu   Update CHANGELOG.md   pep8   trailing whitespace   move to root   move result stupid result object revert to master undo import add ""to"" method to result generalize to try a test try a test Revert ""try a test"" This reverts commit 22e3c1001e6c5774ea18ad925830304c245bf145. Revert ""try a test"" This reverts commit 4d2d8fb2a52d552894809a0cbe51af126d78f070. new test max epochs super epoch end  log in test hanging test undo test initial test that fails on master step end pass step end step end epoch end print step check dev clean up test sanity check wtf is go ing on frustration debugging test test test test test test test test test unused import   move chlog entry   clean   remove outdated changes   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz",1
5365,refactored init (#206),0.6081137,Refactoring,  Added description of saving using ddp   Added code block example to explain DDP saving logic   Fixed underline   Added verbose explanation   Apply suggestions from code review   Added caveat when using custom saving functions   flake8   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz,0
5366,typo JB,0.43134218,Changed overwrite to True (#16009),  test to make sure behaviour is enforced   test_min_steps_override_early_stopping_functionality   make sure Excepted Behaviour is reproduced   remove pollution from extra logging   update docstring   reduce test time   resolve pep8 ,0
5367,[make] Create Makefile (#4620),0.5093744,Create single file in TensorBoardLogger (#777),  remove auto mode from callbacks   chlog   remove auto mode from callbacks   mode   mode   move back   update docs   update docstrings   docstring warning   fix syntax   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   isort   default to 'auto'   syntax   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
5368,[App] Pass LightningWork to LightningApp (#15215),0.8504709,Lightning App,  Update to latest logging format and modify the accuracy method.   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: chaton thomas@grid.ai,1
5369,Warn user when IterableDataset has len defined (#2437),0.5981039,Disabled sampler replacement when using IterableDataset (#11507),  Organize docstring   Update pl_examples/domain_templates/reinforce_learn_Qnet.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: chaton thomas@grid.ai,0
5370,add rank warning (#1428),0.64958704,- Removed deprecated support for passing the `rank_zero_warn` warning category positionally ([#14470](https://github.com/Lightning-AI/lightning/pull/14470)),  :hammer: minor refactor in trainer.   :hammer: Use finally instead of else   :hammer: revert format   :hammer: check should skip inside try   Co-authored-by: chaton thomas@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
5371,Mark Trainer.run_stage as protected (#11000),0.6570127,"Changed these Trainer methods to be protected: call_setup_hook, call_configure_sharded_model, pre_dispatch, dispatch, post_dispatch, call_teardown_hook, run_train, run_sanity_check, run_evaluate, run_evaluation, run_predict, track_output_for_epoch_end",  deprecate hprams setter method   update chlog   isort   update deprecation warning   Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: chaton thomas@grid.ai Co-authored-by: Ananya Harsh Jha ananya@pytorchlightning.ai Co-authored-by: Roger Shieh sh.rog@protonmail.ch Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
5372,ci: adding Muse app (#16392),0.5637819,Add support for listing Lightning AI apps (#13987), Update cloud_io.py  Solves AttributeError: 'PosixPath' object has no attribute 'startswith'  Update cloud_io.py  Co-authored-by: chaton thomas@grid.ai Co-authored-by: Teddy Koker teddy.koker@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
5373,Remove CUDA_LAUNCH_BLOCKING from Lite tests (#16177),0.6688105,"- Trainer queries the CUDA devices through NVML if available to avoid initializing CUDA before forking, which eliminates the need for the `PL_DISABLE_FORK` environment variable introduced in v1.7.4 ([#14631](https://github.com/Lightning-AI/lightning/pull/14631))",  refactor imports of optional dependencies   fix   fix   fix   fix   fix   flake8   flake8   Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: chaton thomas@grid.ai,0
5374,Filter APEX future warning (#15078),0.6271814,Do not override PYTHONWARNINGS (#4700),,0
5375,Remove deprecated LayerSummary and ModelSummary (#12593),0.73697037,Removed deprecated property ModelCheckpoint.period in favor of ModelCheckpoint.every_n_epochs (#9213),  update codeowners   install   Apply suggestions from code review   Apply suggestions from code review   Co-authored-by: chaton thomas@grid.ai   .   Update .github/CODEOWNERS   Co-authored-by: chaton thomas@grid.ai,1
5376,Fix support for symlink save_dir in TensorBoardLogger (#6730),0.6293639,Create single file in TensorBoardLogger (#777),  update logo   Add files via upload   Add files via upload   Delete lightning_logo-large.svg   Delete lightning_logo.svg   Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: Roger Shieh sh.rog@protonmail.ch Co-authored-by: chaton thomas@grid.ai,0
5377,Add transfer_batch_to_device hook to DataModule (#3038),0.7339053,move prepare_data to data connector (#3307),  docs: default_root_path -> default_root_dir   Apply suggestions from code review   fix   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  update notebook  Co-authored-by: Jethro Kuan jethro.kuan@bytedance.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
5378,add trainer flags nb (#4018),0.74928117,adding Trainer.tune() (#3293),  ci typo   v++   Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com,1
5379,Checkpointing interval (#1272),0.63476175,Resuming from checkpoints (#16167),,0
5380,define app 0.6 (#13699),0.6803696,App,,0
5381,bump typing-extensions (#15405),0.5366164,Refactored setup for typing friendly (#6590),  CI: try increase time limit   try min 3.8   no ex   CI   dep   test   deps   deps   drop   drop   Co-authored-by: chaton thomas@grid.ai,0
5382,Explicitly set which Probot job to run (#14756),0.6055521,System customization syncing for jobs run (#16932),  drop sklearn dependency   scipy   Co-authored-by: chaton thomas@grid.ai Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
5383,Rename PL installation to pip install lightning (#17074),0.61374897,Move lightning module to correct device type when using LightningDistributedWrapper (#6070),"  convert xla tensor to cpu before save   move_to_cpu   updated CHANGELOG.md   added on_save to accelerators   if accelerator is not None   refactors   change filename to run test   run test_tpu_backend   added xla_device_utils to tests   added xla_device_utils to test   removed tests   Revert ""added xla_device_utils to test""   This reverts commit 0c9316bb   fixed pep   increase timeout and print traceback   lazy check tpu exists   increased timeout removed barrier for tpu during test reduced epochs   fixed torch_xla imports   fix tests   define xla utils   fix test   aval   chlog   docs   aval   Apply suggestions from code review   Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: chaton thomas@grid.ai Co-authored-by: Sean Naren sean.narenthiran@gmail.com",0
5384,Sampler (#1318),0.6088511,Did not interfere with a default sampler (#1318),"  Add doc fixes   Remove space   Add performance doc, fix flag   Fix up docs   Add install instructions   Update link   Add section for model parallelism, refactor into section   Address code review   fixed underline   Update docs/source/multi_gpu.rst   Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com   Address code review points   Added caveat, increase performance   Update docs/source/multi_gpu.rst   Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com  Update docs/source/multi_gpu.rst  Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com   Add cross reference   Swapped to just fairscale since new release contains all required code   Revert ""Swapped to just fairscale since new release contains all required code""   This reverts commit 21038e72  Update docs/source/multi_gpu.rst  Co-authored-by: chaton thomas@grid.ai  Fairscale install has been fixed  Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com Co-authored-by: chaton thomas@grid.ai",0
5385,Fix flaky test caused by weak reference (#14157),0.5834885,Enabling val/test loop disabling (#2692),  set   cut   env   oonce   env   env   env ,0
5386,BYOC: fix default types for cluster instance types (#14260),0.6531377,Default values and parameter names for Lightning AI BYOC cluster management (#14132),"  Added the function for downloading the badge locally, replacing the url   Fixed the pep8 errors, pointed out during pull request   Update setup.py   refactor   format   test   Added Doctest on the functions   test   test   fix   format   fix   fix   prune   fiix   fiix   flake8   fix   imports   imports   imports   fixx   impoets   win   min   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz",0
5387,Use the strategy's root_device instead of the LightningModule's device property (#11734),0.7019153,Move lightning module to correct device type when using LightningDistributedWrapper (#6070),"  Allow plugin to be chosen via string   Fix implementation, add tests   Fix codefactor issues   Added missing env patch   Skip test for windows   Reword reason   Add skip to invalid test   Create required_plugins function, move sharded amp requirement to plugin   Pass AMPType, fix setter for apex   Better doc strings   Add exception when using apex   Add trainer available_plugins function, warn user when plugins have been added automatically with option to override behaviour   Fixed pep8 indent   Fix codefactor issues   Add env variables   Update pytorch_lightning/cluster_environments/cluster_environment.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   Addressed code review   Update pytorch_lightning/plugins/plugin_connector.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/plugins/plugin_connector.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/plugins/plugin_connector.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   Addressed more code review feedback   Fixed docstrings   Swapped to verbose runtime error   Apply suggestions from code review   Apply suggestions from code review   Update pytorch_lightning/plugins/sharded_plugin.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   Change name   Pass trainer to plugins that may require it   Fix sharded plugin   Added test to ensure string sharded works   Removed trainer typing as this breaks pep8   Fixed doc issues   Fixed tests   Co-authored-by: chaton thomas@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",1
5388,Update psutil requirement from <5.9.4 to <5.9.5 in /requirements (#16372),0.64767027,The psutil package is now required for CPU monitoring (#17010),  create window size dynamically.   pep8   Co-authored-by: chaton thomas@grid.ai,0
5389,feat: finishing #4258 in parts (#4263),0.5739715,"Epoch 3:  50%|█████     | 16/32 [00:36<01:32, 23.12it/s]",  upgrade min deps   unused   replace torchvision and torchtext   loggers   freeze pip   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka Borovec jirka.borovec@seznam.cz Co-authored-by: Sean Naren sean.narenthiran@gmail.com,0
5390,fix type for log_gpu_memory (#6031),0.6680639,- Removed the deprecated `log_gpu_memory` argument from the `Trainer` constructor ([#12657](https://github.com/Lightning-AI/lightning/pull/12657)),,0
5391,Add Training Type Plugins Registry (#6982),0.7698044,Automatically set sync_batchnorm for training_type_plugin (#6536),  add test   resolve logging bug   update   resolve pep8   resolve tests   Co-authored-by: Roger Shieh sh.rog@protonmail.ch,1
5392,docs (#1010),0.7466903,Docs,  update logging docs   experiment   add decorators to base and csv logger methods   fix   doc fix   update docs   update docs   Update pytorch_lightning/loggers/base.py   Co-authored-by: chaton thomas@grid.ai,1
5393,fix logged keys in mlflow logger (#4412),0.6524745,"Our logging mechanism previously supported log(""key"", {""something"": 123}) (not using log_dict). However, this added significant complexity to the implementation with little benefit, as these keys could not be monitored by our Callbacks and most logger implementations do not support this notation. If you were using this feature with a compatible logger, you can still publish data directly to the Logger using self.logger.log_metrics().",  add LightningOptimizer   typo   add mock closure   typo   remove logic in optimizer_step   update   update   update   desactivate LightningOptimizer for hovorod   resolve flake   typo   check optimizer name   change name   added backward to LightningOptimizer   remove use_lightning_optimizer   move update   simplify init   resolve comments   resolve bug   update   update   resolve bugs   resolve flake8   set state   work manual_optimizer_step   add doc   add enable_pl_optimizer   make optimizer_step   add make_optimizer_step   add examples   resolve test   add test_optimizer_return_options_enable_pl_optimizer   add enable_pl_optimizer=True   update   update tests   resolve bugs   update   set Trainer to False   update   resolve bugs   update   remove from doc   resolve bug   typo   update   set to True   simplification   typo   resolve horovod   unwrap horovod   remove Optimizer   resolve horovod   move logic to amp_backend   doesn't seem to be pickable   update   add again   resolve some bugs   cleanup   resolve bug with AMP   change repr   round at -12   udpate   update   update   remove from horovod   typo   add convert_to_lightning_optimizers in each accelerators   typo   forgot   forgot a convert_to_lightning_optimizers   update   update   update   increase coverage   update   resolve flake8   update   remove useless code   resolve comments + add support for LightningOptimizer base class   resolve flake   check optimizer get wrapped back   resolve DDPSharded   reduce code   lightningoptimizer   Update pytorch_lightning/core/optimizer.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   Update pytorch_lightning/core/lightning.py   remove reference to step function   Apply suggestions from code review   update on comments   resolve   Update CHANGELOG.md   add back training_step in apex and native_amp   rename optimizer_step   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: Sean Naren sean.narenthiran@gmail.com,0
5394,CI: install more OS (#14660),0.44859636,The point of this release is more bug fixes ahead of v 1.0.0. We now have CI tests on TPU thanks to @zcain117 from Google! :slightly_smiling_face:,"  drop fairscale for PT <= 1.4   fix   Add extra check to remove fairscale from minimal testing if using minimal torch version 1.3   Update ci_test-full.yml   Update gym to .3 to see if this fixes examples CI   Update omegaconf to minimum for hydra v1.0   Revert ""Update gym to .3 to see if this fixes examples CI""   This reverts commit 4221d4b9  Revert ""Update omegaconf to minimum for hydra v1.0""  This reverts commit 4f579217 Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: SeanNaren sean@grid.ai",0
5395,Release (#467),0.76650923,This release includes:,  freeze DALI   todos   only CI   Update .drone.yml   string   speed   Co-authored-by: Sean Naren sean.narenthiran@gmail.com,1
5396,fixed ckpt tests (#352),0.66000384,"trainer.test(model, ckpt_path=""/path/to/checkpoint.ckpt"")",  fix   fix1   Apply suggestions from code review   Update docs/source/new-project.rst   more fixes   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: chaton thomas@grid.ai,0
5397,updated multiple val dataset docs,0.6238924,val_dataset = trainer.val_dataloaders.dataset,Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com,0
5398,Tune the checkgroup config (#14712),0.61780775,Configuration Validator (#9779),Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com,0
5399,HenryJia: auto-move data decorator (#1905),0.5888102,Removed deprecated auto_move_data decorator (#9231),Correct the text. Co-authored-by: chaton thomas@grid.ai Co-authored-by: Nicki Skafte skaftenicki@gmail.com,0
5400,classification metrics (#4043),0.8727023,Classification metrics overhaul (#4837),Co-authored-by: Sean Naren sean.narenthiran@gmail.com,1
5401,acceleartor fit 1 (#3200),0.5930106,"AcceleratorRegistry.register(""sota_accelerator"", SOTAAccelerator, x=123)",,0
5402,Fix pre-commit isort failure on tests/models/*.py (#5423),0.669948,Dropped official support/testing for PyTorch <1.6 (#8288),  build XLA 1.7   night XLA 1.7   rename   use 1.7   tpu ver ,0
5403,Raise a warning if evaulation is triggered with best ckpt in case of multiple checkpoint callbacks (#11274),0.6098697,- Raised `UserWarning` if evaluation is triggered with `best` ckpt and trainer is configured with multiple checkpoint callbacks ([#11274](https://github.com/PyTorchLightning/pytorch-lightning/pull/11274)),Logging,0
5404,Update validation_epoch_end docs (#12099),0.78628427,validation_end >> validation_epoch_end,Backend unit tests,1
5405,Enable Probot CheckGroup v5.1 (#15763),0.5836882,Configuration Validator (#9779),,0
5406,update readme by v1.2.x (#6728),0.5383289,Updated mlflow with using resolve_tags (#6746),,0
5407,Remove exp_save_path on the LightningModule (#7266),0.9682083,Removed the exp_save_path property from the LightningModule (#7266),,1
5408,swap example (#4029),0.5085001,"merge backends (#3476, #3477, #3478, #3480, #3482)",,0
5409,Better check for programmatic lightningignore (#16080),0.7631849,- Added benchmark for comparing lightning with vanilla implementations,,1
5410,remove deprecated signature for transfer_batch_to_device (#10480),0.7006513,- Removed deprecated signature for `transfer_batch_to_device` hook. The new argument `dataloader_idx` is now required ([#10480](https://github.com/PyTorchLightning/pytorch-lightning/pull/10480)),,1
5411,Update governance.rst,0.7856514,Updated governance docs,,1
5412,Remove legacy pytest markers (#9761),0.68683755,"Removed PyTorch 1.6 support (#10367, #10738)",,0
5413,Fix Enums parsing in generated hparms yaml (#9170),0.7001495,Parsing of enums type hyperparameters to be saved in the haprams.yaml file by TensorBoard and CSV loggers has been fixed and made in line with how OmegaConf parses it (#9170),,1
5414,Simplified setup of optimizers in FSDP (#17309),0.8135685,Generalized Optimizer validation to accommodate both FSDP 1.x and 2.x (#16733),,1
5415,[CLI] Drop ArgumentParser when pickling and save before spawning (#8017),0.70248926,Dropped the LightningCLI ArgumentParser when pickling (#8017),,1
5416,Fix lost compatibility with custom datatypes implementing .to (#2335),0.64885616,Replaced _DataModuleWrapper with __new__ (#7289),  simplify test callback   update   use mock   flake8 ,0
5417,updated doc links,0.70206785,Docs improvements,,1
5418,Add is_wrapped utility function for Fabric (#16953),0.60286194,fabric.launch(),,0
5419,[BUG] estimated_stepping_batches requires distributed comms in configure_optimizers for DeepSpeedStrategy (#13350),0.7218418,"Implemented DeepSpeedPlugin._setup_model_and_optimizers (#10009, #10064)",,1
5420,rename variables nb -> num (#567),0.69721854,and nb_val_batches to num_val_batches (#567),,0
5421,Feature/double precision (#6595),0.6671214,Mixed precision overhaul (#16783),,0
5422,changed lbfgs test,0.5913835,Deprecated the TestTubeLogger (#9065),,0
5423,"Update traitlets requirement from <=5.4.0,>=5.3.0 to >=5.3.0,<5.9.0 in /requirements (#16470)",0.58494663,| Attribute Trainer.tuning                                    | 1.10             | No longer supported         |,,0
5424,Align ddp and ddp-spawn strategies in setting up the environment (#11073),0.792605,- Aligned DDP and DDPSpawn strategies in setting up the environment ([#11073](https://github.com/Lightning-AI/lightning/pull/11073)),,1
5425,Remove access to _short_id in NeptuneLogger (#11517),0.77220935,- Removed access to `_short_id` in `NeptuneLogger` ([#11517](https://github.com/PyTorchLightning/pytorch-lightning/pull/11517)),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
5426,mem clear (#440),0.553867,remember to clear it before continuing,,0
5427,fix tensorboard version (#3132),0.8302475,Improved the error message for installing tensorboard or tensorboardx (#17053),,1
5428,Add RichModelSummary callback (#9546),0.6347185,ModelSummary Callback,,0
5429,Remove reset_train_val_dataloaders from Trainer and move data reloading logic to loop (#9671),0.84065604,"Removed the deprecated Trainer.reset_train_val_dataloaders() in favor of Trainer.reset_{train,val}_dataloader (#16131)",,1
5430,implement forward and update args (#709) (#724),0.61135626,Pass init args to ShardedDataParallel (#9483),  resolve bug   add test docstring   Update tests/trainer/test_dataloaders.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  update test  Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
5431,Remove AcceleratorConnector.root_gpu and deprecate Trainer.root_gpu (#12262),0.7648356,- Removed `AcceleratorConnector.root_gpu` property ([#12262](https://github.com/PyTorchLightning/pytorch-lightning/pull/12262)),,1
5432,Update tests/checkpointing/*.py to use devices instead of gpus or ipus (#11408),0.65548927,Skipped testing with PyTorch 1.7 and Python 3.9 on Ubuntu (#11217),,0
5433,Remove outdated LightningModule animation (#17057),0.7718191,Removed the deprecated LightningDeepSpeedModule (#16041),,1
5434,fix failing on pip (#503),0.66069406,pip install rich,Conflicts: pytorch_lightning/utilities/init.py requirements/extra.txt,0
5435,hotfix: readme formating (#17132),0.5378817,Monir bug fix with print issues and data_loader (#1080),  hydra   omegaconf ,0
5436,Add base IPU dockerfiles (#7252),0.6398388,Graphcore IPU devices,  xla   tpu   fix   fix   flake8 ,0
5437,fix dp issues + update examples and test examples (#3618),0.65813106,"We had a few (subtle) bugs that affected DDP and a few key things in 0.7.2 so we released 0.7.3 to fix them because they are critical for DDP. sorry about that! still, no API changes, but please do skip straight to  0.7.3 upgrade for those fixes",Conflicts: pytorch_lightning/trainer/connectors/precision_connector.py pytorch_lightning/utilities/init.py,0
5438,Remove training_tricks_connector.py (#10112),0.7947502,Removed pytorch_lightning/trainer/training_loop.py (#7985),,1
5439,Update nightly GPU benchmark pool (#12366),0.6961278,GPU training (#2704),  fix import and typo   docs   apex   fix   typo ,0
5440,Decouple utilities from LightningLoggerBase (#11484),0.74641,- Deprecated `LightningLoggerBase.agg_and_log_metrics` in favor of `LightningLoggerBase.log_metrics` ([#11832](https://github.com/PyTorchLightning/pytorch-lightning/pull/11832)),,1
5441,Add separate CI job for slow tests (#10830),0.56857175,CPU stats monitoring,,0
5442,[CLI] Fix registry decorator return value (#9587),0.634881,Deprecated @data_loader decorator  (#926),,0
5443,Fix incorrect tuner error message (#16104),0.6592745,Improved error messages for invalid configure_optimizers returns (#3587),  make device property always return a device with index   pep8   Update test_dtype_device_mixin.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
5444,Check max_time when setting defaults for min/max epochs (#9072),0.66535836,Enforce an epoch scheduler interval when using SWA (#6588),,0
5445,Replace _TORCH_GREATER_EQUAL_DEV_1_10 with _TORCH_GREATER_EQUAL_1_10 (#10157),0.61032426,Disable torch.inference_mode with torch.compile in PyTorch 2.0 (#17215),,0
5446,ci: update coverage scope (#16619),0.5801667,[1.2.0] - 2021-02-18,,0
5447,early stop starts counting once min epochs met,0.66318,EarlyStopping now runs at the end of the training epoch by default (#8286),,0
5448,fix problem with apex closure (#4292),0.65347934,Check if optimizer supports closure (#4981),,0
5449,refactor _configure_schedulers (#11245),0.7046717,refactored dataloader process hook (#3139),,1
5450,Fix Sphinx argument deprecation (#7464),0.5611486,Deprecated prefix argument in ModelCheckpoint (#4765),,0
5451,Typo LightningMoule -> LightningModule (#7038),0.7881671,LightningLite,Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
5452,add default args in Trainer methods doc (#11614),0.8063901,Trainer.add_argparse_args classmethod fixed. Now it adds a type for the arguments (#1147),,1
5453,CI: Use new syntax for setting github output (#15415),0.5496658,"GitHubComponent(api_token=os.environ[""API_TOKEN""])",,0
5454,ci: drop false download artifact (#13473),0.56857324,Deprecated @data_loader decorator  (#926),,0
5455,Fix test suite when running on MPS-enabled hardware (#14708),0.61474,Disabled optimizers setup during testing (#3059),,0
5456,Fix requirements/adjust_versions.py (#7149),0.71135724,Enable PyTorch 1.7 compatibility (#3541),,1
5457,added analysis notebook,0.5953848,Here is an example where users can dynamically create notebooks from the CLI.,,0
5458,Fix rich main progress bar update (#12618),0.73843884,- Fixed an issue in `RichProgressbar` to display the metrics logged only on main progress bar ([#11690](https://github.com/PyTorchLightning/pytorch-lightning/pull/11690)),,1
5459,fixing setup,0.7248384,setup(,,1
5460,Update MPS availability to include check for ARM processors (#13896),0.67851853,The psutil package is now required for CPU monitoring (#17010),,0
5461,update chlog after 1.8.5 (#16125),0.66354084,0.4.0,This reverts commit ba312473,0
5462,add PR template (#204),0.56407684,Allowing decorate model init with saving hparams inside (#4662),,0
5463,Examples: using new API (#1056),0.64920783,A new stateful API,,0
5464,[App] fix lightning open command & better redirects (#16794),0.7261096,Improved the error message when the root LightningFlow passed to LightningApp is missing the run method (#14760),,1
5465,Fix mypy errors attributed to pytorch_lightning.profilers.simple (#14103),0.74262357,+ # pytorch_lightning==1.7.0,,1
5466,CI: debug HPU flow (#13419),0.6164185,Moveed HPU broadcast override to the HPU strategy file (#17011),,0
5467,Fixed NeptuneLogger when using DDP (#11030),0.7810073,Enable NeptuneLogger to work with distributed_backend=ddp (#1753),  resolve bug   update   update   modify one test   remove paramters   update on comments   update changelog   update docstring   Co-authored-by: Nicki Skafte skaftenicki@gmail.com,1
5468,add codeownser for docs - @Felonious-Spellfire (#13717),0.57863605,Introducing CLI commands for apps (#13602)!,,0
5469,"Update s3fs requirement from <=2022.7.1,>=2022.5.0 to >=2022.5.0,<2022.8.3 in /requirements (#14585)",0.58111256,Generalized Optimizer validation to accommodate both FSDP 1.x and 2.x (#16733),,0
5470,"ref: unify slurm and TE under backendPlugin 5/n"" (#4582)",0.736488,"Unify sLURM/TorchElastic under backend plugin (#4578, #4580, #4581, #4582, #4583)",,1
5471,test deprecated - model (#1074),0.7570187,"Deprecated model steps training_end, validation_end and test_end (#1051, #1056)",,1
5472,Deprecate prefix argument in ModelCheckpoint (#4765),0.96712065,Deprecated prefix argument in ModelCheckpoint (#4765),,1
5473,update checkpoint docs (#1016),0.6677907,Resuming from checkpoints (#16167),,0
5474,update example (#5753),0.6059202,Update the Lightning App docs (#13537),,0
5475,Merge pull request #63 from sidhanthholalkere/readme,0.552673,refactored dataloader process hook (#3139),  precision   precision   recall   f beta   confusion matrix   mse   mae   msle   expalained variance   psnr   ssim   text fp fn tp   accuracy   wiki -> sklearn for confusion metrix link   confusion matrix logging note   Co-authored-by: Roger Shieh sh.rog@protonmail.ch,0
5476,allow optimizer fx to return 1 or 2 lists,0.77153397,"    # 2. Return multiple optimizers, same as before",,1
5477,Checking if the parameters are a DictConfig Object (#2216),0.5147146,  validation_if_necessary(),,0
5478,Support fused Adam with mixed precision (#15555),0.6462612,Mixed precision overhaul (#16783),,0
5479,Add dirpath and filename parameter in ModelCheckpoint (#4213),0.76508963,Deprecated filepath in ModelCheckpoint (#4213),,1
5480,update chnagelog (#1169),0.5485265,Update the Lightning App docs (#13537),,0
5481,deprecation warning (#3844),0.99756753,Deprecation warning (#3844),  update test   docstring   Co-authored-by: Ananya Harsh Jha ananya@pytorchlightning.ai,1
5482,Misleading exception raised during batch scaling (#1973),0.6259882,"Raised MisconfigurationException when total length of dataloader across ranks is zero, and give warning when total length is non-zero, but only local rank length is zero. (#9827)","  update logging.rst   logger of choice   Co-authored-by: Nicki Skafte skaftenicki@gmail.com   add metrics reference   trigger ci   Revert ""trigger ci""   This reverts commit 97bf461cf9c00d182b0cc841c6b966a0ca9e85a4. Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: chaton thomas@grid.ai Co-authored-by: Roger Shieh sh.rog@protonmail.ch Co-authored-by: Ananya Harsh Jha ananya@pytorchlightning.ai",0
5483,Fix device placement when setting up FSDP model in Lite (#15822),0.62403244,Changed fsspec to tuner (#4458),,0
5484,Removed double blank,0.5901143,Removed,"  warnings.warn doesn't accept tuples, which causes ""TypeError: expected string or bytes-like object"" when the execution flow gets to this warning. Fixed that.   Try adding a mock test   Try adding a mock test   Co-authored-by: rohitgr7 rohitgr1998@gmail.com Co-authored-by: chaton thomas@grid.ai",0
5485,Add ShardedTensor support in LightningModule (#8944),0.6837337,Deprecated LightningDistributedDataParallel in favor of new wrapper module LightningDistributedModule (#5185),Co-authored-by: chaton thomas@grid.ai,0
5486,"Introduce {Work,Flow}.lightningignore (#15818)",0.7020226,Introduce lightning connect (#14452),  [tests/checkpointing] refactor with BoringModel   [tests/checkpointing] refactor with BoringModel   [tests/checkpointing] refactor with BoringModel   LessBoringModel -> LogInTwoMethods   LessBoringModel -> LogInTwoMethods   LessBoringModel -> TrainingStepCalled   Co-authored-by: chaton thomas@grid.ai Co-authored-by: Ananya Harsh Jha ananya@pytorchlightning.ai,1
5487,Add Trainer max_time argument + Callback (#6823),0.75202376,Deprecate early_stop_callback Trainer argument (#3845),  fix progress bar overshoot   fix updates for partially incomplete main  progress bar when val loop starts   add tests   chlog ,1
5488,Update setup.py,0.7080821,PyTorch 1.5  support,  Add additional check to ensure validation/test step are updated accordingly   Update docs/source/multi_gpu.rst   Co-authored-by: Nicki Skafte skaftenicki@gmail.com  Update docs/source/multi_gpu.rst  Co-authored-by: Nicki Skafte skaftenicki@gmail.com  Update docs/source/multi_gpu.rst  Co-authored-by: Nicki Skafte skaftenicki@gmail.com  Update docs/source/multi_gpu.rst  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
5489,Update progress.py (#2268),0.64441216,- Updated `TQDMProgressBar` to run a separate progress bar for each eval dataloader ([#11657](https://github.com/PyTorchLightning/pytorch-lightning/pull/11657)), Use high refresh rate on Google Colab (#3786)  Automatically override progress_bar_refresh_rate when on Google Colab. Also added a constant IS_COLAB in utilities to check whether it is being run in colab or not. (#3786)   Show a warning instead of overriding when rate is low on colab   Change warning to suggestion and move it   Moved warning to configure_progress_bar instead of on_trainer_init  Apply suggestions from code review  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  add a mock test  Co-authored-by: chaton thomas@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
5490,Fixed docstring for unwatch method (#14920),0.5424398,Replaced _DataModuleWrapper with __new__ (#7289),,0
5491,filter param with no grad (#579),0.5684257,        :param amp_level:,Conflicts: pytorch_lightning/accelerators/accelerator.py pytorch_lightning/accelerators/ddp2_accelerator.py pytorch_lightning/accelerators/ddp_accelerator.py pytorch_lightning/accelerators/ddp_cpu_spawn_accelerator.py pytorch_lightning/accelerators/ddp_hpc_accelerator.py pytorch_lightning/accelerators/ddp_spawn_accelerator.py pytorch_lightning/accelerators/dp_accelerator.py pytorch_lightning/plugins/ddp_plugin.py pytorch_lightning/trainer/connectors/model_connector.py,0
5492,formatting flake8 & isort (#5824),0.5397218,Resolve bug with Finetuning (#5744),  feat(wandb): reinit handled by CLI   fix: typo   docs(wandb): improve formatting   test(wandb): set wandb.run to None   test(wandb): fix tests   style: fix formatting   docs(wandb): fix documentation   Update code markup   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   docs(wandb): update CHANGELOG   test(wandb): init called only when needed   Update CHANGELOG.md   try fix the test   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: chaton thomas@grid.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: rohitgr7 rohitgr1998@gmail.com,0
5493,Change tests/README.md to reflect repo structure change (#13437),0.5642753,Refactoring,"  Encapsulate extracting reference model within the plugin to allow custom wrapper logic to live within the plugin/accelerators   Add missing new lines   Fix call to accelerator   Removed double blank   Use accelerator backend   Handle case where wrapper has not been initialized within the plugin   Added basic get model tests, add better typing   Change model name   Split GPU/DDP test   Add stronger typing, skip ddp test on windows   Fix import   Fix import in dp   Fixed PEP8 definition   Add ddp launcher for ddp testing   Modify accelerator reference model to property, change name to reflect func   Revert property as this is incorrect.=   Revert across accelerators   Modified name to get_model_from_plugin   Code review changes, fix issue with dp   Add verb to function getter   Co-authored-by: chaton thomas@grid.ai",0
5494,Update labeleler config (#16491),0.59212863,Refactored EpochResultStore (#5522),  implementation   init files   more stable reduction   add tests   docs   remove old implementation   pep8   changelog   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Nicki Skafte nugginea@gmail.com Co-authored-by: Teddy Koker teddy.koker@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
5495,[CI SKIP] Fix early stop docs (#3940),0.5751146,Checkpoint and early stopping now work without val. step (#1041),Add batch_arg_name to all calls to _adjust_batch_size,0
5496,Deprecate on_batch_start/on_batch_end callback hooks (#11577),0.7705552,Callback hooks,,1
5497,Sort out the dataloader idx logic for evaluation (#10923),0.6104934,"for data: val_dataloader, test_dataloader, train_dataloader",,0
5498,[fix] Use training type plugin hook when saving (FSDP 1/n) (#6321),0.7055718,"- TrainingTypePlugin.{call_configure_sharded_model_hook, on_reset_*_dataloader, on_save, post_optimizer_step, update_global_step}",,1
5499,add tags argument to MLFlowLogger (#349),0.66448843,MLFlowLogger now accepts run_name as an constructor argument (#7622),,0
5500,release v0.3.6.8,0.7531312,"    version=""0.0.1"",",,1
5501,Advanced GPU Documentation (#7259),0.82585925,GPU training (#2704),,1
5502,[App] Fix e2e test race condition in waiting for openapi (#16381),0.5550862,Updated app testing (#16000),,0
5503,Clean loop fetching usage (#12103),0.61542296,clean up hooks in run_evaluation (#3156),,0
5504,moved cuda flags inside trainer,0.5106368,Removed TrainerProperties mixin and moved property definitions directly into Trainer (#9495),,0
5505,[App] Reduce import depths and add test (#15330),0.57981,| Import pl.core.mixins.DeviceDtypeModuleMixin                                                           | 1.10             | No longer supported |,,0
5506,back 1 tt version,0.65196663,Truncated backpropagation through time (TBPTT) (#16172),"  Fix #4375: Always use trainer.global_step for step   Changelog   Remove superflous use ""epoch""   Update Changelog   Co-authored-by: Nicki Skafte skaftenicki@gmail.com",0
5507,Update ipython[all] requirement from <=8.1.1 to <8.4.1 in /requirements (#14281),0.511371,Refactor cloud dispatch and update to new API (#16456),,0
5508,add Codecov,0.5637207,"adding compute environments (#3837, [#3842)",,0
5509,Add debug flag to TPU Training Plugins (PT_XLA_DEBUG) (#7219),0.65988266,TPU training (#2708),Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
5510,add cloudio pickle patching for unified package (#15309),0.5903028,Dropped the LightningCLI ArgumentParser when pickling (#8017),  Clarify checkpoint deprecation message   Update pytorch_lightning/trainer/connectors/callback_connector.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   Apply suggestions from code review   Apply suggestions from code review   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jeff Yang ydcjeff@outlook.com Co-authored-by: Roger Shieh sh.rog@protonmail.ch Co-authored-by: chaton thomas@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
5511,[CLI] add support for listing apps (#13987),0.8187242,Introducing CLI commands for apps (#13602)!," Implemented ModelSummary total params values  Signed-off-by: George Corrêa de Araújo george.gcac@gmail.com  Fixed documentation, handling modules that are containers for other modules when calculating total params  Signed-off-by: gca george.gcac@gmail.com  Reduced max line length, updated total number of params layout  Signed-off-by: gca george.gcac@gmail.com  Now using only top-level modules of main module to calculate total params  Signed-off-by: gca george.gcac@gmail.com  Added default value for named_modules param in summarize function  Signed-off-by: gca george.gcac@gmail.com  Removed summary function params, removed unused properties  Signed-off-by: gca george.gcac@gmail.com  Changed from np.prod(shape) to numel  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   changelog   Update pytorch_lightning/core/memory.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: chaton thomas@grid.ai Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com",1
5512,Update typing for CometLogger.experiment (#11836),0.80904007,Using .comet.config file for CometLogger (#1913),"  Add prefix parameter in loggers   chlog   pep   patch test   remove args, access via self   try fix the test   try fix the test   try fix the test   prefix test   fix assert has calls   fix assert call Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com",1
5513,Fix checkpoint callback & Trainer.test(_) issue for TPUs (#6654),0.7021089,"Trainer now raises a MisconfigurationException when its methods are called with ckpt_path=""best"" but a checkpoint callback isn't configured (#9841)",  Deprecate prefix in ModelCheckpoint   chlog   Co-authored-by: chaton thomas@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
5514,added slurm managed flag catch for non-slurm peeps,0.60928726,Control SLURM's re-queueing,  Remove the redundant indents in trainer.py   Update pytorch_lightning/trainer/trainer.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: chaton thomas@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Roger Shieh sh.rog@protonmail.ch,0
5515,fixed gan template (#528),0.5540494,We have fixed GAN training - supporting multiple optimizers.,"Show 1999 parameters as 1.9 K and 1000 parameters as 1.0 K, rather than both as 1 K. Co-authored-by: chaton thomas@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Sean Naren sean.narenthiran@gmail.com",0
5516,[bugfix] Prevent on_before_batch_transfer to be called twice (#9715),0.61621946,Raised a MisconfigurationException if batch transfer hooks are overriden with IPUAccelerator (13961),,0
5517,Move logger initialization (#750),0.7202353,moved eval loop logging to loggers (#3408),,1
5518,yapf tests metrics (#5845),0.61339664,Regression metrics (#2221),,0
5519,Remove dead code (#7910),0.69030106,Removed deprecated callbacks (#3979),  increase Parity threshold   typos   increase   increase ,0
5520,Change model name,0.6904446,Renames model steps (#1051),,0
5521,Edited using Colaboratory (#3601),0.49273407,Mixed precision overhaul (#16783),,0
5522,fix deprecated tng and abstract ligntning (#644),0.5637734,Avoid using the deprecated LooseVersion (#16162),  rename   add mnist_datamodule.py   dm   fix   imports   clean   imports   transforms   skip ,0
5523,Update gather_all_tensors to handle tensors of different sizes (#12630),0.7762662,Auto convert tensors to contiguous format when gather_all (#4907),,1
5524,remove obsolete self._device in Trainer (#1849),0.98847985,Removed obsolete self._device in Trainer (#1849),  init fix   init test   more specific dict assert   update changelog   Update tests/checkpointing/test_model_checkpoint.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: chaton thomas@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
5525,Remove reference to outdated Kaggle tutorial (#17390),0.5944116,- Removed the deprecated `pl.strategies.utils.on_colab_kaggle` function ([#16437](https://github.com/Lightning-AI/lightning/pull/16437)),,0
5526,Fix make test (#14273),0.662696,Enabling val/test loop disabling (#2692),  test   poc   add simpler test for ddp   typo   resolve pep8   try coverage testing   trying to add coverage inside ddp   resolve flake8   update   forgot coverage   move .coveragerc   update rcfile path   update   test   update   adding description   add DDPLauncher decorator   add undecorated   push update   update ddp testing   Update tests/backends/launcher.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update tests/backends/launcher.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   update on comments   update on comments   resolve comments   resolve isort   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
5527,Fix typo in loggers.rst (#3557),0.69771475,- pickling errors with loggers (txs @awaelchli),,0
5528,Add fairscale & deepspeed to skipif 4/n (#6281),0.6784986,Support for manual optimization with DeepSpeed (#7970),  add link   typo   Co-authored-by: Nicki Skafte skaftenicki@gmail.com,0
5529,Fix trivial comparison in model checkpoint test (#3464),0.660966,Enable None model checkpoint default (#3669),Co-authored-by: chaton thomas@grid.ai,0
5530,CI: Update docs-related workflows (#13547),0.6497325,Updated governance docs,,0
5531,CI: testing monolotic package (#15213),0.5555155,Implemented ready for components (#16129),,0
5532,packed sequence clarification in train_dataloader (#443),0.6410837,train_dataset = trainer.train_dataloader.loaders.dataset,,0
5533,move lightning_fabric >> lightning/fabric (#16589),0.78451276,Lightning Fabric,,1
5534,"Optimizer Frequencies logic, and new configure_optimizers (#1269)",0.7892203,Working with multiple optimizers (#16539),,1
5535,[App] Maverick (#16701),0.6118197,App,,0
5536,Adding test for legacy checkpoints (#17562),0.6806214,Resuming from checkpoints (#16167),,0
5537,Update changelog after 1.5.7 release (#11204),0.71267235,Full Changelog,,1
5538,[dockers] use inline cache (#4511),0.54151434,"Working with multiple dataloaders (#16800, #16753)",,0
5539,ref: refactor eval loop to use hooks. use test_mode for if so we can split later (#3129),0.98787034,refactor eval loop to use hooks - use test_mode for if so we can split later (#3129),,1
5540,Update defaults for WandbLogger's run name and project name (#14145),0.87289965,"The default project name in WandbLogger is now ""lightning_logs"" (#14145)",,1
5541,Add torch v1.11.0 to the list of versions in adjust_versions.py (#9679),0.7167402,PyTorch 2.0 and torch.compile,,1
5542,CI: Combine conda and full testing into a single workflow (#14387),0.617473,- Full tests that run multiple models in different configs,,0
5543,fix exception raising (#6901),0.5821817,Resolve bug with Finetuning (#5744),,0
5544,ref: ddps train hooks (#3203),0.9508548,DDPs train hooks (#3203),,1
5545,Bump pytest-rerunfailures from 10.2 to 10.3 in /requirements (#15841),0.68738616,Skipped testing with PyTorch 1.7 and Python 3.9 on Ubuntu (#11217),Add masked language modeling (based on Transformers) to community examples Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
5546,Adding hint to the logger's error messages (#16034),0.686943,self.log-ing without a Trainer reference now raises a warning instead of an exception (#9733),  first implementation   add test and changelog   Update tests/loggers/test_base.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   pep8   rounding   increase casting specificity to bool + number   bugfix   changelog formatting   single loop   Update CHANGELOG.md   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: chaton thomas@grid.ai,0
5547,Fix apex installation path in Dockerfile (#11596),0.5906733,Resolved a bug where the install command was not installing the latest version of an app/component by default (#14181),  [metrics] Update SSIM   [metrics] Update SSIM   [metrics] Update SSIM   [metrics] Update SSIM   [metrics] update ssim   dist_sync_on_step True   [metrics] update ssim   Update tests/metrics/regression/test_ssim.py   Co-authored-by: chaton thomas@grid.ai  Update pytorch_lightning/metrics/functional/ssim.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   ddp=True   Update test_ssim.py   Co-authored-by: chaton thomas@grid.ai Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com,0
5548,Standalone Lite: Remaining Utilities (#14492),0.59414047,- Enabled using any Sampler in distributed environment in Lite ([#13646](https://github.com/Lightning-AI/lightning/pull/13646)),  Change version in init.py 1.0.4 -> 1.0.7   fix ver   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
5549,final cleanup for v0.8.0 (#2181),0.68062735,"Cleaning (#5948, #5949, #5950)",,0
5550,(hot fix) Resolve Boring App (#14684),0.5932091,Resolve bug with Finetuning (#5744),  Move init_ddp_connection to DDP Plugin   cluster-env   trainer?   imports   Update ddp_plugin.py   Co-authored-by: Sean Naren sean.narenthiran@gmail.com,0
5551,tests: split examples and pytests (#15774),0.5987153,Skipped testing with PyTorch 1.7 and Python 3.9 on Ubuntu (#11217),  add example for Pytorch Geometric   remove hydra   add docstring   remove description   rename folder   update script to not break test   remove .lock   add Pytorch Geometric to doc   add docstring at the begining   add comments   Update pl_examples/pytorch_ecosystem/pytorch_geometric/README.md   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pl_examples/pytorch_ecosystem/pytorch_geometric/README.md  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pl_examples/pytorch_ecosystem/pytorch_geometric/cora_dna.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  add toml  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai,0
5552,ImageNet Example (#476),0.5158724,"    deserializers = {""x"": Image(224, 224).deserialize}","  Allow ddp plugin to modify optimizer state saving   Rely on the accelerator for optimizer states   Ensure we init the accelerator for the saving function   Better comment for optim state dump   Revert ""Ensure we init the accelerator for the saving function""   This reverts commit af65effa   Added accelerator check to initialize tuner before saving model checkpoint   Simplify comment   Revert ""Added accelerator check to initialize tuner before saving model checkpoint""   This reverts commit f9929c0c   Return single optimizer state to reduce duplication   Fixed docstring   Fixed typing   Fixed comment   Added CHANGELOG.md   Co-authored-by: chaton thomas@grid.ai",0
5553,Fix gradient norm tracking and gradient clipping (#9287),0.77538073,Gradient norm tracking (#16745),"  Allow ddp plugin to move the input to a different device if needed   Swapped name to on_before_forward to align with hooks in the future   Update pytorch_lightning/plugins/ddp_plugin.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   Pass variable arg type to hook, add example   Remove blank space (pep check)   Added blank line   Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",1
5554,fixes Flake8,0.5983959,This release fixes that core issue,  Delay PyPI releasing   Delay PyPI releasing   Co-authored-by: chaton thomas@grid.ai,0
5555,Deprecate Trainer.use_amp (#12312),0.79534715,Deprecated the Trainer.amp_backend property,,1
5556,added slurm doc (#1418),0.5887755,separate SLURM from DDP (#3809),  pytest default color   time   Co-authored-by: chaton thomas@grid.ai,0
5557,Adding doc strings for exceptions raised in trainer.py (#16684),0.62905365,Trainer is now raising a MisconfigurationException instead of a warning if Trainer.{validate/test} is missing required methods (#10016),  Add current_score to ModelCheckpoint.on_save_checkpoint   Update CHANGELOG   [ci skip]  fix  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   fix2   Add test for NaN   Fix failing tests   Simplify line   Add test docstrings   Co-authored-by: chaton thomas@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
5558,Add PredictLoop (#5752),0.72322035,"Refactored prediction loop interface; added new classes PredictionLoop, PredictionEpochLoop (#7700, #8077)",  Update pyproject.toml   Apply isort to files in second level   Co-authored-by: chaton thomas@grid.ai,1
5559,allow decorate model init with saving hparams (#4662),0.9901942,Allowing decorate model init with saving hparams inside (#4662),  test PL examples   minor formatting   skip failing   skip failing   args   mnist datamodule   refactor tests   refactor tests   skip   skip   drop DM   drop DM   Co-authored-by: Sean Naren sean.narenthiran@gmail.com,1
5560,made load from checkpoint flexible (#1307),0.7466265,Checkpoint saving and loading extensibility:,small spelling fix,1
5561,Update LightningModule.all_gather docs (#17046),0.8520965,Update the Lightning App docs (#13537),"Adam's beta 2 parameter was mistakenly referred to as the first order momentum of the gradient, whereas it should be the second order momentum. This has no effect on the correct working of the example.",1
5562,Fix typo in getting started docs (#4882),0.52121013,Docs,"  1) Added experiment_id to NeptuneLogger initialization input arguments. 2) Now function _create_or_get_experiment() overrides ""experiment_name"", ""params"", ""properties"", ""tags"".   Added test case for existing experiment.   Revert ""Added test case for existing experiment.""   This reverts commit 9f3ba2e37b0917ccad18edc980e3763e9cb9d95a.   Added test case for existing experiment.   Fix merging issue.   Moved experiment_id assignment directly to the part with experiment initialization.   Update pytorch_lightning/loggers/neptune.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",0
5563,Use pl.LightningModule in new-project docs (#6656),0.7385645,Deprecated LightningDataParallel in favor of new wrapper module LightningParallelModule (#5670),,1
5564,[feat] Logging refactor 2/n - train (#4495),0.7600773,Refactored logging,  fix state dict   Update docs/source/metrics.rst   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  changelog  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: chaton thomas@grid.ai,1
5565,Updated conda install commands in docs. (#17162),0.4920457,Docs improvements,  addd tests   use boring model   parsing init   chlog   double decorate   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  bug  Co-authored-by: chaton thomas@grid.ai Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Roger Shieh sh.rog@protonmail.ch,0
5566,Resolve: 'DummyExperiment' object does not support item assignment (#10917),0.52042073,"The WandbLogger.name property no longer returns the name of the experiment, and instead returns the project's name (#14145)",,0
5567,Raise RTD doc build warnings as errors (#1823),0.6141904,warning_utils >> warnings,  Add   williamfalcon as owner for API changes   Update .github/CODEOWNERS   Co-authored-by: Teddy Koker teddy.koker@gmail.com  Update CODEOWNERS  Co-authored-by: Teddy Koker teddy.koker@gmail.com Co-authored-by: William Falcon waf2107@columbia.edu,0
5568,"try GH actions cache (#1558, #1624)",0.46827403,Resolve memory leak for evaluation (#6326),  resolve bugs   add should_flush_logs   remove should_flush   should work   update test   use something else   Update pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py   log mock_log_metrics.mock_calls   typo   don't use keys   convert to list   typo   check kwargs   resolve bug   resolve flake8   Co-authored-by: Sean Naren sean.narenthiran@gmail.com,0
5569,Update lm_test_module_mixins.py,0.5465417,| Import pl.core.mixins.DeviceDtypeModuleMixin                                                           | 1.10             | No longer supported |,  Rename distributed_backend to accelerator   Update submit_ddp2_job.sh   Update 05-trainer-flags-overview.ipynb   Co-authored-by: Jeff Yang ydcjeff@outlook.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com,0
5570,use existing logic to configure optimizers in lr_finder (#9789),0.710896,    --optimizer.lr=0.01 \,Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: chaton thomas@grid.ai Co-authored-by: Sean Naren sean.narenthiran@gmail.com,1
5571,Split train- and val progress into separate bars (#16695),0.7646892,now the progress bar has a full bar for the full train + val epochs and a second bar visible only during val.,Fix an error in training_step_end() documentation #4669 Co-authored-by: Sean Naren sean.narenthiran@gmail.com,1
5572,Remove support for logging multiple metrics together (#16389),0.7678504,Un-balanced logging properly supported (#5119),  isolate PL debugger in tests   miss   Co-authored-by: Jeff Yang ydcjeff@outlook.com,1
5573,show mush go on (#7413),0.46310556,"        # 5. ""Truncate""",  Makes automatic optimization a model attribute   Update trainer.py   remove setting property in model   Update pytorch_lightning/core/lightning.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update pytorch_lightning/trainer/trainer.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update trainer.py  Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: Roger Shieh sh.rog@protonmail.ch Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Jeff Yang ydcjeff@outlook.com,0
5574,Fix: Failing test in data_modules(dp) (#5924),0.63720846,"Deprecated DataModule properties: has_prepared_data, has_setup_fit, has_setup_validate, has_setup_test, has_setup_predict, has_teardown_fit, has_teardown_validate, has_teardown_test, has_teardown_predict (#7657)",Co-authored-by: Jeff Yang ydcjeff@outlook.com Co-authored-by: chaton thomas@grid.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
5575,Add datamodule parameter to lr_find() (#3425),0.6806744,tuner.lr_find(...),Co-authored-by: Jeff Yang ydcjeff@outlook.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
5576,fix docs (#2987),0.6210087,This release fixes that core issue,  Fix setup callback hook   Update CHANGELOG.md   Update test_trainer.py   Update test_trainer.py   Update test_trainer.py   fix chlog   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
5577,CI: fix nightly release version (#5260),0.610642,This release fixes that core issue,"  added isort CI job and updated isort config   changed CI check output from files to full diff   added isort pre-commit hook   Added missing first party and restricted files affected by isort   Applied isort to root-level, docs and benchmarks   Apply suggestions from code review   Co-authored-by: Nathan Painchaud nathanpainchaud@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: chaton thomas@grid.ai",0
5578,Add dataclass support to apply_to_collection (#7935),0.69195956,- Added support for dataclasses in `apply_to_collections` ([#11889](https://github.com/PyTorchLightning/pytorch-lightning/pull/11889)),"  logger and api docs   remove gpu_usage_logger, lr_logger   update docstring   fix wandb example   remove step result   charts   add some charts info   Co-authored-by: Teddy Koker teddy.koker@gmail.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com",0
5579,introduce default cluster environment for lightning-specific ddp (#5915),0.70729584,- Added `XLAEnvironment` cluster environment plugin ([#11330](https://github.com/Lightning-AI/lightning/pull/11330)),Co-authored-by: chaton thomas@grid.ai,1
5580,Avoid torchscript export for Metric forward (#4428),0.7180518,Avoid the deprecated onnx.export(example_outputs=...) in torch 1.10 (#11116),  added lambda_closure   move to types   add 2 new tests   make example more complex   add complex example to doc   added more tests   resolve doc   typo   update   update tpu optimizer_step   Apply suggestions from code review   Update pytorch_lightning/core/lightning.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  update  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
5581,Add add_to_queue/get_from_queue for DDP spawn(#7916),0.69466305,"Deprecated add_to_queue, get_from_queue from LightningModule in favor of corresponding methods in the DDPSpawnPlugin (#9118)","  Added abstract precision plugin to expose clip_gradients function, use within accelerator to clip gradients   Exclude model from override, keep optimizer (needed for sharded clip gradients), add override for O2 support apex   Fix doc   Applied codereview changes   Refactored clip function to encapsulate tpu changes with tpu accelerator. Default to standard clip function for vanilla torch   Pass correct grad clip val   Moved var to property   Apply code review suggestions ",0
5582,Fix import deprecation warning (#2800),0.6594004,Re-Enable Logger's ImportErrors (#1938),  wip   update   normalize loss   update test   resolve bug   update test and add TODO   make sure it can be sync   add TODO   update sol ,0
5583,fix loading pkg while setup (#71),0.556993,"Removed PyTorch 1.6 support (#10367, #10738)",  PT 1.8   unfreeze PT   drop nightly from full   add PT 1.8 to workflow   readme table   cuda   skip cuda   test 1.8   unfreeze torch vision   Co-authored-by: ydcjeff ydcjeff@outlook.com Co-authored-by: chaton thomas@grid.ai Co-authored-by: Nicki Skafte skaftenicki@gmail.com,0
5584,[changelog] Update Changelog on release v1.2.3 (#6444),0.7496184,Full Changelog,  update contributing   formatting ,1
5585,Loss keys (#387),0.6217834,Key updates,  [make] Create Makefile   exclude makefile   contributing info   rm .run_local_test.sh ,0
5586,Renamed the DDPSpawnPlugin to DDPSpawnStrategy (#11145),0.86410224,Merged the implementation of DDPSpawnStrategy into DDPStrategy and removed DDPSpawnStrategy (#14952),  update changelog after 1.0.6   fix formatting ,1
5587,Removed redundant line. (#140),0.61334306,The *_epoch_end hooks were removed (#16520), Update CONTRIBUTING.md  Small typo correction.  Update .github/CONTRIBUTING.md  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Roger Shieh sh.rog@protonmail.ch,0
5588,[App] Add utility to get install command for package extras (#15809),0.74806607,- Added support for adding requirements to commands and installing them when missing when running an app command ([#15198](https://github.com/Lightning-AI/lightning/pull/15198),"  Added test/fix for sync_dist raising NotImplementedError   Fixed comments/formatting   Revert base class change, enforce sync tensors across accelerators, added GPU test ",1
5589,Support teardown hook on DataModule (#4673),0.7381317,Updated hooks arguments - breaking for setup and teardown (#2850),  wip   wip check how many tests break   wip   resolve some bugs   resolve more bugs   resolve 2 bugs   resolve   temp fix   update   remove useless code   remove result   try to resolve bug   update changelog   formatting   remove pl   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Sean Naren sean.narenthiran@gmail.com,1
5590,:bug: fix dm prepare_data call (#2811),0.6151381,Changed warnings and recommendations for dataloaders in ddp_spawn (#6762),  move value to cpu to save memory   update   move to cpu   try something   update   update   add back out_dict.update({k: v})   add move_metrics_to_cpu   update   Update pytorch_lightning/utilities/memory.py   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com   resolve comments   Update pytorch_lightning/core/step_result.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Sean Naren sean.narenthiran@gmail.com,0
5591,Profile batch transfer and gradient clipping hooks (#14069),0.66354084,gradient_clip -> gradient_clip_val    ,"  resolve bug   add self._running_manual_optim   update   update tests   update lightning module   resolve bug   update tests   update   resolve pep8   update   replace by ddp_spawn   temporary fix   update   update   move update to training_loop   make both ddp_spawn   introduce manual_optimizer_step   update changelog   added changelog wrong place   add force_optimizer_step   update docstring for tests   update optimizer_step   update zero_grad   resolve flake8   move update into manual_optimizer_step   add zero_grad   remove zero_grad tests   remove manual_backward in AMP, it doesn't help   update   loosen tests   update   update doc   add TODO   Removed unnecessary get model from native amp   Remove try except with pytest raise   Add seed, clean up imports, remove try catch to reproduce error   update code   update test   revert back   formatting   Update pytorch_lightning/core/lightning.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: SeanNaren sean@grid.ai Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",0
5592,fix docs on saving checkpoints manually (#1373),0.7187902,Disable saving checkpoints if not trained (#4372),  fix mock pkgs in docs   sphinx   CI   Co-authored-by: chaton thomas@grid.ai,1
5593,add num_classes argument to confusion matrix (#3450),0.65063834,+ num_devices = trainer.num_devices,  search for attribute in datamodule if not found elsewhere   add test for datamodule   add lightning_getattr test for datamodule   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   Update CHANGELOG.md   Update CHANGELOG.md   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
5594,Prune EvalModelTemplate from callbacks and utilities (#6018),0.6138545,from lightning.pytorch.callbacks import GradientAccumulationScheduler,"  [#4411] fix gpu_log_memory with mlflow logger   sanitize parenthesis instead of removing for all loggers   apply regex for mlflow key sanitization   replace ',' with '.' typo   add single warning and test   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: chaton thomas@grid.ai",0
5595,removed dead code,0.7781536,- Removed the deprecated code in:,Co-authored-by: Nicki Skafte skaftenicki@gmail.com,1
5596,Fix lr scheduler state not being dumped to checkpoint in deepspeed strategy (#11307),0.83941823,- Fixed the lr-scheduler state not being dumped to checkpoint when using the deepspeed strategy ([#11307](https://github.com/PyTorchLightning/pytorch-lightning/pull/11307)),  replace MisconfigurationException with warning   update test   check raising UserWarning   Co-authored-by: chaton thomas@grid.ai Co-authored-by: Nicki Skafte skaftenicki@gmail.com,1
5597,[CLI] Shorthand notation to instantiate optimizers and lr schedulers [2/3] (#9565),0.7881258,Support shorthand notation to instantiate optimizers and learning rate schedulers (#9565),  changes   fix spelling   small note   trying to fix ddp test   fix ddp   fix for test   suggestion   CHANGELOG   Update pytorch_lightning/metrics/metric.py   Co-authored-by: chaton thomas@grid.ai Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: Sean Naren sean@grid.ai,1
5598,document lightiningmodule better (#2920),0.5487751,class LitModel(L.LightningModule):,  skip on fast dev   fix error   changelog   fix recursive issue   combine tests   pep8   move logic to base funcs   fix mistake   Update pytorch_lightning/tuner/lr_finder.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  pep  Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: Nicki Skafte nugginea@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: chaton thomas@grid.ai,0
5599,fixing miss-leading tested acc values (#5876),0.61225474,Removed no return warning from val/test step (#6139),  Add missing load functionality in hpc   Add general file load for hpc   Add mark in CHANGELOG   Fix Typo Lihgtning   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Refactor line separation  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   Fix entangled fixation commit   Fix naming of restore_model_states   Fix amp restore place   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: chaton thomas@grid.ai,0
5600,Clean up fairscale imports (#14476),0.70423687,import argparse,  [dockers] install nvidia-dali-cuda100   Apply suggestions from code review   build DALI   build DALI   build DALI   dali from source   dali from source   use binaries   qq   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai Co-authored-by: Sean Naren sean.narenthiran@gmail.com,1
5601,set logger level for package (#1718),0.6305791,Versioned logs for all loggers.   ,Co-authored-by: stef-ubuntu stef@webempath.com Co-authored-by: Jeff Yang ydcjeff@outlook.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com,0
5602,ref: remove _evaluate fx (#3197),0.97376215,remove _evaluate fx (#3197),Co-authored-by: Jeff Yang ydcjeff@outlook.com,1
5603,drop unused test with result api (#5058),0.64528143,Removed callback metrics from test results obj (#2994),PyTorch Forecasting is a new library that is designed for time series forecasting practitioners and researchers alike. It is based on the awesome work on PyTorch Lightning. Thanks a lot for creating such an asset! Have a look at the documentation for more information. Co-authored-by: chaton thomas@grid.ai Co-authored-by: Jeff Yang ydcjeff@outlook.com,0
5604,remove unused random_split import from tutorial (#15716),0.53635514,Did not interfere with a default sampler (#1318),  note   Update docs/source/metrics.rst   Co-authored-by: chaton thomas@grid.ai Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: Jeff Yang ydcjeff@outlook.com,0
5605,[bugfix] Prevent a DDP failure using copy (#9239),0.59380704,DDP Debugging Improvements,  accelerator docs   accelerator docs ,0
5606,use log no print (#940),0.6852956,switched from print to logging,  ref: unify slurm and TE under backendPlugin 4/n   ref: unify slurm and TE under backendPlugin 5/n ,0
5607,Remove calls to internal dev debugger in training- and eval loop (#9188),0.6309581,Refactored trainer _run_* functions and separate evaluation loops (#8065),,0
5608,[Metrics] MetricCollection (#4318),0.69169044,"docs for all Metrics (#2184, #2209)",,0
5609,Remove deprecated optimizer argument from manual_backward (#8287),0.8134046,Removed the deprecated optimizer_idx from training_step as an accepted argument in manual optimization (#8576),  ref: unify slurm and TE under backendPlugin   ref: unify slurm and TE under backendPlugin ,1
5610,Fix sync,0.6813301,Remove .item which causes sync issues (#1254),  added log_dir shortcut to trainer properties for writing logs   added log_dir shortcut   added log_dir shortcut   added log_dir shortcut   added log_dir shortcut   added log_dir shortcut   added log_dir shortcut   added log_dir shortcut   added log_dir shortcut ,0
5611,[CLI] Avoid warning when configure_optimizers will not be overridden (#9583),0.69911784,Improved error messages for invalid configure_optimizers returns (#3587),,0
5612,Fix #618 Change papi to api (#619),0.5810549,"Removed PyTorch 1.6 support (#10367, #10738)",,0
5613,notices (#4118),0.67770797,Deprecation warning (#3844),,0
5614,feature(docs/app/lit_tabs): add works (#15731),0.5756061,"Show a message when BuildConfig(dockerfile=""..."") is passed but a Dockerfile file is already present in the Work (#15799)",  add congratulations at the end of our notebooks   udpate image ,0
5615,CI: upload report only on failer (#5086),0.58699703,Enabled cp (upload) at project level (#16631),"  add MNIST DALI example, update README.md   Fix PEP8 warnings   reformatted using black   add mnist_dali to test_examples.py   Add documentation as docstrings   add nvidia-pyindex and nvidia-dali-cuda100   replace nvidia-pyindex with --extra-index-url   mark mnist_dali test as Linux and GPU only   adjust CUDA docker and examples.txt, fix import error in test_examples.py   adjust the GPU check   Exit when DALI is not available   remove requirements-examples.txt and DALI pip install   Refactored example, moved to new logging api, added runtime check for test and dali script   Patch to reflect the mnist example module   add req.   Apply suggestions from code review   Removed requirement as it breaks CPU install, added note in README to install DALI   add DALI to Drone   test examples   Apply suggestions from code review   imports   ABC   cuda   cuda   pip DALI   Move build into init function   Co-authored-by: SeanNaren sean@grid.ai Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Sean Naren sean.narenthiran@gmail.com",0
5616,TensorBoardLogger sub_dir parameter for grouping logs (#6195),0.6964644,Changed the default logger to TensorBoardLogger (#609),,0
5617,activated color in all pytest runs (#4254),0.56813735,Removed pytorch_lightning/trainer/evaluation_loop.py (#8056),Co-authored-by: chaton thomas@grid.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
5618,Fix Docker Pipeline (#1765),0.55036426,DDPs train hooks (#3203),"  update logging   solve more bugs   replace Mapping by Dict   update on comments   resolve pep8   Apply suggestions from code review   Co-authored-by: ananthsub ananth.subramaniam@gmail.com  Update pytorch_lightning/trainer/connectors/logger_connector/epoch_result_store.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   update on comments   typo   update for coverage   update test   update   Update tests/models/test_hooks.py   Co-authored-by: Sean Naren sean.narenthiran@gmail.com  Update tests/models/test_hooks.py  Co-authored-by: Sean Naren sean.narenthiran@gmail.com   update on comments   remove deepcopy   remove useless look for   another small optim   extra optim   remove lastest optim, can be source of bug   resolve bug   add docstring   optimize coverage   Update pytorch_lightning/trainer/connectors/logger_connector/epoch_result_store.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/trainer/connectors/logger_connector/epoch_result_store.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update tests/trainer/logging_tests/test_distributed_logging.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/trainer/evaluation_loop.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update tests/trainer/logging/test_logger_connector.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update tests/trainer/logging_tests/test_train_loop_logging_1_0.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   update on comments   update   update on comments   update parity speed   get it down to 0.65   update   0.8 max_dif   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: William Falcon waf2107@columbia.edu",0
5619,CI: Update mypy workflow (#13574),0.5443767,refactored dataloader process hook (#3139),  update PR template   Update .github/PULL_REQUEST_TEMPLATE.md   Co-authored-by: Roger Shieh sh.rog@protonmail.ch  Apply suggestions from code review  Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com Co-authored-by: chaton thomas@grid.ai Co-authored-by: Roger Shieh sh.rog@protonmail.ch Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com,0
5620,Added return in convert_zero_checkpoint_to_fp32_state_dict (#17342),0.5983279,"Validation DataLoader 0:  38%|███      | 12/32 [00:12<00:20,  1.01s/it]",  Fixed early stopping for Horovod   Refactored to sync_dist_if_available   Bump min Horovod version to support hvd.is_initialized   Changelog   Added back change for Horovod   Removed redundant checks for initialization   Implement metrics gathering for Horovod   Added test for EvalResult   Renamed ddp_sync_on_step -> dist_sync_on_step   Added metric test for Horovod   Added option pass callable allgather function to metric base class   Added dist_sync_fn   Fixed calls to private _sync_dist   Fixed Horovod test   Added sync_tensor to the distributed backend   Skip Windows   Insert test path   Removed redundant import   Updated drone   Unset HOROVOD_GPU_ALLREDUCE   Unset   No cache dir   No uninstall   Unset variables   Uninstall Horovod during initialization   Replaced more references to ddp_sync_on_step   Fixed imports   Fixed attribute   Added back default   Lint   Added back docstring   Made gather_all_tensors default   Added whitespace   Update tests/models/test_horovod.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/metrics/metric.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update CHANGELOG.md  Co-authored-by: Teddy Koker teddy.koker@gmail.com Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
5621,update readme typo (#1292),0.5455977,Changed overwrite to True (#16009),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Sean Naren sean.narenthiran@gmail.com,0
5622,Fix sanity check for RichProgressBar (#10913),0.7750851,- Fixed an issue with `RichProgressBar` not resetting the internal state for the sanity check progress ([#15377](https://github.com/Lightning-AI/lightning/pull/15377)),  update changelog   update   Co-authored-by: chaton thomas@grid.ai Co-authored-by: Sean Naren sean.narenthiran@gmail.com,1
5623,Compositional metrics (#5464),0.64496124,Classification metrics overhaul (#4837),  resolve a bug   resolve a bug   remove todo   resolve more bugs   update tests   Update pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py   Co-authored-by: Sean Naren sean.narenthiran@gmail.com  Update pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py  Co-authored-by: Sean Naren sean.narenthiran@gmail.com  resolve pyright  Co-authored-by: Teddy Koker teddy.koker@gmail.com Co-authored-by: Sean Naren sean.narenthiran@gmail.com,0
5624,ref: remove weight loading hack for ddp_cpu (#3808),0.9883961,remove weight loading hack for ddp_cpu (#3808),  is_picklable: catch AttributeError (addresses #3771)   edit   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: chaton thomas@grid.ai,1
5625,Fix for incorrect run on the validation set with overwritten validation_epoch_end and test_end (#1353),0.7425052,validation_end >> validation_epoch_end,,1
5626,Remove deprecated property configure_slurm_dpp from accelerator connector (#10370),0.87292856,- Removed deprecated property `configure_slurm_dpp` from accelerator connector ([#10370](https://github.com/PyTorchLightning/pytorch-lightning/pull/10370)),Co-authored-by: chaton thomas@grid.ai,1
5627,Rename Strategy.reduce to Strategy.all_reduce in Lite (#16370),0.89219844,Renamed Strategy.reduce to Strategy.all_reduce in all strategies (#16370),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jeff Yang ydcjeff@outlook.com,1
5628,Fix version freeze comparison (#13057),0.7234599,Setup: added requirement freeze for the next major version (#14480),  extend release testing   Drone   also PR to release   actions versions ,1
5629,Upgrade GPU CI to PyTorch 1.13 (#15583),0.68687844,refactored GPU backend __step (#3120),  Update metric.py   add test   Update CHANGELOG.md   Update test_metric_lightning.py   Update test_metric_lightning.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
5630,A minor syntax correction (#8925),0.6698785,Syntax changes are: ,  switch to 1.6   readme   1.7   back to normal [ci skip]   horovodrun --verbose   try with apex   add apex test   change base   description   test with 1.7   back to 1.6   no gradient_clip_val   re-add gradient_clip_val   no amp   temp skip torch.cuda.amp + horovod test   Apply suggestion from code review   Co-authored-by: Sean Naren sean.narenthiran@gmail.com   Fix formatting   ddp   Moved extended model outside of function to prevent pickling issue for drone   typo   resolve bug   extract automatic_automization   Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: SeanNaren sean@grid.ai Co-authored-by: chaton thomas@grid.ai,0
5631,[App] Reserve APP_SERVER_PORT in cloud port allocation (#16782),0.5795307,Refactor cloud dispatch and update to new API (#16456),  add a note about argparse   update   Co-authored-by: Jeff Yang ydcjeff@outlook.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
5632,Fix typo in _block_parallel_sync_behavior docstring (#13451),0.56503725,Remove .item which causes sync issues (#1254),  Add CheckpointConnector commentaries   Fix comment format   Fix save/load schema as function comments   Co-authored-by: chaton thomas@grid.ai,0
5633,rename logging -> loggers (#767),0.79469424,Removed LoggerStages (#5673),  replace module_arguments refernces   update hparams docs   add missing save_hyperparameters in example   deprecate instead of remove   Update docs/source/hyperparameters.rst   Co-authored-by: chaton thomas@grid.ai  Update docs/source/hyperparameters.rst  Co-authored-by: Teddy Koker teddy.koker@gmail.com Co-authored-by: chaton thomas@grid.ai Co-authored-by: Sean Naren sean.narenthiran@gmail.com,1
5634,Remove legacy configurations (#10829),0.68636316,Removed deprecated: (#2760),  Add fsspec to tuner   suggestions   pathlib   pep   missed pep ,0
5635,split restore_training_state into logical parts [2 / 2] (#7900),0.6225476,Refactored training loop (#2336),  Disable training when limit_train_batches=0   chlog   pep   limit_train_batches   BoringModel   Co-authored-by: Roger Shieh sh.rog@protonmail.ch,0
5636,Unit test for CLI with subcommands and a common default config file (#12061),0.5850204,- Full tests that run multiple models in different configs,  Disable saving checkpoints if not trained   chlog   update test   fix   Co-authored-by: chaton thomas@grid.ai,0
5637,Properly handle parent modules w/ parameters in BaseFinetuning callback (#7931),0.5557923,  * Removed `opt_idx` argument from `BaseFinetuning.finetune_function` callback method,  adding tests   wip   update   Update tests/trainer/test_trainer.py   Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: Sean Naren sean.narenthiran@gmail.com,0
5638,fixed comet -> mlflow typo in visualize/experiment_managers docs (#14843),0.6747701,Using .comet.config file for CometLogger (#1913),  fix   flags   remove defaults ,0
5639,Update optimization docs (#11174),0.78066844,Refactored optimizer (#4658),  introducing new logging object   typo   typo   Update pytorch_lightning/trainer/logging.py   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com  Update pytorch_lightning/trainer/logging.py  Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com   update on comments   update on comments   add more doctstring   Update pytorch_lightning/core/lightning.py   Co-authored-by: Sean Naren sean.narenthiran@gmail.com   resolve on comments   solve pyright   Update pytorch_lightning/trainer/connectors/logger_connector/epoch_result_store.py   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com   update on comments   Update pytorch_lightning/trainer/connectors/logger_connector/epoch_result_store.py   Co-authored-by: Sean Naren sean.narenthiran@gmail.com  update on comments  Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Sean Naren sean.narenthiran@gmail.com,1
5640,Loop and test restructuring (#9383),0.7441883,Refactored Loops,  detach on buffer   doc update   remove file   changelog   suggestions   Update docs/source/metrics.rst   Co-authored-by: Teddy Koker teddy.koker@gmail.com   fix for 4266   Update docs/source/metrics.rst   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update CHANGELOG.md  Co-authored-by: Teddy Koker teddy.koker@gmail.com Co-authored-by: chaton thomas@grid.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Ananya Harsh Jha ananya@pytorchlightning.ai Co-authored-by: Roger Shieh sh.rog@protonmail.ch Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
5641,[Bugfix] Apply untoggle_optimizer when result is None (#5983),0.6961323,Changed calling of untoggle_optimizer(opt_idx) out of the closure function (#7563),  logging -> logging_tests   warnings -> warnings_tests   Co-authored-by: Jeff Yang ydcjeff@outlook.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
5642,Use root_device in XLAStatsMonitor callback (#11749),0.62672466,xla_device_utils >> xla_device,  move unscale within Native plugin   remove gradient tracking from lightning backward   forgot trainer.fit   typo   update   cleanup   set to 1.6   typo   skip if below 1.6 strict   update changelog   remove useless code   Update tests/plugins/test_amp_plugin.py   Co-authored-by: Sean Naren sean.narenthiran@gmail.com  Update tests/plugins/test_amp_plugin.py  Co-authored-by: Sean Naren sean.narenthiran@gmail.com   update changelog   Update CHANGELOG.md   Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: Jeff Yang ydcjeff@outlook.com,0
5643,Remove torch<=1.4.0 checks (#5998),0.6900871,Prevent modification of torch.backends.cudnn.benchmark when Trainer(benchmark=...) is not set (#13154),  better explanation around tpu_cores   more details on tpu training   Apply suggestions from code review   Co-authored-by: Jeff Yang ydcjeff@outlook.com,0
5644,Skip horovod tests with cuda errors (#16276),0.6115753,horovod deprecation (#16141),"  true final value of global step   ch check   tests   save each validation interval   wip   add test   add test   wip   fix tests, revert old edits, fix merge conflicts, update doctests   test + bugfix   sort files   format test   suggestion by ananth   added changelog   naming   docs   example   suggestion   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   fix test   pep   pep   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com",0
5645,Fix typo in results.rst (#3277),0.5723737,Changed overwrite to True (#16009),,0
5646,docs: add link to lightning-colossalai (#16945),0.719974,LightningCLI additions:,  fix   resolve CodeFormatter   Update pytorch_lightning/loggers/base.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
5647,Add Python 3.10 badge (#15681),0.5805549,"PyTorch 1.10 and Python 3.7 no longer supported (#16492, #16579)",,0
5648,Fix broken links in README (#17292),0.54573655,- Fixed a bug where the upload files endpoint would raise an error when running locally ([#14924](https://github.com/Lightning-AI/lightning/pull/14924)),  docs update   update callbacks docs   docs   notebook examples   warning   line lenght   update deprecation   Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: Roger Shieh 55400948+s-rog@users.noreply.github.com,0
5649,Skip hanging spawn tests (#10838),0.5528355,Enabling val/test loop disabling (#2692),,0
5650,added cpu example,0.6383951,Multiple CPU processes,  timeout for tpu check   added tests   updated CHANGELOG.md   fixed windows tests   Update pytorch_lightning/utilities/xla_device_utils.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  requested changes  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
5651,Regression metrics (#2221),1.0,Regression metrics (#2221),Co-authored-by: Jeff Yang ydcjeff@outlook.com,1
5652,Add additional check to ensure apex is not used with sharded,0.5006104,Ensure that clip gradients is only called if the value is greater than 0 (#6330),  lock cuda version   back to normal ,0
5653,reqs,0.7091334,"    },",,1
5654,replace upload (#5765),0.6775987,Restore original loaders if replaced by entrypoint (#8885),  Add functional multiclass AUROC metric   Add multiclass_auroc tests   fixup! Add functional multiclass AUROC metric   fixup! fixup! Add functional multiclass AUROC metric   Add multiclass_auroc doc reference   Update CHANGELOG   formatting   Shorter error message regex match in tests   Set num classes as pytest parameter   formatting   Update CHANGELOG   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com,0
5655,Fix slack link (#7452),0.6232997,Updated hooks arguments - breaking for setup and teardown (#2850),"  prepare for 1.7 support [ci skip]   tpu [ci skip]   test run 1.7   all 1.7, needs to fix tests   couple with torchvision   windows try   remove windows   1.7 is here   on purpose fail [ci skip]   return [ci skip]   1.7 docker   back to normal [ci skip]   change to some_val [ci skip]   add seed [ci skip]   4 places [ci skip]   fail on purpose [ci skip]   verbose=True [ci skip]   use filename to track   use filename to track   monitor epoch + changelog   Update tests/checkpointing/test_model_checkpoint.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com",0
5656,added lazy decorator,0.5925952,core decorator data_loader,  changelog 1.0.4   changelog 1.0.4 ,0
5657,Keep hidden state in the optimization loops (#9368),0.69720685,        # 4. Perform the optimization in a loop, docs + precision + recall + f_beta + refactor  Co-authored-by: Teddy Koker teddy.koker@gmail.com  rebase  Co-authored-by: Teddy Koker teddy.koker@gmail.com  fixes  Co-authored-by: Teddy Koker teddy.koker@gmail.com   added missing file   docs   docs   extra import   add confusion matrix   add to docs   add test   pep8 + isort   update tests   move util function   unify functional and class   add to init   remove old implementation   update tests   pep8   add duplicate   fix doctest   Update pytorch_lightning/metrics/classification/confusion_matrix.py   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com   changelog   bullet point args   bullet docs   bullet docs   Co-authored-by: ananyahjha93 ananya@pytorchlightning.ai Co-authored-by: Teddy Koker teddy.koker@gmail.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: chaton thomas@grid.ai Co-authored-by: Roger Shieh 55400948+s-rog@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
5658,Remove lightning-dtrun installation (#7018),0.82166624,Removed the deprecated LightningDeepSpeedModule (#16041),,1
5659,Merge pull request #6 from shreyasbapat/management,0.5623499,refactored dataloader process hook (#3139),  first attempt   update tests   support multiple   test bugfix   changelog   pep   pep   import order   import   improve test for resuming   test   update test   add references test   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com  docstring suggestion deprecation  Co-authored-by: Jeff Yang ydcjeff@outlook.com  paramref  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jeff Yang ydcjeff@outlook.com,0
5660,Update websocket-client requirement from <=1.3.3 to <1.5.2 in /requirements (#16640),0.5547323,Refactor cloud dispatch and update to new API (#16456),  distributed_backend -> accelerator   distributed_backend -> accelerator   use_amp -> precision   format   Co-authored-by: rohitgr7 rohitgr1998@gmail.com,0
5661,fix save_hyperparameters(container) if container is empty  (#7268),0.63854504,Moved save_hyperparameters to its own function (#7119),  ddp no-sync   Update pytorch_lightning/trainer/training_loop.py   Co-authored-by: ananthsub ananth.subramaniam@gmail.com   Update training_loop.py   factor enter and exit out to separate context manager   delete _updated_model_last_step   Co-authored-by: justusschock justusschock@pc125.lfb.rwth-aachen.de Co-authored-by: Teddy Koker teddy.koker@gmail.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: chaton thomas@grid.ai Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
5662,Return the output of the optimizer step (#11711),0.81681985,    optimizer1.step()," fix: nb is set total number of devices, when nb is -1.  Refs: #4207  feat: add test code test combination auto_select_gpus, gpus options using Trainer test pick_multiple_gpus function directly    Refs: #4207  docs: modify contents in Select GPU devices  Refs: #4207  refactore: reflect the reuslt of review  Refs: #4207  refactore: reflect the reuslt of review  Refs: #4207  Update CHANGELOG.md  Co-authored-by: chaton thomas@grid.ai Co-authored-by: Roger Shieh 55400948+s-rog@users.noreply.github.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com",1
5663,Merge remote-tracking branch 'carmocca/sync-1.1.5' into release/1.2-dev,0.47791696,Re-introduced fix for Hydra directory sync with multiple process (#5993),Co-authored-by: Jeff Yang ydcjeff@outlook.com,0
5664,fixed os missing,0.5912888,"Fixing missing packaging package in dependencies, which was affecting the only installation to a very blank system.","  use move_data_to_device instead of to; docstring also allow tuple of Tensor; not supported log error when example_inputs is a dict; commented docstring trace example   Use isinstance to check if example_inputs is a Mapping, instead of type   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com   import Mapping for isinstance check   multi-line docstring code to test TorchScript trace()   Fix PEP8 f-string is missing placeholders   minor code style improvements   Use (possibly user overwritten) transfer_batch_to_device instead of move_data_to_device   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   fixed weird comment about trace() log error   Remove unused import   Co-authored-by: Jeff Yang ydcjeff@outlook.com  Remove logger warning about dict not example_inputs not supported by trace  Co-authored-by: stef-ubuntu stef@webempath.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Jeff Yang ydcjeff@outlook.com",0
5665,"Add missing test for ""multiple dataloader + percent_check fix"" (#2226)",0.64206016,enabled multiple dataloaders for validation.    ,  feat(wandb): log in sync with Trainer step   docs: update CHANGELOG   style(test_wandb): fix formatting   parentheses   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
5666,Rename Strategy.lr_schedulers to Strategy.lr_scheduler_configs (#11549),0.6909914,    --lr_scheduler=anneal_strategy=linear,  ananyahjha93 and teddykoker to codeowners for metrics   add Justus   Co-authored-by: Sean Naren sean.narenthiran@gmail.com,0
5667,set html_add_permalinks for docs (#812),0.5658227,Deprecated tags_csv in favor of hparams_file (#1271),  add option to log momentum   add docstring   refactor for cleanliness   Co-authored-by: Nicki Skafte skaftenicki@gmail.com,0
5668,Add typing to data fetching (#11515),0.63585544,"Accessing dataloaders (#16726, #16800)",  warning on states   suggestion   Update metrics.rst   Co-authored-by: Sean Naren sean.narenthiran@gmail.com,0
5669,allow regression metrics to import (#2225),0.7131159,Regression metrics (#2221),  Add optimizer hooks in callbacks   optimizer param   update test   Co-authored-by: Nicki Skafte skaftenicki@gmail.com,1
5670,update ngc for 1.3 (#7414),0.650767,0.4.0,  Add key   Remove unused variables   Update CHANGELOG [skip ci]   best_model_monitor -> monitor   Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
5671,Fix number of total steps shown in progress bar during sanity validation check when number of validation dataloaders >= 2 (#597),0.704083,Allow changing the logged step value in validation_step (#4130),,1
5672,Close SummaryWriter in TensorBoardLogger on finalize (#5696),0.65275913,Create single file in TensorBoardLogger (#777),  Fix  COMET_EXPERIMENT_KEY environment variable usage   Remove unused arg   Update comet.py   Add test by Lothiraldan   remove blank   Co-authored-by: chaton thomas@grid.ai Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Sean Naren sean.narenthiran@gmail.com,0
5673,Promote Fabric.launch() as the default experience in Fabric docs (#16878),0.7784785,fabric.launch(),"  Update ddp_plugin.py   Update ddp_plugin.py   Update ddp_plugin.py   Update test_ddp_plugin.py   Update pytorch_lightning/plugins/ddp_plugin.py   Update pytorch_lightning/plugins/ddp_plugin.py   Fixed imports, make ddp_kwargs protected   Co-authored-by: SeanNaren sean.narenthiran@gmail.com",1
5674,add back compatibility for deprecated metrics 1/n (#5067),0.7341578,Drop duplicate metrics (#5014),"  allow trainer's profiler param to have a str value   add tests   update docs   update exception message   Update CHANGELOG   fix pep8 issues   cleanup test code   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   Add deprecation warning if using bool for profiler   Add deprecation tests and move deprecated tests   Remove bool option to profiler from docs   Deprecate bool args to profiler in CHANGELOG   fixup! Add deprecation warning if using bool for profiler   fixup! Add deprecation tests and move deprecated tests   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   Implement suggestions, remove whitespace   fixup! Implement suggestions, remove whitespace   Allow bool, str (case insensitive), BaseProfiler   Add info about bool deprecation to trainer   fixup! Add info about bool deprecation to trainer   Move deprecate todo to test_deprecated   Test wrong profiler type, improve error message   fixup! Test wrong profiler type, improve error message   Update pytorch_lightning/trainer/connectors/profiler_connector.py   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   Apply suggestions from code review   Readd bool to profiler types, test cli profiler arg   Remove extra whitespace in doc   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Apply suggestions from code review  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update deprecation versions  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com",1
5675,Add support for dataloader_iter to validate and test steps (#11546),0.75350213,enabled multiple dataloaders for validation.    ,Co-authored-by: chaton thomas@grid.ai,1
5676,Refactor get_filesystem to use native fsspec API (#11708),0.6770389,Used fsspec instead of gfile for all IO (#3320),  Add geting help message from docstring   Fix pep8 issue   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Apply suggestions from code review  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Sean Naren sean.narenthiran@gmail.com,0
5677,Profiler summary (#1259),0.67302597, - Profiling: `Trainer(profiler=...)`,  fix   more doc fixes   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: chaton thomas@grid.ai,0
5678,Update import in bug report template (#16616),0.6617503,Module imports are now catching ModuleNotFoundError instead of ImportError (#9867),Co-authored-by: Sean Naren sean.narenthiran@gmail.com,0
5679,Lightning Lite docs (#10176),0.8236821,LightningLite,  add _sanitize_callable_params   add call on _val if callable   clean code formatter   resolve pep8   default return function name   resolve pep8   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update CHANGELOG.md  Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
5680,Upgrade to HPU release 1.8.0 (#16621),0.61921215,Moveed HPU broadcast override to the HPU strategy file (#17011),  wandb finish   experiment   upload at end of run   changelog   comment   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Sean Naren sean.narenthiran@gmail.com,0
5681,Fix torch.compile tests (#16503),0.6973343,"Set PyTorch 1.4 as min requirements, also for testing and examples torchvision>=0.5 and torchtext>=0.5 (#5418)",  upgrade PT version   update docker   docker   try 1.5   badge   fix typo: dor -> for (#3918)   prune   prune   env   echo   try   notes   env   env   env   notes   docker   prune   maintainer   CI   update   just 1.5   CI   CI   CI   CI   CI   CI   CI   CI   CI   CI   CI   docker   CI   CI   CI   CI   CI   CI   CI   CI   CI   push   try   prune   CI   CI   CI   CI   Co-authored-by: Klyukin Valeriy mr.clyukin@gmail.com Co-authored-by: Jeff Yang ydcjeff@outlook.com,0
5682,Make configure_sharded_model implementation in test models idempotent (#17625),0.6927594,- Full tests that run multiple models in different configs,"  enable custom apex, amp plugin   enable custom apex, amp plugin   enable custom apex, amp plugin   enable custom apex, amp plugin ",0
5683,Update docs for new template (#8232),0.56949806,Use correct python version in lightning component template (#13790), Update profilers.py  Enable profilers to use write to remote files with fsspec   Update profilers.py   Update CHANGELOG.md   Update pytorch_lightning/profiler/profilers.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  formatting  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
5684,fix miss-leading imports in tests (#5873),0.65268314,Module imports are now catching ModuleNotFoundError instead of ImportError (#9867),Co-authored-by: Nicki Skafte skaftenicki@gmail.com,0
5685,devel version (#2292),0.6500163,Set version as today (#13906), parent faa357648f49a75341b501ec9c9bbd4985357c10 author ydcjeff ydcjeff@outlook.com 1601049378 +0630 committer ydcjeff ydcjeff@outlook.com 1601469495 +0630  cache docker builds lock horovod at 0.19.5 done [ci skip] [CI SKIP] use --cache-from [ci skip] typo and horovod [ci skip] exclude pt 1.3 py3.8 [ci skip] conda no cache [ci skip] fix   revert   align with master [ci skip]   retry   remove empty continuation lines   add comment   fix build-args ,0
5686,Update Sharded test with RunIf (#6384),0.5890255,Disabled val and test shuffling (#1600), updated header and small fixes  Signed-off-by: ericharper complex451@gmail.com  updated header and small fixes  Signed-off-by: ericharper complex451@gmail.com  updated header and small fixes  Signed-off-by: ericharper complex451@gmail.com  updated header and small fixes  Signed-off-by: ericharper complex451@gmail.com,0
5687,ddp backend refactor (#3210),0.9320499,refactored DDP backend forward (#3119),resolve https://github.com/PyTorchLightning/pytorch-lightning/issues/4345 Co-authored-by: Nicki Skafte skaftenicki@gmail.com,1
5688,[App] Improve LightningTrainerScript start-up time (#15751),0.9732572,Improved LightningTrainerScript start-up time (#15751),  =Add deprecation warning for auc reorder   =Add test for deprecation warning for auc reorder   Update CHANGELOG   Add reorder deprecation warning to auc docstring   Fix pep8 f-string error   remove duplicate import   Co-authored-by: Nicki Skafte skaftenicki@gmail.com,1
5689,ref: decouple apex second attemp part n/n (#4065),0.61663723,"Decoupled Appex (#4052, #4054, #4055, #4056, #4058, #4060, #4061, #4062, #4063, #4064, #4065)",,0
5690,Docs format - Trainer & LModule (#1055),0.6449137,Deprecated TrainerLoggingMixin in favor of a separate utilities module for metric handling (#7180)," repro  debug c d dd d d d ads d d d f rank f v d d d d d d d d d d d set  drop PL_DDP_PID clean up keep set gpus revert Revert ""drop PL_DDP_PID"" This reverts commit 7d88cae469541ef19128f9c20919fd3a6f863039. d pid gpus clean up clean up  misconfig? misconfig clean clean   fix pep   changelog   remove script   Co-authored-by: chaton thomas@grid.ai Co-authored-by: William Falcon waf2107@columbia.edu",0
5691,Fix intellisense for LightningCLI (#11075),0.8024229,LightningCLI improvements,"  Broadcast best model path to ensure we sync with main process + wait for main process to save   Add barrier call to ensure all processes are in sync   Added changelog commit   Move sync of best model path/score to model checkpoint, keep barrier to ensure all processes complete   Ensure we broadcast as tuple   Add init check   Update pytorch_lightning/callbacks/model_checkpoint.py   Co-authored-by: ananthsub ananth.subramaniam@gmail.com  Update pytorch_lightning/callbacks/model_checkpoint.py  Co-authored-by: ananthsub ananth.subramaniam@gmail.com   Removed model checkpoint code, added barrier to trainer to enforce we syncronize and wait for all processes to finish before completing training   Add barrier within teardown call, removed horovod teardown to inherit from base accelerator   Co-authored-by: ananthsub ananth.subramaniam@gmail.com",1
5692,Remove deprecated Trainer.slurm_job_id (#13459),0.79677373,Removed the deprecated TrainerLoggingMixin class (#8609),,1
5693,Remove deprecated DataModule.dims usage in tests (#9948),0.66938055,"Deprecated DataModule properties: train_transforms, val_transforms, test_transforms, size, dims (#8851)",,0
5694,added fallback local init,0.5402137,callback system and init DDP (#3836),  Update data_loading.py   Update data_loading.py   add test + update flag description   add to changelog   Update test_dataloaders.py   fix-pickle   Update test_dataloaders.py   Added missing reference calls   Update tests/trainer/test_dataloaders.py   Apply suggestions from code review   Update data_loading.py   Update test_dataloaders.py   Co-authored-by: Sean Naren sean.narenthiran@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
5695,Better approach to register plugins (#7063),0.7541892,Enabled plugins (#4041),  load directly from fs   if not str or path   pep8   type annotation   Co-authored-by: Sean Naren sean.narenthiran@gmail.com,1
5696,ref: separate slurm from ddp (#3809),0.88377166,separate SLURM from DDP (#3809),  add two tests w/wo tempdir   resolve flake8   this test is failing   update bug report   resolve bug and add test   remove bug_report   resolve flake8   resolve bug   resolve pep8   resolve pep8   Co-authored-by: Teddy Koker teddy.koker@gmail.com,1
5697,drop mypy from .pre-commit-config.yaml (#6542),0.5495319,Drop PyTorch 1.9 support (#15347),  Update init.py   Update pytorch_lightning/init.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
5698,tests: switch imports for pytorch (#16595),0.70757836,from pytorch_lightning.plugins import CheckpointIO,  Add dirpath and filename parameter in ModelCheckpoint   remove old function   chlog   codefactor   update tests   docs   fix doctest and added tests   pathlib dirpath   dep version and docs   try fix doctest   pep   suggestions Co-authored-by: carmocca carlossmocholi@gmail.com   suggestions   fix test   pep   trigger tests   Apply suggestions from code review   Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   suggestions   try fix windows test   add and update some tests   trigger tests   Apply suggestions from code review   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: William Falcon waf2107@columbia.edu,1
5699,Move training_output validation to after train_step_end (#7868),1.0,Move training_output validation to after train_step_end (#7868),  add bug_report_model to bug_report   add notebook should be made public   update ,1
5700,Fix summary hook handles not getting removed (#2298),0.6979102,"Removed output argument from *_batch_end hooks (#3965, #3966)",,0
5701,Model summary: add 1 decimal place (#4745),0.9999999,Model summary: add 1 decimal place (#4745),  Fix bug comparing max_steps to global step which inits at 0   Added test to ensure accumulate grad batch works with max steps   check fix with TODO test   correct call counts   Add check to ensure we've finished accumulation of this global step before exiting loop in conjuction with max steps   Remove + 1 check in test as this was incorrect   Update incorrect expected outputs in lr finder test   Added brackets for clarity   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
5702,prune changelog (#1123),0.6426317,Complete changelog,  limit monitor callback with row_log_interval   try fix gpu test   log_every_n_steps   Apply suggestions from code review   Apply suggestions from code review   rebase and staticmethod   suggestions   Co-authored-by: Jeff Yang ydcjeff@outlook.com,0
5703,Add hook test for reloading with max epochs (#12932),0.652885,refactor eval loop to use hooks - use test_mode for if so we can split later (#3129),  enable custom ddp plugin   enable custom ddp plugin   enable custom ddp plugin   enable custom ddp plugin   enable custom ddp plugin   enable custom ddp plugin   enable custom ddp plugin   enable custom ddp plugin   enable custom ddp plugin   enable custom ddp plugin   enable custom ddp plugin   Co-authored-by: chaton thomas@grid.ai,0
5704,Fix passing _ddp_params_and_buffers_to_ignore (#11949),0.7199834,- Fixed passing `_ddp_params_and_buffers_to_ignore` ([#11949](https://github.com/PyTorchLightning/pytorch-lightning/pull/11949)),  minor doc fix   minor doc fix   fix problem with apex closure (#4292)   Co-authored-by: William Falcon waf2107@columbia.edu   minor doc fix   minor doc fix   Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
5705,Fix(Early Stopping): move best score to device (#7959),0.55955285,device_stats = DeviceStatsMonitor(),Co-authored-by: William Falcon waf2107@columbia.edu,0
5706,[App] Cold start proxy in autoscaler (#16094),0.5779518,Removed mode='auto' from EarlyStopping (#6167),,0
5707,Docs/robots (#6658),0.68425727,:robot: ,"  moved to utility   add files   unify   add desc   update   end of line   Apply suggestions from code review   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com  Apply suggestions from code review  Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com   add back functional test in new interface   pep8   doctest fix   test name fix   unify psnr + add class psnr, TODO: psnr test refactor ala mean squared error   unify psnr   rm unused code   pep8   docs   unify ssim   lower tolerance for ssim   fix import   pep8   docs   flake8   test smaller images   trying to fix test   no ddp test for ssim   pep8   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Teddy Koker teddy.koker@gmail.com",0
5708,skip multi-gpu test when running on single-gpu machine (#5186),0.63108826,GPU training (#2704),  Fix to bug identified in https://github.com/PyTorchLightning/pytorch-lightning/issues/4102   update tests   chlog   Co-authored-by: rohitgr7 rohitgr1998@gmail.com,0
5709,update Loggers (#818),0.8009299,Removed LoggerStages (#5673),  Update optimizer code   Update CHANGELOG   Fix tuple of one list case   Update docs   Fix pep issue   Minor typo [skip-ci]   Use minimal match   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Apply suggestions from code review  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
5710,Refactor plugins backward (#8328),0.7295501,Refactored Accelerators and Plugins (#5743),  closure for all optimizers   rename hook and take care of alternating backwards   add comment   training_loop_fix   closure whenever possible   training_loop   simple tests that count backward calls   fix test to work with closure   remove debugging statement   better place   check grads after backward   start fixing manual optimization   skip step when result returned by closure was None   fix gradient clipping test to work with closure   attribute dict result only for automatic optimization   adjust backward calls in accelerator   adjust where to call gradient clipping   adjust backward calls in tests   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  pass kwargs to xla optimizer  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
5711,Better errors for logging corner cases (#13164),0.69576836,Un-balanced logging properly supported (#5119),  Fix info message when EarlyStopping 'mode' not provided   fixup! Fix info message when EarlyStopping 'mode' not provided   Apply suggestions from code review   Co-authored-by: Jeff Yang ydcjeff@outlook.com Co-authored-by: Jeff Yang ydcjeff@outlook.com,0
5712,refactor accelerator teardown -> training type plugin teardown (#7579),0.8622954,Changed teardown() in Accelerator to allow training_type_plugin to customize teardown logic (#7579),  docs logo   use png   cleaning   vectorial   Apply suggestions from code review   Co-authored-by: Nicki Skafte skaftenicki@gmail.com  Apply suggestions from code review  Co-authored-by: Nicki Skafte skaftenicki@gmail.com,1
5713,Refactor TorchElasticEnvironment.detect to use torch.distributed.is_torchelastic_launched (#12376),0.68162936,Disable torch.inference_mode with torch.compile in PyTorch 2.0 (#17215)," Removed image generation inside the training step.  It was overwriting the image grid generated in on_epoch_end. I also made adversarial_loss a static method.  Incorporated Hyperparameter best practices  Using ArgumentParser and hparams as defined in the Hyperparameters section of the documentation. This way we can set trainer flags (such as precision, and gpus) from the command line.  Incorporated Hyperparameter best practices  Using ArgumentParser and hparams as defined in the Hyperparameters section of the documentation. This way we can set trainer flags (such as precision, and gpus) from the command line.   Split the data part into a LightningDataModule   Update pl_examples/domain_templates/generative_adversarial_net.py   Co-authored-by: Jeff Yang ydcjeff@outlook.com",0
5714,relax docker requirement (#14009),0.54716516,    default_port = 12910,,0
5715,Resolve batch_size in ResultCollection not resetted to 1 on epoch end (#10242),0.71415126,"    batch_size=32,",  =Add iou input checks   =Add test for iou input checks   =Update docstring for iou pred and target ,1
5716,moved env var to before import (#414),0.6151691,"Separated utils: imports & enums (#5256, #5874)",  Add empty lines in docstring for proper docs   Remove Returns:   Remove unnecessary Returns:   Update pytorch_lightning/accelerators/ddp2_accelerator.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  fix returns  Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
5717,Improve requirements parser (#13912),0.59695137,parser = Trainer.add_argparse_args(parser),"  Add strict option to lr_scheduler dict   Update docs   Unnecessary ""else"" after ""raise""   Update CHANGELOG   Fix rebase ",0
5718,Bump google-github-actions/auth from 0 to 1 (#15675),0.5406541,- Removed deprecated support for passing the `rank_zero_warn` warning category positionally ([#14470](https://github.com/Lightning-AI/lightning/pull/14470)),  changelog   Apply suggestions from code review   Co-authored-by: Nicki Skafte nugginea@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
5719,[BUG] Check environ before selecting a seed to prevent warning message (#4743),0.96760774,Check environ before selecting a seed to prevent warning message (#4743),,1
5720,Merge pull request #32 from Separius/patch-1,0.59083784,"merge backends (#3476, #3477, #3478, #3480, #3482)", Update f_beta.py  Added METRIC_EPS in the denominator to avoid nan values in f_beta score.  Update f_beta.py  Made changes flake8 compliant  Update f_beta.py  Makes use of class_reduce for macro f_beta computation to avoid nans  Update f_beta.py  Made flake8 compliant   Corrected F beta computation   Removed offset to make the computation precise ,0
5721,add parsing OS env vars (#4022),0.5491228,  * `env_parse`,,0
5722,Missing profiler attribute in add_argparse_args() ArgumentParser (#1794),0.6967235,"  * Removed class methods from Trainer: `default_attributes()`, `from_argparse_args()`, `parse_argparser()`, `match_env_arguments()`, `add_argparse_args()`",  update stale bot   Apply suggestions from code review   Co-authored-by: chaton thomas@grid.ai  Apply suggestions from code review  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Apply suggestions from code review  Co-authored-by: chaton thomas@grid.ai Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
5723,hotfix: dataloaders - add unimplemented methods (#5352),0.58786786,- Added support for no pre-fetching to `DataFetcher` ([#11606](https://github.com/PyTorchLightning/pytorch-lightning/pull/11606)),  formatting   miss   missing & ver++   path ,0
5724,Fix pre-commit isort failure on pytorch_lightning/accelerators (#5503),0.70703715,"        ""pytorch_lightning.callbacks_factory"": [",,1
5725,Fix missing docs (#2659),0.56068265,"docs for all Metrics (#2184, #2209)",,0
5726,Update tuner docs (#15087),0.6791349,Update the Lightning App docs (#13537),,0
5727,Use hpu hmp for bf16 alone (#13028),0.59575754,Moveed HPU broadcast override to the HPU strategy file (#17011),  activated color in all pytest runs   Update .drone.yml   Co-authored-by: Jeff Yang ydcjeff@outlook.com Co-authored-by: Jeff Yang ydcjeff@outlook.com,0
5728,check omegaconf gpus (#2273),0.55011463,"Validation DataLoader 0:  38%|███      | 12/32 [00:12<00:20,  1.01s/it]",,0
5729,Support auto_select_gpus with accelerator and devices api (#12608),0.9817371,Support auto_select_gpus with the accelerator and devices API (#12608),  prune ignore   try drop loggers ,1
5730,add back compatibility for deprecated metrics 2/n (#5068),0.70445883,Drop duplicate metrics (#5014),,1
5731,handle unknown args passed to Trainer.from_argparse_args (#1932),0.7498841,trainer = L.Trainer.from_argparse_args(args),,1
5732,[DDP] Remove the outdated limitations of DDP communication hook since 1.9 (#8346),0.69623125,Merged the implementation of DDPSpawnStrategy into DDPStrategy and removed DDPSpawnStrategy (#14952),  move base req. to root   check-manifest   check-manifest   manifest   req ,0
5733,fix typo (#1124),0.60734856,Changed overwrite to True (#16009),,0
5734,move Trains logger to Bolts (#2384),0.9328634,Moved TrainsLogger to Bolts (#2384),,1
5735,add Auto rebase action (#845),0.5890779,"Removed deprecated ModelCheckpoint arguments prefix, mode=""auto"" (#6162)",,0
5736,added example and verified,0.65011805,  validation_if_necessary(),  Annotate return type of TrainerProperties.from_argparse_args(...)   Added second empty line between class and typevar   Renamed all uses of the typevar to _T ,0
5737,Update Model Parallel doc (#11465),0.5994438,Data Parallelism,torch.cuda.device_count() returns the number of available GPUs,0
5738,lint,0.49977642,Fabric,,0
5739,No need of warning when saved callback_states is None (#7293),0.72919106,Removed deprecated early_stop_callback (#3982),  add autodoc load_from_checkpoint to docs   autofunction -> automethod   Co-authored-by: Jeff Yang ydcjeff@outlook.com Co-authored-by: Jeff Yang ydcjeff@outlook.com,1
5740,bump version Dev (#16562),0.5417292,Set version as today (#13906),  add persistant flag to add_state in metrics   wrap register_buffer with try catch   pep8   use loose version   test   pep8 ,0
5741,[bugfix] Resolve memory leak for evaluation (#6326),0.9108374,Resolve memory leak for evaluation (#6326),,1
5742,Set minimum PyTorch version to 1.6 (#8288),0.749038,Dropped official support/testing for older PyTorch versions <1.3 (#1917),  :pencil: add newline   Created using Colaboratory   Edited using Colaboratory   Apply suggestions from code review   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com   typo   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
5743,docs: fix past versions location (#17432),0.5217832,Updated governance docs,"  Use Optional for variables set to None by default   Use Optional instead of Union[None, ...] for consistency ",0
5744,Mark internal components as protected (#17009),0.6274423,Marked FitLoop.should_accumulate as protected (#9515),,0
5745,Fix default labels in issue templates (#12434),0.5887206,Resolve bug with Finetuning (#5744),,0
5746,Fix tests related to DDP communication hooks (#12878),0.72210574,DDPs train hooks (#3203),,1
5747,PanelFrontend and Panel Web UI Intermediate docs (#13531),0.70482516,Adds PanelFrontend to easily create complex UI in Python (#13531),,1
5748,Fix docs levels and broken links (without rename) (#17545),0.5875561,Updated mlflow with using resolve_tags (#6746),  add test   fix   sleepy boy   chlog   Apply suggestions from code review   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,0
5749,Remove references to torchtext.legacy from PyTorch Lightning (#10724),0.7610563,Removed deprecated code in pytorch_lightning.utilities.meta (#16038),  remove duplicate metric vs step log   remove duplicate metric vs step log   remove duplicate metric vs step log   fix ddp index issue ,1
5750,Merge DDPStrategy and DDPSpawnStrategy in Lite (#14952),0.8585135,Merged the implementation of DDPSpawnStrategy into DDPStrategy and removed DDPSpawnStrategy (#14952),,1
5751,Fix lr finder for optimizers with states (#3897),0.70334715,Refactored optimizer (#4658),  fix val epoch agg   fix val agg metrics   fix val agg metrics   fix val agg metrics ,1
5752,Trim flaky amp test (#15051),0.64716005,training AMP scaling refactor (#3135),  save initial arguments   typing   chlog   . ,0
5753,Connect progress tracking dataclasses to loops (#8244),0.9824549,"Connect the progress tracking dataclasses to the loops (#8244, #8362)",  namme inputs   sk rename   imports ,1
5754,add dist lib to enable syncing anything across devices (#3762),0.57695454,Made parallel devices optional across all plugins (#6051),,0
5755,add support for sync_bn (#2801),0.71291506,moves sync bn to each backend (#3925),,1
5756,ref: decouple apex second attemp part 9/n (#4063),0.6312687,remove _evaluate fx (#3197),  speedup   something working   update the rest   more desc   recurse tests/metrics again   pep8   Co-authored-by: Teddy Koker teddy.koker@gmail.com,0
5757,Modify LSFEnvironment to use more reliable environment variable (#10825),0.61066425,Changed LSFEnvironment to use LSB_DJOB_RANKFILE environment variable instead of LSB_HOSTS for determining node rank and main address (#10825),"  make current_epoch and global_step to be same as trainer, after model restore.   remove assignment here   test   minor modification   merge with parent's master   doc fix / improve   doc fix!   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com",0
5758,"[Fix] Add delay property for checkpointing, refactor loading checkpoint (DeepSpeed Checkpointing Fix 1/n)  (#8627)",0.72282803,Refactor load in checkpoint connector (#4593), Add trainer flag step  Add step to disable automatic optimization in the trainer @justusschock  Apply suggestions from code review  Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com,1
5759,hotfix on classification metrics (#2878),0.68353903,Classification metrics overhaul (#4837),  new logs   formatting   1.0.1 ,0
5760,Fix missing imports in converting.rst (#11945),0.6138548,Re-Enable Logger's ImportErrors (#1938)," Revert ""temporary drop metrics tests while speeding them up (#4071)""  This reverts commit 86c70622fbae611dd45ccb104830e7e28639fe44.   skip metrics tests   skipping ",0
5761,Fix and refactor test_deepspeed_engine_is_steppable test (#15251),0.65002704,Expose DeepSpeed loss parameters to allow users to fix loss instability (#6115),  Add trace functionality to the function to_torchscript   used wrong parameter name in test   fix indentation to confirm to code style ,0
5762,faster tests (#3604),0.6142907,"PyTorch reports that on average, ""models runs 43% faster in training on an NVIDIA A100 GPU. At Float32 precision, it runs 21% faster on average and at AMP Precision it runs 51% faster on average"" (source). If you want to learn more about torch.compile and how such speedups can be achieved, read the official PyTorch 2.0 blog post.",,0
5763,"Remove the ""Verify linked issue"" bot (#11338)",0.63377976,- Removed the deprecated `TestTubeLogger` ([#12859](https://github.com/Lightning-AI/lightning/pull/12859)),,0
5764,added image,0.5194931,    pil_image = T.ToPILImage()(image),"  Added getstate/setstate method for torch.save serialization, added additional Optional Typing to results object   Added tests to ensure torch.save does not fail   Added flags to ensure compatible ddp cpu environment   Removed torch version check due to minimum already being 1.3, reduced epochs for speed   Moved tests to separate file   Update to accelerator, move to ddp_spawn to prevent hanging ddp ",0
5765,Fix typos in Checkpointing doc (#13827),0.5618615,Changed resume_from_checkpoint warning to an error when the checkpoint file does not exist (#7075),  created minor fixes   adjusted the underline   Update docs/source/amp.rst   suggestion from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
5766,removed dead code in model save,0.7984117,Removed the deprecated save_function property in ModelCheckpoint (#8680),,1
5767,updates to changelog (#2248),0.7856722,Complete changelog,,1
5768,Move result teardown to loops (#8245),0.96962476,Moved result teardown to the loops (#8245),  chlogs   logs   space   date   logs   logs   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   logs   logs   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
5769,added eval and train for redundancy (#464),0.64138067,"EvalResult support for train and val. loop (#2615, #2651)",,0
5770,Update CODEOWNERS (#7302),0.5854929,Update the Lightning App docs (#13537),,0
5771,Remove the deprecated weights_save_path Trainer argument (#14424),0.8482865,- Removed the deprecated `weights_save_path` Trainer argumnent and `Trainer.weights_save_path` property ([#14424](https://github.com/Lightning-AI/lightning/pull/14424)),,1
5772,Remove support for DDP2 strategy (#12705),0.942847,Removed support for the DDP2 strategy,,1
5773,Always use trainer.call_hook (#8498),0.65778375,Trainer.fit hook clean up (#3198),  Mention skipping in docs   Use :class: ,0
5774,ci: update group-check (#16984),0.57807803,group prepare data hook (#3212),,0
5775,[FEAT] Add lambda closure to manual_optimizer_step (#4618),0.7090837,Executing the optimizer_closure is now required when overriding the optimizer_step hook (#9360),,1
5776,update logic to inject FastForwardSampler / CaptureIterableDataset 2/n (#8366),0.65133774,refactored dataloader process hook (#3139),,0
5777,Add axes argument to lr finder plot (#15652),0.82435817,Add an axes argument ax to the .lr_find().plot() to enable writing to a user-defined axes in a matplotlib figure (#15652),Changed trainer.lr_find(...) to trainer.tuner.lr_find(...) Co-authored-by: pbmstrk pbmstrk@users.noreply.github.com,1
5778,Allow frozen data classes in optimizer state dict (#16656),0.72737575,- Added support for frozen dataclasses in the optimizer state ([#16656](https://github.com/Lightning-AI/lightning/pull/16656)),  docs   docs   docs ,1
5779,ref: separate argparse (#3428),0.6915796,argparse_utils >> argparse,,0
5780,Fix missing url (#9602),0.5021835,- Fixed a bug where the upload files endpoint would raise an error when running locally ([#14924](https://github.com/Lightning-AI/lightning/pull/14924)),,0
5781,Prune deprecated metrics (#8586),0.6889028,Removed deprecated metrics (#8586),,0
5782,Update DDP docs (#5046),0.6983801,DDP Debugging Improvements,  docs   docs   docs   docs ,0
5783,Fix mypy errors attributed to pytorch_lightning.loggers.base.py (#13494),0.75869286,Deprecated pytorch_lightning.logging (#767),  enabled manual returns   style   docs ,1
5784,"Run CircleCI with the HEAD sha, not the base (#14625)",0.50420856,Allowing decorate model init with saving hparams inside (#4662),,0
5785,Fix a bug that caused spurious AttributeError when multiple DataLoader classes are imported (#14117),0.697963,Module imports are now catching ModuleNotFoundError instead of ImportError (#9867),,0
5786,ref: decouple apex second attemp part 10/n (#4064),0.6413378,"Decoupled Appex (#4052, #4054, #4055, #4056, #4058, #4060, #4061, #4062, #4063, #4064, #4065)",,0
5787,[metrics] Accuracy num_classes error fix (#3764),0.6399403,Avoid raising the sampler warning if num_replicas=1 (#14097),,0
5788,Lite Example: Model Agnostic Meta Learning (MAML) (#16333),0.57121074,Meta Module,,0
5789,Fix filtration logic for eval results with multiple dataloaders (#10810),0.61261535,data_batch -> batch    ,,0
5790,Update dynamo bug workaround condition (#17065),0.6344737,Improved the error message when the LightningWork is missing the run method (#14759),,0
5791,"Update jsonargparse[signatures] requirement from <=4.7.1,>=4.7.1 to >=4.7.1,<4.7.4 in /requirements (#13052)",0.5819679,Improved error messages in replace_sampler when the DataLoader attributes are not included in the signature or the signature is missing optional arguments (#8519),,0
5792,tweak imagenet docs to match current script (#4895),0.49627542,Reset metrics before each task starts (#9410),,0
5793,unambiguous (#2664),0.61427176,[1.1.6] - 2021-01-26,,0
5794,added warnings to unimplemented methods (#1317),0.6902458,Gave warnings for unimplemented required lightning methods (#1317),,0
5795,Remove deprecated code in pl.utilities.distributed (#16390),0.7580277,- Removed the deprecated code in:,,1
5796,Typo (#2575),0.51415396,(#16002),,0
5797,deprecated Trainer proc_rank (#2269),0.7463138,Removed the deprecated TrainerTrainingTricksMixin class (#8679),  docs   docs   docs   docs   docs ,1
5798,"Revert ""Update Lightning App docs (#13537)"" (#13655)",0.94651467,Update the Lightning App docs (#13537),  docs   docs   docs ,1
5799,Deprecate @auto_move_data in favor of trainer.predict (#6993),0.9798219,Deprecated @auto_move_data in favor of trainer.predict (#6993),,1
5800,Better error message when dataloader and datamodule is None (V2) (#14637),0.7065095,"Improved the error messaging when passing Trainer.method(model, x_dataloader=None) with no module-method implementations available (#14614)",  docs   docs ,1
5801,del iterator on_run_end() (#9915),0.6633967,Support for arbitrary iterables (#16726),  docs   docs   docs   docs   docs   docs   docs   docs   docs ,0
5802,fix deprecation wrapper & tests (#6553),0.6528172,Enabling val/test loop disabling (#2692),,0
5803,delete ref to old update_training_log_metrics (#262),0.721148,Remove epoch from trainer.logged_metrics (#9904),  temporary drop metrics tests while speeding them up   cov   cov   docs ,1
5804,Fix DistribType for ddp_cpu (spawn) (#7492),0.6568891,remove weight loading hack for ddp_cpu (#3808),  docs   docs ,0
5805,Classification metrics overhaul: accuracy metrics (2/n) (#4838),0.8918078,Classification metrics overhaul (#4837),,1
5806,Update uvicorn requirement from <=0.18.2 to <0.19.1 in /requirements (#15562),0.5402806,Only check versions / env when not in the cloud (#15504),,0
5807,Fix Lightning package version label (#15447),0.7529699,Update the Lightning App docs (#13537),  ref: accelerator names   docs ,1
5808,LightingDataModule doc fix (#3948),0.62329924,Removed deprecated LightningModule hparams setter (#6207),,0
5809,removed from_lightning flag,0.72824025,- Removed the experimental `PL_INTER_BATCH_PARALLELISM` environment flag ([#16355](https://github.com/Lightning-AI/lightning/pull/16355)),  docs   docs ,1
5810,[doc] Move each profiler to its own file + Add missing PyTorchProfiler to the doc (#7822),0.74071074,Moved profilers to their own file (#7822),,1
5811,Finish #2549 (#2557),0.6138222,[1.1.6] - 2021-01-26,,0
5812,Allow metrics logged together with hparams (#1630),1.0,Allow metrics logged together with hparams (#1630),  ref: decouple apex second attemp part n/n   ref: decouple apex second attemp part n/n ,1
5813,Get experiment_id from MLFlow only once instead of each training loop (#3394),0.65927684,Refactored result handling in training loop (#7506),  ref: decouple apex second attemp part 9/n   ref: decouple apex second attemp part 9/n   ref: decouple apex second attemp part 9/n ,0
5814,Update jsonargparse to unblock master (#12884),0.5225266,- Changes in `LightningCLI` required for the new major release of jsonargparse v4.0.0 ([#10426](https://github.com/PyTorchLightning/pytorch-lightning/pull/10426)),  ref: decouple apex second attemp part 9/n   ref: decouple apex second attemp part 9/n ,0
5815,Use a unique filename to save temp ckpt in tuner (#9682),0.7047331,The tuner now usees a unique filename to save a temporary checkpoint (#9682),  ref: decouple apex second attemp part 8/n   ref: decouple apex second attemp part 8/n   ref: decouple apex second attemp part 8/n   ref: decouple apex second attemp part 8/n ,1
5816,Deprecate auto_select_gpus (#16147),0.7313437,  * Removed the `Trainer(auto_select_gpus=...)` argument,  autofunction -> automethod   Update pytorch_lightning/core/lightning.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
5817,Update ipython[all] requirement from <8.5.1 to <8.6.1 in /requirements (#15419),0.52386165,Refactor cloud dispatch and update to new API (#16456),  ref: decouple apex second attemp part 7/n   ref: decouple apex second attemp part 7/n   ref: decouple apex second attemp part 7/n ,0
5818,restrict deepspeed version in CI (#8951),0.75983363,Support for manual optimization with DeepSpeed (#7970),  ref: decouple apex second attemp part 6/n   ref: decouple apex second attemp part 6/n ,1
5819,Update test_fetching.py (#11551),0.7422116,Skipped testing with PyTorch 1.7 and Python 3.9 on Ubuntu (#11217),,1
5820,CI: Docs build preview in each PR (#1494),0.54153466,Docs, docs + precision + recall + f_beta + refactor  Co-authored-by: Teddy Koker teddy.koker@gmail.com  rebase  Co-authored-by: Teddy Koker teddy.koker@gmail.com  fixes  Co-authored-by: Teddy Koker teddy.koker@gmail.com   added missing file   docs   docs   extra import   Co-authored-by: Teddy Koker teddy.koker@gmail.com,0
5821,build XLA with py3.6 (#2863),0.60455513,Enable PyTorch 1.7 compatibility (#3541),  Fix to print scaler value in progress bar   chlog   Fix to print scaler value in progress bar   Fix to print scaler value in progress bar ,0
5822,Add --app_args support from the CLI (#13625),0.99999994,Add --app_args support from the CLI (#13625),  ref: decouple apex second attemp part 4/n   ref: decouple apex second attemp part 4/n   Update lightning.py   ref: decouple apex second attemp part 4/n ,1
5823,hparams as dict [blocked by 1041] (#1029),0.60473764,Deprecated tags_csv in favor of hparams_file (#1271),,0
5824,update chlog (#4177),0.58676636,0.4.0,  ref: decouple apex second attemp part 2/n   ref: decouple apex second attemp part 2/n ,0
5825,Mark some loop attributes as protected (#8250),0.6659807,  * The loop classes are now marked as protected ([#16445](https://github.com/Lightning-AI/lightning/pull/16445)),,0
5826,Improve the result printing at the end of evaluation (#11332),0.6001484,print(trainer.num_devices),"  enable custom accelerators   ref: finish decoupling apex, LM and backward   ref: finish decoupling apex, LM and backward   ref: finish decoupling apex, LM and backward ",0
5827,[App] Improve debug triggering (#15951),0.61494786,Updated app testing (#16000),  enable cluster plugins   enable cluster plugins + test backend choices   enable cluster plugins + test backend choices   enable cluster plugins + test backend choices   enable cluster plugins + test backend choices   enable cluster plugins + test backend choices   enable cluster plugins + test backend choices ,0
5828,[Docs] Mention that datamodules can also be used with .test() method (#5286),0.61437035,"for data: val_dataloader, test_dataloader, train_dataloader",  :sparkles: add trainer flags nb   fix typos   fix typos   Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com,0
5829,Remove deprecated on_keyboard_interrupt (#13438),0.7366631,- Removed deprecated `Callback.on_keyboard_interrupt` ([#13438](https://github.com/Lightning-AI/lightning/pull/13438)),  plugin hardware   plugin hardware   plugin hardware ,1
5830,Remove deprecated trainer_optimizer_mixin (#14887),0.84406763,Removed the deprecated TrainerLoggingMixin class (#8609),This reverts commit 7e756ca11f2d3938ef73010fe63aa45d699bf0bc.,1
5831,removed dummy d,0.688014,Removed,  hotfix Drone install Horovod   notes ,0
5832,Removed the deprecated trainer_data_loading_mixin (#14888),0.8553515,Removed trainer.reset_*_dataloader() methods (#16726),  add parsing OS env vars   fix env   Apply suggestions from code review   overwrite init   Apply suggestions from code review ,1
5833,adding license,0.4900452,Pruned requirements duplicity (#13739),  update metrics   pep8   Update pytorch_lightning/metrics/regression/explained_variance.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   add typing for testing utils   change from assert to raise exception   add test for raised shape error   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
5834,Fix MPS availability check (#13947),0.6485675,Updated logic for checking TPUs availability (#6767),,0
5835,changed read me,0.69326866,Changed,module index link in documentation is broken. Co-authored-by: pbmstrk pbmstrk@users.noreply.github.com,0
5836,Added note about custom base images (#14125),0.58404386,Allowing decorate model init with saving hparams inside (#4662),  Update init.py   Update weights_loading.rst   docs for checkpoints ,0
5837,"Unify attribute finding logic, fix not using dataloader when hparams present (#4559)",0.6226822,Fixing critical bugs in newly added hooks and hparams assignment.,,0
5838,Fix PyTorch spelling errors (#13774),0.64461887,PyTorch,This could be a useful utility elsewhere in lightning for calculating the batch size,0
5839,drop deprecated TrainResult (#5323),0.87387466,Removed deprecated TrainResult (#5323),,1
5840,Merge branch 'master' into better_simple_profiler,0.50173575,Moved profilers to their own file (#7822),  working code   add tests   fix scaling   move patch dataloader to utils   renaming   fix tests   add changelog   update docs   pep8 ,0
5841,Fix install setup - push pypi (#2872),0.72645444,"PyTorch 1.10 and Python 3.7 no longer supported (#16492, #16579)",Align the number of steps in README to the documentation Update README Fix grammar Update README.md typos Co-authored-by: pbmstrk pbmstrk@users.noreply.github.com,1
5842,Update docs,0.7347844,Updated governance docs,,1
5843,skipp drafts for full test (#7046),0.53513724,"Changed setup() and teardown() stage argument to take any of {fit,validate,test,predict} (#6386)",,0
5844,[Feat] Add auto_restart for fault tolerant training (#9722),0.724873,Manual Fault-tolerance,"  metric fix, explained variance   one more test   pep8   remove comment   fix add_state condition   Co-authored-by: ananyahjha93 ananya@pytorchlightning.ai",1
5845,Fix typos initialize in docs (#13557),0.52175856,Docs improvements,  lightning module metric tests   whitespace   pep8 ,0
5846,Raise an exception if using amp_level with native amp_backend (#9755),0.91291964,Trainer now raises an exception when requesting amp_level with native amp_backend (#9755),  ref: clean up opts docs   ref: clean up opts docs ,1
5847,Trainer: fix support for non-distributed PyTorch (#14971),0.7844393,- Fixed `Trainer` support for PyTorch built without distributed support ([#14971](https://github.com/Lightning-AI/lightning/pull/14971)),  update metric docs   doc fix   add assert desc   update based on suggestion ,1
5848,Support limit_mode_batches (int) for infinite dataloader (#2840),0.6956985,"Refactor dataloading, supports infinite dataloader (#955)",,0
5849,"Update deepdiff requirement from <6.2.3,>=5.7.0 to >=5.7.0,<6.2.4 in /requirements (#16292)",0.5835351,- Changed minimum supported version of `rich` from `10.14.0` to `12.13.0` ([#16798](https://github.com/Lightning-AI/lightning/pull/16798)),,0
5850,unify LightningEnum (#5389),0.7584511,LightningLite:,,1
5851,Update fastapi requirement from <=0.79.0 to <0.83.0 in /requirements (#14576),0.58464515,Updated precision attributes in DeepSpeedPlugin (#10164),,0
5852,Update changelog after 1.9.2 release (#16777),0.6960262,Full Changelog,"  make current_epoch and global_step to be same as trainer, after model restore.   remove assignment here   test   minor modification   merge with parent's master   [bug-fix]: update trainer properties   minor comment fix   minor comment fix   reset train loader in on_train_epoch_start hook   makes sure the changes work   minor chane   update changelog   adding unit test for reload_dataloaders_every_epoch arg   modified changelog, to add PR number   revert imports   changes to unit test   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com",0
5853,remove last of bad result obj warning (#3073),0.67725784,Removed callback metrics from test results obj (#2994),,0
5854,Fix typo in definition of world size in docs (#15954),0.52114624,Changed the model size calculation using ByteCounter (#10123),,0
5855,rtfd: try another redirect (#17018),0.5681273,Avoid relpath bug on Windows (#16164),,0
5856,Fix CI crash on coverage upload timeout (#2548),0.49235487,Raised a MisconfigurationException if batch transfer hooks are overriden with IPUAccelerator (13961),,0
5857,3/n inter batch parallelism (#9052),0.89698017,Inter Batch Parallelism,,1
5858,[App] Stop App when it has succeeded (#15801),0.6316374,App,,0
5859,Refactor PredictionLoop.on_run_start for consistency (#12732),0.6296874,"The ModelCheckpoint.save_on_train_epoch_end attribute is now computed dynamically every epoch, accounting for changes to the validation dataloaders (#15300)",,0
5860,Typo fix: on_training_epoch_end -> on_train_epoch_end (#17110),0.7273388,training_end >> training_epoch_end,,1
5861,Follow-up changes to #10575 (#10957),0.6328557,[1.5.5] - 2021-12-07,,0
5862,Test deprecated API for 0.8.0 and 0.9.0 (#1071),0.74588984,Removed deprecated API (#2073),,1
5863,neptune.init deprecation fix (#15393),0.59557885,nvidia/apex deprecation (#16039),,0
5864,Added gradient clip test for native AMP (#3754),0.64105535,training AMP scaling refactor (#3135),,0
5865,Fix pip install too,0.7729709,pip install rich,,1
5866,Remove deprecated LightningDataModule.val_transforms (#12763),0.8418179,- Removed the deprecated `val_transforms` argument from the `LightningDataModule` constructor ([#12763](https://github.com/Lightning-AI/lightning/pull/12763)),,1
5867,Allow user to specify 'step' key while logging metrics (#808),0.74547815,"Our logging mechanism previously supported log(""key"", {""something"": 123}) (not using log_dict). However, this added significant complexity to the implementation with little benefit, as these keys could not be monitored by our Callbacks and most logger implementations do not support this notation. If you were using this feature with a compatible logger, you can still publish data directly to the Logger using self.logger.log_metrics().",,1
5868,fixed bug where tuner would not tune lr if also tuning batch_size (#4688),0.65510345,tuner.scale_batch_size(...),,0
5869,Update CHANGELOG after the 2.0.1 release (#17235),0.70237833,Here is a selection of important changes that are not backward compatible with versions < 1.5. The full list of changes and removals are listed in the changelog at the bottom.,,1
5870,Removed subsection in LightningDataModule (#11675),0.741171,Deprecated LightningDataParallel in favor of new wrapper module LightningParallelModule (#5670),,1
5871,Update issues templates (#10537),0.6290922,Use correct python version in lightning component template (#13790),,0
5872,Simplify bug report template (#14925),0.66257375,Simplify optimization Logic (#4984),,0
5873,fix metric docs (#5880),0.79070747,"docs for all Metrics (#2184, #2209)",,1
5874,Improve code quality in AcceleratorConnector._configure_slurm_ddp  (#10102),0.72483635,Deprecated access to the AcceleratorConnector.configure_slurm_ddp method and marked it as protected (#10101),,1
5875,updated support for 1.2.0 (#80),0.69882095,"    version=""0.0.1"",",  videos in trainer api   videos in docs   videos in docs   videos in trainer api   videos in docs   videos in docs   videos in docs   videos in docs   Update new-project.rst   docs   Update new-project.rst ,0
5876,"Remove the ""_precision"" suffix from some precision plugin files (#10052)",0.73344356,precision plugins (#3504),  removed deprecated flags   removed es callback flag ,1
5877,release v,0.6539766,This release includes:, metrics integration into self.log  Co-authored-by: Teddy Koker teddy.koker@gmail.com  ddp and regualr test for self.log + metrics  Co-authored-by: Teddy Koker teddy.koker@gmail.com   pep8   fix log tests   Co-authored-by: Teddy Koker teddy.koker@gmail.com  docs  Co-authored-by: Teddy Koker teddy.koker@gmail.com Co-authored-by: Teddy Koker teddy.koker@gmail.com,0
5878,"stable, dev PyTorch in Dockerfile and conda gh actions (#3074)",0.60054433,PyTorch 1.5  support,,0
5879,Update LightningCLI tests to reflect changes in jsonargparse 4.6.0 (#12704),0.78086936,- Changes in `LightningCLI` required for the new major release of jsonargparse v4.0.0 ([#10426](https://github.com/PyTorchLightning/pytorch-lightning/pull/10426)),,1
5880,Remove the deprecated LightningDataModule.test_transforms  (#12773),0.8806213,- Removed the deprecated `test_transforms` argument from the `LightningDataModule` constructor ([#12773](https://github.com/Lightning-AI/lightning/pull/12773)),  train_batch_end outputs   added tests for the output hooks ,1
5881,mark FitLoop.should_accumulate as protected (#9515),0.9649236,Marked FitLoop.should_accumulate as protected (#9515),,1
5882,Gpu mem (#308),0.5960283,"device parser (#3400, #3405)",,0
5883,Improve @RunIf docs (#10828),0.614242,Docs improvements,  use viewcode instead of linkcode   use viewcode instead of linkcode   Apply suggestions from code review   Co-authored-by: William Falcon waf2107@columbia.edu  Apply suggestions from code review  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: William Falcon waf2107@columbia.edu,0
5884,[IPU] Add special tests for IPUs 2/n (#7833),0.6217348,Graphcore IPU devices,,0
5885,Error if dataset size = 1 batch. (#141),0.6897326,"    batch_size=32,",  sequential data docs fix   TPU support docs fix   Co-authored-by: Iryna Koroliuk irynakoroliuk@Irynas-MacBook-Pro.local,0
5886,PyTorch documentation updates (#11739),0.7978573,PyTorch," Documentation Fixes  Just did some scanning for errors. Fixed indentation spelling, and grammar changes.  Apply suggestions from code review  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com",1
5887,"Add support for init_meta_context, materialize_module (#9920)",0.66403985,Meta Module, initial commit  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  updated  Signed-off-by: ericharper complex451@gmail.com  doc clean up  Co-authored-by: William Falcon waf2107@columbia.edu,0
5888,Refactor CombinedLoader using pytrees (#16714),0.67912513,from lightning.pytorch.utilities import CombinedLoader,  add functional docs   drop   format   Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai,0
5889,Update changelog after v1.5.4 release (#10843),0.7349621,Full Changelog,  missing logs   chlog   add chlo to docs   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   chlog   docs   log   metrics   logs   log   format   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
5890,Fixes #3276 (#4116),0.64763963,Resolve bug with Finetuning (#5744),  latest restore   latest restore ,0
5891,Merge pull request #12920 from PyTorchLightning/rename/lightning_extension,0.70012534,- Added support for returning a single Callback from `LightningModule.configure_callbacks` without wrapping it into a list ([#11060](https://github.com/PyTorchLightning/pytorch-lightning/pull/11060)),  Minor formatting & grammar fixes in docs   Few more tentative doc fixes ,1
5892,Refactor checkgroup to avoid duplicated checks (#15633),0.6165085,Changed type checker with explicit cast of ref_model object (#4457),Co-authored-by: Iryna Koroliuk irynakoroliuk@Irynas-MacBook-Pro.local,0
5893,move optional section (#11011),0.5619465,moved ___step_end hooks (#3130),,0
5894,Fixes #280 (#309),0.6305165,"refactor DDP backend (#3204, #3207, #3208, #3209, #3210)",  Update early_stopping.rst   Update init.py   Update new-project.rst   Update early_stopping.rst   Update init.py   Update early_stopping.rst   Update init.py   Co-authored-by: William Falcon waf2107@columbia.edu,0
5895,[DOCS] Updated link for lightning_lite.rst (#13331),0.77240026,Update the Lightning App docs (#13537),  restore functional metrics   clean   fix ,1
5896,Run standalone tests in batches (#13673),0.6761331,Run batch size finder for validate/test/predict.,,0
5897,Attempt slurm auto resume call when non-shell call fails (#6002),0.97576404,Attempted SLURM auto resume call when non-shell call fails (#6002),  threshold   threshold ,1
5898,"Revert ""[CI] Comment flaky tests (#10084)"" (#10580)",0.58216655,The point of this release is more bug fixes ahead of v 1.0.0. We now have CI tests on TPU thanks to @zcain117 from Google! :slightly_smiling_face:,  skip file   todo   skip   skip   note   move ,0
5899,Fix torch.distributed._* import statements in tests (#11416),0.70694876,import torch,  clean and organize fit   clean and organize fit   clean and organize fit   clean and organize fit   clean and organize fit ,1
5900,Fix build Docker releases (#1783),0.5873145,This release fixes that core issue,,0
5901,reduced accelerator selection (#3211),1.0,reduced accelerator selection (#3211),  added tests for multiple optimizers and dataloaders   added tests for multiple optimizers and dataloaders   added tests for multiple optimizers and dataloaders ,1
5902,Update psutil requirement from <=5.9.1 to <5.9.3 in /requirements (#14665),0.63564575,The psutil package is now required for CPU monitoring (#17010),,0
5903,More robust way of collecting init argument names for LightningModules (#3066),0.6759268,- Added support for passing extra init-parameters to the `LightningDataModule.from_datasets` ([#14185](https://github.com/Lightning-AI/lightning/pull/14185)),  Added test to ensure ckpt filepath contains the correct val score reported from the trainer   Modified to check all saved ckpt files ,0
5904,Fixes #2455 (#2463),0.5959475,Resolve bug with Finetuning (#5744),,0
5905,fix on_fit_start (#3616),0.65436995,| on_pretrain_routine_start  | on_fit_start                 | ,  Update supporters.py   Update CHANGELOG.md   Update supporters.py   Update supporters.py   Update supporters.py   Update supporters.py   Update supporters.py   Update supporters.py   Update CHANGELOG.md   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
5906,update GitHub templates (#601),0.58372706,- Removed the deprecated `BaseProfiler` and `AbstractProfiler` classes ([#14404](https://github.com/Lightning-AI/lightning/pull/14404)),  moves configure ddp to each backend   moves configure ddp to each backend   moves configure ddp to each backend   added torch manual seed in test_mean_error   test for complicated batch structure   test for complicated batch structure   test for complicated batch structure   Co-authored-by: ananyahjha93 ananya@pytorchlightning.ai,0
5907,[feat] Add PyTorch Profiler. (#5560),0.75552434,PyTorch,"  base   add xfail   new test   import   missing import   xfail if not installed   include mkpatch fix test  mock comet  comet mocks fix test remove dep undo merge duplication   line   line   convert doctest   doctest   docs   prune Results usage in notebooks (#3911)   notebooks   notebooks   revamp entire metrics (#3868)   removed metric   Co-authored-by: Teddy Koker teddy.koker@gmail.com  added new metrics  Co-authored-by: Teddy Koker teddy.koker@gmail.com  pep8  Co-authored-by: Teddy Koker teddy.koker@gmail.com  pep8  Co-authored-by: Teddy Koker teddy.koker@gmail.com  docs  Co-authored-by: Teddy Koker teddy.koker@gmail.com  docs  Co-authored-by: Teddy Koker teddy.koker@gmail.com  win ddp tests skip  Co-authored-by: Teddy Koker teddy.koker@gmail.com  win ddp tests skip  Co-authored-by: Teddy Koker teddy.koker@gmail.com  win ddp tests skip  Co-authored-by: Teddy Koker teddy.koker@gmail.com  win ddp tests skip  Co-authored-by: Teddy Koker teddy.koker@gmail.com  reset in compute, cache compute  Co-authored-by: Teddy Koker teddy.koker@gmail.com  reduce_ops handling  Co-authored-by: Teddy Koker teddy.koker@gmail.com  sync -> sync_dist, type annotations  Co-authored-by: Teddy Koker teddy.koker@gmail.com  wip docs  Co-authored-by: Teddy Koker teddy.koker@gmail.com   mean squared error   docstring   added mean ___ error metrics   added mean ___ error metrics   seperated files   accuracy doctest   gpu fix   remove unnecessary mixin   metric and accuracy docstring   Co-authored-by: Teddy Koker teddy.koker@gmail.com  metric docs  Co-authored-by: Teddy Koker teddy.koker@gmail.com  pep8, changelog  Co-authored-by: Teddy Koker teddy.koker@gmail.com   refactor dist utils, pep8   refactor dist utils, pep8   Co-authored-by: Teddy Koker teddy.koker@gmail.com   Callback docs with autosummary (#3908)   callback docs with autosummary   do not show private methods   callback base docstring   skip some docker builds (temporally pass) (#3913)   skip some docker builds   todos   skip   use badges only with push (#3914)   testtube   mock test tube   mock mlflow   remove mlflow   clean up   test   test   test   test   test   test   code blocks   remove import   codeblock   logger   wandb causes stall   Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Ananya Harsh Jha ananya@pytorchlightning.ai Co-authored-by: Teddy Koker teddy.koker@gmail.com Co-authored-by: Jeff Yang ydcjeff@outlook.com",1
5908,Reduce state size (#13970),0.657194,Reduction when batch_size < num_gpus (#1609),  test for complicated batch structure   test for complicated batch structure ,0
5909,Fix ROC metric for CUDA tensors (#2304),0.56056595,Progress bar metrics tensors are now converted to float (#5692),  fixes #3871   :white_check_mark: tests   :white_check_mark: tests   :white_check_mark: tests   :white_check_mark: tests   :white_check_mark: tests   :white_check_mark: tests   :white_check_mark: tests   moves sync bn to each backend   moves sync bn to each backend   Co-authored-by: nateraw nxr9266@g.rit.edu,0
5910,added reduce ddp results on eval (#2434),0.64097166,fix result obj DP auto reduce (#3013),,0
5911,auto state-dict and remove the way the model is loaded during hpc,0.64237833,Deprecated mode='auto' from ModelCheckpoint and EarlyStopping (#4695),,0
5912,Fixed setting of _save_dir when run initiated externally (#7106),0.6253124,  * `save_config_filename`,  base   add xfail   new test   import   missing import   Co-authored-by: William Falcon waf2107@columbia.edu,0
5913,Deprecate LightningModule.summarize() in favor of pl.utilities.model_summary.summarize() (#8513),0.90546787,Deprecated LightningModule.summarize() in favor of pytorch_lightning.utilities.model_summary.summarize() (#8513),,1
5914,fixed none name,0.55422115,    return None,  base   mock test   Co-authored-by: William Falcon waf2107@columbia.edu,0
5915,update,0.6971793,Key updates,,0
5916,Move progress file (#16524),0.66640365,Progress tracking,Co-authored-by: Teddy Koker teddy.koker@gmail.com Co-authored-by: Teddy Koker teddy.koker@gmail.com,0
5917,CODEOWNERS: add wandb (#9374),0.5612881,"Accessing dataloaders (#16726, #16800)", xfail if not installed  include mkpatch fix test  mock comet  comet mocks fix test remove dep undo merge duplication   line   line   convert doctest   doctest   docs ,0
5918,move _HPU_AVAILABLE (#16713),0.66030097,Moveed HPU broadcast override to the HPU strategy file (#17011),  Update loggers.rst   Update loggers.rst   Update index.rst   Create logging.rst   Delete experiment_reporting.rst   Delete experiment_logging.rst   Update init.py ,0
5919,Fix saved filename in ModelCheckpoint if it already exists (#4861),1.0,Fix saved filename in ModelCheckpoint if it already exists (#4861),,1
5920,tracks all outputs including TBPTT and multiple optimizers (#2890),0.9942254,Tracks all outputs including TBPTT and multiple optimizers (#2890),  skip some docker builds   todos   skip ,1
5921,Fix docs typo (#2880),0.5297842,Rename failed -> error in tables (#15608),  callback docs with autosummary   do not show private methods   callback base docstring ,0
5922,Deprecate BaseProfiler in favor of Profiler (#12150),0.64349365,Removed ProfilerConnector (#7654)," removed metric  Co-authored-by: Teddy Koker teddy.koker@gmail.com  added new metrics  Co-authored-by: Teddy Koker teddy.koker@gmail.com  pep8  Co-authored-by: Teddy Koker teddy.koker@gmail.com  pep8  Co-authored-by: Teddy Koker teddy.koker@gmail.com  docs  Co-authored-by: Teddy Koker teddy.koker@gmail.com  docs  Co-authored-by: Teddy Koker teddy.koker@gmail.com  win ddp tests skip  Co-authored-by: Teddy Koker teddy.koker@gmail.com  win ddp tests skip  Co-authored-by: Teddy Koker teddy.koker@gmail.com  win ddp tests skip  Co-authored-by: Teddy Koker teddy.koker@gmail.com  win ddp tests skip  Co-authored-by: Teddy Koker teddy.koker@gmail.com  reset in compute, cache compute  Co-authored-by: Teddy Koker teddy.koker@gmail.com  reduce_ops handling  Co-authored-by: Teddy Koker teddy.koker@gmail.com  sync -> sync_dist, type annotations  Co-authored-by: Teddy Koker teddy.koker@gmail.com  wip docs  Co-authored-by: Teddy Koker teddy.koker@gmail.com   mean squared error   docstring   added mean ___ error metrics   added mean ___ error metrics   seperated files   accuracy doctest   gpu fix   remove unnecessary mixin   metric and accuracy docstring   Co-authored-by: Teddy Koker teddy.koker@gmail.com  metric docs  Co-authored-by: Teddy Koker teddy.koker@gmail.com  pep8, changelog  Co-authored-by: Teddy Koker teddy.koker@gmail.com   refactor dist utils, pep8   refactor dist utils, pep8   Co-authored-by: Teddy Koker teddy.koker@gmail.com",0
5923,Implement partial auroc metric (#3790),0.78320503,Deprecated reorder parameter of the auc metric (#4237),  notebooks   notebooks ,1
5924,xfail if not installed (#3860),0.53875434,- Fixed an issue with the `TPUSpawnPlugin` handling the `XLA_USE_BF16` environment variable incorrectly ([#10990](https://github.com/PyTorchLightning/pytorch-lightning/pull/10990)),  Added test for logging in validation step when using dict dataset with string value   fix recursive issue   fix recursive issue   Co-authored-by: Nathan Painchaud nathanpainchaud@gmail.com Co-authored-by: William Falcon waf2107@columbia.edu,0
5925,Add stronger typing to gradient accumulation scheduler callback (#3558),0.62688637,Simplified logic for updating the learning rate for schedulers (#7682),  nb steps   if   skip   rev   seed   seed ,0
5926,Add note about returning None (#5578),0.76634943,    return None,  add current epoch to __dumped_params   log   reset   add to test   Update CHANGELOG.md   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
5927,Addressed code review points,0.6282978,- Code coverage (99%),"  tpu device check   replaced with xmp spawn   Revert ""replaced with xmp spawn""   This reverts commit 6835380f   replaced all instances of XLA_AVAILABLE   moved inner_f to global scope   made refactors   added changelog   added TPU_AVAILABLE variable   fix codefactor issues   removed form trainer and early stopping   add TORCHXLA_AVAILABLE check   added tests   refactoring   Update pytorch_lightning/utilities/xla_device_utils.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   updated function names   fixed bug   updated CHANGELOG.md   added todo   added type hints   isort and black   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: William Falcon waf2107@columbia.edu",0
5928,separate requirements for logger dependencies (#792),0.68752533,for logger in loggers:,  Makes sure logging doesn't ever happen from non-root zero   Makes sure logging doesn't ever happen from non-root zero   Makes sure logging doesn't ever happen from non-root zero   added bug report model   fix local model   fix local model   fix local model   fix local model ,0
5929,Link full code example for LightningLite (#12568),0.7691509,Introduce lightning connect (#14452),,1
5930,Type Hints for Lightning Core (#946),0.7317296,Introduce lightning connect (#14452),,1
5931,Add BatchSizeFinder callback (#11089),0.7539902,Batch Size Finder (#11089),  Rename row_log_interval -> log_every_n_steps log_save_interval -> flush_logs_every_n_steps   Changelog   fixed title underline length   typo   Update pytorch_lightning/trainer/trainer.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/trainer/trainer.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   pep8 + deprecation test   'todo: remove in 1.1 comment'   1.1 -> 0.11   log   docs   depr API   add depr tests   note   miss   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai,1
5932,build more docker configs (#3533),0.5505241,"Redesigned multi-dataloader support (#16743, #16784, #16939)",include mkpatch fix test,0
5933,Add Fabric.all_reduce (#16459),0.7439604,- Added `Fabric.all_reduce` ([#16459](https://github.com/Lightning-AI/lightning/pull/16459)),,1
5934,pt dpp some ignores,0.63072336,"Enables DP, but with many limitations",,0
5935,Teardown all internal components on exception (#11620),0.5744976,Changed teardown() in Accelerator to allow training_type_plugin to customize teardown logic (#7579),  fix lr finder   changelog   add test ,0
5936,Fix CLI race condition saving the config (#11199),0.59814745,- Fixed an issue when using the CLI without arguments ([#14877](https://github.com/Lightning-AI/lightning/pull/14877)),,0
5937,Fix set_epoch not getting called for prediction dataloaders (#16785),0.6816229,The dataloader wrapper returned from .setup_dataloaders() now calls .set_epoch() on the distributed sampler if one is used (#16101),  fix docker repo issue   docker   docker   docker   no cudnn   no cudnn   try 16.04   Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai,0
5938,fix val logging (#362),0.7786733,bug fix with logging val epoch end + monitor (#3812),,1
5939,Support loading a checkpoint with QAT (#11346),0.67729473,- Fixed an issue with resuming from a checkpoint trained with QAT ([#11346](https://github.com/PyTorchLightning/pytorch-lightning/pull/11346)),"  ref   Mocking Loggers (part 3c, comet) (#3859)   mock comet   new line ",0
5940,[App] Re-wording build config warning in the docs (#15570),0.5936334,"Show a message when BuildConfig(dockerfile=""..."") is passed but a Dockerfile file is already present in the Work (#15799)",  Fix docs for auto_lr_find   change testcode to codeblock   we are not showing a complete example here,0
5941,return simple docs to methods (#3645),0.59170616,"docs for all Metrics (#2184, #2209)",  extensive mlflow test   revert accidental commits ,0
5942,Various test fixes (#15068),0.669978,Deprecated the TestTubeLogger (#9065),"  Fixes #3668, #3887 as a bonus   Fixes #3668, #3887 as a bonus ",0
5943,Fix registry typing annotation (#17489),0.56038105,Refactored setup for typing friendly (#6590),  :sparkles: add self.write_prediction   :sparkles: add self.write_prediction_dict to lightning module ,0
5944,Fix isort failures in loggers (#5527),0.7547525,Removed LoggerStages (#5673),  Fix.   Fix #2550: allow to load model from checkpoint if self.save_hyperparameters() was not called.   Fix? Cleaner way of not calling self.save_hyperparameters in EvalModelTemplate.   Fix? _load_model_state cleanup   Fix?   Fix #2550: allow to load model from checkpoint if self.save_hyperparameters() was not called.   Fix.   Fix? Cleaner way of not calling self.save_hyperparameters in EvalModelTemplate.   Fix? _load_model_state cleanup   Fixed side effect in test_load_model_from_checkpoint_extra_args.   Apply suggestions from code review   fix   try   fixed missing arg in evalmodel   fixed missing arg in evalmodel   fix   update   fix loading   add test   prune   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai Co-authored-by: William Falcon waf2107@columbia.edu,1
5945,scheduled removal of BaseProfiler.output_filename in favor of dirpath… (#9214),0.90143967,Removed deprecated BaseProfiler.output_filename arg from it and its descendants in favor of dirpath and filename (#9214),"  make current_epoch and global_step to be same as trainer, after model restore.   remove assignment here   test   minor modification   Update pytorch_lightning/core/lightning.py   type check, better clarity Co-authored-by: ananthsub ananth.subramaniam@gmail.com  Update pytorch_lightning/core/lightning.py  type check, better clarity Co-authored-by: ananthsub ananth.subramaniam@gmail.com   comments for current_epoch and global_step properties   Update tests/models/test_restore.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   update comments according to the changes made   Update tests/models/test_restore.py   add current_epoch, global_step to jit ignore list   Add comments to CHANGELOG   Update CHANGELOG.md   Update tests/models/test_restore.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",1
5946,CI: Upload video artifacts from e2e tests (#14467),0.48372796,nvidia/apex deprecation (#16039),  add test for checkpoint nan   fix   pep ,0
5947,Bugfix/2956 tpu distrib backend fix (#2959),0.6583766,"refactor DDP backend (#3204, #3207, #3208, #3209, #3210)",  Fixes #2678 - enables training_step to return None   Fixes #2678 - enables training_step to return None ,0
5948,Unroll dict input before call Accelerator X_steps (#10908),0.6396775,Simplify accelerator steps (#5015),*_epoch_out methods expects a return of None.,0
5949,fix rltv imports,0.5990041,from setuptools import setup,,0
5950,Update precision docs (#11010),0.66718745,Docs improvements,,0
5951,prepare space for fused docs (#14160),0.5996648,"docs for all Metrics (#2184, #2209)",,0
5952,pointer to trainer in model,0.6858514,trainer = Trainer(callbacks=[ModelSummary(max_depth=1)]),,0
5953,Update unet.py (#1955),0.61215496,"Removed PyTorch 1.6 support (#10367, #10738)",  fixed model checkpoint frequency   fixed model checkpoint frequency   fixed model checkpoint frequency   fixed model checkpoint frequency   merged ,0
5954,Enhance reduce_boolean_decision to accommodate any-analogous semantics expected by EarlyStopping Callback (#15253),0.9919036,Enhanced reduce_boolean_decision to accommodate any-analogous semantics expected by the EarlyStopping callback (#15253),,1
5955,Update notebooks submodule head (#15432),0.58035105,Moved DeviceDtypeModuleMixin and HyperparametersMixin mixin to core (#8396),  mock neptune base tests   neptune doctest   remove extra   mock loggers   typo   mock import   neptune not compatible with multigpu   add back experiment ,0
5956,Update type hints for multiple dataloaders in .fit() and .test() (#1723),0.71425855,Removed test_dataloaders parameter from Trainer.fit() (#1434),  added tbptt test for logging   added tbptt test for logging ,1
5957,Fix for accuracy calculation (#2183),0.6698186,Renaming of precision recall metric (#3308),  fix #3798   added tbptt test for logging ,0
5958,Use sklearn in runif (#15426),0.56861687,MLFlowLogger now accepts run_name as an constructor argument (#7622),  update tests with EarlyStopping default   imports   revert legacy tests   fix test   revert   revert ,0
5959,"Fix cloud e2e, artifacts and cleanup (#14392)",0.5836617,Refactor cloud dispatch and update to new API (#16456),,0
5960,Fix a typo in README (#4474),0.52561474,Changed overwrite to True (#16009),  Add failing test   force all tbptt vals to be floats for reduce   Co-authored-by: William Falcon waf2107@columbia.edu,0
5961,fix auto scale batch size not working with precision=16 (#3045),0.6863054,Tune batch size,  Add back sanity checks   pep ,0
5962,Fix typo of handling (#2625),0.54834044,Rename failed -> error in tables (#15608),,0
5963,Update pre-commit hook versions (#11202),0.64387274,Updated hooks arguments - breaking for setup and teardown (#2850),  test selecting the correct backend. tem backends while slurm and TE are decoupled   test selecting the correct backend. tem backends while slurm and TE are decoupled ,0
5964,add note (#9141),0.67236817,NOTE,  ref: routed epoch outputs to logger   ref: routed epoch outputs to logger   ref: routed epoch outputs to logger   ref: routed epoch outputs to logger ,0
5965,Fixes lack of logging in logger (#319),0.7963218,Removed LoggerStages (#5673),  use docker in conda CI   update env if needed   update with pip   remove setting pytorch ,1
5966,Fix validation when accelerator is a string (#13417),0.72497255,- Fixed the input validation for the accelerator Trainer argument when passed as a string ([#13417](https://github.com/Lightning-AI/lightning/pull/13417)),,1
5967,Add cpu device parser to validate cpu devices (#12160),0.7054213,"device parser (#3400, #3405)",  ref: adding compute environments (2/n)   ref: adding compute environments (2/n)   ref: adding compute environments (2/n)   ref: adding compute environments (2/n) ,1
5968,Set pep8speaks' max-line-length to 120 (same as black) (#3173),0.5559156,python script.py fit --trainer.max_epochs=123,  Fix val_progress_bar total with num_sanity_val_steps   chlog   Fix val_progress_bar total with num_sanity_val_steps   move test   replaced with sanity flag and suggestions ,0
5969,pypi releasing fix lower (#16406),0.74987435,Drop PyTorch 1.9 support (#15347),  added broadcast option to tpu   add device   moved tpu broadcast to tpu_backend   removed Lightning dist   decode bytes   pep8 fix   fix bug   test for broadcast   updated changelog ,1
5970,fixed restore location,0.53054243,Restore original loaders if replaced by entrypoint (#8885),  ref: adding compute environments (1/n)   ref: adding compute environments (1/n)   ref: adding compute environments (1/n) ,0
5971,Reroute profiler to profilers (#12308),0.7492845,Moved profilers to their own file (#7822),  Explicitly point out where should we set the random seed   Update docs/source/multi_gpu.rst   Co-authored-by: Jeff Yang ydcjeff@outlook.com Co-authored-by: Qinru Li q4li@eng.ucsd.edu Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: Jeff Yang ydcjeff@outlook.com,1
5972,fixing publish pypi (#15361),0.72834885,"PyTorch 1.10 and Python 3.7 no longer supported (#16492, #16579)",  refactored callback system and init ddp   refactored callback system and init ddp   refactored callback system and init ddp   refactored callback system and init ddp ,1
5973,Check if the scheduler already has reduce_on_plateau (#13838),0.7302019,- Improved support for custom `ReduceLROnPlateau` scheduler if `reduce_on_plateau` is set by the user in scheduler config ([#13838](https://github.com/Lightning-AI/lightning/pull/13838)),,1
5974,release v0.12,0.7907394,"    version=""0.0.1"",",  ref: fix epoch logging   verified epoch logging   verified epoch logging   verified epoch logging   verified epoch logging   verified epoch logging   verified epoch logging   verified epoch logging   verified epoch logging ,1
5975,deterministic=True (#2944),0.7567011,Determinism,  ref: finish #3733   remove deprecated test   remove deprecated test   remove deprecated test   remove deprecated test   remove deprecated test   remove deprecated test   remove deprecated test   remove deprecated test   remove deprecated test   remove deprecated test   remove deprecated test   remove deprecated test   remove deprecated test   Update pytorch_lightning/accelerators/ddp_backend.py   Co-authored-by: ananthsub ananth.subramaniam@gmail.com   remove deprecated test   remove deprecated test   remove deprecated test   Co-authored-by: ananthsub ananth.subramaniam@gmail.com,1
5976,Update default gym env version to CartPole-v1 (#7079),0.5934527,| Enum TrainerFn.TUNING                                    | 1.10             | No longer supported         |,  added test_val_check_interval tests   added test_val_check_interval tests   added test_val_check_interval tests ,0
5977,feature: Allow str arguments in Trainer.profiler (#3656),0.71367633,Removed support for passing a bool value to profiler argument of Trainer (#6164),  special case http for torch hub load   Update CHANGELOG.md   Update test.txt ,1
5978,Expose deprecated arguments from logger base interface (#12609),0.70937043,Removed wandb logger's finalize method (#1193),,1
5979,Allow sharing secrets on TPU tests (#15289),0.58489776,TPU training (#2708),  ref: fix metric err   ref: fix metric err   ref: fix metric err   ref: merge   ref: merge   ref: merge   ref: merge   ref: decoupled ddp2   ref: decoupled ddp2   ref: decoupled ddp2   ref: decoupled ddp2   ref: decoupled ddp2   ref: clean up ddp before final fix   ref: clean up ddp before final fix   ref: clean up ddp before final fix   ref: clean up ddp before final fix   ref: clean up ddp before final fix   ref: clean up ddp before final fix   ref: clean up ddp before final fix   ref: clean up ddp before final fix   ref: clean up ddp before final fix   ref: clean up ddp before final fix   ref: clean up ddp before final fix ,0
5980,Invalid cache before listing drive when collecting component names (#13971),0.54897,"Accessing dataloaders (#16726, #16800)",  ref: clean up ddp before final fix   ref: clean up ddp before final fix   ref: clean up ddp before final fix   ref: clean up ddp before final fix   ref: clean up ddp before final fix ,0
5981,[pre-commit.ci] pre-commit autoupdate (#7475),0.52964795,The preemption/termination signal is now configurable (#14626):,,0
5982,Add Open MPI installation details for horovod (#2050),0.6207082,"refactored Horovod backend (#3121, #3122)",  docs update   docs update   suggestions   Update docs/source/introduction_guide.rst   Co-authored-by: William Falcon waf2107@columbia.edu,0
5983,Raise exception if rich is less than 10.2.2 (#10839),0.73145473,Improved exception message if rich version is less than 10.2.2 (#10839),  ref: separate slurm from ddp   ref: separate te from ddp   ref: merge   ref: merge   ref: merge ,1
5984,App: Fix frontends when using multiprocessing in the cloud (#17324),0.8394656,Fix frontend hosts when running with multi-process in the cloud (#17324),,1
5985,Update strategy registry docs (#11311),0.6896858,Updated governance docs,  ref: separate te from ddp   ref: separate te from ddp   ref: separate te from ddp ,0
5986,Integrate lightning_utilities==0.4.2 (#15817),0.82620466,- Integrate the `lightning_utilities` package (,,1
5987,Use single quotes in action job (#10579),0.43815798,    # use the last 4 numbers in the job id as the id,,0
5988,Load app before setting LIGHTNING_DISPATCHED (#16057),0.74575734,Add support for Lightning App Commands through the configure_commands hook on LightningFlow and ClientCommand  (#13602),  init   add test   changelog and docs   fix test   Apply suggestion from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
5989,prune Drone build (#5871),0.4952159,Made TensorBoardLogger and CometLogger pickleable (#2518),  Update ddp_spawn_backend.py   Update ddp_cpu_spawn_backend.py   log   Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai,0
5990,missing logs [to be merged before release][skip ci] (#3131),0.66139984,Removed LoggerStages (#5673),,0
5991,Tests: refactor loggers (#1689),0.72592175,Refactored logging,  ref: part 7 of #3733   ref: part 7 of #3733 ,1
5992,Remove the deprecated pytorch_lightning.callbacks.base module (#16319),0.9554819,Removed the deprecated pytorch_lightning.callbacks.base module in favor of pytorch_lightning.callbacks.callback (#16319),,1
5993,CI: drop unused tests (#15280),0.63814986,Deprecated the TestTubeLogger (#9065),  Use timestamp+pythonVersion to form the docker image tag.   Remove temporary step to check new env var. ,0
5994,Explicitly mention disabling validation in trainer docs (#13148),0.74696434,Deprecated the Trainer.disable_validation property in favor of not Trainer.enable_validation (#8291),,1
5995,removed decorators (#1079),0.6890287,Removed deprecated: (#2760),,0
5996,unblock legacy checkpoints (#15798),0.6871636,Resuming from checkpoints (#16167),  revert backend types   todo   todo ,0
5997,Remove else check,0.642369,Removed,  Update how-to-question.md   Update how-to-question.md   Apply suggestions from code review   typo   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
5998,Remove the deprecated run_stage (#14870),0.7310652,Removed deprecated: (#2760),  typo   path   check   trigger   fix conda   pip ver   fix cuda   fix XLA   fix xla   ci   docker   BIULD   unBIULD   update   py 3.8   apex   apex ,1
5999,fixed tests,0.6725936,- fixed all the .test() calls,  Fix exception chaining   names   Change exception names for consistency   Co-authored-by: Nicki Skafte skaftenicki@gmail.com  Change exception names for consistency  Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai Co-authored-by: Nicki Skafte skaftenicki@gmail.com,0
6000,Fix boring app test: debug=True when running on the cloud (#14751),0.6324415,Updated app testing (#16000),  ref: part 4 of #3733   ref: part 4 of #3733   ref: part 4 of #3733 ,0
6001,ci: adding jupyter component (#16391),0.57952416,Implemented ready for components (#16129),  ref: part 4 of #3733   ref: part 4 of #3733   ref: part 4 of #3733   ref: part 4 of #3733 ,0
6002,Sort simple profiler summary based on mean duration (#11671),0.647335,Changed Simple Profiler report to order by percentage time spent + num calls (#4880), changed to jit_unsed_properties,0
6003,[App] Enable to register data connections (#16670),0.71525556,"Accessing dataloaders (#16726, #16800)",  aggregation testing   add more tests   mse   more tests   fix tests   fix doctest   fix codefactor   fix import error   fix doctest   revert docfix   test for model integration   fix integration test   added test cases   fix rmsle   aggregation testing   add more tests   mse   more tests   fix tests   fix doctest   fix codefactor   fix import error   fix doctest   revert docfix   test for model integration   fix integration test   fix psnr   add warning/valueerror to embedding similarity   fixed f scores   disable some test   fix tests   fixing codefactor   fix pep8   changelog   fix doctest   cleaning test   fix pickle error   pickle fix   fix pickle error   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   code cleanup + changes based on suggestions   update based on suggestion   update based on suggestions   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Nicki Skafte nugginea@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
6004,uniques docs artefact name (#5336),0.54162776,Renamed utils modules (#5199),  ref: #3733 part 2   ref: #3733 part 2 ,0
6005,set device to root gpu (#3042),0.5693954,"device parser (#3400, #3405)",  ref: part a of #3733   ref: part a of #3733 ,0
6006,Add tracking of basic states in Trainer [wip - to-be-merged after v0.9] (#2541),0.70975596,W&B log in sync with Trainer step (#4405),  change accuracy error to warning   changelog ,1
6007,fix progress bar restart with fault-tolerant training enabled (#9310),0.71861154,Changed progress_bar_refresh_rate trainer flag to disable progress bar when setting to 0. (#1108),  skip best_model_path if checkpoint_callback is None   removed test ,1
6008,Update changelog after 1.9.3 and bump version for RC (#16833),0.61168796,Full Changelog: https://github.com/Lightning-AI/lightning/compare/1.8.3...1.8.3,  Add datamodule parameter to lr_find()   Fixed missing import   Move datamodule parameter to end   Add datamodule parameter test with auto_lr_find   Change test for datamodule parameter   Apply suggestions from code review   Co-authored-by: Nicki Skafte skaftenicki@gmail.com  Fix lr_find documentation  Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com   formatting   Add description to datamodule param in lr_find   pep8: remove trailing whitespace on line 105   added changelog   Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Nicki Skafte nugginea@gmail.com Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
6009,Neptune Logger Improvements (#1084),0.7670144,Changed to the NeptuneLogger (#16761):, fixes logging for eval steps,1
6010,Bump pypa/gh-action-pypi-publish from 1.4.1 to 1.5.0 (#13621),0.69347286,PyTorch 1.5  support,  added gradient clip test for fp16   pep8 ,0
6011,Fix inclusion of model_parallel document (#16197),0.6209004,model reference not provided, add dist lib to enable syncing anything across devices,0
6012,[mypy] start introducing mypy on lightning_app (#14267),0.6655802,class MyTrainer(LightningLite):,  Finish #3562   Apply suggestions from code review   Apply suggestions from code review   fix tests   Finish #3562   Apply suggestions from code review   Apply suggestions from code review   fix tests   fix structure   fix structure   make save_last test pass   unnecessary global rank check   fix test   update test   update test   test   test   run save on all   remove assert   tracking saves   check if fails   test   clean up   adjust horovod test   clean up   remove unnecessary makdirs   change   undo   debug   debug   debug   debug   mock   undo debug code   add extra assertions   test   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli adrian.waelchli@inf.unibe.ch Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
6013,"Revert ""add package info (#359)"" (#384)",0.5945494,Refactored EpochResultStore (#5522),  Split out changes from #3563 to make that PR easier to review. This formats the file according to the Black formatter   Store a reference to the trainer on the datamodule   Fixes #3682   Update data_connector.py   Update data_connector.py   Update test_datamodules.py   Add attribute to datamodule for trainer ,0
6014,"Update gcsfs requirement from <=2022.2.0,>=2021.5.0 to >=2021.5.0,<2022.6.0 in /requirements (#13168)",0.533247,Generalized Optimizer validation to accommodate both FSDP 1.x and 2.x (#16733),,0
6015,Deprecate LightningModule.datamodule reference in favor of the trainer one (#6929) (#7168),0.8727888,reference to the Trainer on the LightningDataModule (#3684),,1
6016,Add initial IPU CI job (#7251),0.6170473,Graphcore IPU devices,,0
6017,ref: decouple apex second attemp part 4/n (#4056),0.6214775,"Decoupled Appex (#4052, #4054, #4055, #4056, #4058, #4060, #4061, #4062, #4063, #4064, #4065)",Co-authored-by: Sasikanth sasikanth@Sasikanths-MacBook-Pro.local,0
6018,update docs about transfer_batch_to_device hook while using DP (#7343),0.6935873,Disabled batch transfer in DP mode (#6098),  rename   multi build   multi build   copy   copy   copy   copy   copy   copy   clean   note   docker   formatting   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: William Falcon waf2107@columbia.edu,0
6019,Make sure file and folder exists in Profiler (#10073),0.6060414,Moved profilers to their own file (#7822),  return simple docs to methods   sorting   imports   miss ,0
6020,Unify configure_optimizers docs (#7399),0.6736922,Working with multiple optimizers (#16539),  fix topk=-1 tracking best   update test   clean up   add changelog   enable loading best topk in trainer.test()   make trivial   return right away   make windows test path happy ,0
6021,Add option for weight tying on TPU's (#5441),0.75140035,Support to tie weights after moving model to TPU via on_post_move_to_device hook,  upgrade PT version   update docker   docker   try 1.5   fix docker versions   old   badge ,1
6022,Fix isort failures in core (#5526),0.6671871,This release fixes that core issue,  define type   miss   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   miss   warn   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
6023,[bugfix] remove nan loss in manual optimization (#5121),0.9136769,Remove nan loss in manual optimization (#5121), remove results docs. separate flow from log,1
6024,Drop duplicate docs requirements  (#14644),0.57694364,Docs improvements,  log interval based on global step   test   test   test   test   pep   pep   added changelog   pep   merge   remove unused arg ,0
6025,Bump aws-actions/configure-aws-credentials from 1 to 2 (#17051),0.51104796,"Working with multiple dataloaders (#16800, #16753)",,0
6026,Check for mixed new and old style imports (#17548),0.62918425,import argparse,  ref: test val epoch end   ref: test val epoch end   ref: test val epoch end   ref: test log dict   ref: test log dict   ref: test log dict   ref: test log dict ,0
6027,updated lib name,0.46072248,Changed to the NeptuneLogger (#16761):,  Split out changes from #3563 to make that PR easier to review. This formats the file according to the Black formatter   Store a reference to the trainer on the datamodule   Fixes #3682   Update data_connector.py   Update data_connector.py   Update test_datamodules.py   Split out changes from #3563 to make that PR easier to review. This formats the file according to the Black formatter   support checkpoint hooks for datamodule   refactor on_{save/load}_checkpoint to a separate hook class that both the lightning module and data module inherit add spots in callback connector to call new datamodule hooks if available   hooks formatting   Update hooks.py   Update checkpoint_connector.py   Update lightning.py   update based on upstream/master   checkout upstream/master   Update checkpoint_connector.py   add tests   undo format revert   Updated CHANGELOG.md   add checkpoint hooks   add Dict type   import CheckpointHooks ,0
6028,Added variable interpolation and troubleshooting to LightningCLI doc (#11009),0.74809444,LightningCLI additions:,  ref: test val epoch end   ref: test val epoch end   ref: test val epoch end ,1
6029,cover subproc coverage (#6477),0.5199903,[1.2.7] - 2021-04-06,  Add a more direct test of multi-gpu training working   Update tests/base/develop_pipelines.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
6030,[Feat] Add FastForwardSampler 2/n - Fault Tolerant Training (#8307),0.78771216,Fault-tolerant Training,"  Fix ModelCheckpoint period   Remove comma   Minor changes   skip check   Revert ""skip check""   Already pushed to master This reverts commit 00d9e77b81e7dc9a6cb90c676f744bce23514cb7. Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai",1
6031,update (#6237),0.6200384,[0.5.7] - 2022-08-22, Update lr_finder.rst  Misspelling correction  Update docs/source/lr_finder.rst  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
6032,Add optimizer hooks in callbacks (#4379),0.7555249,Made optimization steps for hooks (#2363),,1
6033,fix package and validate folders (#16299),0.57681876,Prevent to cd into non-existent folders (#16645),  ref: test val epoch end   ref: test val epoch end   ref: test val epoch end   ref: test val epoch end   ref: test val epoch end   ref: test val epoch end ,0
6034,Add SLURM check in ddp_train() and init_ddp_connection() (#1387),0.65604824,Used checkpoint_connector.hpc_save in SLURM (#4217),  ref: make each backend independent for easier debugging and independent debugging   ref: make each backend independent for easier debugging and independent debugging   ref: make each backend independent for easier debugging and independent debugging   ref: make each backend independent for easier debugging and independent debugging   ref: make each backend independent for easier debugging and independent debugging   ref: make each backend independent for easier debugging and independent debugging   ref: test val epoch end   ref: test val epoch end ,0
6035,Remove the deprecated resume_from_checkpoint Trainer argument (#16167),0.91045505,1. Remove resume_from_checkpoint from the Trainer,  disable configure_optimizers during testing   minor changes   hvd and ddp   fix precision during testing   fix ddp   fix amp   fix cpu   update dp   simplify optimizers   add test   codefactor   ref optimizer setup   chlog   suggestions   isort   rebased with master ,1
6036,make RichProgressBar more flexible with Rich.Console (#10875),0.61838835,from pytorch_lightning.callbacks import RichProgressBar,,0
6037,0.8.5 (#2573),0.748353,[0.6.0] - 2022-09-08,  .log in eval   ref   ref: enable self.log in val step ,1
6038,fixed dataset stuff + docs (#1599),0.64618075,Implemented DataParallelPlugin._setup_model (#10010),,0
6039,ref: inner train loop (intermediate step) 19/n (#3385),0.78290033,"inner train loop (#3359, #3361, #3362, #3363, #3365, #3366, #3367, #3368, #3369, #3370, #3371, #3372, #3373, #3374, #3375, #3376, #3385, #3388, #3397)",  Split out changes from #3563 to make that PR easier to review. This formats the file according to the Black formatter   Store a reference to the trainer on the datamodule   Fixes #3682   Update data_connector.py   Update data_connector.py   Update test_datamodules.py   Support more storage backends in trainer.test using best weights   Similar to #3692   Update trainer.py   Update trainer.py   use cloud_io load directly,1
6040,adjust msg for external accelerators (#17465),0.6468388,move specific accelerator code (#3457),"  ref: deprecated results obj, added support for simpler comms. Decouples logging from loops   ref: deprecated results obj, added support for simpler comms. Decouples logging from loops   ref: deprecated results obj, added support for simpler comms. Decouples logging from loops   ref: deprecated results obj, added support for simpler comms. Decouples logging from loops   ref: deprecated results obj, added support for simpler comms. Decouples logging from loops   ref: deprecated results obj, added support for simpler comms. Decouples logging from loops   fix global step err   fix global step err   fix global step err   fix global step err   fix global step err   fix typing err   fix str   fix typing err ",0
6041,Fixed broken link for 9 key lightning tricks. (#787),0.69886446,Introduce lightning connect (#14452), enable tracking original metric when step and epoch are both true,0
6042,Bugfix/swa iterable dset (#8172),0.6049902,Disabled sampler replacement when using IterableDataset (#11507),,0
6043,ref: inner train loop (intermediate step) 15/n (#3374),0.7849026,"inner train loop (#3359, #3361, #3362, #3363, #3365, #3366, #3367, #3368, #3369, #3370, #3371, #3372, #3373, #3374, #3375, #3376, #3385, #3388, #3397)",,1
6044,fix depreated call (#1596),0.7246922,Removed deprecated callbacks (#3979),  fix   fix global step err   fix global step err   fix global step err   fix global step err   fix global step err   fix global step err   Co-authored-by: William Falcon waf2107@columbia.edu,1
6045,"(app) Introduce configure_api and Post, Get, Delete, Put HttpMethods (#13945)",0.7588443,"Add support for Lightning API through the configure_api hook on the LightningFlow and the Post, Get, Delete, Put with HttpMethods (#13945)",  topk default   fix test that doesn't have best available   remove print   3680 changes   fix backward   temp revert   te   add warning by carmocca   format docstring for test   specify monitor in ES test with top k   improve docstring for save_last   remove commented lines   revert passing model to test   undo regex mistake   changelog   fix test covering case monitor=None and savetopk=-1   docstring   fix test for saving all checkpoints   don't save checkpoints for save_top_k=0   add test for savetopk=0   Co-authored-by @carmocca Co-authored-by: Carlos Mocholí carlossmocholi@gmail.com,1
6046,Ipynb update (#8004),0.62278044,    * Renamed the `IPUPlugin` to `IPUStrategy` ([#11193](https://github.com/PyTorchLightning/pytorch-lightning/pull/11193)),  Split out changes from #3563 to make that PR easier to review. This formats the file according to the Black formatter   Store a reference to the trainer on the datamodule   Fixes #3682   Update data_connector.py   Update data_connector.py   Update test_datamodules.py ,0
6047,DeepSpeed Integration (#5954),0.8075312,DeepSpeed Stage 1,  Add ModelCheckpoint.to_json()   Add ModelCheckpoint.to_json() test   Fix W292: Add new line at end of file   Apply suggestions from code review   Apply suggestions from code review   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com   Fixed tests   Update pytorch_lightning/callbacks/model_checkpoint.py   Apply suggestions from code review   fix test   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
6048,added checkpoint test on cpu,0.64921105, - Checkpointing: `Trainer(enable_checkpointing=True)`,  enable None model checkpoint default   enable None model checkpoint default   enable None model checkpoint default   enable None model checkpoint default   enable None model checkpoint default   enable None model checkpoint default   enable None model checkpoint default   enable None model checkpoint default   enable None model checkpoint default   enable None model checkpoint default   enable None model checkpoint default   enable None model checkpoint default   enable None model checkpoint default   enable None model checkpoint default   enable None model checkpoint default   enable None model checkpoint default   enable None model checkpoint default   enable None model checkpoint default   enable None model checkpoint default   enable None model checkpoint default   enable None model checkpoint default   enable None model checkpoint default ,0
6049,Remove the deprecated trainer.*_ckpt_path (#14897),0.7859267,"Deprecated Trainer attribute ckpt_path, which will now be set by weights_save_path (#2681)","  spec Horovod version   MAKEFLAGS=""-j2""   tests   CI   docker   CI   docker ",1
6050,RC & Docs/changelog (#1776),0.61025786,Complete changelog,"  enable pt 1.7   readme   nightly diff version testing, will delete later   nightly diff version testing, will delete later   back to normal [ci skip]   use ignored_properties   define ignored_properties in respective modules   change log   formatting   Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",0
6051,Bump coverage from 6.4.2 to 6.5.0 in /requirements (#15674),0.56000835,[0.5.6] - 2022-08-16,  update stale conf   labels ,0
6052,ci: update install lite & cut pkg dependency (#14517),0.6512505,- The `pyDeprecate` dependency is no longer installed ([#14472](https://github.com/Lightning-AI/lightning/pull/14472)),  mocking for wandb   remove wandb import in amp test   mock loggers in sphinx   check tests   Update extra.txt   setup   dev   min   revert   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai,0
6053,Remove deprecated DistributedType and DeviceType enum classes (#14045),0.7560791,- Removed the deprecated `DistributedType` and `DeviceType` enum classes ([#14045](https://github.com/Lightning-AI/lightning/pull/14045)),  dockers nightly   typo   Apply suggestions from code review   Co-authored-by: Jeff Yang ydcjeff@outlook.com Co-authored-by: Jeff Yang ydcjeff@outlook.com,1
6054,Convert subprocess test to standalone test (#14101),0.57832235,Deprecated the TestTubeLogger (#9065),  Fix ModelCheckpoint period   Test for less epochs ,0
6055,Fix unfreeze_and_add_param_group expects modules rather than module (#6822),0.64804095,  * Removed the deprecated `pl_module` argument from the distributed module wrappers,"  Fix incorrect ""Saving latest checkpoint"" warning   Replace warning with info. Run PyCharm's optimize imports   Remove unused class variable. Refactor logic. Improve test   Fix De Morgan's ",0
6056,Update installation (#14732),0.6464288,Resolved a bug where the install command was not installing the latest version of an app/component by default (#14181),,0
6057,add warning for shuffling in test/val (#1865),0.66329557,Disabled val and test shuffling (#1600),Fix #3652,0
6058,fixes non python type callback metrics and fast_dev_run (#345),0.6338161,Metric compute() method will no longer automatically call reset() (#5409),  Split function   Add docstrings   Add typing annotations   Minor refactor   Make static to add a test ,0
6059,remove deadcode in trainer (#8121),0.7152916,Removed the deprecated TrainerLoggingMixin class (#8609),,1
6060,fix recursive call for apply_to_collection(include_none=False) (#8719),0.5262585,    return None,  use lightning CI docker   exclude py3.8 and torch1.3   torch 1.7   mergify   Apply suggestions from code review   Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
6061,update deprecated messages (#810),0.65131116,Silenced some warnings. verified ddp refactors (#3483),,0
6062,Simplify several profile calls (#11031),0.59261596,Changed Simple Profiler report to order by percentage time spent + num calls (#4880),  test examples   testing   testing   typo   req   exception   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
6063,Test DeepSpeed in Fabric CI (#16458),0.6831607,DeepSpeed Stage 1,  fix examples   fix examples ,0
6064,drop duplicated metric helper (#5366),0.8499528,Drop duplicate metrics (#5014), Update gradient_accumulation_scheduler.py  add types for gradient accumulation scheduler callback  Update gradient_accumulation_scheduler.py,1
6065,Docs clean-up (#2234),0.6902344,Docs improvements,  save to tmpdir   path ,0
6066,remove state_dict,0.6906482,+def state_dict(self):,,0
6067,fix domain_templates (#365),0.54636157,Updated mlflow with using resolve_tags (#6746),  fix examples   fix examples ,0
6068,Add path filters for azure PR jobs (#14544),0.51703894,Changed Checkpoint path parameter from filepath to dirpath (#1016),  Adding clarifying documentation on the usage of second_order_closure   oops typo   making functions more sane   fixing spacing issues - I think   Apply suggestions from code review   suggestions   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
6069,CI: parameterize TPU tests (#15876),0.6849648,Updated logic for checking TPUs availability (#6767),  init   fix call_hook args ,0
6070,Classification metrics overhaul: input formatting standardization (1/n) (#4837),0.84018123,Classification metrics overhaul (#4837),,1
6071,[WIP] Rename overfit_pct to overfit_batches (and fix) and val_percent_check and test_percent_check (and fix) (#2213),0.73184645,overfit_pct in favour of overfit_batches,  fix dp   fix dp   fix dp   fix dp   fix examples   fix examples   fix examples   fix examples   fix examples   fix examples   fix examples   fix examples   fix examples   fix examples   fix examples   fix examples   fix examples   fix examples   fix examples   fix examples   fix examples   fix examples ,1
6072,fix checkpointing to remote file paths (#2925),0.70943534,Changed Checkpoint path parameter from filepath to dirpath (#1016),  update build cases   list   matrix   matrix   builds   docker   -j1   -q   -q   sep   docker   docker   mergify   -j1   -j1   horovod   copy ,1
6073,Add docstrings in mlflow logger class (#9030),0.67555225,"    loggers=[WandbLogger(...), MLFlowLogger(...)]",  enable any logged or written metric to be accessible in callbacks   enable any logged or written metric to be accessible in callbacks   enable any logged or written metric to be accessible in callbacks   enable any logged or written metric to be accessible in callbacks   enable any logged or written metric to be accessible in callbacks   enable any logged or written metric to be accessible in callbacks   enable any logged or written metric to be accessible in callbacks   enable any logged or written metric to be accessible in callbacks   enable any logged or written metric to be accessible in callbacks   enable any logged or written metric to be accessible in callbacks   enable any logged or written metric to be accessible in callbacks   enable any logged or written metric to be accessible in callbacks   enable any logged or written metric to be accessible in callbacks   enable any logged or written metric to be accessible in callbacks   enable any logged or written metric to be accessible in callbacks   enable any logged or written metric to be accessible in callbacks   enable any logged or written metric to be accessible in callbacks   enable any logged or written metric to be accessible in callbacks   enable any logged or written metric to be accessible in callbacks   enable any logged or written metric to be accessible in callbacks   enable any logged or written metric to be accessible in callbacks   clarify forward   clarify forward   clarify forward   clarify forward ,0
6074,CI: lower timeout for e2e (#15483),0.56688535,Increased TPU check timeout from 20s to 100s (#5598),,0
6075,integrate metrics API with self.log (#3961),0.9884845,Integrated metrics API with self.log (#3961),,1
6076,Add additional checkpoint tests,0.6403934,CheckpointIO Plugins,  clarify forward   clarify forward   clarify forward   clarify forward ,0
6077,Fix typos on new-project page (#11942),0.5303032,Rename failed -> error in tables (#15608),  clarify forward   clarify forward ,0
6078,Fix percent_checks (#649),0.63753146,set validation to a fix number of batches,,0
6079,Miscellaneous updates in Fabric docs (#16980),0.6178937,Learn more about Fabric and what it can do in the new docs!,,0
6080,"Update torchvision requirement from <=0.14.0,>=0.11.1 to >=0.11.1,<0.15.0 in /requirements (#16108)",0.75064445,"Set PyTorch 1.4 as min requirements, also for testing and examples torchvision>=0.5 and torchtext>=0.5 (#5418)",  fixes resnet   Fixes formatting and resnet ,1
6081,Remove skipping logic in favor of path filtering (#14170),0.55000407,Avoid relpath bug on Windows (#16164),,0
6082,fix nightly releases & readme (#5922),0.5336892,Set version as today (#13906),  new-project   Update new-project.rst   enable any logged or written metric to be accessible in callbacks   enable any logged or written metric to be accessible in callbacks   enable any logged or written metric to be accessible in callbacks   enable any logged or written metric to be accessible in callbacks   enable any logged or written metric to be accessible in callbacks   enable any logged or written metric to be accessible in callbacks   enable any logged or written metric to be accessible in callbacks   enable any logged or written metric to be accessible in callbacks   enable any logged or written metric to be accessible in callbacks   Co-authored-by: William Falcon waf2107@columbia.edu,0
6083,update docs on checkpoint_callback Trainer argument (#4461),0.78620315,Deprecated passing ModelCheckpoint instance to checkpoint_callback Trainer argument (#4336),  Split out changes from #3563 to make that PR easier to review. This formats the file according to the Black formatter   support checkpoint hooks for datamodule   refactor on_{save/load}_checkpoint to a separate hook class that both the lightning module and data module inherit add spots in callback connector to call new datamodule hooks if available   hooks formatting   Update hooks.py   Update hooks.py   Update checkpoint_connector.py   Update lightning.py   update based on upstream/master   checkout upstream/master   Update lightning.py   Update lightning.py   Co-authored-by: William Falcon waf2107@columbia.edu,1
6084,Implement Explained Variance Metric + metric fix (#4013),0.78698456,Updated explained variance metric (#4024),,1
6085,typing: fix App's core API - api (#16950),0.5848825,Removed deprecated API (#2073),  Update gitignore   Update .gitignore   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
6086,define Yapf config (#5591),0.4881649,Configuration Validator (#9779),,0
6087,Don't use old testing packages in CI (#11366),0.5946699,Avoid using the deprecated LooseVersion (#16162),  :zap: add lightning colab tutorial notebooks   Update notebooks/01-mnist-hello-world.ipynb   Update notebooks/03-basic-gan.ipynb   Update notebooks/04-transformers-text-classification.ipynb   Update notebooks/02-datamodules.ipynb   add jupytext   :bug: comment ipynb related things in conf.py   :fire: remove .py files   Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai,0
6088,Enable fsspec by default for cli config file (#7521),0.6898335,Used fsspec instead of gfile for all IO (#3320),,0
6089,"Raise a MisconfigurationException when trainer functions are called with ckpt_path=""best"" but checkpoint_callback isn't configured (#9841)",0.974945,"Trainer now raises a MisconfigurationException when its methods are called with ckpt_path=""best"" but a checkpoint callback isn't configured (#9841)",,1
6090,Gpu load (#302),0.6851641,GPU training (#2704),,0
6091,Remove deprecated prepare_data_per_node in Trainer (#12536),0.77449656,Removed trainer.reset_*_dataloader() methods (#16726),,1
6092,removing unused imports,0.66996896,import argparse,,0
6093,Remove calls to profile model_forward (#12032),0.65781796,"- Removed calls to `profile(""model_forward"")` in favor of profiling `training_step` ([#12032](https://github.com/PyTorchLightning/pytorch-lightning/pull/12032))",,0
6094,Split init_module into init + sharded_model (#17488),0.62293035,Pass init args to ShardedDataParallel (#9483),,0
6095,sanitize arrays when logging as hyperparameters in TensorBoardLogger (#9031),0.7055416,MLFlowLogger now logs hyperparameters and metrics in batched API calls (#15915),,1
6096,Remove the graveyard (#16138),0.6277582,Removed deprecated: (#2760),,0
6097,Assistant fixes (#15221),0.5543077,Porting fixes to autoscaler component (#16249),,0
6098,Introduce Trainer(barebones=True) (#16854),0.8425245,Barebones Trainer mode (#16854),,1
6099,[doc] Add more reference around predict_step (#7997),0.6016998,"Refactored prediction loop interface; added new classes PredictionLoop, PredictionEpochLoop (#7700, #8077)",,0
6100,Trigger colossalai integration test in CI (#16789),0.52374643,Enabling val/test loop disabling (#2692),,0
6101,Update Multinode Warning (#16091),0.9907695,Updated Multinode Warning (#16091),  docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs ,1
6102,Fix typo in predict_step docs (#12911),0.48634148,"Refactored prediction loop interface; added new classes PredictionLoop, PredictionEpochLoop (#7700, #8077)",  error on multilabel   fix tests   fix pep8   changelog   update doc test   fix doctest   fix doctest   update from suggestion   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   Update test_classification.py   Update test_classification.py   retrigger test   'pep8   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
6103,[Metrics] Class reduction similar to sklearn (#3322),0.860978,Changed class_reduction similar to sklearn for classification metrics (#3322),Split out changes from #3563 to make that PR easier to review,1
6104,add bug_report_model to bug_report (#4307),0.5893199,Fixing critical bugs in newly added hooks and hparams assignment.,Split out changes from #3563 to make that PR easier to review. This formats the file according to the Black formatter,0
6105,app: adding silent dependencies (#16302),0.7450817,Improved support for running apps when dependencies aren't installed (#15711), Black format pytorch_lightning/core/hooks.py  Split out changes from #3563 to make that PR easier to review. This formats the file according to the Black formatter  Split out changes from #3563 to make that PR easier to review. This formats the file according to the Black formatter,1
6106,Update trainer.py (#233),0.73502314,Removed pytorch_lightning/trainer/training_loop.py (#7985),  ref: result 1/n (make monitor default to checkpoint_on to simplify result syntax)   ref: result 1/n (make monitor default to checkpoint_on to simplify result syntax)   ref: result 1/n (make monitor default to checkpoint_on to simplify result syntax)   ref: result 1/n (make monitor default to checkpoint_on to simplify result syntax)   ref: result 1/n (make monitor default to checkpoint_on to simplify result syntax)   ref: result 1/n (make monitor default to checkpoint_on to simplify result syntax)   ref: result 1/n (make monitor default to checkpoint_on to simplify result syntax)   ref: result 1/n (make monitor default to checkpoint_on to simplify result syntax)   ref: result 1/n (make monitor default to checkpoint_on to simplify result syntax)   ref: result 1/n (make monitor default to checkpoint_on to simplify result syntax)   ref: result 1/n (make monitor default to checkpoint_on to simplify result syntax)   ref: result 1/n (make monitor default to checkpoint_on to simplify result syntax)   ref: result 1/n (make monitor default to checkpoint_on to simplify result syntax)   ref: result 1/n (make monitor default to checkpoint_on to simplify result syntax)   ref: result 1/n (make monitor default to checkpoint_on to simplify result syntax)   ref: result 1/n (make monitor default to checkpoint_on to simplify result syntax)   ref: result 1/n (make monitor default to checkpoint_on to simplify result syntax)   ref: result 1/n (make monitor default to checkpoint_on to simplify result syntax)   ref: result 1/n (make monitor default to checkpoint_on to simplify result syntax)   Update pytorch_lightning/callbacks/model_checkpoint.py   Co-authored-by: ananthsub ananth.subramaniam@gmail.com   ref: result 1/n (make monitor default to checkpoint_on to simplify result syntax)   force crash when max_epochs < epochs in a checkpoint   Co-authored-by: ananthsub ananth.subramaniam@gmail.com,1
6107,IPU Integration 5/5 (#7867),0.6798545,Graphcore IPU devices,  force crash when max_epochs < epochs in a checkpoint   force crash when max_epochs < epochs in a checkpoint ,0
6108,Logger default (#351),0.7897584,Return the default_root_dir as the log_dir when the logger is a LoggerCollection (#8187),  docs   set env variable   fix   changelog ,1
6109,apply_to_collection improvements and add apply_to_collections (#7769),0.5959898,Changed metrics_to_scalars to work with any collection or value (#7888),,0
6110,Remove duplicate CHANGELOG entry (#17069),0.72591245,Complete changelog,  example   ex   example   sampler   fix   fix   remove example   changelog ,1
6111,Update CHANGELOG after the 1.6.3 release (#12968),0.68749183,Here is a selection of important changes that are not backward compatible with versions < 1.5. The full list of changes and removals are listed in the changelog at the bottom.,,0
6112,Update github url for new project template (#90),0.54034424,- Deprecated `BaseProfiler.profile_iterable` ([#12102](https://github.com/PyTorchLightning/pytorch-lightning/pull/12102)),  nightly   nightly   ls ,0
6113,Change trainer.should_stop to not stop in between an epoch and run until min_steps/min_epochs only (#13890),0.76254904,- Changed `trainer.should_stop` to not stop in between an epoch and run until `min_steps/min_epochs` only ([#13890](https://github.com/Lightning-AI/lightning/pull/13890))," Update gradient_accumulation_scheduler.py  add types for gradient accumulation scheduler callback  Apply black formatting to model checkpoint callback  auto-format, no other changes  Update gradient_accumulation_scheduler.py  drop other changes Co-authored-by: William Falcon waf2107@columbia.edu",1
6114,Stricter FxValidator and add hooks (#7874),0.6160307,Fixing critical bugs in newly added hooks and hparams assignment.," Update gradient_accumulation_scheduler.py  add types for gradient accumulation scheduler callback  Apply black formatting to model checkpoint callback  auto-format, no other changes  Update gradient_accumulation_scheduler.py  drop other changes   Add type hints to model checkpoint callback   Update model_checkpoint.py   remove trainer/lightning modules types to avoid circular import",0
6115,Make parallel devices optional across all plugins (#6051),0.98361826,Made parallel devices optional across all plugins (#6051),,1
6116,Fixes #2943 (#2970),0.61555773,Resolve bug with Finetuning (#5744),,0
6117,Update accelerator.py (#7318),0.73202,Moved accelerators and plugins to its legacy pkg (#5645),  Allow kwargs in WandbLogger   isort   kwargs docstring   typo   kwargs for other loggers   pep and isort   formatting   fix failing test   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
6118,Fix CC-bot for non-forks (#14710),0.5801439,- Added a more descriptive error message when attempting to fork processes with pre-initialized CUDA context ([#14709](https://github.com/Lightning-AI/lightning/pull/14709)),  drop v0.10 deprecated   import   missed ,0
6119,Resolve Lightning App with remote storage (#17426),0.9889127,Resolved Lightning App with remote storage (#17426),  :bug: make dims a property   :bug: fix ,1
6120,Run plugin closure before on_before_optimizer_step [1/2] (#9288),0.7536809,Executing the optimizer_closure is now required when overriding the optimizer_step hook (#9360), Updated explanation to enabling early stopping via boolean flag.  Now also includes case of returning Result objects.   Improved API documentation for checkpoint_on and early_stp_on in results.   Apply suggestions from code review   Fix terminology.   Fix wrong documentation. Strict checking is disabled when using structured results.   element typo   update remaining edits from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
6121,Remove deprecation warnings being called for on_{task}_dataloader (#9279),0.98381066,Removed deprecation warnings being called for on_{task}_dataloader (#9279),  Improve Comet Logger pickled behavior   Delay the creation of the actual experiment object for as long as we can.  Save the experiment id in case an Experiment object is created so we can   continue the same experiment in the sub-processes.  Run pre-commit on the comet file.   Handle review comment   Make most Comet Logger attribute protected as they might not reflect the final Experiment attributes. Also fix the typo in the test name.   Ensure that CometLogger.name and CometLogger.version always returns str   Add new test for CometLogger.version behavior   Add new tests for CometLogger.name and CometLogger.version   Apply review suggestions   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   Remove extraneous comments in Comet logger tests   Fix lint issues   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
6122,release v0.1.dev15,0.6738089,"    version=""0.0.1"",",  Fix ModelCheckpoint's name formatting   Fix failing tests   Add dot to CHECKPOINT_SUFFIX   Set variables to their default values at the end of tests   Fix logic for filepath='' and filename=None. Add test   Fix Windows tests   Fix typo. Remove leading line break and zeroes   Remove CHECKPOINT_SUFFIX   Fix typos. Use appropriate f-string format   Apply suggestions from code review   Fix broken tests after #3320   Finish changes suggested by Borda   Use explicit test var names   Apply suggestions   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Apply suggestions  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   Update CHANGELOG   Apply suggestions from code review   for   prepend whitespace in warn msg   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
6123,"Mocking Loggers (part 4c, mlflow) (#3889)",0.74029905,"Mocking loggers (#3596, #3617, #3851, #3859, #3884, #3853, #3910, #3889, #3926)"," Fixes the test for early stopping without val step.  The expression which checked, if early stopping was triggered, had an off-by-one error and hence was true even if early stopping was not triggered. Furthermore set patience to 0 and max epochs to 10, to ensure loss has enough time to flatten.  Fixes early stopping without val step.  The issue has been, that only early_stop_on key was checked and not an arbitrary monitor key.  Fixes branch, which checks whether early stopping is done during validation.  Before only val_early_stop_on was checked. Since arbitrary keys can be used, the set of possible validation keys cannot be exhaustive. Hence this disables ""early stopping on_train_epoch_end"" via an instance attribute if early stopping was executed in on_validation_epoch_end. Furthermore adds a test, which ensures arbitrary keys work.  Improve check whether eval results are used.  Only disable early checking with train results if eval results are actually used. Before they were always disabled in on_validation_epoch_end. Rename and document instance variable, to make it more clear.   Remove wrong documentation on behaviour of early stopping with train result' dict.   Apply suggestions from code review   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",1
6124,Avoid torchelastic warning message when importing lightning (#15610),0.7412734,Gave warnings for unimplemented required lightning methods (#1317),  :pencil: docs   :pencil: docs   :pencil: docs   :pencil: docs   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
6125,Add link to PL forum in GH questions template (#3708),0.5114206,Allow passing hparams as a keyword argument to LightningModule when loading from checkpoint (#1639),,0
6126,test pickling (#1636),0.9484049,Tested pickling (#1636),  dockerfile and actions file   dockerfile and actions file   added pytorch conda cpu nightly   added pytorch conda cpu nightly   recopy base reqs   gh action include torch nightly   add pytorch nightly & conda gh badge   rebase   fix horovod   proposal refactor   Update .github/workflows/ci_pt-conda.yml   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update .github/workflows/ci_pt-conda.yml  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   update   update   fix cmd   filled &&   fix   add -y   torchvision >0.7 allowed   explicitly install torchvision   use HOROVOD_GPU_OPERATIONS env variable   CI   skip 1.7   table   Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
6127,Fixes #250 (#253),0.5356293,Mixed precision overhaul (#16783),  try   try   drop 0.20   drop 0.19.5   -U   Fixed Horovod in CI due to wandb==0.10.0 sys.path modifications (#3525)   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   format   wb freeze   types   Co-authored-by: Travis Addair taddair@uber.com,0
6128,Remove deprecated stochastic_weight_avg example from the docs (#10502),0.66838914,Deprecated passing stochastic_weight_avg to the Trainer constructor in favor of adding the StochasticWeightAveraging callback directly to the list of callbacks (#8989)," Fix IoU score for classes not present in target or pred  Fixes #3097  Allow configurable not_present_score for IoU for classes   not present in target or pred. Defaults to 1.0.  Also allow passing num_classes parameter through from iou   metric class down to its underlying functional iou   call.   Changelog: move IoU not-present score fix to [unreleased]   IoU: avoid recomputing class presence in target and pred   Use already-computed support, true positives, and false positives to determine if a class is not present in either target or pred.  Test IoU against sklearn jaccard_score  Also add TODO to test our IoU's not_present_score against sklearn's jaccard_score's zero_division when it beecomes available.  IoU: remove_bg -> ignore_index  Fixes #2736  Rename IoU metric argument from remove_bg -> ignore_index. Accept an optional int class index to ignore, instead of a bool and   instead of always assuming the background class has index 0.  If given, ignore the class index when computing the IoU output,   regardless of reduction method.   Improve documentation for IoU not_present_score   Update default IoU not_present_score to 0.0   Add note about IoU division by zero   Rename IoU not_present_score -> absent_score   Update IoU absent score changelog wording   Condense IoU absent_score argument docstring   Remove unnecessary IoU ignore_index comment   docstrings   isort   flake8   Fix test of IoU against sklearn jaccard   Use macro instead of micro averaging in sklearn's jaccard score, to match multi-class IoU, which conventionally takes per-class scores before averaging. Co-authored-by: rohitgr7 rohitgr1998@gmail.com",0
6129,ref: organize args 4/n (#3456),0.8825123,"organize args (##3435, #3442, #3447, #3448, #3449, #3456)",  tensorboard version   WIP test tb hparams logs (#3040)   optional   req   tensorboard>=2.2.0   data   data   TB   Co-authored-by: Rosario Scalise rosario@cs.washington.edu,1
6130,Move tracking epoch end outputs logic to the EvaluationEpochLoop (#9261),0.6709275,Log epoch metrics before the on_evaluation_end hook (#7272),,0
6131,"Add support for reloading the last checkpoint saved by passing ckpt_path=""last"" (#12816)",0.8667682,"- Added support for reloading the last checkpoint saved by passing `ckpt_path=""last""` ([#12816](https://github.com/Lightning-AI/lightning/pull/12816))",,1
6132,fix plateau scheduler stepping on incomplete epoch (#8861),0.6566544,"Resolved schedule step bug for PyTorch Profiler (#6674, #6681)",  fix + test   changelog   Apply suggestions from code review   Co-authored-by: Tim Chard timchard@hotmail.com  improve test  Co-authored-by: Tim Chard timchard@hotmail.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com,0
6133,Torch inference mode for prediction (#15719),0.9746053,Set Torch inference mode for prediction (#15719),  ref: precision plugins 1/n   ref: precision plugins 1/n ,1
6134,[DeepSpeed] Do not fail if batch size could not be inferred for logging (#10438),0.98110336,Do not fail if batch size could not be inferred for logging when using DeepSpeed (#10438),  new class reduce interface   update docs   pep8   update_class_metrics   fix doctest   changelog   fix docs   fix codefactor   fix codefactor   formatting   fix typo   fix typo   typo pr -> per   update from suggestion   fix error   Apply suggestions from code review   Update CHANGELOG.md   formatting   timeouts   docstring formatting for reg metrics   pep   flake8   revert workflow changes   suggestions   Co-authored-by: Nicki Skafte nugginea@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai Co-authored-by: rohitgr7 rohitgr1998@gmail.com,1
6135,adding framework level dp,0.5808998,DDP custom implementation support (override these hooks):,  Pass epoch argument   Copy epoch instead of inplace pop   Remove whitespace   Add test for epoch logging   add docstring   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
6136,Add strategy argument to Trainer (#8597),0.714602,New Trainer Arguments: Strategy and Devices,,1
6137,load_spawn_weights only in proc rank 0 (#1385),0.6151124,Loading Model Weights,  ref: apex plugin   ref: apex plugin   ref: apex plugin ,0
6138,formatting 3/n: PL modules (#5716),0.6795021,Simplify the PL examples structure (shallower and more readable) (#1247),  build docs on master   fomatting ,0
6139,Remove unused test argument (#13296),0.6734951,Removed the deprecated model argument from ModelCheckpoint.save_checkpoint (#8688),  Disable train dataloader shuffle when overfit_batches is active.   pep8   Co-authored-by: William Falcon waf2107@columbia.edu,0
6140,Disable pl optimizer temporarily to fix AMP issues (#5163),0.94348454,enable_pl_optimizer=False by default to temporarily fix AMP issues (#5163),  metric aggregation   metric aggregation   add at_least_1d   fix output formatting   add metric tests   add missing test case   remove reduce_op frm metric classes   fix reduce_op stuff   start test fixing   fix tests due to aggregation   fix faulty import   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   remove reduce_op docstrings   add compute   remove import   remove collection metric   update base class   update tests   Update metric.py   Update metric.py   Apply suggestions from code review   change default aggregate   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com,1
6141,Add remote filesystems to docs (#10752),0.62992287,Using gfile to support remote directories (#2164),Co-authored-by: David Waterworth david.waterworth@cim.io,0
6142,release v0.2.3,0.7870234,0.4.0,  fix(docs): change to configure_sync_batchnorm for sync bn hook   Update docs/source/lightning-module.rst   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   distinguish pypi and conda download badges   python version badge and original pypi download badge   Update docs/source/lightning-module.rst   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
6143,Have checkgroup pull the latest runs (#16033),0.58628345,Early stopping checks on_validation_end (#1458),  replace nans to 0 at conf. matrix & update tests   cm.isnan() -> torch.isnan(cm)   fix row-wise division while normalize   update tests   pep8 fix   Update tests/metrics/test_classification.py   add comment to test Co-authored-by: Nicki Skafte skaftenicki@gmail.com  Update tests/metrics/functional/test_classification.py  Co-authored-by: Nicki Skafte skaftenicki@gmail.com  Update pytorch_lightning/metrics/functional/classification.py  Co-authored-by: Nicki Skafte skaftenicki@gmail.com  final update  Co-authored-by: Nicki Skafte skaftenicki@gmail.com,0
6144,Fix mypy in utilities.device_dtype_mixin (#8127),0.5445873,| Import pl.core.mixins.DeviceDtypeModuleMixin                                                           | 1.10             | No longer supported |,Co-authored-by: Nicki Skafte nugginea@gmail.com,0
6145,trainer: module fix.,0.7711603,trainer = pl.Trainer(plugins=SLURMEnvironment(auto_requeue=False)),,1
6146,fix fast_dev_run parsing from cli (#7240),0.6560466,Tuner algorithms will be skipped if fast_dev_run=True (#3903),  update docs for log_save_interval   formatting   empty   link reporting ,0
6147,Fix endpoint information tab not showing up in AutoScaler UI (#16128),0.642663,Porting fixes to autoscaler component (#16249),  cleaning up stale logger tests   cleaning up stale logger tests   cleaning up stale logger tests   cleaning up stale logger tests   cleaning up stale logger tests   cleaning up stale logger tests ,0
6148,Fix protobuf incompatibility blocking CI (#16441),0.6112419,Raised a MisconfigurationException if batch transfer hooks are overriden with IPUAccelerator (13961),  ref: ddp verify   ref: ddp verify   ref: ddp verify   ref: ddp verify   ref: ddp verify   ref: ddp verify   ref: ddp verify   ref: ddp verify   ref: ddp verify   ref: ddp verify   Update ddp_base_backend.py ,0
6149,Remove InternalDebugger.track_event (#9654),0.64809304,Removed deprecation warnings being called for on_{task}_dataloader (#9279),,0
6150,Properly set LightningModule.device after model replacement (#7188),0.79793507,Move lightning module to correct device type when using LightningDistributedWrapper (#6070),,1
6151,Add bfloat16 support to Lightning Trainer (#9049),0.7549874,reference to the Trainer on the LightningDataModule (#3684),,1
6152,"Deprecate DataModule properties: train_transforms, val_transforms, test_transforms, dims, and size (#8851)",0.97996897,"Deprecated DataModule properties: train_transforms, val_transforms, test_transforms, size, dims (#8851)",  ref: merge backends x/n   ref: merge backends x/n   ref: merge backends x/n   ref: merge backends x/n ,1
6153,Add yapf to pre-commit (#5747),0.5014146,Full commit list: https://github.com/PyTorchLightning/pytorch-lightning/compare/1.6.0...1.7.0,,0
6154,Merge pull request #1636 from PyTorchLightning/callback,0.65286547,| Import pytorch_lightning.callbacks.base.Callback                                                           | 1.9             | pytorch_lightning.callbacks.callback.Callback |,  ref: slurm connector 1/n   ref: slurm connector 1/n   ref: slurm connector 1/n   ref: slurm connector 1/n ,0
6155,ref: decouple apex second attemp part 2/n (#4054),0.61175483,"Epoch 8:  53%|█████    | 17/32 [5.13/s, v_num=2, loss=0.5643]",  ref: checkpoint connector methods 4/n   ref: checkpoint connector methods 4/n   ref: checkpoint connector methods 4/n   ref: checkpoint connector methods 4/n   ref: checkpoint connector methods 4/n   ref: checkpoint connector methods 4/n   ref: checkpoint connector methods 4/n   ref: checkpoint connector methods 4/n   ref: checkpoint connector methods 4/n ,0
6156,Update Lightning Lite docs (5/n) (#16291),0.8647059,Update the Lightning App docs (#13537),,1
6157,Squashed commit of the following: (#2164),0.61463815,[1.1.6] - 2021-01-26,  ref: accelerator connector methods 3/n   ref: accelerator connector methods 3/n ,0
6158,Fix mypy errors attributed to pytorch_lightning.utilities.distributed (#13678),0.7330775,+ # pytorch_lightning==1.7.0,,1
6159,ref: part 4 of #3733 (#3773),0.7277377,[1.7.4] - 2022-08-31,  ref: accelerator connector methods x/n   ref: accelerator connector methods x/n ,1
6160,ref: inner train loop (intermediate step) 3/n (#3363),0.7810612,"inner train loop (#3359, #3361, #3362, #3363, #3365, #3366, #3367, #3368, #3369, #3370, #3371, #3372, #3373, #3374, #3375, #3376, #3385, #3388, #3397)",,1
6161,make PyTorch Lightning PEP 561 Compliant  (#3187),0.74957585,+ # pytorch_lightning==1.7.0,,1
6162,[App] Resolve multi-node cloud bug (#15619),0.70252645,Resolved a bug where the wrong client was passed to collect cloud logs (#14684),We were comparing keys across the same checkpoint dict instead of ckpt_last vs ckpt_last_epoch All other changes here are formatting,1
6163,Update bfloat16 docs (#10330),0.5882673,Update the Lightning App docs (#13537),  ref: organize args x/n   ref: move specific accelerator code x/n   ref: move specific accelerator code x/n   ref: move specific accelerator code x/n ,0
6164,Iterate dictionary directly (#8155),0.55989206,    # Return a list of dictionaries with commands:,,0
6165,docker use $(nproc) (#7606),0.5854878,"# if user gave a port number, use that one instead",  embedding similarity class + test   fix tests   fix pep8   add docs   noindex   Update docs/source/metrics.rst   Update pytorch_lightning/metrics/self_supervised.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update pytorch_lightning/metrics/self_supervised.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   suggestions   changes to init   move all   fix imports   Apply suggestions from code review   assert typo   change import   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Nicki Skafte nugginea@gmail.com,0
6166,removed printing. added auto process gen if slurm tasks do not match,0.5363884,Control SLURM's re-queueing,,0
6167,Add dataclass support to _extract_batch_size (#12573),0.7908026,- Added dataclass support to `extract_batch_size` ([#12573](https://github.com/Lightning-AI/lightning/pull/12573)),,1
6168,Prepare v1.8.0rc0 (#14918),0.64139223,Implemented ready for components (#16129),,0
6169,Remove support for passing strategy strings to accelerator (#12696),0.71048844,- Removed support for passing strategy names or strategy instances to the accelerator Trainer argument ([#12696](https://github.com/Lightning-AI/lightning/pull/12696)),  add num_classes arg to confusion matrix   update ConfusionMatrix test   final update) ,1
6170,Remove deprecated hpc_load in CheckpointConnector (#10525),0.8461503,Deprecated the use of CheckpointConnector.hpc_load() in favor of CheckpointConnector.restore() (#7652),  rename   fix badges   add docker build   mergify   update   env   ci   times   CI   name   comment ,1
6171,Feature/5275 clean progress bar print (#5470),0.788067,Better progress bar (#16695),  Fix batch_outputs with optimizers frequencies   optimizers   fix batch_outputs with optimizer frequencies   clean test   suggestion   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   chlog   failing doctest   failing doctest   update doctest   chlog   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
6172,Avoid printing ModelCheckpoint log with monitor=None and verbose=True (#6109),0.85595226,Do not print top-k verbose log with ModelCheckpoint(monitor=None) (#6109),  ref: organize args 3/n   ref: organize args 3/n   ref: organize args 3/n   ref: organize args 3/n   ref: organize args 3/n   ref: organize args 3/n ,1
6173,Replace lightning logo asset (#4844),0.66463995,- Removed the deprecated `LightningIPUModule` ([#14830](https://github.com/Lightning-AI/lightning/pull/14830)),  ref: organize args 2/n   ref: organize args 2/n   ref: organize args 2/n ,0
6174,App: Move AutoScaler dependency to extra requirements (#15971),0.6740621,Porting fixes to autoscaler component (#16249),"  Updated ""Models defined by data""   Fixed WARNING: Inline substitution_reference start-string without end-string.   Co-authored-by: David Waterworth david.waterworth@cim.io",0
6175,Bugfix/fix gan example (#2019),0.65088063,Tons of bug fixes,,0
6176,make gpus=str in Trainer consistent with command line parsing of string (#6388),0.62173164,parser = Trainer.add_argparse_args(parser),  ref: organize args 2/n   ref: organize args 2/n   ref: organize args 2/n   ref: organize args 2/n ,0
6177,Use getter instead of python property for the dataloaders (#275),0.62319314,"  dataloader = DataLoader(PyTorchDataset(...), ...)",  ref: organize args 2/n   ref: organize args 2/n ,0
6178,add tests for single scalar return from training (#2587),0.63117,trainer.test(),  ref: organize args 1/n   ref: organize args 1/n ,0
6179,fix: corrected attribute in *_dataloader in datamodule (#2748),0.66282827,refactored dataloader process hook (#3139)," 📝Update Readme.md (Code Error Minimal Example)  to use gpus we need to use trainer = pl.Trainer(max_epochs=1, gpus=8) but In readme only Trainer() is used   📝Update Readme.md(added import in GPU/TPU stmts)   📝Update Readme.md ( Added Import to all imports)   as per the suggestion from @awaelchli Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com",0
6180,Add ddp launcher for ddp testing,0.71604633,DDP Debugging Improvements,  ref: move lr_finder   ref: move lr_finder   ref: move lr_finder   ref: move lr_finder   ref: move lr_finder   ref: move lr_finder   ref: move lr_finder ,1
6181,Add GitHub Actions build (#823),0.5830855,- Added support for adding requirements to commands and installing them when missing when running an app command ([#15198](https://github.com/Lightning-AI/lightning/pull/15198),  ref: separate properties   ref: separate properties   ref: separate properties   ref: separate properties ,0
6182,Minimal Transformer Example (#17282),0.5482517,Introduce lightning connect (#14452),,0
6183,ENG-627: Docs for CloudCompute Mount Argument (#15182),0.59952444,Resolved a bug where the wrong client was passed to collect cloud logs (#14684),  Added py.typed.   Move py.typed inclusion to MANIFEST.in   Added path separator for inclusion ,0
6184,Update hash for caching (#12405),0.5191835,Refactor cloud dispatch and update to new API (#16456),  ref: trainer argparse 1/n   ref: trainer argparse 1/n   ref: trainer argparse 1/n   ref: trainer argparse 1/n   ref: trainer argparse 1/n   ref: trainer argparse 1/n   ref: trainer argparse 1/n   ref: trainer argparse 1/n ,0
6185,[feat] Named Parameter Groups in LearningRateMonitor (#7987),0.63914704,Changed LearningRateLogger to LearningRateMonitor (#3251),,0
6186,Isolate optimizer step logic to the PrecisionPlugin (#10029),0.80418783,- PrecisionPlugin.{post_optimizer_step},,1
6187,Add CI app cloud e2e & fix setup UI download (#13499),0.571055,"Apps without UIs no longer activate the ""Open App"" button when running in the cloud (#15875)",  ref: trainer 1/n   ref: separate trainer docstrings poc   ref: separate trainer docstrings poc   ref: separate trainer docstrings poc   ref: separate trainer docstrings poc ,0
6188,Loss format from .3f to .3g in the tqdm (#4972),0.57473874,Changed default TQDM to use tqdm.auto for prettier outputs in IPython notebooks (#752),  ref: moved eval loop 2/n   ref: moved eval loop 2/n   ref: trainer 1/n   ref: trainer 1/n   ref: trainer 1/n ,0
6189,Allow separate config files for parameters with class type when LightningCLI is in subclass_mode=False (#10286),0.994665,Allowed separate config files for parameters with class type when LightningCLI is in subclass_mode=False (#10286),  ignore types in files   CI timeout ,1
6190,Added max number of steps in Trainer (#728),0.7720822,Changed default value of the max_steps Trainer argument from None to -1 (#9460),  Get experiment_id from MLFlow only once instead of each training loop.   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   add test that asserts mlflow client is called to retrieve experiment id only once   make pep8 happy   logs   Co-authored-by: Patrick Orlando patrick.orlando@rea-group.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai,1
6191,Update to the latest playwright container v1.27.1 (#15129),0.4797163,- Fixed torchscript error with containers of LightningModules ([#14904](https://github.com/Lightning-AI/lightning/pull/14904)),  fix   fix and test   fix merge error   test for max dataset size   changelog   update docs   fix merge   unused imports   imports ,0
6192,Standalone Lite: DataParallel Strategy (#14681),0.7063008,The slow and clunky data-parallel strategy (#16748),,1
6193,Fix docs for early stopping (#865),0.62532175,"Disabled checkpointing, earlystopping and logging with fast_dev_run (#5277)",  ref: added model connector   ref: added model connector   ref: added model connector ,0
6194,update changelog after 1.4.8 release (#9650),0.6850897,Full Changelog,  ref: device to gpus   ref: device to gpus   ref: device to gpus   ref: device to gpus   ref: device to gpus ,0
6195,Rename the TPUSpawnStrategy to XLAStrategy (#16781),0.76480347,- Renamed `TPUSpawnStrategy` to `XLAStrategy` ([#16781](https://github.com/Lightning-AI/lightning/pull/16781)),  Added check for apex AMP and unit tests for Horovod + AMP   Changelog   Fixed order of Horovod and Apex optimizer wrapping ,1
6196,Refactor LightningDataParallel (#5670),0.8167956,Deprecated LightningDataParallel in favor of new wrapper module LightningParallelModule (#5670),  ref: train loop refactors part 2: 1/n   ref: device parser   ref: device parser   ref: device parser   ref: device parser   ref: device parser   ref: device parser   ref: device parser   ref: device parser ,1
6197,add lightning colab tutorial notebooks (#3591),0.7256437,Updated LightningTemplateModel to look more like Colab example (#1577),  ref: remove inner train loop 1/n   ref: remove inner train loop 1/n ,1
6198,Address black version conflict (#11505),0.5553731,Set version as today (#13906),  ref: all results+dict actions in a connector 1/n   ref: all results+dict actions in a connector 2/n   ref: all results+dict actions in a connector 2/n   ref: all results+dict actions in a connector 2/n ,0
6199,mini refactor for _running_stage access (#5724),0.64436495,refactored dataloader process hook (#3139),,0
6200,Remove dead check in ModelCheckpoint (#10930),0.82494724,Removed the deprecated model argument from ModelCheckpoint.save_checkpoint (#8688), Fix sample code of LightningModule DataLoaders  In the LightningModule DataLoaders sample code the train DataLoader is again passed to DataLoader constructor which is incorrect.   Pass transform to MNIST dataset in documenattion.   Apply suggestions from code review   minor changes   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
6201,Docker release (#1613),0.575593,This release includes:,  ref: inner train loop (intermediate step) 20/n   ref: inner train loop (intermediate step) 21/n   ref: inner train loop (intermediate step) 21/n   ref: inner train loop (intermediate step) 21/n   ref: inner train loop (intermediate step) 21/n   ref: inner train loop (intermediate step) 21/n ,0
6202,Add LightningCLI(run=False|True) (#8751),0.7985457,LightningCLI(auto_registry=True),  ref: inner train loop (intermediate step) 19/n   Update debugging.py   ref: inner train loop (intermediate step) 19/n ,1
6203,clean up docs (#1614),0.63209367,clean up data reset (#3161),  ref: inner train loop (intermediate step) 17/n   ref: inner train loop (intermediate step) 17/n   ref: inner train loop (intermediate step) 17/n ,0
6204,Fix setup_model typos in Fabric (#17498),0.6030661,Replace MisconfigurationException with warning in ModelCheckpoint Callback (#4560),,0
6205,Progress bar callback (#1450),0.80670875,Deprecated ProgressBar callback in favor of TQDMProgressBar (#10134),  ref: inner train loop (intermediate step) 16/n   ref: inner train loop (intermediate step) 16/n   ref: inner train loop (intermediate step) 16/n   ref: inner train loop (intermediate step) 16/n   ref: inner train loop (intermediate step) 16/n   ref: inner train loop (intermediate step) 16/n ,1
6206,Re-introduce fix for Hydra directory sync with multiple process (#5993),0.9930749,Re-introduced fix for Hydra directory sync with multiple process (#5993),,1
6207,Docs: Fix import for scikit in XGBoost template (#15693),0.6475252,from setuptools import setup,  ref: inner train loop (intermediate step) 15/n   ref: inner train loop (intermediate step) 15/n ,0
6208,Fix TPU tests on master builds (#15349),0.69501245,Updated logic for checking TPUs availability (#6767),  ref: inner train loop (intermediate step) 14/n   ref: inner train loop (intermediate step) 14/n ,0
6209,Update setup logic in training type plugins (deepspeed) [2 / n] (#10009),0.74191093,"Implemented DeepSpeedPlugin._setup_model_and_optimizers (#10009, #10064)",  ref: inner train loop (intermediate step) 12/n   ref: inner train loop (intermediate step) 12/n   ref: inner train loop (intermediate step) 12/n   ref: inner train loop (intermediate step) 12/n   ref: inner train loop (intermediate step) 12/n   ref: inner train loop (intermediate step) 12/n ,1
6210,"Revert ""join coverage (#2460)"" (#2499)",0.64405483,[1.2.6] - 2021-03-30,  ref: inner train loop (intermediate step) 12/n   ref: inner train loop (intermediate step) 12/n   ref: inner train loop (intermediate step) 12/n ,0
6211,Move trainer functions (#7295),0.681272,Deprecated @auto_move_data in favor of trainer.predict (#6993),  ref: inner train loop (intermediate step) 11/n   ref: inner train loop (intermediate step) 11/n ,0
6212,Update API references for Core API (#11352),0.6532811,Refactor cloud dispatch and update to new API (#16456),,0
6213,cleanup (#10081),0.82112646,"Cleaning (#5948, #5949, #5950)",  ref: inner train loop (intermediate step) 9/n   ref: inner train loop (intermediate step) 9/n   ref: inner train loop (intermediate step) 9/n   ref: inner train loop (intermediate step) 9/n ,1
6214,Update Contributors (#1274),0.6880466,Contributors,  ref: inner train loop (intermediate step) 7/n   ref: inner train loop (intermediate step) 8/n ,0
6215,lr_finder: Fix typo in docstring (#1746),0.63486725,tuner.lr_find(...),  ref: inner train loop (intermediate step) 6/n   ref: inner train loop (intermediate step) 6/n   ref: inner train loop (intermediate step) 6/n ,0
6216,Docs (#1164),0.75526524,Docs,,1
6217,Data connection mounts for jobs running from CloudSpaces (#17006),0.59587586,"Working with multiple dataloaders (#16800, #16753)",,0
6218,Prune EvalModelTemplate (3/n) (#10971),0.57575333,Refactored EpochResultStore (#5522),  ref: inner train loop (intermediate step) 3/n   ref: inner train loop (intermediate step) 3/n   ref: inner train loop (intermediate step) 3/n   ref: inner train loop (intermediate step) 3/n   ref: inner train loop (intermediate step) 3/n   ref: inner train loop (intermediate step) 3/n ,0
6219,Add PL mastercalss to readme (#2873),0.55434406,Simplify the PL examples structure (shallower and more readable) (#1247),,0
6220,Add .git-blame-ignore-revs (#16709),0.5504062,  * Removed `Loop.replace()` ([#16361](https://github.com/Lightning-AI/lightning/pull/16361)),,0
6221,Trainer.test should return only test metrics (#5214),0.79395956,trainer.test(),,1
6222,Introduce the graveyard :headstone:  (#15061),0.4755054,[1.5.8] - 2022-01-05, Correct documentation examples of optimizer_step  Without the default arguments set in optimizer_step the examples fail due to the arguments not being provided  Apply suggestions from code review  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
6223,Add estimated_stepping_batches property to Trainer (#11599),0.88050103,Trainer.estimated_stepping_batches,,1
6224,Fix early stopping with training step's return dict (#3347),0.7396142,Removed training loop explicitly calling EarlyStopping.on_validation_end if no validation is run (#7069),  pull data hooks up into a common interface   fix multiple inheritance ordering   docs reference datahooks ,1
6225,corrected example usage of save_hyperparameters from List[str] to seperate str (#2353),0.75011945,Moved save_hyperparameters to its own function (#7119),  Refactor GPUMonitor to improve training speed   added gpu ids to monitor   update tests   added deprecation warning   pep   fix test   fix docs   fix log_gpu_memory   move deprecation check   chlog   Update CHANGELOG.md   suggestions and fix   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
6226,Update TQDM progress bar tracking with multiple dataloaders (#11657),0.8170834,Run main progress bar updates independent of val progress bar updates in TQDMProgressBar (#12563),,1
6227,Enableself.log in most functions. (#4969),0.93845713,Enabled self.log in most functions (#4969),  fix datamodule hasattr   fix patch check   fix setattr   update docs   revert patch fix   changelog   fix datamodule passed in as fit arg   docs   set datamodule batch size in lightning_setattr   fix merge   check with has_attr   access datamodule via trainer   pass fit args down to tuner   docs   fix typos in docs   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
6228,complete test (#1705),0.697082,Fully tested!,"  script   docs   simple test   move test   fix doctest   no grad context   extend tests   test test   datamodule test   clean up test   docs   name   fix import   update changelog   fix import   skip pytorch 1.3 in test   update codeblock   skip bugged 1.4   typehints   doctest not working on all pytorch versions   rename TestGAN to prevent pytest interference   add note about pytorch version   fix torchscript version inconsistency in tests   reset training state + tests   update docstring   Apply suggestions from code review   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com   update docstring, dict return   add docs to index   add link   doc eval mode   forward   optional save to file path   optional   test torchscript device   test save load with file path   pep   str   Commit typing suggestion   Co-authored-by: ananthsub ananth.subramaniam@gmail.com  skip test if cuda not available  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: ananthsub ananth.subramaniam@gmail.com",0
6229,Add hydra experimental to correct location,0.5077233,Resolve interpolation bug with Hydra (#5406) ,  Change LearningRateLogger to LearningRateMonitor   file rename   docs   add LearningRateLogger with deprecation warning   deprecated LearningRateLogger   move deprecation check   chlog   Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai,0
6230,[Metrics] Fix/4237 auc unstable reorder (#4281),0.7397932,Deprecated reorder parameter of the auc metric (#4237), use fsspec instead of gfile for all IO  This better supports remote (and local) file operations with a dedicated package  Apply suggestions from code review  Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com  chlog  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai,1
6231,Update progress.py,0.66892904,prog -> progress   ,  Fix: gather_all_tensors cross GPUs in metrics   add a test case for gather_all_tensors_ddp in #3253 ,0
6232,Enable auto parameters tying for TPUs (#9525),0.75601786,Enable Auto Parameters Tying,   clarifying doc    improved documentation clarity   docs minor correction   remove device from type_as   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
6233,Add accelerator method teardown() (#11935),0.7962109,Changed teardown() in Accelerator to allow training_type_plugin to customize teardown logic (#7579),  add conda badge to readme   Update README.md   Update README.md   Apply suggestions from code review   make same color with pypi package   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
6234,Upgrade to HPU release 1.7.0 (#15616),0.59157467,0.4.0,  cast pl AttributeDict to dict   fix for omegaconf ,0
6235,fix myst-parser warning blocking docs ci (#7967),0.54866016,Do not override PYTHONWARNINGS (#4700),  ref: moving train loop to own object 2/n (intermediate steps)   ref: moving train loop to own object 2/n (intermediate steps) ,0
6236,add package info (#395),0.62696564,"    name=""my-package"",",  ref: moving train loop to own object 2/n (intermediate steps)   ref: moving train loop to own object 2/n (intermediate steps) ,0
6237,Remove Trainer._strategy_type (#11990),0.8171773,Removed the deprecated TrainerTrainingTricksMixin class (#8679),  ref: moving train loop to own object (intermediate steps)   ref: moving train loop to own object (intermediate steps)   ref: moving train loop to own object (intermediate steps)   ref: moving train loop to own object (intermediate steps) ,1
6238,apply PEP8,0.45847633,The psutil package is now required for CPU monitoring (#17010),,0
6239,Lazy import check for hydra dependency (#13812),0.5638527,import argparse,  ref: moved accelerator   ref: moved accelerator   ref: moved accelerator   ref: moved accelerator ,0
6240,Enable Probot CheckGroup v5 (#15670),0.59038293,Configuration Validator (#9779),  rename metrics   update docs ,0
6241,Update CODEOWNERS (#5561),0.56244,Deprecated @data_loader decorator  (#926),  ref: moved argparse code to central class   ref: moved argparse code to central class   ref: moved argparse code to central class ,0
6242,Force ModelCheckpoint callback to run last (#5731),0.71538764,Replace MisconfigurationException with warning in ModelCheckpoint Callback (#4560),"  change t() to transpose() as xla devices do not support .t() on 1-dim tensor   detach tensor before copying   Revert ""detach tensor before copying""   This reverts commit 37cc7bbe   changed dims   added test_result_obj_on_tpu   detach before copying   detach before copying   detach before copying   replace torch.cat with sum ",1
6243,Tests/drop macos py38 (#2061),0.5686306,Skipped testing with PyTorch 1.7 and Python 3.9 on Ubuntu (#11217), Added missing term 'Data' in 'LigthningModuleAPI'  This could be a possible typo!  Update datamodules.rst  Co-authored-by: William Falcon waf2107@columbia.edu,0
6244,CI: filter mypy triggers (#15481),0.5292498,the default signal is SIGUSR1,Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,0
6245,update GitHub templates (#612),0.56128097,- Removed the deprecated `BaseProfiler` and `AbstractProfiler` classes ([#14404](https://github.com/Lightning-AI/lightning/pull/14404)),,0
6246,CI: skip jobs in draft (#15529),0.39456627,[1.5.2] - 2021-11-16,  ref: move train outside of setup training   ref: move train outside of setup training   ref: move train outside of setup training   ref: move train outside of setup training ,0
6247,Bump mypy from 1.1.1 to 1.2.0 in /requirements (#17535),0.5406151,"    version=""0.0.1"",",  ref: .tune()   ref: run_pretrain_routine -> setup_training ,0
6248,moved COMET_DISABLE_AUTO_LOGGING out of modeule for flake8 compliance (#410),0.6205611,Using .comet.config file for CometLogger (#1913),  ref: .tune()   ref: .tune()   ref: .tune()   ref: .tune()   ref: .tune()   ref: .tune() ,0
6249,Add map location option to checkpoint upgrade utility (#17527),0.57777655,Saves apex states automatically and restores it for a checkpoint.,  ref: modular is_overridden   ref: modular is_overridden   ref: modular is_overridden   ref: modular is_overridden ,0
6250,flake8 & isort (#5647),0.59220666,[1.1.8] - 2021-02-08,  ref: added data connector   ref: added data connector   ref: added data connector   ref: added data connector   ref: added data connector   ref: added data connector   ref: added data connector   ref: added data connector   ref: added data connector   ref: added data connector ,0
6251,New bug form (#14193),0.5853812,Fixing critical bugs in newly added hooks and hparams assignment.,,0
6252,Update evaluation docs (#11173),0.68336093,Docs improvements,,0
6253,cleaning up demos (#312),0.6805446,"Cleaning (#5948, #5949, #5950)",  updated docs   updated docs   updated docs   updated docs   updated docs ,0
6254,Update CHANGELOG after the 1.5.9 release (#11558),0.71273804,Here is a selection of important changes that are not backward compatible with versions < 1.5. The full list of changes and removals are listed in the changelog at the bottom.,  updated docs   updated docs ,1
6255,Deprecate save_function from model checkpoint callback (#7201),0.9225538,Deprecated the save_function property from the ModelCheckpoint callback (#7201),  updated docs   updated docs   updated docs   updated docs   updated docs   updated docs ,1
6256,call on_load_checkpoint() when resuming from checkpoint (#1666),0.8550073,Trainer now calls on_load_checkpoint() when resuming from a checkpoint (#1666),  updated docs   updated docs   updated docs ,1
6257,pkg: bump 2.1dev & chlog sections (#17109),0.6242552,Moved accelerators and plugins to its legacy pkg (#5645),"  Parse Union[bool, str] arguments   Address review   Co-authored-by: William Falcon waf2107@columbia.edu",0
6258,Support any lr_scheduler,0.74867964,          lr_scheduler.step(),  initial draft   fix test   Update pytorch_lightning/trainer/callback_hook.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   fix tests   remove old code   untested upgrade script   document limitations   clean up and add tests   Update pytorch_lightning/trainer/training_io.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   reflect PR comments   fix formatting   Update docs/source/callbacks.rst   clarify docs   revert change for loading checkpoints   small edits   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
6259,remove deprecated args to learning rate step function (#890),0.9992982,Remove deprecated args to learning rate step function (#890),  Changed standard open to cloud_open   Changed how version numbers are extracted to remove terminal / from paths   formatting   Co-authored-by: James Bockman james@aiml.team Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
6260,Updated metrics/classification/precision_recall.py (#5348),0.7014112,Renaming of precision recall metric (#3308),  refactored clean_namespace   Update try except to handle pickling error   Consolidated clean_namespace. Added is_picklable   PEP8   Change warning to use rank_zero_warn. Added Test to ensure proper hparam filtering   Updated imports   Corrected Test Case ,1
6261,Update introduction_guide.rst (#7453),0.54793334,"    version=""0.0.1"",",  Follow up of #2892   typo   iterabledataset ,0
6262,Replace unwrapping logic in strategies (#13738),0.5720682,Renamed Strategy.reduce to Strategy.all_reduce in all strategies (#16370),,0
6263,Lightning: make type hints public (#17100),0.6802636,Secrets for Lightning Apps,  Fix GpuUsageLogger   docstrings   misconfigexception   add basic tests   skip doctest   fix parameter and docstring   rm cl   skip doctest   cleanup   chlog   add suggestions from review   add test from suggestions   fix import   fix test   fix test   fix test   fix test   rename GpuUsageLogger to GPUStatsMonitor   doc fix   Apply suggestions from code review   update docs format   update docs   miss   merge   fix title formatting   unindent   punctuation   simplify if statements   fix test   suggestions   pep   Update CHANGELOG.md   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   fix on_train_batch_*   use AttributeDict   usage   rank zero   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   import   minor changes   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Adrian Wälchli adrian.waelchli@inf.unibe.ch Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
6264,v0.1.3.0rc3 + changelogs (#7388),0.704504,Full Changelog: https://github.com/Lightning-AI/lightning/compare/1.8.3...1.8.3,,1
6265,Add fairscale requirement as zip before release,0.5237173,Avoided requiring the FairScale package to use precision with the fsdp native strategy (#14092),,0
6266,Split jobs into two workflows (#12449),0.5761827,System customization syncing for jobs run (#16932),  tests to ensure correct dataloading interval and sequence   tests to ensure correct dataloading interval and sequence   tests to ensure correct dataloading interval and sequence   tests to ensure correct dataloading interval and sequence   tests to ensure correct dataloading interval and sequence ,0
6267,Add typing to TQDMProgressBar (#11369),0.6766083,Deprecated ProgressBar callback in favor of TQDMProgressBar (#10134),,0
6268,Fix amp tests (#661),0.70136887,enable_pl_optimizer=False by default to temporarily fix AMP issues (#5163),  Fix typo   ref: group prepare data hook (6) (#3212)   group prepare data hook   group prepare data hook   group prepare data hook   group prepare data hook   group prepare data hook   group prepare data hook   group prepare data hook   Fix typo   Co-authored-by: William Falcon waf2107@columbia.edu,1
6269,Add typing to accelerators/gpu.py (#11333),0.6626371,"    accelerator=""gpu"", ",  group prepare data hook   group prepare data hook   group prepare data hook   group prepare data hook   group prepare data hook   group prepare data hook   group prepare data hook ,0
6270,set default work version v1.12 (#15431),0.60823894,Set version as today (#13906),,0
6271,Fix unimplemented type() on TPU (#1396),0.59447104,Renamed TPUHalfPrecisionPlugin to TPUBf16PrecisionPlugin (#10026),,0
6272,Update core/lightning.py to core/module.py (#12740),0.7496816,Removed the deprecated pytorch_lightning.core.lightning module in favor of pytorch_lightning.core.module (#16318),,1
6273,Use non-deprecated options in tests (#9949),0.59632003,test_percent_check in favour of limit_test_batches,,0
6274,Update README to 1.3 (#7489),0.55572426,"    version=""0.0.1"",",  ddp backend refactor   ddp backend refactor ,0
6275,Make Trainer.__test_using_best_weights use cloud_io's load to support more storage backends (#3694),0.579224,Swaped torch.load for fsspec load in cloud_io loading (#3692),,0
6276,Move accelerator-specific parsing functions with their accelerators (#14753),0.80398697,move specific accelerator code (#3457),,1
6277,Move optimizer step and clipping into the PrecisionPlugin (#10143),0.845025,"Moved the optimizer_step and clip_gradients hook from the Accelerator and TrainingTypePlugin into the PrecisionPlugin (#10143, #10029)",  ddps train   ddps train ,1
6278,fixes #3871 (#3919),0.62285215,Resolve bug with Finetuning (#5744),,0
6279,Update tpu_cores flag with accelerator and devices flag (#12158),0.68736076,- Removed `AcceleratorConnector.tpu_cores` property ([#12437](https://github.com/PyTorchLightning/pytorch-lightning/pull/12437)),,0
6280,Document SLURM interactive mode (#16955),0.5737326,Read more about our SLURM integration here.,  eval loop clean up   eval loop clean up   eval loop clean up   eval loop clean up ,0
6281,[App] Connect and Disconnect node (#16700),0.9089768,Connect and Disconnect node (#16700),  remove _evaluate   remove _evaluate   remove _evaluate   remove _evaluate   remove _evaluate   remove _evaluate   remove _evaluate   remove _evaluate ,1
6282,hpu: add extra warning after removal (#17372),0.63763803,Don't raise a warning when nn.Module is not saved under hparams (#12669),  added to(device)   added test   fix test on gpu   Update pytorch_lightning/core/lightning.py   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com  Update pytorch_lightning/core/lightning.py  Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com  remove multi gpu check  Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com   updated message   Update pytorch_lightning/core/lightning.py   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com   updated test   onxx to onnx   Update pytorch_lightning/core/lightning.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update tests/models/test_onnx.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  add no grad  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   add isinstance back   chlog   error is input_sample is not Tensor   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai,0
6283,added lightning docs,0.8153311,LightningCLI additions:,  moved hooks around in eval loop   moved hooks around in eval loop   moved hooks around in eval loop   moved hooks around in eval loop ,1
6284,Use PL 1.8.0 instead of 1.8.0rc in app example requirements (#15454),0.56554604,Changed deprecated enable_pl_optimizer=True (#5244),  fix rmsle   Updated test to match rmsle fix   Updated RMSLE example result to match functional   chlog   add randomized test   fix pep8   Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai Co-authored-by: Nicki Skafte skaftenicki@gmail.com,0
6285,[TPU] Refactor availability check (#17384),0.8753655,Updated logic for checking TPUs availability (#6767),"The previous implementation trained a auto encoder and evaluated classificator. I try to fix this by replacing the evaluation metric with an auto encoder metric. Hence, no classification is done. I'm not 100% sure what the original authors intent was, since he extends a classification model (LitMNIST) but does not use it. The following model is an AutoEncoder and does not do any classification.  Small textual changes. forward() now implements encoding and not decoding (as it was described  in the text.) _shared_eval uses MSE loss instead of class loss, since no  classification weights are learned. initialized MSE in init, since calling MSE directly is not  supported.",1
6286,"changed spelling of ""licence"" (#5937)",0.52443635,Re-enabled naming metrics in ckpt name (#3060),  new base structure   missing packages   updated interface   revert some changes   fixes   add changelog   fix bug   added description   test for pickable   fixing test   fixing test   fix pickle issue   reduceop typehints back   remove redundant module arg   add save/load test   add aggregate method   text clarification   fix doctest   Apply suggestions from code review   change test to results obj   fix docs   formatting   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   formatting   pep   Update CHANGELOG.md   suggestions   fix tests   fix pep8   fix tests   Co-authored-by: Nicki Skafte nugginea@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
6287,[docs] add community example : pl + ms nni (#2340),0.50924695,"For a full tutorial and running example, visit our docs. TODO: add to docs",  Update training_tricks.py   pep   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,0
6288,Update .codecov.yml,0.5331116,Updated app URLs to the latest format (#16568),,0
6289,Add Changelog entry for #12716 (#12813),0.71885705,Full Changelog, restore eval loop hook,1
6290,Introduce Upload File endpoint (#14703),0.75737095,Add support to upload files to the Drive through an asynchronous upload_file endpoint (#14703),,1
6291,Remove data_pipeline attribute patch (#12204),0.61468565,Removed deprecated TrainResult (#5323),  remove on_eval_start hook   remove on_eval_start hook ,0
6292,Fix broken links to PyTorch Lightning Bolts (#9634),0.7359232,Removed pytorch_lightning.trainer.connectors.OptimizerConnector (#10120),,1
6293,decouple returns from each step (#307),0.60277534,"Changed the order of backward, step, zero_grad to zero_grad, backward, step (#6147)",,0
6294,refactored dataloader process hook (#3139),1.0,refactored dataloader process hook (#3139),  clean up data reset   clean up data reset ,1
6295,"Update pandas requirement from <=1.4.3,>1.0 to >1.0,<1.5.1 in /requirements (#14787)",0.72867197,Removed dependency on pandas (#736),  gan updated for lightning-0.9   bugs fixed ,1
6296,tiny spelling error (#295),0.41373873,Updated Multinode Warning (#16091),  Added missing parameter 'minimize' docs in TrainResult   Added missing docs for parameters in TrainResult and EvalResult ,0
6297,Make DDP subprocess the default launcher for multi-device (#16780),0.6603451,"When using multiple devices, the strategy now defaults to ""ddp"" instead of ""ddp_spawn"" when none is set (#16388)",  clean up hooks in run_evaluation   clean up hooks in run_evaluation   clean up hooks in run_evaluation   clean up hooks in run_evaluation   clean up hooks in run_evaluation   clean up hooks in run_evaluation   clean up hooks in run_evaluation ,0
6298,Fixes #234 (#311),0.6205032,Resolve bug with Finetuning (#5744),  final inner eval loop hooks   final inner eval loop hooks ,0
6299,CI: adjust names & skip app if pytorch (#13317),0.6275743,Improved PyTorchProfiler chrome traces names (#8009),,0
6300,prune ignore (#4240),0.6150674,Sanitize None params during pruning (#6836),  refactored dataloader process hook   refactored dataloader process hook   refactored dataloader process hook ,0
6301,[ddp] Support multi-node distributed execution under torchelastic (#1811),0.6906299,"In a similar fashion, if installing torch>=1.11, you can enable DDP static graph to apply special runtime optimizations:",,0
6302,added on_after_backward,0.60005593,"The on_before_optimizer_step hook previously ran before the entire optimization closure, including backward. This was unintended behavior and if you rely on this, move your code to the new on_before_backward` hook.",  added eval loop   added eval loop   added eval loop   added eval loop   added eval loop   added eval loop ,0
6303,feature: add _generate_works_json method (#15767),0.4878323,- `LightningCLI` changed to use jsonargparse native support for list append ([#13129](https://github.com/Lightning-AI/lightning/pull/13129)),,0
6304,drop failing e2e quick app (#17409),0.5287152,| Enum RunningStage.TUNING                                    | 1.10             | No longer supported         |,,0
6305,Move metrics_to_scalars to a dedicated utilities file (#7180),0.6751291,Changed metrics_to_scalars to work with any collection or value (#7888),,0
6306,Simplify test for AMP plugins (#6311),0.6830125,enable_pl_optimizer=False by default to temporarily fix AMP issues (#5163),  moved eval hooks   moved eval hooks   moved eval hooks   moved eval hooks   moved eval hooks   moved eval hooks   moved eval hooks ,0
6307,formatting 5/n: Core (#5721),0.5440103,Implemented ready for components (#16129),  moved eval hooks   moved eval hooks   moved eval hooks   moved eval hooks ,0
6308,Remove twine dependency from requirements (#13050),0.53964216,Avoid using the deprecated LooseVersion (#16162),  added hook base method   added hook base method ,0
6309,Fix fit loop restart logic to enable resume using the checkpoint (#12821),0.7766855,Call reset_on_restart in the loop's reset hook instead of when loading a checkpoint (#9561),  simplified training_forward   simplified training_forward   simplified training_forward ,1
6310,fix parallel devices return type & add copyright (#6215),0.5753822,Made parallel devices optional across all plugins (#6051),  remove obscure forward call in eval   remove obscure forward call in eval   remove obscure forward call in eval   remove obscure forward call in eval   remove obscure forward call in eval   remove obscure forward call in eval ,0
6311,Support CLI shorthand natively (#12614),0.75108224,Introducing CLI commands for apps (#13602)!,,1
6312,moves init apex from LM to apex connector (#3923),0.99999994,moves init apex from LM to apex connector (#3923),  Make trainer.state a read-only property   Update states.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
6313,[App] Improve PythonServer info message on startup (#15989),0.5726608,- Added an error message when attempting to launch processes with `python -i` and an interactive-incompatible strategy ([#15293](https://github.com/Lightning-AI/lightning/pull/15293)),  refactored horovod backend   refactored horovod backend ,0
6314,Improve typing for loops (#10749),0.66876185,Loop customization improvements,  refactored gpu backend __step   refactored gpu backend __step   refactored gpu backend __step   refactored gpu backend __step ,0
6315,Update introduction docs (#11140),0.6526365,Docs improvements,,0
6316,Add functional regression metrics (#2492),0.7916783,Regression metrics (#2221),  moved tpu training_step   refactored eval step   refactored eval step   refactored eval step ,1
6317,fixes slurm weights saving (#2339),0.65816087,Control SLURM's re-queueing,,0
6318,[bugfix] Resolve bug with multiple optimizers and toggle. (#5574),0.6935468,Resolve bug with Finetuning (#5744),  Only try to delete jobs if there are any to delete.   Reorder jobs.   Remove cleanup from the jobs that run on every commit. ,0
6319,Update lightning-utilities requirement from ==0.3. to ==0.4. in /requirements (#15420),0.77521884,Gave warnings for unimplemented required lightning methods (#1317),Automating,1
6320,[App] testing lightning in lightning-app package (#15286),0.83254266,Lightning App,,1
6321,remove legacy plugins (#5950),0.75368756,- The base Plugin class has been removed. , log_hyperparams add default metric  also adds scalar support   fix typos and style   another typo   keep original logging implementation   remove missed line   fix capitalization   add step to leg_metrics for tests   disable hp metric none (-1) logging   to pass tests   initial arg implementation   add step to log_metrics   add hp_metric case to log test   add docs    and minor formatting   fix broken else   pep8 style   edit tests   Update pytorch_lightning/loggers/tensorboard.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Update pytorch_lightning/loggers/tensorboard.py  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
6322,Introduce new precision layout in PL (#16783),0.66875106,Mixed precision overhaul (#16783),,0
6323,refactor tests,0.717272,"Refactored training_batch + tests to verify correctness (#2327, #2328)",,1
6324,Finalize logger (#337),0.7862611,Removed wandb logger's finalize method (#1193),,1
6325,document: fix  callback signature (#2113),0.6139066,Improved error messages in replace_sampler when the DataLoader attributes are not included in the signature or the signature is missing optional arguments (#8519),  fix(docs): test_dataloader use transformed dataset   suggestion   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,0
6326,docs: clarify closure usage in gan example (#8521),0.5764941,Check if optimizer supports closure (#4981),  Fix num_sanity_val_steps according to limit_val_steps   fix test   add num_sanity_batches   pep   update docstring in test   add more test   chlog   update comments and docstring in test   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Adrian Wälchli adrian.waelchli@inf.unibe.ch Co-authored-by: Ananya Harsh Jha ananya@pytorchlightning.ai,0
6327,Update governance.rst (#7111),0.7743377,Updated governance docs,  clean   ver ,1
6328,move get_active_optimizers to utilities (#9581),0.6286235,- Added utility functions for moving optimizers to devices ([#11758](https://github.com/PyTorchLightning/pytorch-lightning/pull/11758)),,0
6329,parse_gpu_ids fix (#382),0.6321322,Deprecated PrecisionPlugin.master_params() in favor of PrecisionPlugin.main_params() (#10105),,0
6330,Fix docs typo (#4930),0.50288355,Refactored setup for typing friendly (#6590),  fix   fix   fix   fix   temp   fix   0.9.0 readme   0.9.0 readme   0.9.0 readme   Co-authored-by: William Falcon waf2107@columbia.edu,0
6331,ref: precision plugins 1/n (#3504),0.9497336,precision plugins (#3504),  Readme changes   Update README.md   Update README.md   0.9.0 readme   0.9.0 readme   Co-authored-by: William Falcon waf2107@columbia.edu,1
6332,[feat] 1/2 Add trainer.predict  (#5579),0.74635607,"trainer = Trainer(accumulate_grad_batches={""1"": 5, ""10"": 3})",  0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme   0.9.0 readme ,1
6333,Remove special handling of loss in progress bar (#16192),0.83668804,Logging the loss to the progress bar (#16192),  miss   fix wrong changelog entry   miss   update changelog   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
6334,"Clarify limit_{train,val,test}_batches behaviour (#2630)",0.7104457,test_percent_check in favour of limit_test_batches,,1
6335,Add remaning sklearn metrics (#2562),0.83136225,Sklearn metrics classes (#1327),  fixed bad warn for result obj   fixed bad warn for result obj ,1
6336,fix fairscale compatible with PT 1.8 (#5996),0.64175105,Avoided requiring the FairScale package to use precision with the fsdp native strategy (#14092),,0
6337,opt_idx cleanup after optimizer loop changes (#16597),0.77339435,"    optimizer_idx,",,1
6338,spec Horovod version (#3661),0.7806757,"refactored Horovod backend (#3121, #3122)",  deconflict   fix links   deconflict   deconflict   deconflict   deconflict   deconflict   deconflict   deconflict   deconflict   deconflict   deconflict   deconflict   deconflict   Co-authored-by: William Falcon waf2107@columbia.edu,1
6339,Reset dataloaders on failure in tuner (#14372),0.6720965,Reset val_dataloader in tuner/batch_size_scaling (#9857),  :sparkles: add dm to_device logic in trainer   :fire: remove unnecessary comment   :sparkles: add to_device logic to datamodule   :white_check_mark: add test   updated docs   Co-authored-by: William Falcon waf2107@columbia.edu,0
6340,CI: block app contrib. (#13310),0.59532595,Introducing CLI commands for apps (#13602)!,  flake8 fixes   fix pep8   fix pep8   Co-authored-by: William Falcon waf2107@columbia.edu,0
6341,[App] Remove SingleProcessRuntime (#15933),0.9351295,Removed the SingleProcessRuntime (#15933),,1
6342,Type the Loop base class as generic (#9418),0.7030475,"Base classes (#1326, #1877)","When a LightningModule inherits from a class that implements __new__() such as typing.Generic, inspect.signature(cls) short-circuits and returns the signature of __new__() instead of __init__(). So, we need to be more specific and call inspection directly on the init function.",1
6343,ref: .tune() (temporary) (#3293),0.575675,adding Trainer.tune() (#3293),"  fix 3018, 3032   changed progress bar for 3032 ",0
6344,Add dim to pytorch_lightning.metrics.PSNR (#5957),0.7474024,+ # pytorch_lightning==1.7.0,,1
6345,Mark SignalConnector as protected (#11513),0.68030095,"- Marked the `{Accelerator,Signal,Callback,Checkpoint,Data,Logger}Connector` classes as protected ([#17008](https://github.com/Lightning-AI/lightning/pull/17008))",,0
6346,add warning when Trainer(log_every_n_steps) not well chosen (#7734),0.7214128,"trainer = pl.Trainer(callbacks=EarlyStopping(..., log_rank_zero_only=True))",  Update new-project.rst   Update new-project.rst   Create 3_steps.rst   revert   remove the callbacks vid   fix blank line   change ref   spelling   spelling   Update docs/source/new-project.rst   Co-authored-by: Nathan Raw nxr9266@g.rit.edu   spelling   spelling   spelling   spelling   spelling   spelling   spelling   Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: Nathan Raw nxr9266@g.rit.edu,1
6347,Re-enable Lite CLI on Windows + PyTorch 1.13 (#15645),0.6557373,- Fixed bug that forced overriding `configure_optimizers` with the CLI ([#11672](https://github.com/PyTorchLightning/pytorch-lightning/pull/11672)),  re-enabled naming metrics in ckpt name   re-enabled naming metrics in ckpt name   re-enabled naming metrics in ckpt name   re-enabled naming metrics in ckpt name   re-enabled naming metrics in ckpt name   re-enabled naming metrics in ckpt name ,0
6348,Add auto_device_count and device name support (#13423),0.5934005,Support auto_select_gpus with the accelerator and devices API (#12608),  add methods   log in trainer   add tests   changelog   fix tests   fix tests   fix tests   fix tests   fix tests   fix tests   fix tests   text   added argument   update tests   fix styling   improve testing ,0
6349,Merge pull request #68 from williamFalcon/williamFalcon-patch-1,0.5232296,"merge backends (#3476, #3477, #3478, #3480, #3482)",  lightning attr fix   revert refactor   create test   separate test   changelog update   tests   revert   Update pytorch_lightning/trainer/training_tricks.py   Co-authored-by: William Falcon waf2107@columbia.edu,0
6350,Fix typo (#6178),0.57601005,Changed overwrite to True (#16009),  Support sphinx toggle and copy buttons   add buttons to conf ,0
6351,hotfix Drone install Horovod (#4038),0.5898204,"refactored Horovod backend (#3121, #3122)",  return last logged value   Update test_results.py   Update step_result.py   Update step_result.py   pep8   pep8 ,0
6352,tests: switch imports for apps (#16554),0.6104704,| Import pl.core.mixins.DeviceDtypeModuleMixin                                                           | 1.10             | No longer supported |,  add test   test   test   add fix   changelog   check batch size changed ,0
6353,Docs: Update tutorial to match PyTorchProfiler changes (#15440),0.7068808,Changed PyTorchProfiler to use torch.autograd.profiler.record_function to record functions (#6349),  test for gethering results   fix gather   document tests   changelog   assert dtype   default to concat   additional test ,1
6354,Update Docs for current checkpointing behaviour (#445),0.65184647,Disable saving checkpoints if not trained (#4372),,0
6355,ci: use randon seed (#17571),0.57133317,Default seed_everything(workers=True) in the LightningCLI (#7504),"  Fix typo in Quick Start/Step-by-step walk-through   Fix typo in Quick Start/Step-by-step walk-through   Fix snippets in lightning module   Remove testblock    doctest does not have torch with CUDA, so x.cuda() will fail  Remove test code  ""..."" is not python, so doctests fail   Fix #3005   Fix indentation, stage in docs   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Teddy Koker teddy.koker@gmail.com Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com",0
6356,fix some minor typos in docs (#5369),0.5503496,Resolve bug with Finetuning (#5744),  freeze tb to 2.2.0   Update environment.yml ,0
6357,fix Neptune logger creating multiple experiments when gpus > 1 (#3256),0.57958823,Cleaning up stale logger tests (#3490),Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,0
6358,Fix mypy in utilities.argparse (#8124),0.5772925,Removed deprecated pytorch_lighting.utilities.argparse_utils module (#9166),  updated code example   added warning when changing monitor and using results obj   added warning when changing monitor and using results obj   added warning when changing monitor and using results obj   added warning when changing monitor and using results obj   added warning when changing monitor and using results obj   added warning when changing monitor and using results obj   added warning when changing monitor and using results obj   added warning when changing monitor and using results obj   added warning when changing monitor and using results obj   added warning when changing monitor and using results obj   added warning when changing monitor and using results obj   added warning when changing monitor and using results obj ,0
6359,Hotfix for torchvision (#6476),0.73895085,Removed dependency on torchvision (#797),,1
6360,Fix mypy typing for utilities.debugging (#8672),0.533168,Removed pytorch_lightning.utilities.debugging.InternalDebugger (#9680),  fix result for dp   fix result for dp   fix result for dp   fix result for dp   fix result for dp   fix result for dp   fix result for dp   fix result for dp   fix result for dp   fix result for dp   fix result for dp   fix result for dp   added warning when changing monitor and using results obj ,0
6361,Add docs and message for DDP static graph (#12411),0.61110806,"In a similar fashion, if installing torch>=1.11, you can enable DDP static graph to apply special runtime optimizations:",  added warning when changing monitor and using results obj   added warning when changing monitor and using results obj   added warning when changing monitor and using results obj ,0
6362,add python 3.8 testing (#915),0.7748102,python script.py test,,1
6363,Remove unused mixed precision class (#14790),0.6585094,Mixed precision overhaul (#16783),  added lr scheduler test using dev debugger   added lr scheduler test using dev debugger   added lr scheduler test using dev debugger ,0
6364,Add warning section for checkpoint_callback in Trainer docs (#10041),0.7480274,Deprecated checkpoint_callback from the Trainer constructor in favor of enable_checkpointing (#9754),  add ddp script variations   add ddp test   rename   shell   test   test   try call   try without subprocess   test   display the error   list all variations   try string   try copy env   debug   pythonpath   path   update test   change   simple ddp test   replace   remove random port   random port   str   clean up   check run spawn   clean up   docs   docs   update test   docs   changelog   changelog ,1
6365,Remove remaining old-style AcceleratorConnector properties (#13412),0.7464894,- Removed the `AcceleratorConnector.device_type` property ([#12081](https://github.com/PyTorchLightning/pytorch-lightning/pull/12081)),  updated docs   updated docs ,1
6366,fixed native amp + ddp (#1788),0.6039956,Enable forward compatibility with the native AMP (PyTorch 1.6).,,0
6367,Drop the DataLoader iterator when pickling (#17130),0.6730869,"        iterables = [DataLoader(), DataLoader()]",  updated docs   updated docs ,0
6368,fixed type hint for weights_summary arg (#1313),0.6301314,Trainer.add_argparse_args classmethod fixed. Now it adds a type for the arguments (#1147),,0
6369,Rename spawn-based launchers (#13743),0.55949223,"We redesigned the process creation for spawn-based strategies such as DDPSpawnStrategy and TPUSpawnStrategy (#10896). All spawn-based strategies now spawn processes immediately upon calling Trainer.{fit,validate,test,predict}, which means the hooks/callbacks prepare_data, setup, configure_sharded_model and teardown all run under an initialized process group. These changes align the spawn-based strategies with their non-spawn counterparts (such as DDPStrategy).",,0
6370,changed apex level (#2362),0.80420566,Changed default apex level to 'O2' (#2362),,1
6371,Delete deprecated TrainerLoggingMixin (#8609),0.92439735,Removed the deprecated TrainerLoggingMixin class (#8609),,1
6372,force crash when max_epochs < epochs in a checkpoint (#3580),0.64119214,"This also impacts checkpoints saved during an epoch (e.g. on_train_epoch_end). For example, a Trainer(max_epochs=1, limit_train_batches=1) instance that saves a checkpoint will have the current_epoch=0 value saved instead of current_epoch=1.",,0
6373,root_module: fix comma splits.,0.5123551,Fixing critical bugs in newly added hooks and hparams assignment.,,0
6374,.fit() returns last not best weights in ddp_spawn (#2565),0.58879447,"trainer.fit(..., ckpt_path=""last"")",  removed callback metrics from test results obj   removed callback metrics from test results obj ,0
6375,[TPU] Remove error check for IterableDatasets (#17331),0.7313794,Disabled sampler replacement when using IterableDataset (#11507),  fixed build   fixed build ,1
6376,LightningCLI documentation improvements (#8303),0.86354786,LightningCLI improvements,  build py3.6   info   conda   update   version   version   builds   builds   builds   builds   builds ,1
6377,Enable apex O2 + dp (#493),0.7132255,Changed default apex level to 'O2' (#2362),"  first attempt   update changelog   fix pep8 and tests   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   added new tests   fixed tests   Apply suggestions from code review   used num_training_batches   fixed pep8   fixed with is_last_batch suggested by @awaelchli   fixed with num_training_batches   fixed with num_training_batches   cleanup   fix test and update docs   fixed for alignment, update docs   minor changes   update doc   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com",1
6378,prune func calls in meta pkg init (#13742),0.4983718,Introducing CLI commands for apps (#13602)!,,0
6379,Update tests/callbacks/*.py to use devices instead of gpus or ipus (#11387),0.6438236,Dropped official support/testing for PyTorch <1.6 (#8288),,0
6380,"Update DeepSpeed version, fix failing tests (#9898)",0.6959483,Updated precision attributes in DeepSpeedPlugin (#10164),  add val step arg to metrics   add val step arg to metrics   add val step arg to metrics   add val step arg to metrics   add val step arg to metrics   add val step arg to metrics   add val step arg to metrics   add val step arg to metrics   add val step arg to metrics   add step metrics   add step metrics ,0
6381,ignoring dist parallel forward,0.61926425,Sequence Parallelism,,0
6382,Prepare for the 1.6.0 release,0.70392513,"    version=""0.0.1"",",,1
6383,"[Metrics] PrecisionRecallCurve, ROC and AveragePrecision class interface (#4549)",0.61162186,Gradient norm tracking (#16745), Save test predictions on multiple GPUs,0
6384,Update Changelog for v1.2.7 (#6874),0.7598113,Complete changelog,,1
6385,Fix tensor printing in trainer.test() (#5138),0.7156614,Fix reset TensorRunningAccum (#5106), fix gpus index error,1
6386,Update multi_gpu.rst (#4201),0.5757675,Updated Multinode Warning (#16091),,0
6387,Inline the ModelIO interface (#16999),0.6313876,Direct support for compiled models (#15922),,0
6388,Skip length checks for non-sized iterables (#17218),0.69748473,Support for arbitrary iterables (#16726),  Fixes #2942   doc fix ,0
6389,Avoid deprecated warnings from accelerator and checkpoint connector #10142,0.6581079,"Raise MisconfigurationException when the accelerator is available but the user passes invalid ([]/0/""0"") values to the devices flag (#12708)",,0
6390,Merge pull request #11046 from PyTorchLightning/docs/security,0.63946223,- Removed `configure_sync_batchnorm` from `ParallelStrategy` and all other strategies that inherit from it ([#11754](https://github.com/PyTorchLightning/pytorch-lightning/pull/11754)),,0
6391,"[WIP] ref: deprecated results obj, added support for simpler comms (1/n) (#3681)",0.8861891,"deprecated results obj, added support for simpler comms (#3681)",  add docs   fix spelling ,1
6392,"Update torchmetrics requirement from <=0.7.2,>=0.4.1 to >=0.4.1,<0.9.2 in /requirements (#13275)",0.7411837,"Set PyTorch 1.4 as min requirements, also for testing and examples torchvision>=0.5 and torchtext>=0.5 (#5418)",  override dist backend when using tpus   added test   updated doc string   drop redundant info...   more redundant info   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: rohitgr7 rohitgr1998@gmail.com,1
6393,feat: Add ModelSummary Callback (#9344),0.8260022,ModelSummary Callback,  Add label   add ref   add ref   add ref   add label   add label   add label   add label   Update fast_training.rst   label   label   label   label   label   label   label   label   label   label   label   Update performance.rst   Update production_inference.rst   Update profiler.rst   Update results.rst   Update sequences.rst   Update single_gpu.rst   Update slurm.rst   Update test_set.rst   Update tpu.rst   Update trainer.rst   Update training_tricks.rst   Update transfer_learning.rst   Update weights_loading.rst   Update governance.rst   Update hooks.rst   Update bolts.rst   Update child_modules.rst   Update hyperparameters.rst   Update transfer_learning.rst ,1
6394,[CLI] Add support for --key.help=class (#10767),0.6557367,Introducing CLI commands for apps (#13602)!,,0
6395,DeepSpeed ZeRO Docs update (#6752),0.8972707,"Updated DeepSpeed ZeRO (#6546, #6752, #6142, #6321)", update docs,1
6396,Fix MissingFieldException in offline mode (#14919),0.7593546,- Fixed MissingFieldException in offline mode for the `NeptuneLogger()` ([#14919](https://github.com/Lightning-AI/lightning/pull/14919)),  template   typo   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
6397,Unpin Pillow after the 8.3.1 release (#8324),0.46702206,"    version=""0.0.1"",",  fix(docs): docstring for amp_backend   fix(docs): early_stop_checkpoint -> early_stop_callback   docs   Co-authored-by: ananyahjha93 ananya@pytorchlightning.ai,0
6398,[App] Fix app name in URL (#16575),0.68604255,Updated app URLs to the latest format (#16568),"This function has the if statement if (train_dataloader or val_dataloaders) and datamodule:. The issue is similar to that in https://github.com/PyTorchLightning/pytorch-lightning/pull/1560. The problem is that the if(dl) translates to if(bool(dl)), but there's no dataloader.bool so bool() uses dataloader.len > 0. But... dataloader.len uses IterableDataset.len for IterableDatasets for which len is undefined. The fix is also the same, the if dl should be replaced by if dl is not None. Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com",0
6399,enable on_load_checkpoint for datamodule for all trainer_fn (#10238),0.8332344,Enabled on_load_checkpoint for LightningDataModule for all trainer_fn (#10238),"  Added strict=False and hparams_file accepcts dict   Apply suggestions from code review   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com   Type check fix   Added tests   Linting & test fix   Removed redundant code & test   Added strict=False and hparams_file accepcts dict   Apply suggestions from code review   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com   Type check fix   Added tests   Linting & test fix   Removed redundant code & test   Apply suggestions from code review   tests   tests   chlog   Update tests/models/test_restore.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   update test comments   Added docstring for the strict attribute   Added supplementary tests   Update saving.py   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  pep8, removed extra func  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: ananyahjha93 ananya@pytorchlightning.ai",1
6400,Enable inference mode for testing and predicting (#8813),0.64907557,Set Torch inference mode for prediction (#15719),"  Added strict=False and hparams_file accepcts dict   Apply suggestions from code review   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com   Type check fix   Added tests   Linting & test fix   Removed redundant code & test   Added strict=False and hparams_file accepcts dict   Apply suggestions from code review   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com   Type check fix   Added tests   Linting & test fix   Removed redundant code & test   Apply suggestions from code review   tests   tests   chlog   Update tests/models/test_restore.py   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   update test comments   Added docstring for the strict attribute   Added supplementary tests   Update saving.py   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  pep8, removed extra func  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: ananyahjha93 ananya@pytorchlightning.ai",0
6401,ref: inner train loop (intermediate step) 17/n (#3376),0.7914145,"inner train loop (#3359, #3361, #3362, #3363, #3365, #3366, #3367, #3368, #3369, #3370, #3371, #3372, #3373, #3374, #3375, #3376, #3385, #3388, #3397)",,1
6402,Fix disabled grads after call to predict (#6657),0.52366805,Deprecated automatically detaching returned extras with grads (#7994),,0
6403,Fix user warning produced by apex + scheduler combination (#1873),0.6535764,Improved error messages for invalid configure_optimizers returns (#3587),  add apex test   rename   level   events   wrap   evt   miss   apex   apex   apex   apex   apex   apex   Update tests/models/test_amp.py   Co-authored-by: William Falcon waf2107@columbia.edu   notes   notes   Co-authored-by: William Falcon waf2107@columbia.edu,0
6404,fix docker builds (#2383),0.57372147,Updated hooks arguments - breaking for setup and teardown (#2850), updated docs,0
6405,Ignore num_nodes when running MultiNode components locally (#15806),0.89389193,The MultiNode components now warn the user when running with num_nodes > 1 locally (#15806),  thr deterministic=True   clean   clean   Apply suggestions from code review   Co-authored-by: Vadym Stupakov vadim.stupakov@gmail.com  Apply suggestions from code review  Co-authored-by: Vadym Stupakov vadim.stupakov@gmail.com,1
6406,CI: skip examples with draft (#14453),0.4758035,[1.3.6] - 2021-06-15,,0
6407,Fix support for passing -1 to find_usable_cuda_devices function (#16866),0.57835615,"device = ""cuda"" if torch.cuda.is_available() else ""cpu""",,0
6408,Add typing to ModelPruning callback (#7529),0.77787954,ModelSummary Callback,  GPU utilisation Callback   GPU utilisation Callback   Fixing style   Fixing style   Fixing CodeFactor: partial executable path   Fix a misspelling in the Class name ,1
6409,fix examples (#3631),0.65252733,Resolve bug with Finetuning (#5744),,0
6410,Refactor (#407),0.73831797,Refactoring,  fix hparams loading for model that accepts *args   add test case   changelog   pep   fix test   Co-authored-by: William Falcon waf2107@columbia.edu,1
6411,Bump loader-utils from 1.4.0 to 1.4.2 in /src/lightning_app/cli/pl-app-template/ui (#16375),0.6278231,Resolved the memory leak issue with the Lightning Cloud package and bumped the requirements to use the latest version (#14697), change to OmegaConf API  Co-authored-by: Omry Yadan omry@fb.com   Swapped Container for OmegaConf sentinel; Limited ds copying   Add Namespace check.   Container removed. Pass local tests.   Co-authored-by: Omry Yadan omry@fb.com,0
6412,Change reference to depricated result object to self (#3989),0.6357671,Updated references to self.forward() to instead use the __call__ interface. (#1211), track batch size in result obj,0
6413,cleaned up pep8 issues,0.60720176,This release fixes that core issue,  Update CONTRIBUTING.md   Update CONTRIBUTING.md   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
6414,Reset the progress tracking state after sanity checking (#11218),0.784719,- Reset the validation progress tracking state after sanity checking ([#11218](https://github.com/PyTorchLightning/pytorch-lightning/pull/11218)),,1
6415,Fix batch_arg_name bug (#4812),0.5564859,argparse_utils >> argparse,,0
6416,ref: run_pretrain_routine -> setup_training (#3294),0.91828966,move run_pretrain_routine -> setup_training (#3294), updated docs,1
6417,"Use f-""""""-string in a Trainer comment (#6377)",0.64866805,"trainer = Trainer(precision=""bf16"")",  Use kubectl to get logs from TPU CI instead of gcloud logging.   Update Github Action to read logs from kubectl rather than gcloud logging. ,0
6418,Update checkgroup config (#14587),0.6106212,Configuration Validator (#9779),  add docs   non blocking only on tensor   changelog   add test case   add test comment   update changelog   changelog chlog,0
6419,tng and val steps now have batch nbs,0.567101,Renames model steps (#1051),Variable defined as mnist_dm but used as mnist. Change to use mnist_dm.,0
6420,Update mypy version (#15161),0.58660364,Set version as today (#13906),,0
6421,[FEAT] DDP: Create DDPLauncher (#4515),0.7442278,DDP2 implementation (inspired by parlai and @stephenroller).,  added a map dict   added a map dict ,1
6422,Avoid redundant callback restore warning while tuning (#13026),1.0000002,Avoid redundant callback restore warning while tuning (#13026),  fix missing return statement. Do not normalize remote paths   Update pytorch_lightning/utilities/cloud_io.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   Add some documentation that we now support s3 and hdfs paths   suggestion from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,1
6423,"Revert ""Do not modify MANIFEST.in on install (#15549)"" (#15644)",0.64015263,Avoid using the deprecated LooseVersion (#16162),,0
6424,updated parser help,0.6628381,parser = Trainer.add_argparse_args(parser),  fix root_seed in reset_seed   seed value ,0
6425,Fix misuse of transforms in docs (#3546),0.5819184,Docs improvements,  Minor language fixes   Typo fix ,0
6426,Ddp2 fix (#448),0.7443795,DDP(2) backend (#2796)," Update lr_logger.py  when logging learning_rate, we should provide different choices to log including 'step' and 'epoch'  Update lr_logger.py  add some type annotations and docstrings  Update lr_logger.py  fixed a bug where on_train_batch_start() can't be triggered, instead, we should use on_batch_start(); add interval args so that we can record learning_rates with respect to global_step or current_epoch.  Update lr_logger.py  restore _extract_lr()   suggestion   Update lr_logger.py   modify _extract_lr(), it no more need to pass interval parameter.  Update test_lr_logger.py  SkafteNicki 's suggetion   log_interval now supports None, step, epoch   change log_interval to logging_interval   Update test_lr_logger.py   Update lr_logger.py   put types check into on_train_start()   cleanup   docstring typos   minor changes from suggestions   Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai Co-authored-by: rohitgr7 rohitgr1998@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com",1
6427,docs: update pytorch_lightning imports (#16864),0.7931456,from pytorch_lightning import LightningModule,  missing   miss ,1
6428,Remove meta device utilities in favor of torchdistx (#13868),0.7092473,Removed dependency on torchvision (#797),,1
6429,[bugfix] Add support for omegaconf and tpu (#6741),0.5808943,Updated logic for checking TPUs availability (#6767),,0
6430,add CLI test for examples (#2285),0.6270364,- Improved Trainer CLI arguments handling (generalization),"  Add initial tracking of states in Trainer.   Add INTERRUPTED state, improve tests, move state switching from callback to a trainer.   Move part of a trainer state switching to a decorator.   Add documentation.   Fix docs, rename state enum, restore state to previous on exit if None, add tests for decorator only.   Fix callback typing.   Co-authored-by: William Falcon waf2107@columbia.edu",0
6431,Add required for positional arguments in argparse logic (#12504),0.73633206,- Make positional arguments required for classes passed into the `add_argparse_args` function. ([#12504](https://github.com/Lightning-AI/lightning/pull/12504)),,1
6432,Add functional metric docs [skip ci] (#3946),0.6956816,"Add deprecated metric utility functions back to functional (#5067, #5068)",,0
6433,Deprecate pl/utilities/xla_device (#14514),0.7171786,xla_device_utils >> xla_device,commit 29fb0506cd38a15c359e369cc8bc4435916b0c78 Author: Brendan Fahy bmfahy@gmail.com Date:   Sat Aug 8 19:35:30 2020 +0000 fix checking for version for docs to build  commit 467fd640db02275972c7111af031c86bb59333e9 Author: Brendan Fahy bmfahy@gmail.com Date:   Sat Aug 8 18:56:05 2020 +0000 remove no local test  commit a7cc9f88de00feec1a5406874d05313c42bd004c Author: Brendan Fahy bmfahy@gmail.com Date:   Sat Aug 8 18:46:44 2020 +0000 fix  commit 3fdbb729da79ae9348c83410a138666bad467951 Author: Brendan Fahy bmfahy@gmail.com Date:   Sat Aug 8 18:23:30 2020 +0000 revert requirements  commit 9b8686bd83e2bc243cf329e26f1c667c6949cf67 Author: Brendan Fahy bmfahy@gmail.com Date:   Sat Aug 8 18:16:42 2020 +0000 make it a fixture  commit eec74953d24c8b25268d3b6dde3cc4affdd5cb8f Author: Brendan Fahy bmfahy@gmail.com Date:   Sat Aug 8 18:01:32 2020 +0000 fix up the testing  commit 896d94a0e60083d52c81db2a036b7f1e015cad11 Author: Brendan Fahy bmfahy@gmail.com Date:   Sat Aug 8 17:47:28 2020 +0000 fix some tests  commit 6d22bde19767bf2b71dfd44839b01efdf6888f83 Merge: 6175d4e2 6ebe0d72 Author: Brendan Fahy bmfahy@gmail.com Date:   Sat Aug 8 10:20:47 2020 +0000 Merge remote-tracking branch 'origin/master' into tb_use_gfile  commit 6175d4e26b15a43c412c26d501762cd0b570616a Author: Brendan Fahy bmfahy@gmail.com Date:   Fri Aug 7 10:16:36 2020 +0000 Use tensorboard.compat.gfile to support remote writing,1
6434,Fix version comparison for python version in pl.utilities.imports (#12754),0.6250103,Compatibility for Python 3.10,  fix reduction docstring   Update docstring and some cleanup   miss   suggestion from code review   Co-authored-by: Ananya Harsh Jha ananya@pytorchlightning.ai Co-authored-by: Ananya Harsh Jha ananya@pytorchlightning.ai,0
6435,remove deprecated API for v0.8 (#2073),0.9390391,Removed deprecated API (#2073),"  fix imagenet example: lr_scheduler, loader workers, batch size when ddp   Fix evaluation for imagenet example   add imagenet example test   cleanup   gpu   add imagenet example evluation test   fix test output   test is fixed in master, remove unecessary hack   CHANGE   Apply suggestions from code review   image net example   update imagenet example   update example   pep   imports   type hint   docs   obsolete arg   [wip] fix imagenet example: lr_scheduler, loader workers, batch size when ddp (#2432)   fix imagenet example: lr_scheduler, loader workers, batch size when ddp   Fix evaluation for imagenet example   add imagenet example test   cleanup   gpu   add imagenet example evluation test   fix test output   test is fixed in master, remove unecessary hack   CHANGE   Apply suggestions from code review   Co-authored-by: Jirka jirka@pytorchlightning.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   update chlog   add missing chlog   pep   pep   Co-authored-by: Ruotian Luo rluo@ttic.edu Co-authored-by: Jirka jirka@pytorchlightning.ai",1
6436,Fix mypy errors attributed to pytorch_lightning.trainer.connectors.callback_connector.py (#13750),0.762308,Removed pytorch_lightning.trainer.connectors.OptimizerConnector (#10120),  pl 0.9 update   pl 0.9 update   pl 0.9 update   pl 0.9 update   pl 0.9 update   pl 0.9 update   pl 0.9 update   pl 0.9 update   pl 0.9 update   pl 0.9 update   pl 0.9 update   pl 0.9 update   pl 0.9 update   pl 0.9 update   pl 0.9 update   pl 0.9 update   pl 0.9 update   pl 0.9 update   pl 0.9 update   pl 0.9 update   pl 0.9 update ,1
6437,Don't register signal in thread (#10610),0.55615115,Making threadpool non-default from LightningCloud client  (#14757),Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,0
6438,Update Fabric CPU tests to work on GPU machines (#17391),0.7564287,GPU training (#2704),  docs clarify what gpus=0 means   add example suggested by @ydcjeff ,1
6439,Remove deprecated test_tube dependency (#14513),0.79555273,Deprecated the TestTubeLogger (#9065)," Fix docstring  ""mean absolute loss"" rather than ""root mean absolute loss""  minor docstring fix  Co-authored-by: rohitgr7 rohitgr1998@gmail.com",1
6440,supports --num-nodes on DDPSequentialPlugin() (#5327),0.8024346,Deprecated num_nodes and sync_batchnorm arguments in DDPPlugin and DDPSpawnPlugin (#7026),"  save_last should be last   changelog   seed, docs   retrigger ci   compare filenames   move constants   fix test   epoch, global step   improve test ",1
6441,Remove test's proxy boring classes import (#13297),0.6028749,Did not interfere with a default sampler (#1318),  Faster classfication stats   Faster accuracy metric   minor change on cls metric   Add out-of-bound class clamping   Add more tests and minor fixes   Resolve code style warning   Update for #2781   hotfix   Update pytorch_lightning/metrics/functional/classification.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   Update about conversation   Add docstring on stat_scores_multiple_classes   Fixing #2862   Co-authored-by: Younghun Roh yhunroh@mindslab.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
6442,"Mocking Loggers (part 4a, mlflow) (#3884)",0.7611642,"Mocking loggers (#3596, #3617, #3851, #3859, #3884, #3853, #3910, #3889, #3926)",  wip   setup   type   name   wip   docs   imports   fix if   fix if   use_amp   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Apply suggestions from code review  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   fix tests   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   fix tests   todos   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
6443,simplify examples structure (#1247),0.8174389,Simplify the PL examples structure (shallower and more readable) (#1247),  Fix Trainer arg name in docs   Fix a PR comment ,1
6444,Refactor launching tests to use our launchers (#14954),0.6389865,Deprecated the TestTubeLogger (#9065),,0
6445,Changed basic_examples to use LightningCLI (#6862),0.77138114,LightningCLI changes:,  Update index.rst   add thumbanil   update readme   Update README.md   Update index.rst   image   Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,1
6446,ref: added hook base method (#3127),0.6792604,Updated hooks arguments - breaking for setup and teardown (#2850),,0
6447,Update CONTRIBUTING.md (#2855),0.5720518,Update the Lightning App docs (#13537),  fix setup install   fix setup install   :pencil: edit docs install command   Co-authored-by: nateraw nxr9266@g.rit.edu Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,0
6448,Add support for optimizers and learning rate schedulers to LightningCLI (#8093),0.7910556,"LightningCLI now supports registries for callbacks, optimizers, learning rate schedulers, LightningModules and LightningDataModules. This greatly improves the command line experience as only the class names and arguments are required as follows:",  clean imports   miss ,1
6449,Fix converting only float type tensors in Lite (#10429),0.6199225,Switch from tensorboard to tensorboardx in TensorBoardLogger (#15728),  simplify   tmpdir   revert   clean   accel   types   test   edit test acc   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update test acc  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
6450,Support **DictConfig hparam serialization (#2519),0.9928938,Support **DictConfig for hparam serialization (#2519),,1
6451,join coverage (#2460),0.531133,[1.1.6] - 2021-01-26,,0
6452,Fix-multiple-loggers-typo (#16305),0.5538337,- Fixed logging to loggers with multiple eval dataloaders ([#12454](https://github.com/PyTorchLightning/pytorch-lightning/pull/12454)),  fix https://github.com/PyTorchLightning/pytorch-lightning/issues/2622   Update training_loop.py ,0
6453,Fix progress bar print error when called before training (#7674),0.68019897, - Progress bar: `Trainer(enable_progress_bar=True)`,  modified hooks   modified hooks   modified hooks   modified hooks   modified hooks   modified hooks   modified hooks   modified hooks   modified hooks ,0
6454,Adding clarifying documentation on the usage of second_order_closure (#3551),0.5316702,[1.2.2] - 2021-03-02," Add support to Tensorboard logger for OmegaConf hparams  Address https://github.com/PyTorchLightning/pytorch-lightning/issues/2844 We check if we can import omegaconf, and if the hparams are omegaconf instances. if so, we use OmegaConf.merge to preserve the typing, such that saving hparams to yaml actually triggers the OmegaConf branch   avalaible   chlog   test   Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai",0
6455,enable None model checkpoint default (#3669),0.9997548,Enable None model checkpoint default (#3669),,1
6456,feat: customize gradio components with lightning colors (#17054),0.5779409,Updated LightningTemplateModel to look more like Colab example (#1577),,0
6457,update CHANGELOG.md (#5482),0.7194939,Complete changelog,  Support limit_mode_batches(int) for infinite dataloader   flake8   revert and update   add and update tests   pep8   chlog   Update CHANGELOG.md   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   Add suggestions by @awaelchli   docs   Apply suggestions from code review   Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk   Apply suggestions from code review   fix   max   check   add and update tests   max   check   check   check   chlog   tests   update exception message   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai,1
6458,Fixed PYTHONPATH for ddp test model (#4528),0.7112574,"Implemented {DDPShardedPlugin,DDPShardedSpawnPlugin}._setup_model_and_optimizers (#10028, #10064)", Update CONTRIBUTING.md  Added docker option to testing section.  Apply suggestions from code review  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
6459,Fix codes in 'Lightning in 2 steps' docs (#4894),0.6302464,Improved the error message when the LightningWork is missing the run method (#14759),  Use .comet.config file or env var for API key.   Make CometLogger API key changes backwards compatible.   Fix line too long.   Add documentation about loading from ~/.comet_config.   Update required comet_ml version.   Comet logger: allow offline experiments with config file.   This adds a new argument to the logger to control the online / offline mode explicitly so that if you give an API key and a save_dir (e.g. to control where checkpoints go while having ~/.comet.config) you can specify which mode you want.   Make CometLogger API key changes backwards compatible.   Comet logger: change online argument to be offline.   For consistency with other loggers.  chlog  Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai,0
6460,[App] Hot fix. Resolve platform CI (#16699),0.66587484,This release fixes that core issue,,0
6461,device property (#1791),0.6849791,"            device_ids=device_ids,",  Override the default gather method to support scalars   add computing average of a list   bug: change if to elif   add some tests   change style   change documentation   use apply_to_collection in DP gather   use apply_to_collection in DP gather   fix warning msg   override gather method in DP   add tests for python scalars   add python scalars to docstring   Update message   override gather method in DP   formatting   chlog   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai,0
6462,Merge pull request #7990 from PyTorchLightning/refactor/loops/loops_everywhere_eval,0.62642425,    * Add an utility to collect the states across processes ([#10639](https://github.com/PyTorchLightning/pytorch-lightning/pull/10639)),  add install matrix   nb tests   win   cfg   torch   link   Update .github/workflows/install-pkg.yml   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com   try   try   try   try   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,0
6463,CI: resolving Docs (#15508),0.6474832,Docs,  fix hparams lr finder bug   add tests for new functions   better tests   fix codefactor   fix styling   fix tests   fix codefactor   Apply suggestions from code review   modified hook   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,0
6464,change cluster creation log message (#14672),0.72330886,Made cluster creation/deletion async by default (#16185),  update contributing.md   Update CONTRIBUTING.md   Update CONTRIBUTING.md   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com   Update .github/CONTRIBUTING.md   suggestion from code review   minor changes   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
6465,testing new pretrain order,0.51891446,test_end >> test_epoch_end,,0
6466,fixed epoch continuation from checkpoint,0.65459687,Reset current progress counters when restarting an epoch loop that had already finished (#9371),,0
6467,Prepare for ShardedTensor deprecation (#16892),0.56320894,# Create model here and wrap the large layers for sharding,  :art: use package extras   :art: get extras from reqs   :art: .   :pencil: docs   :art: . ,0
6468,Use correct python version in lightning component template (#13790),0.99999994,Use correct python version in lightning component template (#13790),  add test for none checkpoint in ddp_spawn   fix code style   make sure checkpoint_callback is none   Fix tests   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,1
6469,updated test models with lazy decorators,0.55270374,- Full tests that run multiple models in different configs,  Added basic file logger #1803   fixup! Added basic file logger #1803   fixup! Added basic file logger #1803   fixup! Added basic file logger #1803   fixup! Added basic file logger #1803   fixup! Added basic file logger #1803   csv   Apply suggestions from code review   tests   tests   tests   miss   docs   Co-authored-by: xmotli02 xmotli02@users.noreply.github.com Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
6470,[Feat] Improve TPU CI (#6078),0.7438059,Enabled manual optimization for TPUs (#8458),  Faster classfication stats   Faster accuracy metric   minor change on cls metric   Add out-of-bound class clamping   Add more tests and minor fixes   Resolve code style warning   Update for #2781   hotfix   Update pytorch_lightning/metrics/functional/classification.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   Update about conversation   Add docstring on stat_scores_multiple_classes   Co-authored-by: Younghun Roh yhunroh@mindslab.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
6471,fix CONTRIBUTING link and silence checkpoint callback message (#325),0.67494714,Avoid redundant callback restore warning while tuning (#13026),,0
6472,"Update arrow requirement from <=1.2.2,>=1.2.0 to >=1.2.0,<1.2.4 in /requirements (#14770)",0.5168973,Improved exception message if rich version is less than 10.2.2 (#10839),  add ddp sync for logging in result step   pep8   pep8   make ddp tests run also on cpu (except windowws)   create class instance in ddp test   revert automated formatting   pep8 ,0
6473,Docs update (#13959),0.722384,Docs improvements,  modified hook   modified hook   modified hook   modified hook   modified hook   modified hook   modified hook   modified hook   modified hook   modified hook   modified hook   modified hook   modified hook ,1
6474,Call Strategy.process_dataloader in data_connector.py (#12251),0.78594077,- Move `Strategy.process_dataloader` function call from `fit/evaluation/predict_loop.py` to `data_connector.py` ([#12251](https://github.com/PyTorchLightning/pytorch-lightning/pull/12251)),  updated sync bn   updated sync bn   updated sync bn   updated sync bn   updated sync bn   updated sync bn   updated sync bn   updated sync bn   added ddp_spawn test   updated test   clean   clean   Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai,1
6475,added coverage file,0.48621285,New features,,0
6476,Proper check for availability of bagua (#15220),0.6587558,The Bagua Strategy," Revert ""Support limit_mode_batches (int) for infinite dataloader (#2787)""  This reverts commit de9c9f0864418a83f295e4c87be50e12645bd83a.  Update training_tricks.py",0
6477,generalize closure api in Lightning (#8642),0.6572874,"Deprecated LightningLoggerBase.close, LoggerCollection.close in favor of LightningLoggerBase.finalize, LoggerCollection.finalize (#9422)",  circleci config   Apply suggestions from code review   miss ,0
6478,Introduce Lightning App (#13303),0.8875399,Lightning App,  title tweak   remove changes in new-project   Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,1
6479,fix porgressbar postfix order (#1874),0.51427126,Refactored EpochResultStore (#5522),,0
6480,Fixed num_sanity_val_steps affecting reproducibility of training data shuffling (#7014),0.65989685,Refactored result handling in training loop (#7506),,0
6481,multiple optimizer restart with fault-tolerant training (#9537),0.729993,Disabled optimizers setup during testing (#3059),  make ssim fast   remove padding   pep8   add comments for readability   plus -> coef ,1
6482,ref: moving train loop to own object (intermediate steps) (#3312),0.86194026,"train loop refactor - moving train loop to own object (#3310, #3312, #3313, #3314)",,1
6483,Load fix (#74),0.60727805,- Fixed a bug where the upload files endpoint would raise an error when running locally ([#14924](https://github.com/Lightning-AI/lightning/pull/14924)),  initial commit for sync_bn   updated changelog   tests   tests   ddp tests hanging with script tests   updated trainer   updated params   test   passingtests   passing tests   passing tests   passing tests   tests   removed apex   doc   doc   doc   doc   docs   tests   tests   tests ,0
6484,Update LightningCLI test for new support in latest release of jsonargparse (#13805),0.79478836,- Changes in `LightningCLI` required for the new major release of jsonargparse v4.0.0 ([#10426](https://github.com/PyTorchLightning/pytorch-lightning/pull/10426)),  Support limit_mode_batches(int) for infinite dataloader   flake8   revert and update   add and update tests   pep8   chlog   Update CHANGELOG.md   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   Add suggestions by @awaelchli   docs   Apply suggestions from code review   Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk   Apply suggestions from code review   fix   max   check   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai,1
6485,Update logging.py (#602),0.7398895,Deprecated pytorch_lightning.logging (#767),  added docs   Update docs/source/multi_gpu.rst   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  testcode change to example  Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,1
6486,docs: fix return type description of Trainer.validate/test (#8522),0.75576466,Trainer is now raising a MisconfigurationException instead of a warning if Trainer.{validate/test} is missing required methods (#10016),check checkpoint_callback before setting best_model_path,1
6487,Add magicleap/atlas to community examples (#2937),0.5060179,    * Added `CollaborativeStrategy` ([#12842](https://github.com/Lightning-AI/lightning/pull/12842)),"  added balanced accuracy   added dcg score   added mean absolute error   added mean squared error   fix   added mean squared log error   add median absolute error and r2 score   switch arguments   added mean poisson deviance   add mean gamma deviance and mean tweedie deviance   fix styling   added explained variance score   added cohen kappa score   added hamming, hinge, jaccard   fix styling   update sklearn requirement to newer version   update requirement   fix doctest   fix tests   added balanced accuracy   added dcg score   added mean absolute error   added mean squared error   fix   added mean squared log error   add median absolute error and r2 score   switch arguments   added mean poisson deviance   add mean gamma deviance and mean tweedie deviance   fix styling   added explained variance score   added cohen kappa score   added hamming, hinge, jaccard   fix styling   update sklearn requirement to newer version   fix doctest   fix tests   fix doctest   fix failing docs   fix test   trying to fix errors   Apply suggestions from code review   format   Co-authored-by: Nicki Skafte nugginea@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai",0
6488,Add auto wrapping support for DDPFullyShardedStrategy (#14383),0.82035434,- Added support for auto wrapping for `DDPFullyShardedStrategy` ([#14383](https://github.com/Lightning-AI/lightning/pull/14383)),  Update converters.py   Update test_converters.py   pep8   pep8 tests   Update test_datamodules.py   Update test_converters.py   Update converters.py   Update test_datamodules.py   Update test_converters.py   Update test_converters.py   fix tests   fix ddp tests on windows   chlog   Co-authored-by: Jirka Borovec jirka@pytorchlightning.ai,1
6489,[App] Enable state broadcast with MultiNode (#15607),0.9114249,Enabled MultiNode Components to support state broadcasting (#15607),,1
6490,Add strict option to lr_scheduler dict (#3586),0.71858037,Disabled lr_scheduler.step() in manual optimization  (#6825),,1
6491,update changelog after 1.5.10 release (#11830),0.71018827,Full Changelog: https://github.com/Lightning-AI/lightning/compare/1.8.0...1.8.0.post1,,1
6492,latest restore func metrics (#3949),0.5980904,"Add deprecated metric utility functions back to functional (#5067, #5068)",  docs update and follow up of #2789   pep8   Update trainer.py   Update trainer.py   Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,0
6493,Avoid passing unnecessary params from TPUSpawn to DDPSpawn (#8192),0.6179059,Merged the implementation of DDPSpawnStrategy into DDPStrategy and removed DDPSpawnStrategy (#14952),  More clear docstring for val_check_interval   Update trainer.py ,0
6494,Fix all_gather for tpu_cores=8 (#6587),0.60530114,Accelerator all_gather supports collection (#5221),  Fix docs typo   Fix docs typo ,0
6495,Fix LightningCLI docs after overhaul of the documentation (#14976),0.78196347,LightningCLI improvements,,1
6496,Fix Configuring Learning Rate Schedulers (#1177),0.8466392,Simplified logic for updating the learning rate for schedulers (#7682),,1
6497,re-trigger build (#2988),0.6540841,refactored dataloader process hook (#3139),  update gpu PT 1.6   fix docker   use PT 1.5   Update tests/install_AMP.sh   Co-authored-by: Nathan Raw nxr9266@g.rit.edu Co-authored-by: Nathan Raw nxr9266@g.rit.edu,0
6498,Drop FairScale sharded parity tests (#16069),0.63791156,FairScale deprecation (in favor of PyTorch's FSDP implementation) (#16353), ddp refactor,0
6499,Improving Hydra+DDP support (#11617),0.7804528,better support for Hydra,  missing   miss   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   miss   note   notes   update CI testing with pip upgrade (#2380)   try pt1.5   cpu   upgrade   tpu   user   [blocked by #2380] freeze GPU PT 1.4 (#2780)   freeze   user   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
6500,Remove deprecated torch_distributed_backend logic (#14693),0.65577346,Setting the torch-distributed backend,,0
6501,release v0.4.1,0.8209121,"    version=""0.0.1"",", Fix num_classes warning  Put to_categorical before get_num_classes in metrics/functional/classification.py  Update classification.py  Remove whitespaces in blank line.,1
6502,"[LAI] Rename instances of ""lightning_app"" and ""pytorch_lightning"" (#15208)",0.7048873,Renamed lightning.app.components.LiteMultiNode to lightning.app.components.FabricMultiNode (#16505),  Fix shuffle for distributed sampler   add test   test   chlog   update test   update test   update test   assertions via callback   define callback outside for pickling   skip ddp test on windows   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
6503,DummyLogger can be called with unknown methods (#13224),0.59554267,- Added support for calling unknown methods with `DummyLogger` ([#13224](https://github.com/Lightning-AI/lightning/pull/13224),  fix https://github.com/PyTorchLightning/pytorch-lightning/issues/2407   Update pytorch_lightning/trainer/distrib_data_parallel.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
6504,Connect the model to the training type plugin at the start of run (#8536),0.77034914,Automatically set sync_batchnorm for training_type_plugin (#6536),  added logging docs   added logging docs   added logging docs   added logging docs ,1
6505,Fix tuner.scale_batch_size not finding batch size attribute when using datamodule (#5968),0.76667017,tuner.scale_batch_size(...),  added logging docs   added logging docs   added logging docs   added logging docs   added logging docs   added logging docs   added logging docs   added logging docs   added logging docs   added logging docs ,1
6506,Hydra changes to lightning-lite (#14950),0.7011813,LightningLite:,,1
6507,3/n Simplify spawn plugins: Merge pre_dispatch and setup logic  (#11137),0.7046162,"    * The hooks/callbacks `prepare_data`, `setup`, `configure_sharded_model` and `teardown` now run under initialized process group for spawn-based plugins just like their non-spawn counterparts",  :sparkles: call dm hooks in trainer implicitly   :white_check_mark: update tests   :pencil: remove unused stage arg from dm docs   :white_check_mark: update tests   :white_check_mark: update tests   :construction: include stage in datamodule.setup   :pencil: docs   :pencil: docs   added more dm tests   added more dm tests   :bug: call dm.setup everywhere   :fire: pickle tests now implied by accelerator tests   :art: set dm as attr of trainer   :bug: .   :construction: wip   add can prepare test   add can prepare test   verified setup in fit   fixed setup call   fixed setup call   fixed setup call   Co-authored-by: William Falcon waf2107@columbia.edu,1
6508,Fix 1.0.0 changelog (#4180),0.7411196,Complete changelog,,1
6509,save_dir fix for MLflowLogger + save_dir tests for others (#2502),0.62936413,- Allow logging to an existing run ID in MLflow with `MLFlowLogger` ([#12290](https://github.com/PyTorchLightning/pytorch-lightning/pull/12290)),  try pt1.5   cpu   upgrade   tpu   user   [blocked by #2380] freeze GPU PT 1.4 (#2780)   freeze   user ,0
6510,Increase parity to match logging refactor (#4651),0.7269834,Refactored logging,  exist images   names   images   args   pt 1.6 dev   circleci   update   refactor   build   fix   MKL ,1
6511,Clean save (#2933),0.70179456,clean up data reset (#3161),  Test using torchtext.data.Field with include_lengths=True/False   Fix issue that Tensors in a Batch generated by torchtext with torchtext.data.Field configured as include_lengths=True   Add description for fix of issue #2688   changes to accomodate CodeFactor issues   Another attemt to make last CodeFactor issue pass (it's a false alarm)   temporarly disable test of test_grad_tracking to check if testing will pass   reenable test in test_grad_norm   Update CHANGELOG.md   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   Renamed get_torchtext_data_iterator to _get_torchtext_data_iterator as suggested by @borda   Update pytorch_lightning/utilities/apply_func.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   adding tests more specific to batch_move_data_to_device with tochtext Batch   added check that Tensors were moved to target device   removed tests using RNN models to be moved into a separate PR   fixing FLAKE8 errors that showed up after merge from master branch     modified:   tests/base/datamodules.py     modified:   tests/callbacks/test_model_checkpoint.py   parameterized test to reduce code duplication   Added check only if length tensor exist. Removed left over comments.   rearranged device parameterization and added pytest.param   Try to figure out why only one device is tested on Linux machines   Testing on CPU and GPU devices (GPU test is skip if no cuda device is available.   added test for TPU device (experimental)   Adding test parameterization for TPU test (experimental)   change import statement to limit what is imported for a TPU environment   made test work with TPU   Change to trigger CI   Change to trigger CI   uncommented TPU test to check CI   reenabling TPU test   small change to trigger CI build   small change to trigger CI build   small change to trigger CI build   adding tests/utilities/test_apply_func_torchtext.py to CI TPU test   try to make test not skipped on CI with TPU   remove testing on TPU   undo an accidental change to test_tpu.py (file should not have been touched)   small change to trigger CI build   small change to trigger CI build   Update tests/utilities/test_apply_func_torchtext.py   Revert to previous version   Apply suggestions from code review   Change to trigger CI   Co-authored-by: Thomas Schaaf tschaaf@mmm.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Thomas Schaaf tschaaf@cs.cmu.edu,1
6512,Document access to Fabric attributes inside LightningModule (#17440),0.68275785,"Renamed the class LightningLite to Fabric (#15932, #15938)",  re-enable skipped   timeout ,0
6513,[test] Add checks for gpus=1 (#7105),0.5719328,    # x is 1/nb_gpus of the full batch,,0
6514,mark todo exceptions (#5320),0.6657202,except Exception:,  conda speedup   cache   add pip cache   suggestion   cache   cache   req ,0
6515,Lightning 1.8.0 release (#15435),0.80266106,Lightning 2.0 is the official release for Lightning Fabric :tada:,  export model to onnx   prepare data before exporting   support for dataloaders and tensors   added tests   use example_input_array add to changelog   updated docstring   added onnx inference tests   temp commit   removed schema valid test   add onnxruntime to environment.yml   moved onnxruntime to environment.yml pip   add example in doc   add lines between code block   added PR to changelog   is file check   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  remove *  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   infer example outputs   added doctest for onnx   fix windows tests   moved eval within condition block   self.forward to self   added docs   fixed docs error   added to toctree   Update CHANGELOG.md   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
6516,Remove deadlock detection / process reconciliation logic (#16204),0.7686224,- Removed deadlock detection / process reconciliation (`PL_RECONCILE_PROCESS=1`) ([#16204](https://github.com/Lightning-AI/lightning/pull/16204)),  pt 1.6   don't use the new zipfile serialization for now   quick flake8 fixes   remove unnecessary f   coalesce strings   remove comma   remove extra commas   Apply suggestions from code review   Co-authored-by: Peter Yu 2057325+yukw777@users.noreply.github.com   set _use_new_zipfile_serialization to False only for pytorch 1.6.0   remove unnecessary comments   flake8 fixes   use pkg_resources instead of packaging   readme   format   version   chlog   Co-authored-by: Peter Yu peter@asapp.com Co-authored-by: Peter Yu 2057325+yukw777@users.noreply.github.com,1
6517,Trainer only references accelerator (#6039),0.9999999,Trainer only references accelerator (#6039),,1
6518,Add a trainer.ckpt_path setter for stateful loading (#16187),0.7631772,"Deprecated Trainer attribute ckpt_path, which will now be set by weights_save_path (#2681)",  remove deprecated in v0.9   data_loader   import   hook   args ,1
6519,refactor CheckpointConnector.restore_weights  (#7862),0.7906523,Refactor load in checkpoint connector (#4593), Fix typo on tpu.rst  There're 3 ways :)  Update docs/source/tpu.rst  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,1
6520,Update help steps (#5564),0.58551425,The accelerator and training type plugin update_global_step hook has been removed (#8856),"The speed up is achieved by: - Moving the ""where"" out of the loop (and replacing with min for simplicity). - Replacing manual sum and pow with torch.norm. Even though this results   in unnessecary computation (computing pow(root)) this is still a lot   faster. - Preallocating the output gives a slight speed up. Note that calling .to for all parameters results in a small speed penalty (~4 ms in my case) but allows parameters on different devices. Overall this reduces the time used for gradient clipping from 206ms to 74 ms for my model (Resnet50 + few additional vars, all vars on GPU).",0
6521,Fix deploy production intermediate doc (#13204),0.597098,Porting fixes to autoscaler component (#16249), Misleading exception raised during batch scaling  Use batch_size from model.hparams.batch_size instead of model.batch_size   Improvements considering #1896   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
6522,Remove the deprecated pl.utilities.cli module (#16116),0.7326175,Removed the deprecated pytorch_lightning.utilities.cli module in favor of pytorch_lightning.cli (#16116),,1
6523,add option to log momentum (#4384),0.60807097,moved eval loop logging to loggers (#3408),,0
6524,[feat] Add BasePredictionWriter 3/3 (#7127),0.63224155,"Base classes (#1326, #1877)",  Add missing methods to logger collection   Update CHANGELOG.md   Fix errors after merge   Fix codefactor issues   Update CHANGELOG.md   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
6525,Update README.md (#3986),0.5223268,refactored dataloader process hook (#3139),,0
6526,De-duplicate DistributedSampler mentions (#7636),0.6673016,Sampler replacement in distributed strategies (#16829),,0
6527,Bump pypa/gh-action-pypi-publish from 1.5.0 to 1.5.1 (#13954),0.6935049,PyTorch 1.5  support,,0
6528,Nightly PyTorch version is now 2.0 (#16017),0.76304984,PyTorch 1.5  support,,1
6529,remove trainer hidden state | sanity refactor [2 / n] (#7507),0.7082282,Removed deprecated property Trainer.running_sanity_check in favor of Trainer.sanity_checking (#9209),"  when hydra is enabled, set the cwd of subprocesses to the original cwd for ddp   move imports up   clean up imports ",1
6530,Update Strategy doc (#12950),0.649183,Renamed Strategy.reduce to Strategy.all_reduce in all strategies (#16370),  truncate version number   add docs and example   extend docs   docs   docs   changelog   show last   Update pytorch_lightning/core/lightning.py   Update pytorch_lightning/core/lightning.py   Co-authored-by: William Falcon waf2107@columbia.edu,0
6531,advanced profiler describe + cleaned up tests (#837),0.59762704,"Refactored training_batch + tests to verify correctness (#2327, #2328)",,0
6532,Prune metrics: regression 8/n (#6636),0.7208955,Regression metrics (#2221)," make the error message readable  make the error message readable by adding spaces, fixing a type ""his -> this"",   cleanup   Update pytorch_lightning/trainer/auto_mix_precision.py   Apply suggestions from code review   Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk",1
6533,Replace docs gifs with videos snippets so user can play at own speed (#2966),0.5092097,Refactor GPUStatsMonitor to improve training speed (#3257),  freeze pt 1.5   torchtext   Apply suggestions from code review   Co-authored-by: Peter Yu 2057325+yukw777@users.noreply.github.com  timeout  Co-authored-by: Peter Yu 2057325+yukw777@users.noreply.github.com,0
6534,Docs & Changelog (#2176),0.74494326,Full Changelog,,1
6535,Refactor Strategy._move_optimizer_states as utility functions (#11758),0.70057774,Refactored optimizer (#4658),,1
6536,fix appveyor - install pytorch,0.70354164,PyTorch 1.5  support,,1
6537,docs/fix_CONTRIBUTING.md (#1984),0.6477265,Docs improvements,,0
6538,testing hpc save load,0.5520778,Used checkpoint_connector.hpc_save in SLURM (#4217),,0
6539,Fix mypy errors attributed to pytorch_lightning.core.mixins.device_dtype_mixin (#13704),0.7122164,- Removed deprecated `pytorch_lightning.utilities.device_dtype_mixin.DeviceDtypeModuleMixin` in favor of `pytorch_lightning.core.mixins.device_dtype_mixin.DeviceDtypeModuleMixin` ([#10442](https://github.com/PyTorchLightning/pytorch-lightning/pull/10442)),  tests: add default_root_dir=tmpdir   remove duplicate tmpdir args   add missing fixture   test requires multi gpu   typo   resize   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
6540,Cleanup the is_distributed property [TPU] (#17381),0.57282674,Ensured process_dataloader is called when tpu_cores > 1 to use Parallel DataLoader (#6015),  circleci config   circleci config   circleci config   circleci config ,0
6541,Docs for auto_select_gpu (#2836),0.70491636,Support auto_select_gpus with the accelerator and devices API (#12608),  added tests   added tests   added tests   added tests   added tests   added tests   added tests   added tests   added tests   added tests   added tests   added tests   added tests   added tests   added tests   added tests   added tests   added tests   added tests   added tests   added tests   added tests   added tests   added tests   added tests   added tests   added tests   added tests ,1
6542,Make the is_picklable function more robust (#17270),0.9999998,Make the is_picklable function more robust (#17270),  init   rename   tpu_core_idx   idx 8   idxs   @pl_multi_process_test   assert   assert   deamon   no close   imort   msg   use_single_gpu   dataset   idx   fix idx   dataset   format   add pickable   typo   apex   typo   wip   wip   wip   wip   wip   wip   wip   wip   docs   typo   tests   tests   tests   tests   tests   tests   tests   tests   tests   tests   tests   tests   tests   tests   tests   tests   tests   docs   docs   Apply suggestions from code review   Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk Co-authored-by: Rohit Gupta rohitgr1998@gmail.com  Apply suggestions from code review  Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk   docs   Apply suggestions from code review   Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk Co-authored-by: Rohit Gupta rohitgr1998@gmail.com,1
6543,Fix save_hyperparameters for multiple inheritance and mixins (#16369),0.63236237,Moved save_hyperparameters to its own function (#7119),"  Fix fast_dev_run to run for all val_dataloaders   fast_dev_run check   changelog   explicit   limit_batches with fast_dev_run in init   add test   whitespace and comment fix   comment and assertion   added tests   Fix fast_dev_run to run for all val_dataloaders   fast_dev_run check   changelog   explicit   limit_batches with fast_dev_run in init   add test   whitespace and comment fix   comment and assertion   added tests   added tests   added tests   added tests   update rtol   Revert ""update rtol""   This reverts commit 4320329540798c112cf45dcbd6f677993e4c6ad6.  added tests  Co-authored-by: William Falcon waf2107@columbia.edu",0
6544,Deprecate ProgressBar and rename it to TQDMProgressBar (#10134),0.89132434,Deprecated ProgressBar callback in favor of TQDMProgressBar (#10134),,1
6545,Bump JamesIves/github-pages-deploy-action from 4.1.4 to 4.4.0 (#13953),0.550275,- Checkpoint saving and loading redesign ([#16434](https://github.com/Lightning-AI/lightning/pull/16434)),  fix weights_save path and drop ckpt_path   add tests   unused import   update docs   changelog   pep8   fix horovod test   make backward compatible   perform same test for all loggers   fix for when logger=False and weights_save_path is set   update changelog   update docs   update tests   do not set save dir dynamically   remove duplicate test   remove duplicated tests   update tests   update tests   remove remaining ckpt_path references   move defaults to init as suggested by @Borda   test deprecation ,0
6546,"Remove default optimizer, add None optimizer option (#1279)",0.7977054,Changed default behaviour of configure_optimizers to use no optimizer rather than Adam. (#1279),  Add a GKE cleanup workflow to run once per hour.   Add fixes. Temp use workflow as triggered by commit so we can see that command works.   Add back in schedule. ,1
6547,Move strategy tests from accelerators to strategies directory (#11329),0.68747413,"The Accelerator and PrecisionPlugin have moved into Strategy. All strategies now take an optional parameter accelerator and precision_plugin (#11022, #10570).", updated docs,0
6548,Pick queue type only if specified (#15295),0.46478128,Allow dataloaders without sampler field present (#1907),  cpu backend   cpu backend   cpu backend ,0
6549,bump base NGC image (#13346),0.51165056,Pinning starsessions to 1.x (#14333), refactor ddp_spawn,0
6550,ci: update OS for pkg release (#17455),0.5573106,Dropped official support/testing for older PyTorch versions <1.3 (#1917),,0
6551,Fix titles capitalization in docs,0.50738525,Docs,  reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator ,0
6552,fixed metrics request not forced anymore,0.64034724,Metric compute() method will no longer automatically call reset() (#5409),  reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator ,0
6553,added slurm no process warning,0.6225766,Fix an issue with the SLURM srun detection causing permission errors (#15485),,0
6554,sets default ddp mode to spawn (#2168),0.68022704,"When using multiple devices, the strategy now defaults to ""ddp"" instead of ""ddp_spawn"" when none is set (#16388)",  reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator   reactor into gpu accelerator ,0
6555,Fix rich import error in Google Colab (#17156),0.6197515,| Import pl.core.mixins.DeviceDtypeModuleMixin                                                           | 1.10             | No longer supported |,,0
6556,increase acc (#2039),0.5715011,[0.6.3] - 2022-10-07,  :art: warn instead of error out on loaders   :bug: test misconfiguration should still fail   :construction: .   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   updated docs with new result obj   Co-authored-by: William Falcon waf2107@columbia.edu,0
6557,Added Optional sphinx docs linkcheck (#16234),0.6384475,Pinned sphinx-autodoc-typehints with <v1.15 (#11400),,0
6558,README.md typo correction (#3147),0.5290897,Changed overwrite to True (#16009),  fix setup call while testing   changelog   drop if condition   add test to check setup call   flake8   update test to check model stage   Co-authored-by: William Falcon waf2107@columbia.edu,0
6559,[Feat] Add graceful detection of signal to exit + SignalConnector and merge SlurmConnector. (#9566),0.59725964,The preemption/termination signal is now configurable (#14626):,,0
6560,Remove AcceleratorConnector.has_ipu (#12111),0.8226155,- Removed `AcceleratorConnector.has_ipu` property ([#12111](https://github.com/PyTorchLightning/pytorch-lightning/pull/12111)),,1
6561,docker: fix building PL image (#17353),0.53925,Porting fixes to autoscaler component (#16249),,0
6562,Support predict_dataset in LightningDataModule.from_datasets (#12942),0.8699311,- Added missing `predict_dataset` argument in `LightningDataModule.from_datasets` to create predict dataloaders ([#12942](https://github.com/Lightning-AI/lightning/pull/12942)),  :sparkles: Add copy of pl_bolts datamodule to lightning   :sparkles: add datamodule to necessary init files   :construction: add datamodule property to LightningModule   :construction: .   :art: Let DataModule do its own thing   :construction: add back setup and run both hooks implicitly   :construction: .   :bug: fix add_argparse_args   :lipstick: apply black formatting and isort   :pencil: docstrings   :pencil: .   :pencil: .   :bug: overwrite cls prepare_data instead of instance   :pencil: .   :white_check_mark: add some tests   Update datamodule.py   Update datamodule.py   Update datamodule.py   Co-authored-by: William Falcon waf2107@columbia.edu,1
6563,Amp2 (#1580),0.67329216,Apex mixed precision gets replaced with AMP (#16149),  remove duplicate test   remove duplicated tests ,0
6564,Fix mypy errors in pytorch_lightning/cli.py (#14653),0.7438867,+ # pytorch_lightning==1.7.0,  quick-fix for --gpus flag bug   warning added   warning added   set on_gpu using data_parallel_device_ids   self.on_gpu repositioned   Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,1
6565,Fixes #2479 (#3856),0.66289437,Resolve bug with Finetuning (#5744),  Horovod: Adjust base LR used by schedulers to match that of the optimizer after scaling by number of workers   Added unit test   Removed debug statements   Updated changelog   Apply suggestions from code review   Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
6566,Support optimizer step progress tracking with manual optimization (#11848),0.79727554,- Added support for optimizer step progress tracking with manual optimization ([#11848](https://github.com/PyTorchLightning/pytorch-lightning/pull/11848)),  metrics: add SSIM   Update CHANGELOG.md   fix codefactor issue fix doctest fix doctest fix test  added test for raise Error,1
6567,update Win CI req. (#123),0.54210764,Moved base req. to root (#4219),  add circleCI   wip   CircleCI setup that worked on my private repo. Use a working pytorch-lightning commit   Fix the orb imports   Update circleci header comment   Try to pull the GITHUB_REF from the CI_PULL_REQUEST   Use null instead of space for 'sed'   Add TODO for codecov   Remove echo of GKE_CLUSTER since it will be redacted by CircleCI.   Try running codecov upload.   Try using codecov orb   Use pip install codecov   Use codecov orb again since it should be approved   dockers/tpu-tests/Dockerfile   action   suggestions   drop suggestion   suggestion   Co-authored-by: Jirka jirka@pytorchlightning.ai,0
6568,add myself (#3999),0.5045296,        return 7  # lucky number 7,  support sanity_val_step=-1   fix list size   simplification   simplify   add test for num_sanity_val_steps=-1   update test   update docs   extend tests to multiple dataloaders   changelog   Update tests/trainer/test_trainer.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   improve test   refactor the sanity check decision   fix merge   Update trainer.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: William Falcon waf2107@columbia.edu,0
6569,add on fit_start on fit_end hooks (#2217),0.6137448,Made optimization steps for hooks (#2363), add EvalResult to support to val/test loops,0
6570,Improved model initialization API for Fabric (#17462),0.7170147,fabric.launch(),,1
6571,Remove deprecated Strategy.post_dispatch (#13461),0.70538944,- Removed the deprecated `Strategy.post_dispatch()` hook ([#13461](https://github.com/Lightning-AI/lightning/pull/13461)),"  metrics: added bleu score and test bleu   metrics: fixed type hints in bleu   bleu score moved to metrics/functional/nlp.py   refactor with torch.Tensor   Update test_sequence.py   refactor as Borda requests and nltk==3.2   locked nltk==3.3   nltk>=3.3, parametrized smooth argument for test   fix bleu_score example   added class BLEUScore metrics and test   added class BLEUScore metrics and test   update CHANGELOG   refactor with torchtext   torchtext changed to optional import   fix E501 line too long   add else: in optional import   remove pragma: no-cover   constants changed to CAPITALS   remove class in tests   List -> Sequence, conda -> pip, cast with tensor   add torchtext in test.txt   remove torchtext from test.txt   bump torchtext to 0.5.0   bump torchtext to 0.5.0   Apply suggestions from code review   ignore bleu score in doctest, renamed to nlp.py   back to implementation with torch   remove --ignore in CI test, proper reference format   apply justus comment   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",1
6572,Update callbacks.rst,0.6591005,  callbacks:,  dataloader_idx typo   typo   update test_step docs   missing optimizer_idx ,0
6573,changed root image (#1061),0.5911844,    root_node = '127.0.0.2',,0
6574,App: prune requirements duplicity (#13739),0.78162694,Pruned requirements duplicity (#13739),  recursive dtype device apply   simplify   simple test   submodule test   rename   explicit   type hints   test for dp backend   fix test skip   rename   add ddp_spawn test   fix None index in test   try fix ddp_spawn test   changelog   move _dtype and _device to mixin   additional doctest ,1
6575,Add link to benchmarks docs (#17239),0.63567704,"docs for all Metrics (#2184, #2209)",  r   r   r   patched optimizer closure with sr   patched optimizer closure with sr   patched optimizer closure with sr   added train step structured result   added train step structured result   added train step structured result   added train step structured result   added train step structured result   added train step structured result   added train step structured result   added train step structured result   added train step structured result   added train step structured result   added train step structured result   added train step structured result   added train step structured result   added train step structured result   added train step structured result   added train step structured result   added train step structured result   added train step structured result   added train step structured result   added train step structured result   added autoreduce for train step   added auto reduce on train   added auto reduce on train   added auto reduce on train   added auto reduce on train   added auto reduce on train   added auto reduce on train   added hooks   added hooks   added hooks   added hooks   added hooks   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   cache   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   Update pytorch_lightning/callbacks/early_stopping.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/callbacks/early_stopping.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/callbacks/early_stopping.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   Update pytorch_lightning/callbacks/model_checkpoint.py   Update pytorch_lightning/core/step_result.py   finished tests for structured results on train epoch   finished tests for structured results on train epoch   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com  Apply suggestions from code review  Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com  Apply suggestions from code review  Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com   simple   finished tests for structured results on train epoch   simple   simple   revert   finished tests for structured results on train epoch   finished tests for structured results on train epoch   Update tests/base/deterministic_model.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   finished tests for structured results on train epoch   docstring typos   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   finished tests for structured results on train epoch   Update pytorch_lightning/core/step_result.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update pytorch_lightning/overrides/data_parallel.py  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka jirka@pytorchlightning.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,0
6576,Disable benchmark ci on PRs (#9430),0.58967084,Stopped optimizer_zero_grad from being called after IPU execution (#12913),Co-authored-by: Kai Fricke kai@anyscale.com,0
6577,Default EarlyStopping.check_on_train_epoch_end=True (#8286),0.74853575,Changed EarlyStopping callback from by default running EarlyStopping.on_validation_end if only training is run. Set check_on_train_epoch_end to run the callback at the end of the train epoch instead of at the end of the validation epoch (#7069),,1
6578,More EpochResultStore refactors! :tada: (#5522),0.8550182,Refactored EpochResultStore (#5522),Co-authored-by: Kai Fricke kai@anyscale.com,1
6579,New modular metric interface (#2528),0.6326985,Deprecated TrainerLoggingMixin in favor of a separate utilities module for metric handling (#7180)," Fix local rank zero casting  The environment variable 'LOCAL_RANK' can be a string, causing the if rank_zero_only.rank == 0 check to fail  Update distributed.py  address comment",0
6580,Remove the deprecated trainer.verbose_evaluate (#14884),0.7978642,Deprecated the Trainer.disable_validation property in favor of not Trainer.enable_validation (#8291),,1
6581,"Update omegaconf requirement from <=2.1.*,>=2.0.5 to >=2.0.5,<2.3.0 in /requirements (#13167)",0.48959744,"Epoch 3:  21%|██        | 28/128 [00:36<01:32, 23.12it/s, loss=0.163]",,0
6582,Fix pre-commit isort failure on tests/checkpointing/*.py (#5427),0.6817832,Skipped testing with PyTorch 1.7 and Python 3.9 on Ubuntu (#11217), Update index.rst  Add QA Finetuning in community examples.  Update README.md,0
6583,constant root seed in reset_seed (tests) (#2895),0.61919194,seed_everything now fails when an invalid seed value is passed instead of selecting a random seed (#8787),  add tests for single scalar return from training   add tests for single scalar return from training   add tests for single scalar return from training   fixing val step only   fixing val step only   fixing val step only   fixing val step only   fixing val step only   fixing val step only   fixing val step only   fixing val step only   fixing val step only   fixing val step only   fixing val step only   fixing val step only   fixing val step only   fixing val step only   fixing val step only   fixing val step only   fixing val step only   fixing val step only   fixing val step only   fixing val step only   fixing val step only   fixing val step only ,0
6584,Fix mid-epoch warning call while resuming (#11556),0.7413856,Avoid redundant callback restore warning while tuning (#13026),  add simple CircleCI   ignore some   note ,1
6585,Actually show deprecation warnings and their line level [2/2] (#8002),0.7926105,Deprecation warning (#3844),,1
6586,Update fit with no validation hook test (#7738),0.67031336,"Validation DataLoader 0:  38%|███      | 12/32 [00:12<00:20,  1.01s/it]",  add tests for single scalar return from training   add tests for single scalar return from training   add tests for single scalar return from training   add tests for single scalar return from training   add tests for single scalar return from training ,0
6587,deprecate the TestTubeLogger (#9065),0.970504,Deprecated the TestTubeLogger (#9065),  register buffer hint   testcode   clarify init   code block   make it unambigous ,1
6588,add & apply flake8-simplify (#17386),0.583371,Simplify the PL examples structure (shallower and more readable) (#1247),  missing   miss   miss   new ,0
6589,code guideline (#1949),0.60974693,- Code coverage (99%),  Typo   Update docs/source/performance.rst   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
6590,Add ElasticTraining documentation (#1818),0.5206519,Changed (renamed and refactored) TensorRunningMean -> TensorRunningAccum: running accumulations were generalized. (#1278),  remove grad scaling tpu   remove grad scaling tpu ,0
6591,ref: inner train loop (intermediate step) 16/n (#3375),0.78695357,"inner train loop (#3359, #3361, #3362, #3363, #3365, #3366, #3367, #3368, #3369, #3370, #3371, #3372, #3373, #3374, #3375, #3376, #3385, #3388, #3397)",  remove grad scaling tpu   remove grad scaling tpu   remove grad scaling tpu   remove grad scaling tpu   remove grad scaling tpu   remove grad scaling tpu   remove grad scaling tpu   remove grad scaling tpu   remove grad scaling tpu ,1
6592,is_picklable: catch AttributeError (addresses #3771) (#4508),0.55884874,Make the is_picklable function more robust (#17270),,0
6593,Raise exception for ddp_cpu not supported for TPUs (#8530),0.64181775,Updated logic for checking TPUs availability (#6767),  nones   fix   fix   test   test   test   fix   eps   tpu   eps   type   test tpu   Update init.py   Co-authored-by: William Falcon waf2107@columbia.edu,0
6594,fabric: upstream runif to pkg (#17504),0.627925,fabric.backward(loss),  enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint ,0
6595,Update docs for map_location (#920),0.5327649,Changed Checkpoint path parameter from filepath to dirpath (#1016),  enable none checkpoint   enable none checkpoint ,0
6596,Rename reset_on_epoch to reset_on_run (#9658),0.9827299,Renamed reset_on_epoch to reset_on_run (#9658),  Add functional regression metrics   add functional tests   add docs   changelog   init   pep8   docs   docs   setup docs   docs   Apply suggestions from code review   Apply suggestions from code review   typo   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka jirka@pytorchlightning.ai,1
6597,ref: decouple apex second attemp part n/n (#4062),0.6078324,remove _evaluate fx (#3197),  added base tests for tpu   added base tests for tpu   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint   enable none checkpoint ,0
6598,Update syntax errors in docs code snippets (#10036),0.57458687,Fixing critical bugs in newly added hooks and hparams assignment.,,0
6599,Reset the total fit-validation batch progress on epoch (#11244),0.7739886,Reset epoch progress with batch size scaler (#13846),  mlflow rework   logger save_dir   folder   mlflow   simplify   fix test   add a test for file dir contents   new line   changelog   docs   Update CHANGELOG.md   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   test for comet logger   improve mlflow checkpoint test   prevent  commet logger error on pytest exit   test tensorboard save dir structure   wandb save dir test   skip test on windows   add mlflow to pickle tests   wandb   code factor   remove unused imports   remove unused setter   wandb mock   wip mock   wip mock   wandb tests with mocking   clean up   clean up   comments   include wandblogger in test   clean up   missing argument   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
6600,testing loop docs,0.6616243,Enabling val/test loop disabling (#2692),  Start accumulate gradients schedule at epoch 0   Undo change in #2375   Update test_trainer.py::test_gradient_accumulation_scheduling   Fix pep8 formatting   Remove 'Datasets/' folder   Split args for readability   Fix pep8 formatting ,0
6601,tests: randomized order for PT & Fabric (#17460),0.59152675,Fabric,  Add failing test for bug   Fix bug ,0
6602,"Removed line, dont abs",0.5725581,Removed,,0
6603,Adding pytorch-forecasting to community examples (#4575),0.64272285,  model = PyTorchModel(...),  fix ci crash on codecov timeout   Update .github/workflows/tpu-testing.yml   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
6604,Fix mypy errors attributed to pytorch_lightning.core.module.py (#13603),0.7275678,+ # pytorch_lightning==1.7.0,  added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu ,1
6605,limit monitor callback with log_every_n_steps (#3881),0.64585537,"Allow ModelCheckpoint monitor to be None, meaning it will always save ([3630)", removed spawns for test_converters and verified tests  Co-authored-by: Ananya Harsh Jha ahj265@nyu.edu Co-authored-by: zcain zcain@google.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka jirka@pytorchlightning.ai,0
6606,Make sure save_dir can be empty str (#15638),0.7690438,Make sure save_dir can be empty str (#15638](https://github.com/PyTorchLightning/pytorch-lightning/issues/15638)),  Fix parameters and docs in metrics   doc improvements   whitespace   doc indentation   Apply suggestions from code review   Co-authored-by: Nicki Skafte skaftenicki@gmail.com   zero   drop defaults   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Jirka jirka@pytorchlightning.ai,1
6607,Fix inconsistency when user specifies weights_save_path (#12372),0.7128161,- Changed checkpoints save path in the case of multiple loggers and user-provided weights_save_path from `weights_save_path/name1_name2/version1_version2/checkpoints` to `weights_save_path/checkpoints` ([#12372](https://github.com/Lightning-AI/lightning/pull/12372)),"  Add stub PSNR metric   Fix linter   Add data range as parameter   Add tests   Add scikit-image   Add PSNR to regression metrics and add functional   Refactor to functional   Fix linter   Fix linter, again   Fix linter, again   Fix typo in test   Fix typo in another test   Add scikit-image to conda   Lift numpy requirement   Add random tests   Update CHANGELOG.md   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",1
6608,CI: Update labeler bot (#13624),0.58299875,refactored dataloader process hook (#3139),  wandb logging fix   Changelog fix   change test ,0
6609,Update DeepSpeed docs after single saving PR (#7036),0.7209939,DeepSpeed single file saving (#6900),,1
6610,docs: move assets_lightning to pl-public-data (#16419),0.5688535,- Deprecated public utilities in `pytorch_lightning.utilities.cli.LightningCLI` in favor of equivalent copies in `pytorch_lightning.cli.LightningCLI` ([#13767](https://github.com/Lightning-AI/lightning/pull/13767)),  skip ckpt test on rank  > 0   fx test   add extra assert   code factor   add back removed   add old loading code   add back old   unused import   add same skip to run_model_without_loggers   test if horovod now works with python 3.8   test remove all 3.8 skips   remove spawn   fix   fix test   move load check up   fix test multigpu   rename   fix gpu mode   on gpu fix when on cpu   move ,0
6611,Fix is_interactive_compatible logic after AcceleratorConnector rewrite (#12008),0.7864243,AcceleratorConnector rewrite,  added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   fix deprecation warnings   added base tests for tpu   added base tests for tpu   Update pytorch_lightning/trainer/trainer.py   Co-authored-by: Jeremy Jordan 13970565+jeremyjordan@users.noreply.github.com   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   added base tests for tpu   Co-authored-by: Jirka jirka@pytorchlightning.ai Co-authored-by: Jeremy Jordan 13970565+jeremyjordan@users.noreply.github.com,1
6612,[Test] Add extra test for val_check_interval in distributed scenario (#7863),0.65727055,Enabling val/test loop disabling (#2692),  drop py3.6   use base image   typo   skip extra   drop cache ,0
6613,Collective's PREMUL_SUM support with PyTorch 1.13 (#15201),0.6046823,PyTorch 1.5  support,  refactor   add TPU base   wip   builds   typo   extras   simple   unzip   rename ,0
6614,feature/ Add note about Argparse. (#4321),0.67008007,argparse_utils >> argparse,  fix worker warning   improve tests   suggestion   Co-authored-by: Jirka jirka@pytorchlightning.ai,0
6615,ref: moved hooks around in eval loop (#3195),0.95116824,moved hooks around in eval loop (#3195),Co-authored-by: Vijay Rajaram vrajaram3@gatech.edu,1
6616,add PR Gatekeeper (#12646),0.553729,Configuration Validator (#9779),  state updates to logger   change log   changelog ,0
6617,Merge branch 'master' of https://github.com/PyTorchLightning/pytorch-lightning,0.6508399,- Moved ownership of the lightning optimizers from the `Trainer` to the `Strategy` ([#11444](https://github.com/PyTorchLightning/pytorch-lightning/pull/11444)),  fix dtype conversion   changelog ,0
6618,Using internal ip + port in a load balancer instead of URL exposed (#16119),0.8809887,The LoadBalancer now uses internal ip + port instead of URL exposed (#16119),,1
6619,"Update wandb requirement from <=0.12.11,>=0.8.21 to >=0.8.21,<0.12.17 in /requirements (#13051)",0.5677981,Removed wandb logger's finalize method (#1193),  fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang   fix tpu hang ,0
6620,GPU suggestion does not require devices anymore (#17217),0.7139293,GPU training (#2704),This reverts commit 355918af8dcee6bb21cbdc497b7b33243fec8db3.,1
6621,[bugfix] Properly name PyTorchProfiler traces (#8009),0.8337945,Improved PyTorchProfiler chrome traces names (#8009),This reverts commit 944ffba305418a3bb63071e1a64e051cf83c0490.,1
6622,Pin protobuf version (#13177),0.6512881,Renamed TPUHalfPrecisionPlugin to TPUBf16PrecisionPlugin (#10026),  join coverage   full TPU test   codecov   typo   report   docker   timeout   base   show   cd dir   req   docker   docker   docker   coverage   upload   drop main   report   report   python   upload   drone   drone   drone   drone   drone   drone   drone   drone   drone ,0
6623,Update contributing guide (#2830),0.6809596,Contributors,  added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test ,0
6624,Add profiling to dataloader next() (#12124),0.6964815,- Added profiling to the loops' dataloader `__next__` calls ([#12124](https://github.com/Lightning-AI/lightning/pull/12124)),  interpreter   chlog ,0
6625,Optimize Payload Time locally (#15601),0.623417,Disabled lr_scheduler.step() in manual optimization  (#6825),,0
6626,Fixes broken LM links (#4117),0.5833975,Updated mlflow with using resolve_tags (#6746),  Fixes #2455   Fixes #2455   Fixes #2455   Fixes #2455   Fixes #2455   Fixes #2455   Fixes #2455   Fixes #2455   Fixes #2455   Fixes #2455   Fixes #2455   Fixes #2455   Fixes #2455   Fixes #2455   Fixes #2455   Fixes #2455   Fixes #2455   Fixes #2455   Fixes #2455   Fixes #2455   Fixes #2455   Fixes #2455   Fixes #2455   Fixes #2455   Fixes #2455   Fixes #2455   Fixes #2455   added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test   added early stop tpu test ,0
6627,Weights path (#211),0.5612472,"Deprecated Trainer attribute ckpt_path, which will now be set by weights_save_path (#2681)",  fix gpu example   make cpu_template and gpu_template differnt   Co-authored-by: Adrian Wälchli adrian.waelchli@inf.unibe.ch,0
6628,Standalone Lite: Strategy base classes and registry (#14662),0.6472237,class Lite(LightningLite):,,0
6629,Deprecate LightningLoggerBase.close (#9422),0.8525523,"Deprecated LightningLoggerBase.close, LoggerCollection.close in favor of LightningLoggerBase.finalize, LoggerCollection.finalize (#9422)",,1
6630,[App] Resolve Multi Node examples (#15605),0.65018,The MultiNode components now warn the user when running with num_nodes > 1 locally (#15806),  Add Github Action to run TPU tests.   Trigger new Github Actions run.   Clean up more comments.   Use different fixed version of ml-testing-accelerators and update config to match.   use cluster in us-central1-a   Run 'gcloud logging read' directly without 'echo' to preserve newlines.   cat coverage.xml on the TPU VM side and upload xml on the Github Action side   Use new commit on ml-testing-accelerators so command runs fully.   Preserve newlines in the xml and use if: always() temporarily to upload codecov   Use pytorch_lightning for coverage instead of pytorch-lightning   Remove the debug cat of coverage xml   Apply suggestions from code review   jsonnet rename   name   add codecov flags   add codecov flags   codecov   codecov   revert codecov   Clean up after apt-get and remove old TODOs.   More codefactor cleanups.   drone   drone   disable codecov   cleaning   docker py versions   docker py 3.7   readme   bash   docker   freeze conda   py3.6   Stop using apt-get clean.   Dont rm pytorch-lightning   Update docker/tpu/Dockerfile   Longer timeout in the Github Action to wait for GKE to finish.   job1   job2   job3   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka jirka@pytorchlightning.ai,0
6631,Update embeddings.py,0.6293802,PyTorch 1.5  support,,0
6632,"Enable ZeRO tests for CI, fix to/half function calls (#6070)",0.5271823,"Updated DeepSpeed ZeRO (#6546, #6752, #6142, #6321)","  add coverage, test failing   fix test   badges   typo   freeze conda ",0
6633,Fix lr key name in case of param groups (#1719),0.53769433,Override some of the params with new values,,0
6634,Fix IoU score for classes not present in target or pred (#3098),0.9197247,Changed IoU score behavior for classes absent in target and pred (#3098),  add warning when getting checking len   added test   changelog   pep   do not show warning below 1.4   try version parse   comments   xfail   Update requirements/base.txt   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/trainer/data_loading.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  version  Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka jirka@pytorchlightning.ai,1
6635,skip best_model_path if checkpoint_callback is None (#2962),0.98690885,Skipped best_model_path if checkpoint_callback is None (#2962),  enabled no returns from eval   fixed docs   fixed docs   fixed docs   fixed docs   fixed docs   fixed docs   fixed docs   fixed docs   fixed docs   fixed docs   fixed docs   fixed docs ,1
6636,release v0.1.dev182,0.6748152,"    version=""0.0.1"",",,0
6637,Github Actions deprecation (#5183),0.6044271,- Removed deprecated callback hooks ([#14834](https://github.com/Lightning-AI/lightning/pull/14834)),  missing   miss ,0
6638,Typing for tests 1/n (#6313),0.65618634,"nb_test_batches to num_test_batches,",  base tests   pil   wip   wip   wip   ignore   ignore   win   link   win   cpu   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
6639,"Mocking loggers (part 1, wandb) (#3596)",0.79708064,"Mocking loggers (#3596, #3617, #3851, #3859, #3884, #3853, #3910, #3889, #3926)",  fix and test for ddp block logging rank > 0   rename   use the dummy logger   dummy logger test   set the logger in  model   decorator for rank zero experiment   simplify check   simplify   fix problem with None in checkpoint path   revert configure logger   unused import   offline   try rank 0 decorator in checkpoint   try fix test   imgs   add asserts to make sure log zero only saves checkpoints   add asserts to make sure log zero only saves checkpoints   add asserts to make sure log zero only saves checkpoints   add asserts to make sure log zero only saves checkpoints   add asserts to make sure log zero only saves checkpoints   fix tpu tests   fix tpu tests   Co-authored-by: William Falcon waf2107@columbia.edu,1
6640,simplify tests & cleaning (#2588),0.68341386,"Cleaning (#5948, #5949, #5950)",,0
6641,added val loop options,0.69840896,Loop customization:,,0
6642,docs: logits -> probs in Accuracy metric documentation (#5340),0.70130277,Allow logging of metrics together with hparams (#1630),  Adding importing ipywidgets before importing tqdm.auto to make sure ipywidgets is installed.   Updated CHANGELOG.md   Updated ipywidgets importing checks to @awaelchli comments.   Co-authored-by: William Falcon waf2107@columbia.edu,1
6643,test for complicated batch structure (#3928),0.64481187,"    batch_size=32,",  added reduce ddp results on eval   added reduce ddp results on eval   added reduce ddp results on eval   added reduce ddp results on eval   added reduce ddp results on eval   added reduce ddp results on eval   added reduce ddp results on eval   added reduce ddp results on eval   added reduce ddp results on eval   added reduce ddp results on eval   added reduce ddp results on eval ,0
6644,model checkpint on rank_zero_only & global rank state (#1408),0.6046631,- Fixed environment variable priority for global rank determination ([#11406](https://github.com/PyTorchLightning/pytorch-lightning/pull/11406)),  fix outputs   fix outputs   fix outputs   fix outputs   fix outputs   fix outputs   fix outputs   fix outputs   fix outputs   fix outputs   fix outputs   fix outputs   fix outputs   fix outputs   fix outputs   fix outputs   fix outputs   fix outputs   fix outputs ,0
6645,Remove deprecated trainer.should_rank_save_checkpoint (#14885),0.90716946,Removed should_rank_save_checkpoint property from Trainer (#9433),  drop CircleCI   add PT testing   fix   cpu   conda   conda   req   base   conda   conda   conda   conda   conda   conda   conda   name   req   info   tests   pt 1.6   drop 1.6   info ,1
6646,Allow loading checkpoints from urls (#1667),0.5769305,    # put all logic related to loading a checkpoint here,  fix outputs   fix outputs ,0
6647,reduce only loss with dp (#11594),0.64819354,fix result obj DP auto reduce (#3013),,0
6648,ref: (results 1/n) enable tracking original metric when step and epoch are both true (#3685),0.9428358,enable tracking original metric when step and epoch are both true (#3685),,1
6649,Remove duplicate import checks (#16648),0.6816448,Wrapped imports for traceability (#13924),,0
6650,[CLI] Save only the configuration used (#11532),0.6093383,Saved the LightningCLI config on setup and only on the main process (#8017),,0
6651,Multinode on MPS (#15748),0.66595364,Updated Multinode Warning (#16091),,0
6652,Replace automatic nan check with optional flag (#1475),0.6755475,Changed the default behaviour to no longer include a NaN check with each training iteration. (#1475),,0
6653,(app) Add s3 drive type (1/2) (#14002),0.51316774,- Added Apple Silicon Support via `MPSAccelerator` ([#13123](https://github.com/Lightning-AI/lightning/pull/13123)),  missing   RC1   RC1   format ,0
6654,IPU hotfix for #9721 (#9759),0.58311236,Graphcore IPU devices,,0
6655,CI: fix typo in workflow & trigger (#15465),0.57335466,Raised a MisconfigurationException if batch transfer hooks are overriden with IPUAccelerator (13961),,0
6656,Small improvements to TB and CSV loggers (#11764),0.65376675,- Added support for writing logs remote file systems on `CSVLoggers`. ([#16880](https://github.com/Lightning-AI/lightning/pull/16880)),  add state_dict for early stopping   move best attr after monitor_op defined   improve early stopping and model checkpoint callbacks   fix formatting   fix attr init order   clean up setting of default_root_dir attr   logger needs default root dir set first   reorg trainer init   remove direct references to checkpoint callback   more fixes   more bugfixes   run callbacks at epoch end   update tests to use on epoch end   PR cleanup   address failing tests   refactor for homogeneity   fix merge conflict   separate tests   tests for early stopping bug regressions   small fixes   revert model checkpoint change   typo fix   fix tests   update train loop   cannot pass an int as default_save_path   refactor log message   fix test case   appease the linter   fix some doctests   move config to callback   fixes from rebase   fixes from rebase   chlog   docs   reformat   formatting   fix   fix   fixes from rebase   add new test for patience   Update pytorch_lightning/callbacks/model_checkpoint.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/callbacks/model_checkpoint.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update tests/callbacks/test_early_stopping.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   fix formatting   remove enable_early_stop attribute   add state_dict for early stopping   move best attr after monitor_op defined   improve early stopping and model checkpoint callbacks   fix formatting   fix attr init order   clean up setting of default_root_dir attr   logger needs default root dir set first   reorg trainer init   remove direct references to checkpoint callback   more fixes   more bugfixes   run callbacks at epoch end   update tests to use on epoch end   PR cleanup   address failing tests   refactor for homogeneity   fix merge conflict   separate tests   tests for early stopping bug regressions   small fixes   revert model checkpoint change   typo fix   fix tests   update train loop   fix test case   appease the linter   fix some doctests   move config to callback   fixes from rebase   fixes from rebase   chlog   docs   reformat   formatting   fix   fix   fixes from rebase   add new test for patience   Update pytorch_lightning/callbacks/model_checkpoint.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/callbacks/model_checkpoint.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update tests/callbacks/test_early_stopping.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   fix formatting   remove enable_early_stop attribute   fix test with new epoch indexing   fix progress bar totals   fix off by one error (see #2289) epoch starts at 0 now   added missing imports   fix hpc_save folderpath   fix formatting   fix tests   small fixes from a rebase   fix   tmpdir   tmpdir   tmpdir   wandb   fix merge conflict   add back evaluation after training   test_resume_early_stopping_from_checkpoint TODO   undo the horovod check   update changelog   remove a duplicate test from merge error   try fix dp_resume test   add the logger fix from master   try remove default_root_dir   try mocking numpy   try import numpy in docs test   fix wandb test   pep 8 fix   skip if no amp   dont mock when doctesting   install extra   fix the resume ES test   undo conf.py changes   revert remove comet pickle from test   Update CHANGELOG.md   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   Update weights_loading.rst   Update weights_loading.rst   Update weights_loading.rst   renamed flag   renamed flag   revert the None check in logger experiment name/version   add the old comments   _experiment   test chckpointing on DDP   skip the ddp test on windows   cloudpickle   renamed flag   renamed flag   parentheses for clarity   apply suggestion max epochs   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jeremy Jordan jtjordan@ncsu.edu Co-authored-by: Jirka jirka@pytorchlightning.ai Co-authored-by: Jeremy Jordan 13970565+jeremyjordan@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: William Falcon waf2107@columbia.edu,0
6657,Use fsspec in checkpoint connector for fault-tolerant training (#11776),0.7115325,Fault-tolerant Training,  fix #2386   extra test   extra case   extra test   chlog   fix test ,1
6658,ignoring multi-node flag,0.6591053,Slightly safer multi node (#15538),,0
6659,Update mypy job to 1.1.1 (#16974),0.5213081,Resolved a bug where the work statuses will grow quickly and be duplicated (#13970),  fix #2334   chlog ,0
6660,Move test_dtype_device_mixin to lite (#14511),0.63374025,Moved DeviceDtypeModuleMixin and HyperparametersMixin mixin to core (#8396),  remove warnings   remove warnings   added doc lines   added doc lines ,0
6661,Annotate return type of TrainerProperties.from_argparse_args(...) (#4192),0.79439825,trainer = L.Trainer.from_argparse_args(args),  skip if no amp   dont mock when doctesting   install extra ,1
6662,Refine remote fs doc (#11393),0.58731776,Changed fsspec to tuner (#4458),  torchtext   Update pytorch_lightning/utilities/apply_func.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update apply_func.py  Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
6663,Update flake8 version (#15816),0.55014014,This release fixes that core issue,  test   fix   fix ,0
6664,Remove using_lbfgs argument from optimizer_step module hook (#16538),0.71165276,- Removed the `using_lbfgs` argument from `LightningModule.optimizer_step` hook ([#16538](https://github.com/Lightning-AI/lightning/pull/16538)),  Handle torchtext.data.Batch on GPU   Update CHANGELOG.md   Apply code review requests   Correct the docs   Change requirements ,1
6665,Fix min_max gpu memory logging bug (#453),0.6815429,Changed min-max GPU memory to be on their own plots (#1358),  move from CircleCI   req   tex   tex   sudo   extra   recom   pic   dvipng ,0
6666,Cast to fp16 before moving to device with deepspeed (#14000),0.8276534,Casted tensors to fp16 before moving them to device with  DeepSpeedStrategy (#14000),  remove warnings   remove warnings   remove warnings   remove warnings   remove warnings   remove warnings   remove warnings   remove warnings   remove warnings   remove warnings ,1
6667,Run only CUDA tests on Azure GPU CI (#13651),0.59880686,GPU training (#2704),  move Trains logger   chlog ,0
6668,adding LAI test (#13321),0.5701252,"nb_test_batches to num_test_batches,",  cli examples   ddp   CI   CI   req   tests   skip DDP   Co-authored-by: William Falcon waf2107@columbia.edu,0
6669,Prevent loss to be moved to the cpu before backward call. (#9308),0.6403037,remove obscure forward call in eval + CPU backend ___step (#3123),,0
6670,added hooks docs,0.714471,Fixing critical bugs in newly added hooks and hparams assignment.,,1
6671,Make internal torchscript check a class attribute (#14904),0.5704868,Dropped torchvision dependency in tests and added own MNIST dataset class instead (#986),save_top_k should be an int and have been mentioned as save_top_k=True in the snippet provided under 'Saving and Loading Weights' docs. Changed it to its default value (1) to make it consistent. Signed-off-by: Kshitij Patil kshitijpatil98@gmail.com,0
6672,Sharded Accelerator 1/n: Expose clip gradients to plugins via abstract class (#4639),0.641624,gradient_clip -> gradient_clip_val    ,  native amp   typo   imports   apex ,0
6673,Ensure we do windows check first,0.6254339,Run ddp_spawn dataloader checks on Windows (#6930),  no cov   no cov   ReduceOp   group   reduce_op.sum   Update sklearns.py   formatting   horovod   Apply suggestions from code review   horovod   horovod   horovod   horovod   ci   print   ci   timeout   timeout   time   fix   distributed cpu   pipes   time   cpu   spawn   spawn   spawn   tp   separate   os   os   npm   Fix load_from_checkpoint() not working with URL on Windows   Update CHANGELOG   Update CHANGELOG.md   Co-authored-by: Peter Yu 2057325+yukw777@users.noreply.github.com   Apply suggestions from code review   fix   fix meta tags creating empty lines   pyright   node   fix httpserver address   drop tutils.default_trainer_options   imports   Better fix for load_from_checkpoint() not working with absolute path on Windows (#2294)   Fix load_from_checkpoint() not working with URL on Windows   Update CHANGELOG   Update CHANGELOG.md   Co-authored-by: Peter Yu 2057325+yukw777@users.noreply.github.com  Apply suggestions from code review  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Peter Yu 2057325+yukw777@users.noreply.github.com  drop duplicate  Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: airium airium@outlook.com Co-authored-by: Peter Yu 2057325+yukw777@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: AIRIUM 38249940+airium@users.noreply.github.com,0
6674,drop bots (#5679),0.5873028,"decoupled DDP, DDP spawn (#3733, #3766, #3767, #3774, #3802, #3806)",  get dataloader size   pyright ,0
6675,[metrics] change default behaviour of state dict (#4685),0.877844,Metric states are no longer as default added to state_dict (#4685),  deal with NotImplementedError raised by torchtext   deal with NotImplementedError raised by torchtext   Added tests for dataloader which raise NotImplementedError in len()   Fixed some typos   enabled tests for dataloader raising NotImplementedError in len and corrected match string for raised exception   deleted empty line for style compliance   refactored CustomNotImplementedErrorDataloader to derive from CustomInfDataloader   enabled reduced number of not_implemented_error dataloader test to reduce runtime for continuous integration   reduced test number of not_implemented_error dataloader test further to reduce test time   reduced test number of not_implemented_error dataloader test to one to reduce test time   disabled all not_implemented_error dataloader test to see if test pass in time   added next with a reduced number (5) of elements after which CustomNotImplementedErrorDataloader stops to speedup test.   enabling all not_implemented_error dataloader test   added brief description of change and relation of torchtext   CustomNotImplementedErrorDataloader reduced number of batches served to 2.   Update CHANGELOG.md   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   Apply suggestions from code review   Update CHANGELOG.md   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Disable parallelism in dataloader  Suspect that it might cause pytest to hang more frequent   added max_steps=None to Trainer in not_implemented_error dataloader tests   rearranged not_implemented_error test in file to group them together   disabled parallel data loading Reason: testing if that stops the test framework from hanging.   Apply suggestions from code review   Apply suggestions from code review   Co-authored-by: Thomas Schaaf tschaaf@cs.cmu.edu Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
6676,"[WIP] ref: decoupled ddp, ddp spawn (finish 3733) (#3819)",0.809873,"decoupled DDP, DDP spawn (#3733, #3817, #3819, #3927)",,1
6677,Update deepspeed precision test (#12727),0.8244997,Updated precision attributes in DeepSpeedPlugin (#10164),*simplified optimizer step and zero grad overriding,1
6678,Refactor: skipif for Windows 2/n (#6268),0.5779066,Changed fsspec to tuner (#4458),,0
6679,Correctly reset metric objects in self.log (#7055),0.7557825,Integrated metrics API with self.log (#3961),  fixes hparam logging   fixes hparam logging   fixes hparam logging   fixes hparam logging   fixes hparam logging   Apply suggestions from code review   skipif   rename   Update test_tensorboard.py   Update test_tensorboard.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka jirka@pytorchlightning.ai,1
6680,Lazy initialize Strategy.parallel_devices (#11572),0.6826315,- Changed `parallel_devices` property in `ParallelStrategy` to be lazy initialized ([#11572](https://github.com/PyTorchLightning/pytorch-lightning/pull/11572)),  swaps lr sched order   Update optimizers.py   added amdim encoder choice ,0
6681,Fix docs metrics formatting (#5077),0.7162617,"docs for all Metrics (#2184, #2209)",https://github.com/PyTorchLightning/pytorch-lightning/issues/2329,1
6682,Let metadata score be serializable by wand (#15544),0.59317803,Prevent WandbLogger from dropping values (#5931),,0
6683,Remove unused hook keyword argument (#13059),0.75128716,"Removed output argument from *_batch_end hooks (#3965, #3966)",  docs about Python logging   add link to Python logging docs ,1
6684,Deprecate AbstractProfiler in favor of BaseProfiler (#12106),0.65035284,- Removed the deprecated `BaseProfiler` and `AbstractProfiler` classes ([#14404](https://github.com/Lightning-AI/lightning/pull/14404)),Co-authored-by: David Waterworth david.waterworth@cim.io,0
6685,Add remove_checkpoint to CheckpointIO plugin to simplify ModelCheckpo… (#9373),0.7509571,Removed the deprecated model argument from ModelCheckpoint.save_checkpoint (#8688),  generalize data transfer   added test   update docs   fix spelling error   changelog   update docs ,1
6686,made ddp the default if no backend specified with multiple GPUs (#1789),0.9855724,Made DDP the default if no backend specified with multiple GPUs (#1789),  refactoring training epoch   refactored training epoch   refactored training epoch   refactored training epoch   refactored training epoch   refactored training epoch   fixes slurm weights saving   fixes slurm weights saving ,1
6687,Fix example argument parser in docs (#1692),0.66631067,parser = argparse.ArgumentParser(), fixes rank zero issue,0
6688,Fix pre-commit trailing-whitespace and end-of-file-fixer hooks. (#5387),0.64354485,Fixing critical bugs in newly added hooks and hparams assignment.,,0
6689,updated args,0.7013215,argparse_utils >> argparse,  added tpu params test   added tests   removed xla imports   added test cases for TPU   fix pep 8 issues   refactorings and comments   add message to MisconfigurationException   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   test if device is set correctly   added TPU device check removed mark.spawn   removed device selection   remove xla_device call   readded spawn due to test failures   add TODO for tpu check   Apply suggestions from code review   Apply suggestions from code review   flake8   added tpu args to cli tests   added support for tpu_core selection via cli   fixed flake formatting   replaced default_save_path with default_root_dir   added check for data type for tpu_cores   fixed flake indent   protected   protected   added tpu params test   added tests   removed xla imports   test if device is set correctly   added support for tpu_core selection via cli   replaced default_save_path with default_root_dir   added check for data type for tpu_cores   chlog   fixed tpu cores error   rebased with latest changes   flake fix   Update pytorch_lightning/trainer/distrib_parts.py   added suggesstion Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka jirka@pytorchlightning.ai,1
6690,temp suspend NVIDIA CI build (#7350),0.5322005,GPU training (#2704),"  Init fix num_batches   Fix num_batches in case of multiple dataloaders   Apply suggestions from code review   Changes based on suggestions   Flake8   Add test to check num_batches   generalize dataloader percent check test   fix formatting   remove hparams   tests   CHANGELOG   Update CHANGELOG.md   max_batches can be int   conflict and rebase   add back the test   fix fix message 0.0 works Revert ""fix message"" This reverts commit 839cacf8b8610f4e697e654ef6f3d2501bf23984.   update changelog   Update CHANGELOG.md   Fix num batches in case of multiple dataloaders and percent_check (#1920)   git conflict   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   missing union   doc update suggestion by @rohitgr7   extend test   changelog   docs add note about multiple loaders   update changelog   remove unused variable   Co-authored-by: rohitgr7 rohitgr1998@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",0
6691,fix install,0.6529918,"Fixing missing packaging package in dependencies, which was affecting the only installation to a very blank system."," Checking if the parameters are a DictConfig Object  This is in reference to #2058 .  To be honest, I have no idea how I should go about writing a test for this.  Update pytorch_lightning/loggers/base.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  fix ...  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka jirka@pytorchlightning.ai",0
6692,update chlog v1.2.5 (#6742),0.68783355,0.4.0,  update docs   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
6693,Convert property into attribute (#10412),0.53087604,Removed attributes and methods:,  refactored training_bath   refactored training_bath   refactored training_bath   refactored training_bath   refactored training_bath   refactored training_bath   refactored training_bath   refactored training_bath   refactored training_bath   refactored training_bath   refactored training_bath ,0
6694,tests fix,0.74817103,- fixed all the .test() calls," Fix ROC metric for CUDA tensors  Previously roc metric (and auroc) errors when passed in CUDA tensors, due to torch.tensor construction without specifying device. This fixes the error by using F.pad instead.   Update test_classification.py   Update test_classification.py   chlog   Update test_classification.py   Update test_classification.py   Update tests/metrics/functional/test_classification.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update test_classification.py  Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: Jirka jirka@pytorchlightning.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",1
6695,fixed basic trainer,0.7504703,1. Create the Trainer,"  Fixed average_precision metric, parenthesis were missing. Added test test that failed with the old implementation   Modified CHANGELOG.md   Update CHANGELOG.md   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",1
6696,Implemented ModelSummary total params values (#4521),0.68706465,        :param model:,,0
6697,clean up side menu (#12892),0.64827466,"Cleaning (#5948, #5949, #5950)",  detach hooks after completion   detach hook   update docs   add test   docs   changelog ,0
6698,Test to ensure ckpt filepath contains correct val score (#3933),0.6121337,"trainer.test(model, ckpt_path=""/path/to/checkpoint.ckpt"")",,0
6699,docs: Add BackboneLambdaFinetuningCallback (#5553),0.7527105,Changed computer_vision_fine_tunning example to use BackboneLambdaFinetuningCallback (#5377),  check omegaconf gpus   test   test   Apply suggestions from code review   Co-authored-by: Omry Yadan omry@fb.com Co-authored-by: Omry Yadan omry@fb.com,1
6700,try to update failing dockers (#5611),0.5670252,Resolved a bug where the install command was not installing the latest version of an app/component by default (#14181),  cli gpus   test   test ,0
6701,fix incomplete RunningMean (#1309),0.66307485,| Enum RunningStage.TUNING                                    | 1.10             | No longer supported         |,,0
6702,Add support for async checkpointing (#13658),0.82759154,Asynchronous Checkpointing," Revert ""deprecated: epoch indexing from 1 (#2206)""  This reverts commit f94b919b   chlog   grad index   Apply suggestions from code review   tests   fix   test ",1
6703,remove redundant whitespace in new-project.rst (#4346),0.6031307,Remove unnecessary intermediate layers in Dockerfiles (#5697),  deal with NotImplementedError raised by torchtext   deal with NotImplementedError raised by torchtext   Added tests for dataloader which raise NotImplementedError in len()   Fixed some typos   Co-authored-by: Thomas Schaaf tschaaf@cs.cmu.edu,0
6704,Fix import error when distributed module not available (#12794),0.77213144,Module imports are now catching ModuleNotFoundError instead of ImportError (#9867),Fixes a minor bug introduced in #2213,1
6705,release 1.1.7 (#5761),0.746124,"    version=""0.0.1"",",  deprecated   test ,1
6706,Add missing super().__init__() calls (#12948),0.72240055,    super().__init__(),  move backward   refactor backward to remove 16 bit from user override   refactor backward to remove 16 bit from user override   Update pytorch_lightning/core/hooks.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
6707,fix for multiple types in trainer add_argparse_args (#3077),0.8724039,Trainer.add_argparse_args classmethod fixed. Now it adds a type for the arguments (#1147),  Attempt to add broken test   use wandb logger   Update test_amp.py   Co-authored-by: William Falcon waf2107@columbia.edu,1
6708,Remove unused code (#17001),0.7524425,- Removed the deprecated code in:,  cleaning   docs   docs   types   mixins   mixins   docs   typo ,1
6709,wandb: Fix example rendering for docs (#5905),0.59239316,Changed computer_vision_fine_tunning example to use BackboneLambdaFinetuningCallback (#5377),,0
6710,Add trace functionality to the function to_torchscript (#4142),0.7053875,Add code_dir argument to tracer run (#15771),,1
6711,App: Update changelog post release (0.6.1) (#14844),0.6919111,Full Changelog: https://github.com/Lightning-AI/lightning/compare/1.8.0...1.8.0.post1,  fix missing arg   fix missing arg   fix missing arg   fix missing arg   fix missing arg   fix missing arg   fix missing arg ,0
6712,Avoid enforcing shuffle=False for eval dataloaders (#11575),0.82939565,Enforced eval shuffle warning only for default samplers in DataLoader (#12653),  miss   miss   chlog   chlog ,1
6713,Fix mypy errors attributed to pytorch_lightning.core.saving (#13932),0.7190105,Deprecated pytorch_lightning.logging (#767),,1
6714,Fix a typo in warning message inside Trainer.reset_train_dataloader (#12645),0.7249111,Trainer is now raising a MisconfigurationException instead of a warning if Trainer.{validate/test} is missing required methods (#10016),,1
6715,bug fix #10872 (#10965),0.6764948,Resolve bug with Finetuning (#5744),,0
6716,missing changes (#2283),0.5941524,Replaced _DataModuleWrapper with __new__ (#7289),,0
6717,Update docutils package version in requirements.txt (#10158),0.5770885,Parsed local package versions (#13933),  remove barriers   remove barriers   remove barriers   remove barriers   remove barriers   remove barriers   remove barriers   remove barriers   remove barriers   remove barriers ,0
6718,improve early stopping verbose logging (#6811),0.85146284,Improved verbose logging for EarlyStopping callback (#6811),,1
6719,Explicitly point out where should we set the random seed (#3839),0.6048403,seed_everything now fails when an invalid seed value is passed instead of selecting a random seed (#8787),  remove frame inspection on self.hparams   remove frame inspection on self.hparams   remove frame inspection on self.hparams   remove frame inspection on self.hparams   remove frame inspection on self.hparams   remove frame inspection on self.hparams ,0
6720,ref: part 7 of #3733 (#3802),0.697806,[1.7.1] - 2022-08-09,,0
6721,Don't copy the batch when training on a single gpu (#1576),0.65478146,"where you can use the outputs of training_step (performed on each GPU with a portion of the batch),",,0
6722,Add pyupgrade to pre-commit (#8557),0.6591803,PyTorch 1.5  support,  made fx public   made fx public   made fx public ,0
6723,Merge pull request #8 from shreyasbapat/further_changes,0.54596305,Removed teardown from ParallelPlugin (#8943),  added barrier   blank line   added barrier   added barrier   made fx public   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
6724,CI: exclude docs requirements changes (#15488),0.5700693,Docs improvements,,0
6725,[App] Fixing race condition while setting servers to be free for next batch in the Loadbalancer (#16279),0.5950489,Resolved a bug about a race condition when sending the work state through the caller_queue (#14074),  chlog   docs   ver++   docs   url   docs   readme   docs --- ,0
6726,Move plateau schedulers epoch update to the training epoch loop (#8424),0.6884383,ModelCheckpoint now runs at the end of the training epoch by default (#8389),"There was a typo in metrics description: RMSE titled metric was actually RMSLE and vice-versa. So I've fixed it, changing two characters.",0
6727,Fix materialize_module recursively setting its child module (#12870),0.58592063,Module imports are now catching ModuleNotFoundError instead of ImportError (#9867),  Fixed the load_from_checkpoint path detected as URL bug   Fixed the load_from_checkpoint path detected as URL bug   fixed Caps lock typo   Added .absolute() to checkpoint path to force hard drive prefix in string ,0
6728,Add leave argument to RichProgressBar (#10301),0.5766603,from pytorch_lightning.callbacks import RichProgressBar,  Change PR template   Update .github/PULL_REQUEST_TEMPLATE.md   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
6729,Update new-project.rst (#1655),0.58146477,Refactored EpochResultStore (#5522),,0
6730,Update LightningModule docs (#10637),0.9111525,Update the Lightning App docs (#13537),  add iou function   update stat scores   add iou class   add iou tests   chlog   Apply suggestions from code review   tests   docs   Apply suggestions from code review   docs   Co-authored-by: Jirka jirka@pytorchlightning.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
6731,Fix for hanging issue on TPU Pod (#16844),0.5652811,Fix hanging in DDP HPC accelerators (#5157),  update docs   update docs   update docs   update docs   update docs   update docs ,0
6732,docs (#1001),0.7365186,Docs,  final clean for v0.8.0   chlog   chlog   date   rename stage   date   missing ,1
6733,move example inputs to correct device when tracing module (#4360),0.6171875,"            device_ids=device_ids,",  init the port using a seed that matches process id for ddp   init the port using a seed that matches process id for ddp   init the port using a seed that matches process id for ddp   init the port using a seed that matches process id for ddp   init the port using a seed that matches process id for ddp   init the port using a seed that matches process id for ddp   init the port using a seed that matches process id for ddp   Co-authored-by: Zhaofeng Wu zfw7@cs.washington.edu,0
6734,Fix invalid value for weights_summary (#5296),0.5867785,Improved error messages in replace_sampler when the DataLoader attributes are not included in the signature or the signature is missing optional arguments (#8519),  add tpu view   add tpu view   add tpu view   add tpu view   add tpu view ,0
6735,update readme with conda installation instruction (#2099),0.46654487,The psutil package is now required for CPU monitoring (#17010),,0
6736,Merge pull request #6885 from PyTorchLightning/v1.3.0rc,0.6914164,- Removed `configure_sync_batchnorm` from `ParallelStrategy` and all other strategies that inherit from it ([#11754](https://github.com/PyTorchLightning/pytorch-lightning/pull/11754)),,0
6737,Some docs update (#3794),0.6740811,Docs improvements,  allow regression metrics to import   allow regression metrics to import   allow regression metrics to import   allow regression metrics to import   allow regression metrics to import   allow regression metrics to import   allow regression metrics to import   allow regression metrics to import   allow regression metrics to import ,0
6738,refactor,0.89842856,Refactoring,  allow regression metrics to import   allow regression metrics to import   docs   Apply suggestions from code review   Co-authored-by: Jirka jirka@pytorchlightning.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
6739,fix gradient norm tracking for row_log_interval > 1 (#3489),0.76084673,Gradient norm tracking (#16745),,1
6740,Add warnings regarding unsupported keys in optim config and OneCycleLR (#9666),0.6438126,Improved error messages for invalid configure_optimizers returns (#3587),,0
6741,"Fix incorrect ""Saving latest checkpoint"" warning (#3588)",0.7261048,Changed resume_from_checkpoint warning to an error when the checkpoint file does not exist (#7075),  add regression metrics   solve tests   add docs ,1
6742,changelog for release 0.9 (#2998),0.67114556,Full Changelog,  drop train_percent_check   drop train_percent_check   drop train_percent_check   drop train_percent_check   drop train_percent_check   drop train_percent_check   drop train_percent_check   drop train_percent_check   drop train_percent_check   drop train_percent_check   drop train_percent_check   drop train_percent_check   drop train_percent_check   drop train_percent_check   drop train_percent_check   drop train_percent_check   drop train_percent_check   chlog   deprecated   deprecated   deprecated   tests   tests   Apply suggestions from code review   tests   hydra support   tests   hydra support   hydra support   hydra support   tests   typo   typo   Update test_dataloaders.py   docs   docs   docs   docs   Co-authored-by: Jirka jirka@pytorchlightning.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
6743,set the default value of progress_bar_refresh_rate to 1 (#1100),0.8060993,Changed progress_bar_refresh_rate trainer flag to disable progress bar when setting to 0. (#1108),  hydra support   hydra support   hydra support   hydra support   hydra support   hydra support   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
6744,Bump hivemind from 1.1.2 to 1.1.5 in /requirements (#16293),0.626395,- Hivemind Strategy,  Enable gradients at train start   Update training_loop.py   Co-authored-by: William Falcon waf2107@columbia.edu,0
6745,Remove truncated backpropagation from loops (#16337),0.67025036,Moved result teardown to the loops (#8245),  hydra ddp support   hydra ddp support   hydra ddp support   hydra support   hydra support   hydra support ,0
6746,[bugfix] Resolve metrics not being properly resetted on validation epoch end (#9717),0.6880717,"Validation DataLoader 0:  38%|███      | 12/32 [00:12<00:20,  1.01s/it]",  fixed percent check for val/test   fixed percent check for val/test   fixed percent check for val/test   fixed percent check for val/test   overfit_pct now uses train loaders for val and test and does not shuffle   overfit_pct now uses train loaders for val and test and does not shuffle   overfit_pct now uses train loaders for val and test and does not shuffle   overfit_pct now uses train loaders for val and test and does not shuffle   overfit_pct now uses train loaders for val and test and does not shuffle   overfit_pct now uses train loaders for val and test and does not shuffle   overfit_pct now uses train loaders for val and test and does not shuffle   overfit_pct now uses train loaders for val and test and does not shuffle   overfit_pct now uses train loaders for val and test and does not shuffle   overfit_pct now uses train loaders for val and test and does not shuffle   overfit_pct now uses train loaders for val and test and does not shuffle   overfit_pct now uses train loaders for val and test and does not shuffle   overfit_pct now uses train loaders for val and test and does not shuffle   overfit_pct now uses train loaders for val and test and does not shuffle   overfit_pct now uses train loaders for val and test and does not shuffle   overfit_pct now uses train loaders for val and test and does not shuffle   overfit_pct now uses train loaders for val and test and does not shuffle   overfit_pct now uses train loaders for val and test and does not shuffle   overfit_pct now uses train loaders for val and test and does not shuffle   overfit_pct now uses train loaders for val and test and does not shuffle   overfit_pct now uses train loaders for val and test and does not shuffle   overfit_pct now uses train loaders for val and test and does not shuffle   overfit_pct now uses train loaders for val and test and does not shuffle   add on fit_start on fit_end hooks   add on fit_start on fit_end hooks   add on fit_start on fit_end hooks   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
6747,Cleanup CHANGELOG (#12507),0.7141769,"Cleaning (#5948, #5949, #5950)",This reverts commit f8103f9c7dfc35b4198e951a1789cae534c8b1db.,1
6748,Bump pytest from 7.2.0 to 7.2.2 in /requirements,0.7307607,Skipped testing with PyTorch 1.7 and Python 3.9 on Ubuntu (#11217), Misleading exception raised during batch scaling  Use batch_size from model.hparams.batch_size instead of model.batch_size   Improvements considering #1896   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Rohit Gupta rohitgr1998@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
6749,Fix docs for auto_lr_find (#3883),0.64693093,tuner.lr_find(...),  add on fit_start on fit_end hooks   add on fit_start on fit_end hooks   add on fit_start on fit_end hooks ,0
6750,Set DistributedSampler seed if seed_everything was called (#7024),0.85439277,pl.seed_everything will now also set the seed on the DistributedSampler (#7024),  fix docs   Update docs/source/metrics.rst   Update docs/source/metrics.rst   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   Update docs/source/metrics.rst   Update docs/source/metrics.rst   Update metrics.rst   title   fix   fix for num_classes   chlog   nb classes   hints   zero division   add tests   Update metrics.rst   Update classification.py   Update classification.py   prune doctests   docs   Apply suggestions from code review   Apply suggestions from code review   flake8   doctests   formatting   cleaning   formatting   formatting   doctests   flake8   docs   rename   rename   typo   Co-authored-by: Nicki Skafte nugginea@gmail.com Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka jirka@pytorchlightning.ai Co-authored-by: edenlightning 66261195+edenlightning@users.noreply.github.com,1
6751,remove redundant methods in RichProgressBar (#11100),0.65048933,Remove EpochResultStore and HookResultStore in favor of ResultCollection (#7909),"  Configure isort   Fix whitespace   Line length, make THIRDPARTY the default ",0
6752,Bump max deepspeed version to 0.8.0 (#16469),0.7316749,DeepSpeed Stage 1,,1
6753,update python version (#6399),0.71014905,- Removed support for Python 3.7 ([#16579](https://github.com/Lightning-AI/lightning/pull/16579)),  fixes   fixes   fixes   fixes   fixes   fixes   fixes   fixes   fixes   fixes   fixes   fixes   fixes   fixes   fixes   fixes   fixes   fixes   fixes   fixes   Apply suggestions from code review   Co-authored-by: Nicki Skafte skaftenicki@gmail.com   add workers fix   add workers fix   add workers fix   add workers fix   add workers fix   add workers fix   add workers fix   add workers fix   add workers fix   add workers fix   Update docs/source/metrics.rst   Co-authored-by: Nicki Skafte skaftenicki@gmail.com  Update docs/source/metrics.rst  Co-authored-by: Nicki Skafte skaftenicki@gmail.com  Update docs/source/metrics.rst  Co-authored-by: Nicki Skafte skaftenicki@gmail.com  Update docs/source/metrics.rst  Co-authored-by: Nicki Skafte skaftenicki@gmail.com   add workers fix   add workers fix   add workers fix   doctests   add workers fix   add workers fix   fixes   fix docs   fixes   fixes   fixes   fixes   fixes   fixes   fixes   fixes   fixes   fixes   fixes   fixes   fixes   fixes   fixes   fixes   fixes   Apply suggestions from code review   Co-authored-by: Nicki Skafte skaftenicki@gmail.com   add workers fix   Update docs/source/metrics.rst   Co-authored-by: Nicki Skafte skaftenicki@gmail.com   doctests   add workers fix   fix docs   fixes   fixes   fix doctests   Apply suggestions from code review   fix doctests   fix examples   bug   Update docs/source/metrics.rst   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update docs/source/metrics.rst  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update docs/source/metrics.rst  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   fixes   fixes   fixes   fixes   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Jirka jirka@pytorchlightning.ai Co-authored-by: Nicki Skafte nugginea@gmail.com,1
6754,Update pl CPU testing matrix (#15312),0.6658132,The psutil package is now required for CPU monitoring (#17010),  save hparams to yaml   import   resolves   req   Update requirements/base.txt   Co-authored-by: Omry Yadan omry@fb.com Co-authored-by: Omry Yadan omry@fb.com,0
6755,[test] Accumulated gradient optimization tests (#4477),0.6596776,"            model, optimizers, opt_level=amp_level,",  epoch indexing from 1   chlog   fix tests   fix tests   self.min_epochs ,0
6756,Improve checkpoint docs (#10916),0.6679225,Checkpoint and early stopping now work without val. step (#1041),  reduce test warnings   Update test_trainer.py   Update test_trainer.py   Co-authored-by: William Falcon waf2107@columbia.edu,0
6757,release v0.7.2rc5,0.7455379,0.4.0,,1
6758,Sync our torchmetrics wrappers after the 0.4 release (#8205),0.68619585,"Unify sLURM/TorchElastic under backend plugin (#4578, #4580, #4581, #4582, #4583)",,0
6759,fix TypeError cause failure in singal_connector teardown (#10961),0.6067289,Improved the error message when the LightningWork is missing the run method (#14759),,0
6760,Update docs on arg train_dataloader in fit (#6076),0.636214,Changed Trainer arg and functionality from reload_dataloaders_every_epoch to reload_dataloaders_every_n_epochs (#5043)," squash  variant a variant b add test revert rename add changelog docs move changelog entry to top use hooks wip wipp layer summary clean up, refactor type hints rename remove obsolete code rename unused imports simplify formatting of table and increase readability doctest superclass object update examples print unknown sizes more docs and doctest testing unknown layers add rnn test remove main restore train mode test device wip device constant simplify model forward transfer return summary object in method extend tests fix summary for empty module extend tests refactor and added hook variant a variant b add test revert rename add changelog docs move changelog entry to top remove hardcoded string simplify test unknown shapes and all others comments for tests fix hparams attribute   update default   unused import   clean up   replace hardcoded strings   fix doctest   fix top/full   black   fix rnn test   fix rnn   update debugging docs   update docs typo update docs update docs   add changelog   extract constant   setter and getter   move parity models to test folder   parameterize mode ",0
6761,added copyright notices (#3062),0.50378203,@property,"  First attempt at auto-moving data for inference   Correct my copypaste errors   Correct for if device is CPU   Get rid of the WIP code I accidentally added   Add tests   Make tests more foolproof   Make sure we stick with pep8 formatting   Clarify docs a little   Apply suggestions from code review   Get everything working again hopefully   refactor and added hook   variant a variant b add test revert rename add changelog docs   move changelog entry to top   Move data transfer to utilities   Add back in warnings for autotransfer   Get rid of the test code I ended up accidentally commiting again   Add docs any changelog   Correct PR number in Changelog   Correct changelog   Update data.py   Update test_cpu.py   make a decorator   type hint   changelog   changelog   remove old function   import   test for decorator   fix test   remove old test   doctest   apply decorator directly   convert doctest to code block   prevent side effects in tests   fix merge   update forward docs   update docs   added docs in section ""deployment / prediction""   update changelog   Co-authored-by: Hengjian Jia henryjia18@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: William Falcon waf2107@columbia.edu",0
6762,Upgrade CI to PyTorch 1.13 (#15403),0.7262101,Drop PyTorch 1.9 support (#15347),  hide doctest decoration   Update docs/source/conf.py   Update docs/source/_static/copybutton.js   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
6763,timeout for tpu check (#4340),0.8468725,Increased TPU check timeout from 20s to 100s (#5598)," Add ckpt_path option to LightningModule.test()  If ckpt_path is ""best"" (default), it loads the best weights saved by ModelCheckpoint for the test loop. If ckpt_path is a path to a checkpoint file, it loads the weights from the file for the test loop. If ckpt_path is None, it uses the weights from the end of training for the test loop. If model parameter is set, ckpt_path is ignored.  Update test_set.rst  Co-authored-by: William Falcon waf2107@columbia.edu",1
6764,upload checkpoint files to neptune from stream (#17430),0.6344262,Enabled cp (upload) at project level (#16631),  add workers fix   add workers fix ,0
6765,Fixes .test() for ddp (#2570),0.66815746,Run ddp_spawn dataloader checks on Windows (#6930), Handle KeyboardInterrupt during training  Fixes #2079.   chlog   Fix whitespace   Update callback_hook.py   Update base.py   Update training_loop.py   Update test_trainer.py   Update CHANGELOG.md   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   Update CHANGELOG.md   on_keyboard_interrupt   Co-authored-by: Jirka jirka@pytorchlightning.ai Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
6766,Simplify GPU and TPU accelerator (#5024),0.7712496,Update Gradient Clipping for the TPU Accelerator (#6576),  log row might be a bottleneck depending on network. 50 unblocks this and is small enough for small datasets   log row might be a bottleneck depending on network. 50 unblocks this and is small enough for small datasets ,1
6767,Removes the old HPO content (#14754),0.63354445,Removed deprecated: (#2760),  accuracy_fix   fix line length   Apply suggestions from code review   Update test_classification.py   Co-authored-by: Nicki Skafte nugginea@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: William Falcon waf2107@columbia.edu,0
6768,rename integrations (#16312),0.61974084,Rename failed -> error in tables (#15608),  past checkpoints   omegaConf save   enforce type   resolve=True   Co-authored-by: Omry Yadan omry@fb.com   test omegaconf   tests   test past   Co-authored-by: Omry Yadan omry@fb.com,0
6769,Introduce base collective and main subclasses (#15016),0.7524679,"Base classes (#1326, #1877)",,1
6770,Add quick docs for deepspeed infinity (#8323),0.69021165,Support for manual optimization with DeepSpeed (#7970),  missed   format   math   req   notes   fix CI   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
6771,drop IPU docker for runner (#17583),0.56522137,"- When training with `precision=16` on IPU, the cast has been moved off the IPU onto the host, making the copies from host to IPU cheaper ([#13880](https://github.com/Lightning-AI/lightning/pull/13880))",  [trainer] Call prepare_data once per node in DDP/DDP2 training   refactored DDP routes   renamed proc_rank to local_rank   renamed proc_rank to local_rank   renamed proc_rank to local_rank   renamed proc_rank to local_rank   renamed proc_rank to local_rank   renamed proc_rank to local_rank   renamed proc_rank to local_rank   renamed proc_rank to local_rank   renamed proc_rank to local_rank   renamed proc_rank to local_rank   renamed proc_rank to local_rank   renamed proc_rank to local_rank   spawn message   spawn message   spawn message   fixes   fixes   fixes   fixes   fixes   Update trainer.py   Co-authored-by: ananthsub ananth.subramaniam@gmail.com,0
6772,Fix Checkpoint issue when using Horovod distributed backend (PyTorchLightning#6947) (#6958),0.7267962,- Fixed check for horovod module ([#12377](https://github.com/PyTorchLightning/pytorch-lightning/pull/12377)),  typo   typo ,1
6773,Remove outputs in on_train_epoch_end hooks (#8587),0.8323529,Removed the deprecated outputs argument in both the LightningModule.on_train_epoch_end and Callback.on_train_epoch_end hooks (#8587),  clean requirements   missing   missing   req   min   default >> base   base.txt ,1
6774,Fix on_train_batch_start hook to end epoch early (#3700),0.7633936,EarlyStopping now runs at the end of the training epoch by default (#8286),  fix loss param order   valid loss ,1
6775,CombinedLoader example fix (#9906),0.6046194,"combined_loader = CombinedLoader(iterables, mode=""min_size"")",,0
6776,Rename trainer._launch to trainer._run (#7265),0.78835654,Renamed several Trainer atributes:  (#567),  Create metric.py   Create utils.py   Create init.py   Create init.py   Create init.py   add tests for metric utils   add tests for metric utils   add docstrings for metrics utils   add docstrings for metrics utils   add function to recursively apply other function to collection   add function to recursively apply other function to collection   add tests for this function   add tests for this function   add tests for this function   update test   update test   Update pytorch_lightning/metrics/metric.py   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   update metric name   remove example docs   fix tests   fix tests   add metric tests   fix to tensor conversion   fix to tensor conversion   fix apply to collection   fix apply to collection   Update pytorch_lightning/metrics/metric.py   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   remove tests from init   remove tests from init   add missing type annotations   rename utils to convertors   rename utils to convertors   rename utils to convertors   rename utils to convertors   Update pytorch_lightning/metrics/convertors.py   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/metrics/convertors.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/metrics/convertors.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/metrics/convertors.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/metrics/convertors.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/metrics/convertors.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/metrics/metric.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update tests/utilities/test_apply_to_collection.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update tests/utilities/test_apply_to_collection.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update tests/utilities/test_apply_to_collection.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update tests/utilities/test_apply_to_collection.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update tests/metrics/convertors.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update tests/metrics/convertors.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Apply suggestions from code review  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Apply suggestions from code review  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Apply suggestions from code review  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   add doctest example   rename file and fix imports   rename file and fix imports   added parametrized test   added parametrized test   replace lambda with inlined function   rename apply_to_collection to apply_func   rename apply_to_collection to apply_func   rename apply_to_collection to apply_func   Separated class description from init args   Apply suggestions from code review   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   adjust random values   suppress output when seeding   remove gpu from doctest   Add requested changes and add ellipsis for doctest   Add requested changes and add ellipsis for doctest   Add requested changes and add ellipsis for doctest   forgot to push these files...   forgot to push these files...   forgot to push these files...   add explicit check for dtype to convert to   add explicit check for dtype to convert to   fix ddp tests   fix ddp tests   fix ddp tests   remove explicit ddp destruction   remove explicit ddp destruction   New metric classes (#1326)   Create metrics package   Create metric.py   Create utils.py   Create init.py   add tests for metric utils   add docstrings for metrics utils   add function to recursively apply other function to collection   add tests for this function   update test   Update pytorch_lightning/metrics/metric.py   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   update metric name   remove example docs   fix tests   add metric tests   fix to tensor conversion   fix apply to collection   Update CHANGELOG.md   Update pytorch_lightning/metrics/metric.py   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   remove tests from init   add missing type annotations   rename utils to convertors   Create metrics.rst   Update index.rst   Update index.rst   Update pytorch_lightning/metrics/convertors.py   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/metrics/convertors.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/metrics/convertors.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/metrics/metric.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update tests/utilities/test_apply_to_collection.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update tests/utilities/test_apply_to_collection.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update tests/metrics/convertors.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Apply suggestions from code review  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   add doctest example   rename file and fix imports   added parametrized test   replace lambda with inlined function   rename apply_to_collection to apply_func   Separated class description from init args   Apply suggestions from code review   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   adjust random values   suppress output when seeding   remove gpu from doctest   Add requested changes and add ellipsis for doctest   forgot to push these files...   add explicit check for dtype to convert to   fix ddp tests   remove explicit ddp destruction   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   add function to reduce tensors (similar to reduction in torch.nn)   add functionals of reduction metrics   add functionals of reduction metrics   add more metrics   pep8 fixes   rename   rename   add reduction tests   add first classification tests   bugfixes   bugfixes   add more unit tests   fix roc score metric   fix tests   solve tests   fix docs   Update CHANGELOG.md   remove binaries   solve changes from rebase   add eos   test auc independently   fix formatting   docs   docs   chlog   move   function descriptions   Add documentation to native metrics (#2144)   add docs   add docs   Apply suggestions from code review   formatting   add docs   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka jirka@pytorchlightning.ai   Rename tests/metrics/test_classification.py to tests/metrics/functional/test_classification.py   Rename tests/metrics/test_reduction.py to tests/metrics/functional/test_reduction.py   Add module interface for classification metrics   add basic tests for classification metrics' module interface   pep8   add additional converters   add additional base class   change baseclass for some metrics   update classification tests   update converter tests   update metric tests   Apply suggestions from code review   tests-params   tests-params   imports   pep8   tests-params   formatting   fix test_metrics   typo   formatting   fix dice tests   fix decorator order   fix tests   seed   dice test   formatting   try freeze test   formatting   fix tests   try spawn   formatting   fix   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: J. Borovec jirka.borovec@seznam.cz Co-authored-by: Xavier Sumba c.uent@hotmail.com Co-authored-by: Jirka jirka@pytorchlightning.ai Co-authored-by: Nicki Skafte nugginea@gmail.com,1
6777,nb. devices (#2973),0.70560837,    devices=1,  set ddp_spawn as default   spawn message   spawn message   spawn message   spawn message   spawn message   spawn message   spawn message   spawn message ,1
6778,Use torch.testing.assert_close everywhere (#15031),0.62521756,"Set PyTorch 1.4 as min requirements, also for testing and examples torchvision>=0.5 and torchtext>=0.5 (#5418)",  multi_gpu.rst   update lr_finder   Update docs/source/lr_finder.rst   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update docs/source/multi_gpu.rst  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update docs/source/multi_gpu.rst  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update docs/source/multi_gpu.rst  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update docs/source/multi_gpu.rst  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update docs/source/multi_gpu.rst  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update docs/source/multi_gpu.rst  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
6779,fix typo in docs migration 1_6_regular (#17186),0.50170887,Remove unnecessary intermediate layers in Dockerfiles (#5697),,0
6780,Reference the compatibility matrix (#17091),0.53388715,Configuration Validator (#9779),  remove deprecated API   chlog   times   missed   formatting check   missing   missing   miss   fix docs build error   fix pep whitespace error   docs   wip   amp_level   amp_level   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
6781,Make Trainer readable and debuggable (2/n) (#14862),0.71691257,Allow easy trainer re-instantiation (#7508),  Fix pyright member access errors in training module   Fix Trainer instantiation error due to inheritence order   Add GH workflow for pyright   Fix more pyright errors in trainer module   Add pyrightconfig and setup python environment in type-check workflow   Exclude pyrightconfig.json   suggestions   Co-authored-by: Jirka jirka@pytorchlightning.ai,1
6782,update rng state save/load test to also run on cuda gpu (#14396),0.60389674,- Included `torch.cuda` rng state to the aggregate `_collect_rng_states()` and `_set_rng_states()` ([#14384](https://github.com/Lightning-AI/lightning/pull/14384)),,0
6783,Disable training with zero num_training_batches when insufficient limit_train_batches (#5703),0.9540511,Disabled training with zero num_training_batches when insufficient limit_train_batches (#5703),,1
6784,Docs [Fix]: use bytes instead of strings while writing (#14505),0.57977724,    buffered = BytesIO(),  allow loading checkpoints from urls   tmpdir_server fixture   test cases for loading checkpoints from url   dir => root_dir   default map_location to None   test case for resume_from_checkpoint   changelog   doc update   monkeypatch TORCH_HOME to avoid caching   Use a threading server with random ports so that it is easier to clean up   test fixes   pep8 fix   ThreadingHTTPServer support in 3.6   pep8 fix   fix changelog   separate tests for urls   typo   Co-authored-by: Peter Yu 2057325+yukw777@users.noreply.github.com  Apply suggestions from code review  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
6785,Add logging messages to notify when FitLoop stopping conditions are met (#9749),0.86174524,- Added logging messages to notify when `FitLoop` stopping conditions are met ([#9749](https://github.com/Lightning-AI/lightning/pull/9749)),  Create utils.py   Create init.py   redo sklearn metrics   add some more metrics   add sklearn metrics   Create init.py   redo sklearn metrics   New metric classes (#1326)   Create metrics package   Create metric.py   Create utils.py   Create init.py   add tests for metric utils   add docstrings for metrics utils   add function to recursively apply other function to collection   add tests for this function   update test   Update pytorch_lightning/metrics/metric.py   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   update metric name   remove example docs   fix tests   add metric tests   fix to tensor conversion   fix apply to collection   Update CHANGELOG.md   Update pytorch_lightning/metrics/metric.py   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   remove tests from init   add missing type annotations   rename utils to convertors   Create metrics.rst   Update index.rst   Update index.rst   Update pytorch_lightning/metrics/convertors.py   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/metrics/convertors.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   add doctest example   rename file and fix imports   added parametrized test   replace lambda with inlined function   rename apply_to_collection to apply_func   Separated class description from init args   Apply suggestions from code review   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   adjust random values   suppress output when seeding   remove gpu from doctest   Add requested changes and add ellipsis for doctest   forgot to push these files...   add explicit check for dtype to convert to   fix ddp tests   remove explicit ddp destruction   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   add sklearn metrics   start adding sklearn tests   fix typo   return x and y only for curves   fix typo   add missing tests for sklearn funcs   imports   all   imports   fix sklearn arguments   fix imports   update requirements   Update CHANGELOG.md   Update test_sklearn_metrics.py   formatting   formatting   format   fix all warnings and formatting problems   Update environment.yml   Update requirements-extra.txt   Update environment.yml   Update requirements-extra.txt   fix all warnings and formatting problems   Update CHANGELOG.md   docs   inherit   docs inherit.   docs   Apply suggestions from code review   Co-authored-by: Nicki Skafte skaftenicki@gmail.com   docs   req   min   Apply suggestions from code review   Co-authored-by: Tullie Murrell tulliemurrell@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka jirka@pytorchlightning.ai Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Tullie Murrell tulliemurrell@gmail.com,1
6786,Add a check for TPU Spawn barrrier (#7241),0.57295096,Updated logic for checking TPUs availability (#6767),  cloudpickle   ci tests ,0
6787,[docs] Add Torch Distributed Run (#9890),0.68509525,Setting the torch-distributed backend,  miss chlog   miss chlog   docs   miss   formatting ,0
6788,license (#3423),0.4849048,"trainer/separate argparse (#3421, #3428, #3432)",,0
6789,slow tpu train (#2033),0.64730024,Speed up single-core TPU training by loading data using ParallelLoader (#2033),  [docs] Add Cotratron to community examples   Apply suggestions from code review   Co-authored-by: Nicki Skafte skaftenicki@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com,0
6790,docker: fix Torch url (#16927),0.63447773,- Fixed issue where the CLI fails with certain torch objects ([#13153](https://github.com/Lightning-AI/lightning/pull/13153)),,0
6791,drop deprecated fbeta metrics (#5322),0.8277701,"Removed deprecated Fbeta, f1_score and fbeta_score metrics (#5322)",,1
6792,Allow MLFlowLogger to work with mlflow-skinny (#16513),0.78065354,- Added support for running the `MLFlowLogger` with the `mlflow-skinny` package ([16513](https://github.com/Lightning-AI/lightning/pull/16513)),,1
6793,made fx public (#2247),0.6170914,Renamed xxx_AVAILABLE as protected (#5082),,0
6794,codeowner: update test/boring model (#5869),0.63168305,Deprecated the TestTubeLogger (#9065),,0
6795,Update trainer.rst (#4952),0.7342875,"Refactor RunningStage and TrainerState usage (#4945, #7173)",  Add torchvision and gym dependencies   Add pl_examples/requirements.txt to the list of dependencies for running local tests ,1
6796,Update issue and PR templates (#8528),0.5799124,Use correct python version in lightning component template (#13790),  Remove explicit flush from tensorboard logger   Update changelog ,0
6797,Remove the deprecated pl.loggers.base module (#16120),0.7744136,Removed the deprecated pytorch_lightning.loggers.base module in favor of pytorch_lightning.loggers.logger (#16120),,1
6798,Avoid Pillow 8.3.0 due to errors with numpy (#8234),0.5460635,Avoid raising the sampler warning if num_replicas=1 (#14097),  fixes early stopping bug   fixes early stopping bug   fixes early stopping bug   fixes early stopping bug   fixe docs   fixe docs   fixe docs   fixe docs   fixe docs   fixe docs   fixe docs   fixe docs   fixe docs   fixe docs   fixe docs   fixe docs   fixe docs   fixe docs   fixe docs   fixe docs   fixe docs   fixe docs   fixe docs   fixe docs   added test ,0
6799,allow codecov upload to fail (#4221),0.6394335,Enabled cp (upload) at project level (#16631),,0
6800,fix warning so the user has a clear next step (#16751),0.6722096,warning_utils >> warnings,  training batch clean up   training batch clean up   training batch clean up   training batch clean up   training batch clean up   training batch clean up   training batch clean up   training batch clean up   training batch clean up   training batch clean up   training batch clean up   training batch clean up   training batch clean up   training batch clean up   training batch clean up   training batch clean up   training batch clean up   training batch clean up   training batch clean up   training batch clean up   adding spawn   adding spawn   adding spawn   adding spawn   adding spawn   adding spawn   adding spawn   adding spawn ,0
6801,removed .egg,0.6393258,Removed,  training batch clean up   training batch clean up   training batch clean up ,0
6802,Refactor RunningStage usage in advance of implementing Trainer.validate() (#4945),0.9165864,"Refactor RunningStage and TrainerState usage (#4945, #7173)",,1
6803,Fix local rank zero casting (#2640),0.58739436,The default value of accumulate_grad_batches has changed from 1 to None (#9652).,  update readme with conda installation instruction   fix team header   bibtex spelling   Update README.md   Co-authored-by: William Falcon waf2107@columbia.edu,0
6804,Add test for compiling FSDP model in Fabric (#17394),0.60322475,Direct support for compiled models (#15922),  DictConf   inits   Apply suggestions from code review   Co-authored-by: Omry Yadan omry@fb.com   wip   wip   wip   wip   wip   wip   wip   wip   wip   wip   atrib   wip   wip   wip   added hparams test   wip   wip   wip   wip   wip   wip   wip   wip   wip   wip   wip   wip   wip   wip   wip   wip   wip   wip   wip   wip   wip   Update test_hparams.py   added hparams test   added hparams test   pep8   pep8   pep8   docs   wip   wip   clean   review @omry   Update docs/source/hyperparameters.rst   Co-authored-by: Omry Yadan omry@fb.com Co-authored-by: Omry Yadan omry@fb.com Co-authored-by: William Falcon waf2107@columbia.edu,0
6805,Update 8-bit optimizer docs (#15155),0.75429636,Refactored optimizer (#4658),  cleaning   optim imports   fix   typo   on   mergify ,1
6806,0.113,0.72253835,0.5.1,"trainer.fit uses the parameter val_dataloaders but in the documentation it is val_dataloader, which is invalid.",1
6807,docs: building releases (#16967),0.5880705,"docs for all Metrics (#2184, #2209)", Explain the default value for scheduler  Co-authored-by: Qinru Li q4li@eng.ucsd.edu,0
6808,release v0.3.4,0.81729746,0.4.0,"  fixed new amp bugs   fixed new amp bugs   fixed new amp bugs   try exit   larger dataset   full mnist   full mnist   trainer   assert   .05   .10, #4   5   5   5   refactor   abs diff   speed   speed   speed   speed   Co-authored-by: J. Borovec jirka.borovec@seznam.cz Co-authored-by: Jirka jirka@pytorchlightning.ai",1
6809,fix PR link (#2858),0.54189605,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ","  do not include local vars in auto collection   add test   add test for model with ""self"" renamed to ""obj""   skip decorator   changelog   changelog   update docs   remove obsolete child collection   generalize args, kwargs names   docs   also update varargs passed in   Revert ""also update varargs passed in""   This reverts commit 3d7a30dbee07a513ee13e1cc3e08ca5ccdb85734.  update test",0
6810,Prune metyrics: regression 9/n (#6637),0.63831997,Regression metrics (#2221)," black  Added throught black.toml other options are hard so far No caching for black github action Moved from black.toml to pyproject.toml Exclude not only yml but also yaml Update pyproject.toml Co-authored-by: Thomas Johansen thomasjo@gmail.com Update .github/workflows/code-formatting-check.yml mergify Remove formating check E231 error ignoring because of black formating Updated CONTRIBUTING to the master   Update .github/workflows/code-formatting-check.yml   Bump black to 19.10b0 version   resolved incorrect merge of CONTRIBUTING,   Black skipping string normalization   Minor fixes in CONTRIBUTING, two typos   Update setup.cfg   chlog   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka jirka@pytorchlightning.ai",0
6811,prune installation artifact (#15558),0.575318,Sanitize None params during pruning (#6836),  tests drop macOS py38   ignore single test   try freeze env   drop   drop   drop   drop   drop skips   imports   fix ,0
6812,Update multi_node_own_slurm_script.py,0.54456913,Slightly safer multi node (#15538),  increase acc   try 0.45   @pytest   @pytest   try .50   duration   pytest ,0
6813,Narrower CI timeouts (#15231),0.5857937,Enabled users to have more control over scaling out/in intervals (#16093),  tests drop macOS py38   ignore single test   try freeze env   drop   drop   drop   drop   drop skips   drop macOS py38   imports ,0
6814,past checkpoints (#2160),0.80553746,Resuming from checkpoints (#16167), refactor and added hook  variant a variant b add test revert rename add changelog docs   resolve merge duplication   overridden typo   fix test   tpu id   raise if TPU not available   re-use apply_to_collection function for parsing collections   comment   make utility function available to user   documentation   move changelog entry to top   fix tpu transfer call   fix call   remove hardcoded string   improve test   call model hook by default   Apply suggestions from code review   rename utility function   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
6815,Add support for CPU AMP autocast (#9084),0.5727782,enable_pl_optimizer=False by default to temporarily fix AMP issues (#5163)," Raise an error when lightning replaces an existing sampler  Currently, Trainer replaces the existing sampler with DistributedSampler if running distributing training and replace_sampler_ddp=True (default behaviour). If a user has configured an existing sampler, this would lead to widely different results if running a distributed vs non-distributed training. This PR fixes this by raising an Error if user has configured a sampler and uses replace_sampler_ddp=True. The recommended behavior from now on is to either remove the sampler or set replace_sampler_ddp=False   Fix tests   Simpler fix   Fix tests   Make inner method protected   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",0
6816,Add on_exception hook to documentation (#9365),0.72172177,An on_exception Callback hook has been added which allows the user to perform custom exception handling.,  fix grad norm formula   grad-norm tracker test   fixed seed and explicit rtol in grad norm tracking test   a docstring for grad-norms and forced cast to float of norm_type   support for inf-norm   renamed the grad norm test   docs   fixed language in docstring   Apply suggestions from code review   Co-authored-by: Jirka jirka@pytorchlightning.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
6817,Add CombinedLoader to the API reference (#17062),0.57497334,duplicate data interface definition up into DataHooks class (#3344),  merge multi-gpu docs   extend slurm docs   update links to elastic   format docs and type hints in distrib parts   reference multi-gpu/slurm in trainer args docs   fix doctest   typo   doctest   Apply suggestions from code review   Co-authored-by: Lucas Vazquez lucasgouvaz@gmail.com   wall time   Update docs/source/slurm.rst   Co-authored-by: Lucas Vazquez lucasgouvaz@gmail.com   fix title   update docs for weights summary   update changelog   Co-authored-by: Lucas Vazquez lucasgouvaz@gmail.com,0
6818,Simplify data fetching (#11466),0.68035066,Data-Loading improvements,,0
6819,Inspect correct function in wrap_init (#12716),0.5306809,refactored inner eval loop (#3141),"  use parallel loader   Revert ""use parallel loader""   This reverts commit ed6e7583   select tpu id for pl   condition if tpu_id is None   added info to changelog   Revert ""condition if tpu_id is None""   This reverts commit 1fb6e586  Apply suggestions from code review  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",0
6820,formatting @Will (#3719),0.47721455,@rohitgr7 @tchaton ,  fix bug_report template   article ,0
6821,Make default tqdm dict overridable (#749),0.7011665,"Moved the default tqdm_dict definition from Trainer to LightningModule, so it can be overridden by the user (#749)",  import   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
6822,refactor 1/n for v1.0.0 (#2704),0.70633775,"    version=""0.0.1"",", fix(wandb): use same logger on multiple training loops  New training loops reset step to 0 which would previously try to overwrite logs fix #2015  docs(changelog.md): add reference to PR 2055,1
6823,Fixes EarlyStopping With Precision=16 (#1996),0.6385666,EarlyStopping now runs at the end of the training epoch by default (#8286),  Fix domain_templates   Fix type of fake labels   type   args ,0
6824,Resolve collectives test issues (#15195),0.5349237,test_percent_check in favour of limit_test_batches,  replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   replace ddp spawn with subprocess   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix ,0
6825,[dockers] install nvidia-dali-cudaXXX (#4532),0.6651096,This required installing nvidia/apex,,0
6826,add more detail to tbptt example (#755),0.5204916,Truncated backpropagation through time (TBPTT) (#16172),This reverts commit bf39cb26c57f1fd5968420e99ba351a9e0df9541.,0
6827,Fix compiler support test (#15927),0.59347767,Enabling val/test loop disabling (#2692),"  Patch for issue 1815, which will allow EarlyStopping to work on precision=16   Added a whitespace to the end of the line so CICD can rerun. No reason for the latest macos test to have been cancelled.   Format. ",0
6828,[CI] Make the bot less aggressive (#9575),0.5469331,:robot: ," Add an additional attribute to ModelCheckpoint to keep track of the best model's path  Currently, only the best metric value is directly tracked. This new attribute will help in uses cases where the trained model needs to be used or tracked right after training.   Add small description and usage example to docs   Fix PEP8 issues   Fix doctest example   Fix expected output in doctest   Apply suggestions from code review   Show example as code block instead of doctest   Apply suggestions from code review   Update CHANGELOG.md   Rename ModelCheckpoint.best to ModelCheckpoint.best_model_score   Also rename ModelCheckpoint.best_model (added in this PR) to ModelCheckpoint.best_model_path, for consistency, and kth_best_model to kth_best_model_path.  Update pytorch_lightning/trainer/training_io.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Apply suggestions from code review  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Add warning when loading checkpoint from an old version  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",0
6829,ref: callback system and init ddp (1/n) (#3836),0.9316274,callback system and init DDP (#3836),"  :bug: fixed fake example type assigning and hparams arg   fixed GAN example to work with dp, ddp., ddp_cpu   Update generative_adversarial_net.py   Co-authored-by: William Falcon waf2107@columbia.edu",1
6830,pkg: include lite in PL (#14536),0.6086631,Moved accelerators and plugins to its legacy pkg (#5645),  ogc install   cleaned up tests   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix   hot fix ,0
6831,dp doesnt support amp with any setting,0.6516528,enable_pl_optimizer=False by default to temporarily fix AMP issues (#5163),  fix chlog   test for #1729   hist   update   Document use case of passing test dataloaders to Trainer.test() (#1992)   Issue 1990 Doc patch.   Codeblock directive.   Update to reflect current state of pytorch-lightning   Final grammar cleaning. I hope these commits are squashed.   Apply suggestions from code review   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: authman uapatira@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
6832,Set dataset attribute to MpDeviceLoader used in TPU Spawn (#10151),0.64994615,Implemented DataParallelPlugin._setup_model (#10010),,0
6833,removed hparams req,0.6217445,Removed, Update CONTRIBUTING.md  typos  Apply suggestions from code review  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
6834,prune warning & deprecation wrapper (#6540),0.693864,Sanitize None params during pruning (#6836),"  Fraceful shutdown on python interpreter exit   Update CHANGELOG.md   Update training_loop.py   Update training_loop.py   Update CHANGELOG.md   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   pep8, move to constant   Update training_loop.py   Update training_loop.py   Update training_loop.py   pep8, move to constant   pep8   timeout   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz",0
6835,Fix apply_to_collection(defaultdict) (#10316),0.5868821,Did not interfere with a default sampler (#1318),  Add source comments   Update training_tricks.rst ,0
6836,Break lazy accumulation of graphs (#12938),0.57312435,Update the logic to check for accumulation steps with deepspeed (#9826),,0
6837,updated to latest tt version,0.540688,Updated logic for checking TPUs availability (#6767),  code rule   Apply suggestions from code review   Co-authored-by: Jeremy Jordan 13970565+jeremyjordan@users.noreply.github.com  chlog  Co-authored-by: Jeremy Jordan 13970565+jeremyjordan@users.noreply.github.com Co-authored-by: Nicki Skafte skaftenicki@gmail.com,0
6838,Add link to TPU Pods tutorial. (#2477),0.6637311,TPU training (#2708),,0
6839,fix import failer (#12676),0.73020554,Module imports are now catching ModuleNotFoundError instead of ImportError (#9867),  unify tests   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
6840,Remove deprecated TrainerCallbackHookMixin (#14401),0.89587194,Removed the deprecated TrainerLoggingMixin class (#8609),  fixed undesired behaviour due to dict.fromkeys   a test for log length consistency   runtime-warn if no schedulers are configured   chlog   move   Co-authored-by: Jirka jirka@pytorchlightning.ai,1
6841,Merge pull request #21 from williamFalcon/r,0.5386539,"merge backends (#3476, #3477, #3478, #3480, #3482)",  Removing unecessary early stopping calls   Update CHANGELOG.md   Co-authored-by: Mateusz Pieniak mateusz.pieniak@evidenceprime.com Co-authored-by: William Falcon waf2107@columbia.edu,0
6842,typing: fix App's core API - Queues (#16948),0.5907128,- Fixed a bug when launching an app on multiple clusters ([#15226](https://github.com/Lightning-AI/lightning/pull/15226)),  fix default   formatting errors   update   flake8 ,0
6843,Decouple device parsing logic from Acc connector to Trainer (#8180),0.91221595,Decoupled device parsing logic from Accelerator connector to Trainer (#8180),This reverts commit d0ec11b9d656a185b10a113a08dfba3042b4da01.,1
6844,Resolve e2es V3 (#14153),0.54699457,except Exception as e:,  updated docs   added mixed   added mixed ,0
6845,fixes no val loader,0.63644457,Reset val_dataloader in tuner/batch_size_scaling (#9857),,0
6846,ref: final inner eval loop hooks (#3154),0.9763802,final inner eval loop hooks (#3154),,1
6847,Support BaguaStrategy with external implementation (#17029),0.6395574,"The Bagua Strategy is a deep learning acceleration framework that supports multiple, advanced distributed training algorithms with state-of-the-art system relaxation techniques. Enabling Bagua, which can be considerably faster than vanilla PyTorch DDP, is as simple as:",  filter valid args   error on unknown manual args   added test   changelog   update docs and doctest   simplify   doctest   doctest   doctest   better test with mock check for init call   fstring   extend test   skip test on 3.6 not working   Co-authored-by: William Falcon waf2107@columbia.edu,0
6848,remove deprecated reload_dataloaders_every_epoch from Trainer (#10481),0.8396484,- Removed deprecated `reload_dataloaders_every_epoch` from `Trainer` in favour of `reload_dataloaders_every_n_epochs` ([#10481](https://github.com/PyTorchLightning/pytorch-lightning/pull/10481)),,1
6849,readme (#1931),0.5940055,[1.4.5] - 2021-08-31," Fixes PyTorchLightning/pytorch-lightning#490  EarlyStopping should check the metric of interest on_validation_end rather than on_epoch_end.  In a normal scenario, this does not cause a problem, but in combination with check_val_every_n_epoch>1 in the Trainer it results in a warning or in a RuntimeError depending on strict.   Highlighted that ES callback runs on val epochs in docstring   Updated EarlyStopping in rst doc   Update early_stopping.py   Update early_stopping.rst   Update early_stopping.rst   Update early_stopping.rst   Update early_stopping.rst   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   Update docs/source/early_stopping.rst   fix doctest indentation warning   Train loop calls early_stop.on_validation_end   chlog   Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka jirka@pytorchlightning.ai",0
6850,gh: fix duplicate id in bug issue (#17495),0.5042753,Resolved a bug where the work statuses will grow quickly and be duplicated (#13970),  wip protected progress bar settings   remove callback attr from LRfinder   whitespace   changelog ,0
6851,Sort the arguments in the Trainer docs (#17047),0.57579905,"Trainer arguments: add_row_log_interval, default_save_path, gradient_clip, nb_gpu_nodes, max_nb_epochs, min_nb_epochs, nb_sanity_val_steps",  saves model every epoch   implement test for save_last   Update CHANGELOG.md   Update CHANGELOG.md   changes test description   Co-authored-by: Jeremy Jordan 13970565+jeremyjordan@users.noreply.github.com Co-authored-by: Jeremy Jordan 13970565+jeremyjordan@users.noreply.github.com,0
6852,Update test_pruning.py to use devices instead of gpus or ipus (#11341),0.66047335,Skipped testing with PyTorch 1.7 and Python 3.9 on Ubuntu (#11217),"  tests, fix logger bug and prepare data bug   add CHANGELOG.md   Co-authored-by: Nicki Skafte nugginea@gmail.com",0
6853,update isort config (#5335),0.6088029,This release fixes that core issue,,0
6854,refactoring setup (#6590),0.7821954,Refactoring,  update logger imports   pep8 fixes   pep8 ,1
6855,update chlog for future 1.1.3rc (#5242),0.63775146,0.4.0,  remove the need for hparams   remove the need for hparams   remove the need for hparams   remove the need for hparams   replace self.hparams   replace self.hparams   replace self.hparams   replace self.hparams   replace self.hparams   replace self.hparams   replace self.hparams   replace self.hparams   replace self.hparams   replace self.hparams   replace self.hparams   replace self.hparams   replace self.hparams   replace self.hparams   replace self.hparams   replace self.hparams   replace self.hparams   replace self.hparams   replace self.hparams   replace self.hparams   replace self.hparams   replace self.hparams   replace self.hparams   replace self.hparams   replace self.hparams   replace self.hparams   replace self.hparams   replace self.hparams   replace self.hparams   replace self.hparams   replace self.hparams   replace self.hparams   fixed   fixed   fixed   fixed   fixed   fixed   fixed   fixed   fixed   fixed   fixed   fixed   fixed   fixed   finished moco   basic   testing   todo   recurse   hparams   persist   hparams   chlog   tests   tests   tests   tests   tests   tests   review   saving   tests   tests   tests   docs   finished moco   hparams   review   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   hparams   overwrite   transform   transform   transform   transform   cleaning   cleaning   tests   examples   examples   examples   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   chp key   tests   Apply suggestions from code review   class   updated docs   updated docs   updated docs   updated docs   save   wip   fix   flake8   Co-authored-by: Jirka jirka@pytorchlightning.ai Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
6856,Remove unused argument model (#13454),0.7677513,Removed the deprecated model argument from ModelCheckpoint.save_checkpoint (#8688),  readme   add governance.rst   Co-authored-by: Nicki Skafte nugginea@gmail.com,1
6857,Graceful shutdown on python interpreter exit (#1631),0.6861522,Run graceful training teardown on interpreter exit (#1631),  fix user error produced by apex + scheduler combination   add changelog   added reinit to every configure_apex call   fix styling   Co-authored-by: Nicki Skafte nugginea@gmail.com,0
6858,Deprecate LightningModule.on_pretrain_routine_{start/end} (#12122),0.7725992,- Deprecated `LightningModule.on_pretrain_routine_start` and `LightningModule.on_pretrain_routine_end` hooks in favor of `on_fit_start` ([#12122](https://github.com/PyTorchLightning/pytorch-lightning/pull/12122)),  set min PT 1.3   circleCI   mergify   min   chlog   skip ,1
6859,added hook on_sanity_check_start,0.7971339,Deprecated the on_sanity_check_start hook in ModelHooks (#598),  suppress epub warn   fail on warn   suppress epub warn   fail on warn   disable epub   typo   remove obsolete suppress ,1
6860,update CI testing with pip upgrade (#2380),0.6060233,The psutil package is now required for CPU monitoring (#17010)," Allow dataloaders without sampler field present  Sometimes we have a custom dataloader that doesn't have a sampler, better to check that the field is there before reading it.  chlog  Co-authored-by: Jirka jirka@pytorchlightning.ai",0
6861,[feat] Support time-based checkpointing during training (#7515),0.6575803,Trainer now calls on_load_checkpoint() when resuming from a checkpoint (#1666),,0
6862,Fix truncated_bptt_steps hiddens detach() and improve docs (#8145),0.6254467,Removed the deprecated Trainer.truncated_bptt_steps in favor of LightningModule.truncated_bptt_steps (#8826),,0
6863,fix: get project (#17617),0.571591,Enabled cp (upload) at project level (#16631),  New metric classes (#1326)   Create metrics package   Create metric.py   Create utils.py   Create init.py   add tests for metric utils   add docstrings for metrics utils   add function to recursively apply other function to collection   add tests for this function   update test   Update pytorch_lightning/metrics/metric.py   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   update metric name   remove example docs   fix tests   add metric tests   fix to tensor conversion   fix apply to collection   Update CHANGELOG.md   Update pytorch_lightning/metrics/metric.py   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   remove tests from init   add missing type annotations   rename utils to convertors   Create metrics.rst   Update index.rst   Update index.rst   Update pytorch_lightning/metrics/convertors.py   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/metrics/convertors.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/metrics/convertors.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/metrics/metric.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update tests/utilities/test_apply_to_collection.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update tests/utilities/test_apply_to_collection.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update tests/metrics/convertors.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Apply suggestions from code review  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   add doctest example   rename file and fix imports   added parametrized test   replace lambda with inlined function   rename apply_to_collection to apply_func   Separated class description from init args   Apply suggestions from code review   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   adjust random values   suppress output when seeding   remove gpu from doctest   Add requested changes and add ellipsis for doctest   forgot to push these files...   add explicit check for dtype to convert to   fix ddp tests   remove explicit ddp destruction   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   move dtype device mixin to more general place   refactor to general device dtype mixin   add initial metric package description   change default to none for mac os   pep8   fix import   Update index.rst   Update ci-testing.yml   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   Update CHANGELOG.md   Update pytorch_lightning/metrics/converters.py   readme   Update metric.py   Update pytorch_lightning/metrics/converters.py   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka jirka@pytorchlightning.ai,0
6864,remove flake 8 (#3687),0.5928017,Removed deprecated: (#2760),  Remove NaNs from loss in LRFinder   np.isfinite   chlog   add test   chlog   Co-authored-by: Jirka jirka@pytorchlightning.ai,0
6865,Fix save/load/resume from checkpoint for DeepSpeed Plugin (#8397),0.7977103,- Added support for saving and loading DeepSpeed checkpoints through `Fabric.save/load()` ([#16452](https://github.com/Lightning-AI/lightning/pull/16452)),  fix codecov   upgrade codecov   upgrade codecov ,1
6866,Make examples runnable (#16981),0.6056346,Expose RunWorkExecutor to the work and provides default ones for the MultiNode Component (#15561),,0
6867,Added the function for downloading the badges locally and replace the url with downlaod path (#4250),0.55361414,Changed Checkpoint path parameter from filepath to dirpath (#1016),,0
6868,Call TrainingTypePlugin collective functions directly instead of going through the Accelerator  (#9677),0.877731,"The Trainer now calls TrainingTypePlugin collective APIs directly instead of going through the Accelerator reference (#9677, #9901)"," added tpu_id  added tpu_id to mixins   train on individual tpu   parallel loader if tpu_id is None   removed progress_bar_refresh_rate   chlog   replaced num_tpu_cores with tpu_cores   set tpu_id to None if int   changed num_tpu_cores to tpu_cores in docs   updated docs   updated init.py removed self.tpu_id for ParallelLoader   Update pytorch_lightning/trainer/init.py   check if tpu_cores is a list   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   xla device conditional   num_tpu_cores deprecation   removed duplicate warning   fixed pep8 error   Revert ""removed duplicate warning""   This reverts commit 8adb0a9b   deprecated api update   fixed recursion error   fixed tests   fixed flake errors   removed current_tpu_index   Update CHANGELOG.md   Update trainer.py   Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: William Falcon waf2107@columbia.edu",1
6869,Snapshot selected globals and restore them in spawned process (#13921),0.6163291,- Fixed an issue causing deterministic algorighms and other globals to get reset in spawned processes ([#13921](https://github.com/Lightning-AI/lightning/pull/13921)),  fix trainer.test()   Update trainer.py   Co-authored-by: William Falcon waf2107@columbia.edu,0
6870,Add Windows Support for DeepSpeed (#8488),0.7177353,Support for manual optimization with DeepSpeed (#7970),,1
6871,Reduce flakiness of memory test (#8651),0.7002935,Resolve memory leak for evaluation (#6326),  Fixed the default value of auto_lr_find in docs   Update lr_finder.rst   Co-authored-by: William Falcon waf2107@columbia.edu,1
6872,Update requests requirement from <=2.28.1 to <2.28.3 in /requirements (#16547),0.56799287,Refactor cloud dispatch and update to new API (#16456),  Adding a new section to the docs: Example Lightning Project Structures   Update index.rst   Update index.rst   Co-authored-by: William Falcon waf2107@columbia.edu,0
6873,fixed clip grad warning,0.59387714,warning_utils >> warnings, Add flag to dump_checkpoint for only including weights  ModelCheckpoint then passes self.save_weights_only to the save function.   Fix tests and add changelog entry   Add check and descriptive message when training state is restored from a weights only checkpoint   Also add a test for making sure ModelCheckpoint.save_weights_only works as expected.   Fix weights-only test to properly match expected exception   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
6874,simplify storage import  (#14638),0.5819453,Wrapped imports for traceability (#13924),  remove kwargs   remove useless test   rename unknown trainer flag   trainer inheritance and test   blank line   test for unknown arg   changelog ,0
6875,Caching MNIST dataset for testing (#917),0.57647145,"for data: val_dataloader, test_dataloader, train_dataloader",  miss   miss   miss   update   format ,0
6876,Error messages for removed DataModule hooks (#15072),0.74825436,Removed deprecated model hooks (#3980),  Fix test configuration check and testing   Fix test configuration check and testing   Remove check_testing_configuration during test   Fix docstring   fix function name   remove conflicts ,1
6877,removed hparams assignment example (#7639),0.6617204,Deprecated tags_csv in favor of hparams_file (#1271),  remove unused device attribute   dtype   move on_gpu to model ,0
6878,Update mlflow with using resolve_tags (#6746),0.9873226,Updated mlflow with using resolve_tags (#6746),,1
6879,Ignore step param in Neptune logger's log_metric method (#5510),0.97832763,Ignored step param in Neptune logger's log_metric method (#5510),  release 0.7.6rc2   release 0.7.6   include img   smaller image   missing   miss   miss   miss   up ,1
6880,Assert availability via imports,0.73105395,import argparse,,1
6881,[App] Add annotations endpoint (#16159),0.5159203,Precision Plugins (#5718),  Update distrib_parts.py   Update CHANGELOG.md ,0
6882,Shubhamagarwal92 master (#1349),0.50885546,"trainer/separate argparse (#3421, #3428, #3432)",  extend arg parser   flake8   tests   example   fix test ,0
6883,Make codecov patch threshold 5%,0.5927475,- Code coverage (99%),,0
6884,[TPU] Remove CPU conversion of metrics (#17572),0.6300005,Enabled manual optimization for TPUs (#8458),  add warn   Apply suggestions from code review ,0
6885,Fix reset_seed() converting the PL_SEED_WORKERS environment variable str read to bool (#10099),0.5729499,seed_everything now fails when an invalid seed value is passed instead of selecting a random seed (#8787),Co-authored-by: Nicki Skafte nugginea@gmail.com,0
6886,Fix ModelCheckpoint not being fixable.,0.7757983,Replace MisconfigurationException with warning in ModelCheckpoint Callback (#4560),  missing pkg   update CI   strict RTD   strict RTD   make   missing   ignore   ignore   mock   typo ,1
6887,rename app _exit 2/2 (#16398),0.53186154,Application storage prefix moved from app_id to project_id/app_id (#14583),  Update unet.py   Update semantic_segmentation.py ,0
6888,Activation checkpointing in FSDP without boilerplate (#15826),0.65485746,Deprecated checkpoint_callback from the Trainer constructor in favor of enable_checkpointing (#9754),,0
6889,enable testing DDP examples (#4995),0.7237468,DDP Debugging Improvements,  debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug ,1
6890,[bugfix] Re-compute accumulated_grad_batches (#8493),0.5935071,Tons of bug fixes,  :sparkles: Use store_true for bool args   debug   Co-authored-by: Nate Raw nxr9266@g.rit.edu,0
6891,Group trainer call methods as functions (#16702),0.66155773,"trainer = Trainer(callbacks=GradientAccumulationScheduler({""1"": 5, ""10"": 3}))",,0
6892,Use apply_to_collection in metrics_to_scalars (#7888),0.80038106,Changed metrics_to_scalars to work with any collection or value (#7888),,1
6893,removed validation call,0.6705647,"Removed the on_epoch guard from the ""should stop"" validation check (#7701)",The intention of the code is to output a warning message when hparams is null or not set. Instead the code now fatals when model.hparams = None. Prevent that.,0
6894,[bug] All_gather support tensor on cpu (#6416),0.64446986,Accelerator all_gather supports collection (#5221),,0
6895,Fix minor typos in sharing_components.rst (#16468),0.5153621,Fixing critical bugs in newly added hooks and hparams assignment.,  fix suggestion being too naive   fix accumulation error and added new tests   fix styling   update CHANGELOG.md   update based on review   fix tests   Apply suggestions from code review   Apply suggestions from code review   Apply suggestions from code review   Apply suggestions from code review   Co-authored-by: Nicki Skafte nugginea@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
6896,lightning typo (#2228),0.7576377,LightningLite:,"The changes are quite local and limited in nature -- viz., checking for some indicator environment variables. We check for (SLURM_LOCALID, NODE_RANK, GROUP_RANK) in order. If multiple are found set, a warning is logged. This patch also fixes a minor bug with comparing the WORLD_SIZE environment variable. This can be a string type.",1
6897,updated trainer docs (#4570),0.72550386,"Refactor RunningStage and TrainerState usage (#4945, #7173)",  Update README.md   Update README.md   committed suggestion Co-authored-by: William Falcon waf2107@columbia.edu  Update README.md  Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com  Update README.md  Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,1
6898,added model for tests,0.7529825,- Full tests that run multiple models in different configs,  Add support for hierarchical dict   Support nested Namespace   Add docstring   Migrate hparam flattening to each logger   Modify URLs in CHANGELOG   typo   Simplify the conditional branch about Namespace   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update CHANGELOG.md  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   added examples section to docstring   renamed _dict -> input_dict   mata_tags.csv -> hparams.yaml   code style fixes   add pyyaml   remove unused import   create the member NAME_HPARAMS_FILE   improve tests   Update tensorboard.py   pass the local test w/o relavents of Horovod   formatting   update dependencies   fix dependencies   Apply suggestions from code review   add savings   warn   docstrings   tests   Apply suggestions from code review   saving   Apply suggestions from code review   use default   remove logging   typo fixes   update docs   update CHANGELOG   clean imports   add blank lines   Update pytorch_lightning/core/lightning.py   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update pytorch_lightning/core/lightning.py  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   back to namespace   add docs   test fix   update dependencies   add space   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,1
6899,Add CSVLogger for Lightning Lite (#16346),0.77075744,LightningCLI V2,  added override for hparams in load_from_ckpt   override hparams   override hparams   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   update doctest   typo   chlog   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka jirka.borovec@seznam.cz,1
6900,Fix usage of fs.listdir in CheckpointConnector (#15413),0.74141127,Refactor load in checkpoint connector (#4593),  device property   add/copy properties   inherit   rename   Apply suggestions from code review   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com   dtype   prop   pt api   Co-authored-by: Justus Schock 12886177+justusschock@users.noreply.github.com,1
6901,Fix attribute error for _gpus_arg_default loading checkpoint prior to 1.2.8 (#7043),0.6172204,Enable None model checkpoint default (#3669),,0
6902,Force keyword-only usage to init Fabric (#17023),0.520205,"2. Instantiate Fabric directly, without subclassing",,0
6903,docs refactor 3/n (#12795),0.6434957,Docs improvements,,0
6904,Support inspecting the signature of decorated hooks (#17507),0.55611366,New Hooks,,0
6905,Deprecate write_predictions on the LightningModule (#7066),0.8821185,Deprecated LightningModule.write_predictions and LightningModule.write_predictions_dict (#7066),,1
6906,Fixed wrong on_fit_start callback reference in documentation (#7001),0.6341307,Replace MisconfigurationException with warning in ModelCheckpoint Callback (#4560),  Fixed typing annotation by adding boolean type. After that Profiler flag will be added to argparse.   Updated CHANGELOG.md   Updated git_init_arguments_and_types() to pass doctests.   Added doctest example to add_argparse_parser() ,0
6907,missing changes in chlog (#2430),0.54032594,Corrected call to torch.no_grad (#5124),,0
6908,Improve SWA docs (#8717),0.6921611,Docs improvements,,0
6909,Add LambdaCallback (#5347),0.57793593,Changed computer_vision_fine_tunning example to use BackboneLambdaFinetuningCallback (#5377),,0
6910,changed examples scripts,0.60305256,Syntax changes are: ,,0
6911,Add Strategy page to docs (#11441),0.58166075,Updated governance docs,,0
6912,Lightning App Fixes from Training Studio App dev (#14532),0.75912416,Improved the error message when the root LightningFlow passed to LightningApp is missing the run method (#14760)," Option to provide seed to random generators to ensure reproducibility  I added small function in utilities which imports torch, numpy, python random and sets seed for all of the libraries to ensure reproducibility of results.   Apply recommendations from core contributors on seeding   Moved the seeding code to another file  Make deterministic as a parameter for trainer class Add assertions for seeding numpy Added warnings  torch.manual_seed should be enough for seeding torch   Revert ""Apply recommendations from core contributors on seeding""   This reverts commit a213c8e6882eec8a9e7408b9418926d2db7c5461.  Revert ""Revert ""Apply recommendations from core contributors on seeding""""  This reverts commit 59b2da53c62878de7aab0aa3feb3115e105eea06.   Change in test, for correct seeding   Allow seed equal to 0   Allow seed to be uint32.max   Added deterministic to benchmarks   Cuda manual seed as in benchmark seeding   Seeding should be done before model initialization   cuda manual_seed is not necessary   Fixing seed test_cpu_lbfgs   On some seeds seems like lbfgs doesn't converge. So I fixed the seed during testing.   rebasing issue with old reproducibility.py   Improved documentation and ability to seed before initializing Train class   Change in docs   Removed seed from trainer, update for documentation   Typo in the docs   Added seed_everything to all   Fixing old changes   Model initialization should be earlier then Trainer   Update pytorch_lightning/trainer/init.py   From Example to testcode Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   Fixing according to the contributors suggestions   Moving horovod deterministic to Trainer class   deterministic flag affects horovod docs update   Improved static typing   Added deterministic to test runners of horovod   It is failing on some versions, not very predictable   static seeds for horovod tests   Change for reset_seed function in tests   Seeding horovod using reset_seed from tutils   Update pytorch_lightning/trainer/init.py   chlog   Update trainer.py   change ""testcode"" to ""Example"" in trainer init documentation   Update pytorch_lightning/trainer/seed.py, first line in comment   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz Co-authored-by: William Falcon waf2107@columbia.edu",1
6913,Fix Multi-GPU join for horovod (#6954),0.65889245,"refactored Horovod backend (#3121, #3122)",,0
6914,Add automatic optimization property setter to lightning module (#5169),1.0000001,Add automatic optimization property setter to lightning module (#5169),,1
6915,fix version + yapf (#6999),0.61739135,This release fixes that core issue,,0
6916,Remove result serialization (under fault tolerance) (#16516),0.6748538,"Removed experimental fault-tolerance support (#16516, #16533)",,0
6917,Remove epoch from trainer.logged_metrics (#9904),1.0,Remove epoch from trainer.logged_metrics (#9904),  add metric logging   Use pytorch built-in method   Update tensorboard.py   Update tensorboard.py ,1
6918,Remove deprecated is_overridden(model=...) (#10507),0.76044834,Deprecated is_overridden(model=...) in favor of is_overridden(instance=...) (#7918),  gh act - if   gh act - if   gh act - steps   gh act - steps   gh act - steps   name   name   reorder   docker   timeout   repo   show   show   ver   rc   tag ,1
6919,CI: Azure - multiple configs (#12984),0.6007557,"Working with multiple dataloaders (#16800, #16753)",  made ddp the default if no backend specified with multiple GPUs   fix   spawn   Co-authored-by: Jirka jirka.borovec@seznam.cz,0
6920,Enable PyTorch 1.7 in conda CI (#3541),0.82982326,Enable PyTorch 1.7 compatibility (#3541),  Join Horovod workers at the end of trainer.fit() to prevent race conditions following training   flake8   flake8   Co-authored-by: Jirka jirka.borovec@seznam.cz,1
6921,Fix to avoid val progress bar disappear after validate (#11700),0.7254811,Run main progress bar updates independent of val progress bar updates in TQDMProgressBar (#12563),  fixed native amp + ddp   fixed native amp + ddp ,1
6922,Remove TrainingTypePlugin.on_save and Accelerator.on_save (#9023),0.98233134,Removed TrainingTypePlugin.on_save and Accelerator.on_save (#9023),  move logging config to trainer class init   alternate logging config ,1
6923,Docs for Collaborative Training (#12996),0.621542,A Collaborative Training strategy using Hivemind,  added self.device   added docs ,0
6924,ref: adding compute environments (2/n) (#3842),0.9190527,"adding compute environments (#3837, [#3842)",  Removed test_dataloader call   Check if test_dataloader is actually overriden   Fixed method spelling   Replaced lambdas   Replaced None with super method   Fixed testpass ,1
6925,Add back-compatibility for checkpoint io plugins in pl/plugins/io (#14519),0.6964088,CheckpointIO Plugins,  dataloaders with fast_dev_run   dataloaders with fast_dev_run   dataloaders with fast_dev_run   fix   pep 8 ,0
6926,Add circle CI for building PyTorch 1.1/1.2/1.3 (#502),0.6199026,Enable PyTorch 1.7 compatibility (#3541),  missing   RC   tol   Apply suggestions from code review   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  test  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
6927,"Revert ""Support limit_mode_batches (int) for infinite dataloader"" (#2839)",0.75173193,"Refactor dataloading, supports infinite dataloader (#955)",Saving was introduced in #1561.,1
6928,flake8,0.46910247,Mixed precision overhaul (#16783),  fix val dataloader   Update evaluation_loop.py ,0
6929,fix mypy typing errors in pytorch_lightning/strategies/ddp2.py (#13535),0.7015531,+ # pytorch_lightning==1.7.0,,1
6930,Fix broken link in Examples Readme (#327),0.52574414,Fixing critical bugs in newly added hooks and hparams assignment.,,0
6931,update for v1.1.2 (#5240),0.73073643,"    version=""0.0.1"",",,1
6932,Refine debugging doc (#11390),0.653947,DDP Debugging Improvements,  Fix lr key name in case of param groups   Add tests   Update test and added configure_optimizers__param_groups   Update CHANGELOG ,0
6933,Fix docs to fix #11081 (#11229),0.5807549,Fix reset TensorRunningAccum (#5106),  Update and rename docker_builds.yml to docker_nightly_builds.yml   Update and rename docker_nightly_builds.yml to docker_builds.yml   Update docker_builds.yml   Update .github/workflows/docker_builds.yml   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
6934,Fix: Allow hashing of metrics with lists in their state (#5939),0.6357885,Changed metrics_to_scalars to work with any collection or value (#7888),,0
6935,"Update jinja2 requirement from <3.1.0,>=3.0.0 to >=3.0.0,<3.2.0 in /requirements (#16546)",0.5298515,Obligatory post 1.0 minor release. Main fix is to make Lightning module fully compatible with Jit (had some edge-cases we had not covered).,training_dataloader -> train_dataloader Co-authored-by: Alexander Kreuzer alexander.kreuzer@sap.com,0
6936,unify extras & minor CI cleaning: move env. var (#15942),0.5709156,some minor cleaning,  group argument wandb   formatting fix ,0
6937,ref: train loop refactor (mid-step) (#3310),0.80102134,"train loop refactor - moving train loop to own object (#3310, #3312, #3313, #3314)",  wip   cleaning   optim imports   -   default hparams   fix restore   fix imports ,1
6938,prune unused methods (#5860),0.664984,Sanitize None params during pruning (#6836),  auto batch finder   fix styling   add description   add different modes   fix copy paste error   better organised code   fix styling   add tests   fix   fix   add some documentation   added CHANGELOG.md   some documentation   update based on review   Update trainer.py   Update docs/source/training_tricks.rst   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com  Update tests/trainer/test_trainer_tricks.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Update tests/trainer/test_trainer_tricks.py  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Apply suggestions from code review  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com   use EvalModelTemplate   param tests   rename   wrap params   rename function   rename   rename param   fix   abs   rename   refactor code   add docs   try   arg   loop   exept   loop   drop bool   docs   docs   added check and test for passing dataloader to fit   styling fix   update based on review   Co-authored-by: Nicki Skafte nugginea@gmail.com Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka jirka.borovec@seznam.cz,0
6939,Update README.md (#1798),0.45833364,0.4.0,  update prog. bar metrics on train epoch end   changelog   wip test   more thorough testing   comments   update docs   move test   Co-authored-by: Jirka jirka.borovec@seznam.cz,0
6940,Make dims a property in datamodule (#3547),0.6150232,Support shorthand notation to instantiate datamodules (#10011),  added warning for None dataloader   fixed variable style   updated warning message   remove unused import   Co-authored-by: ybrovman ybrovman@ebay.com,0
6941,Start accumulate gradients schedule at epoch 0 (continued) (#2513),0.7001213,Reset current progress counters when restarting an epoch loop that had already finished (#9371),,1
6942,Fixed link to trainer.py github code (#386),0.64364123,- Deprecated `Trainer.should_rank_save_checkpoint` Trainer property ([#11068](https://github.com/PyTorchLightning/pytorch-lightning/pull/11068)),,0
6943,track batch size (#2954),0.7079988,Batch Size Finder (#11089),,1
6944,[Docs] Warning on metric states (#4388),0.73797405,"docs for all Metrics (#2184, #2209)",  mock all packages on RTD   update ,1
6945,Mark trainer.config_validator as protected (#9779),0.6967113,Configuration Validator (#9779),  removed if dl from _reset_eval_dataloader()   changed to if dl != None to be more safe   hints from pep8speaks   Co-authored-by: ybrovman ybrovman@ebay.com,0
6946,Add missing docstring for LightningWork.stop() (#13368),0.70918053,Improved the error message when the LightningWork is missing the run method (#14759),  improve pickle tests for callbacks   set mode dict as a class attr ,1
6947,"[3/3] Update lightning callbacks to Stateful, deprecations for old on_save/load_checkpoint signatures (#11887)",0.72339225,- Removed support for returning a value in `Callback.on_save_checkpoint` in favor of implementing `Callback.state_dict` ([#14835](https://github.com/Lightning-AI/lightning/pull/14835)),,1
6948,[Feat] Add utilities for CombinedLoader state dict and dataloader state dict 1/n (#8364),0.6700335,    return Dataloader(ds),,0
6949,Fix torch.distributed._sharded_tensor DeprecationWarning (#13261),0.67547864,- Fixed issue where the CLI fails with certain torch objects ([#13153](https://github.com/Lightning-AI/lightning/pull/13153)),  refactor default model   drop redundant seeds   refactor dataloaders tests   fix multiple   fix conf   flake8   Apply suggestions from code review   Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: William Falcon waf2107@columbia.edu,0
6950,Fix pre-commit isort failure on tests/callbacks/*.py (#5428),0.65744644,Dropped official support/testing for PyTorch <1.6 (#8288),"  add doctest to circleci   Revert ""add doctest to circleci""   This reverts commit c45b34ea911a81f87989f6c3a832b1e8d8c471c6.  Revert ""Revert ""add doctest to circleci""""  This reverts commit 41fca97fdcfe1cf4f6bdb3bbba75d25fa3b11f70.   doctest docs rst files   Revert ""doctest docs rst files""   This reverts commit b4a2e83e3da5ed1909de500ec14b6b614527c07f.   doctest only rst   doctest debugging.rst   doctest apex   doctest callbacks   doctest early stopping   doctest for child modules   doctest experiment reporting   indentation   doctest fast training   doctest for hyperparams   doctests for lr_finder   doctests multi-gpu   more doctest   make doctest drone   fix label build error   update fast training   update invalid imports   fix problem with int device count   rebase stuff   wip   wip   wip   intro guide   add missing code block   circleci   logger import for doctest   test if doctest runs on drone   fix mnist download   also run install deps for building docs   install cmake   try sudo   hide output   try pip stuff   try to mock horovod   Tranfer -> Transfer   add torchvision to extras   revert pip stuff   mlflow file location   do not mock torch   torchvision   drone extra req.   try higher sphinx version   Revert ""try higher sphinx version""   This reverts commit 490ac28e46d6fd52352640dfdf0d765befa56988.   try coverage command   try coverage command   try undoc flag   newline   undo drone   report coverage   review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   remove torchvision from extras   skip tests only if torchvision not available   fix testoutput torchvision   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",0
6951,Add LightningLite to README (#11164),0.7726147,LightningLite:,  move generated files to subfolder   remove if exists   reformat argv   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   update rebase   rebase yml   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: J. Borovec jirka.borovec@seznam.cz,1
6952,running ddp tests,0.72486115,DDP Debugging Improvements,  refactor default model   drop redundant seeds   path   refactor callback tests   update   fix sch   wip   fix return   review ,1
6953,Fixes #4141 (#4169),0.6222663,Resolve bug with Finetuning (#5744),  lr   optim   wip   wip   fix mean   flake8 ,0
6954,[App] Introduce Bi-directional queues (#15582),0.55803174,App,  Fix Horovod backend to disable progress bar on all ranks except 0   Add join barriers   Added changelog   Make protected and add verbosity   Refactor to disable progress bar callback in train   Removed vebose setting   Add cache check for Horovod   Test run again   Updated comment   Always skip cache for Horovod   Only reinstall when necessary   Added separate step   Fixed spacing   Skip Python 3.8 ,0
6955,Update changelog for recent releases (#7387),0.70500535,Full Changelog,parser.parse_known_args() actually returns a tuple of the Namespace of known args and a list of unknown args. We only want the former.,1
6956,Update to latest logging format and modify the accuracy method. (#4816),0.7272899,Refactored logging,  fix early stopping bug   allow val dataloader   update CHANGELOG.md   fix early stopping bug   allow val dataloader   update CHANGELOG.md   Co-authored-by: Nicki Skafte nugginea@gmail.com,1
6957,Add error handling for all trainer entry points (#8819),0.6912737,Trainer is now raising a MisconfigurationException instead of a warning if Trainer.{validate/test} is missing required methods (#10016),  refactor default model   drop redundant seeds   drop redundant seeds   refactor models tests   refactor models tests   imports   fix conf   Apply suggestions from code review ,0
6958,Fix reference link in Benchmarking doc (#12486),0.5371393,Prevent modification of torch.backends.cudnn.benchmark when Trainer(benchmark=...) is not set (#13154),  update typehints   change log ,0
6959,Fix conda badge in README (#17345),0.4857641,Porting fixes to autoscaler component (#16249),  refactor default model   drop redundant seeds   path   refactor loggers tests   imports ,0
6960,Prune EvalModelTemplate (#11153),0.5800613,Refactored EpochResultStore (#5522),,0
6961,[1/2] Collaborative Strategy (#12842),0.69297314,    * Added `CollaborativeStrategy` ([#12842](https://github.com/Lightning-AI/lightning/pull/12842)),  refactor trainer checks   opt   none   Apply suggestions from code review   imports   fix tensors ,0
6962,changelog (#1643),0.8216274,Complete changelog,  reduce if <= num_gpus   add test with explanation   chlog   fix changelog   Co-authored-by: J. Borovec jirka.borovec@seznam.cz,1
6963,Fix default ckpt path when logger exists (#771),0.69241405,Return the default_root_dir as the log_dir when the logger is a LoggerCollection (#8187),  Fine tuning example.   Fix (in train method) + Borda's comments (added argparse + fixed docstrings).   Updated CHANGELOG.md   Fix + updated docstring.   Fixes (awaelchli's comments) + docstrings.   Fix train/val loss.   Fix. ,0
6964,Update mypy job to torch 2.0 (#16933),0.6611302,PyTorch 2.0 and torch.compile,  fix typo   Typo   typo Borda   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
6965,Update init.py,0.5541612,PyTorch 1.5  support,"  Removed unnecessary 'global_step' from wandb logger.   Fixed wrong step implementation in wandb and missing metric skipping in logger base.   simplified metric check in base logger   Added Fix Description in CHANGELOG.md   Updated wandb logger tests.   udpate test, step=3   Moved Fix Description in CHANGELOG.md to unreleased.   Update CHANGELOG.md   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",0
6966,Fix install latest version of app/component (#14181),0.82733786,Resolved a bug where the install command was not installing the latest version of an app/component by default (#14181),  change module params to dict   tiny change   reverse ,1
6967,Corrected f_beta computation (#4183),0.61628366,Renamed class metric Fbeta >> FBeta (#4656),  fix LightningTemplateModel   update CHANGELOG.md   update LightningTemplate   update changelog   update changelog   loss fix ,0
6968,Parametrize fit hook test with different precision plugins (#8070),0.66229784,Precision Plugins (#5718),  Update docker_builds.yml   Update docker_builds.yml   nightly   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
6969,Update README and approval config (#13311),0.5452691,Refactor load in checkpoint connector (#4593),  refactor default model   drop redundant seeds   formatting   path   formatting   rename ,0
6970,Add bagua support for CUDA 11.6 images (#14529),0.5116012,Add --app_args support from the CLI (#13625)," Trigger automatic rebase on issue comment  Instead of pull_request event (created, closed, etc.). Fixes https://github.com/cirrus-actions/rebase/issues/43  Removed workaround",0
6971,Add support for missing return obj from to function on custom batch objects (#8433),0.55567586,Removed callback metrics from test results obj (#2994),  params   drop acc   Fix Horovod distributed backend to set the root_gpu   Fixed test   Fixed tests   Fixed lint   Set root_gpu during initialization   chlog   Co-authored-by: Jirka jirka.borovec@seznam.cz,0
6972,added arg docs,0.66616416,argparse_utils >> argparse,  move unnecessary dict trainer_options   fix tests   fix tests   formatting   missing ,0
6973,Fix typing in pl.callbacks.timer (#10798),0.63104624,Removed deprecated early_stop_callback (#3982),  params   drop acc   acc ,0
6974,update PR template (#2965),0.5918826,Use correct python version in lightning component template (#13790),  base implementation   docs + implementation   fix styling   add lr string   renaming   CHANGELOG.md   add tests   Apply suggestions from code review   Co-Authored-By: Adrian Wälchli aedu.waelchli@gmail.com   Apply suggestions from code review   Update pytorch_lightning/callbacks/lr_logger.py   Update pytorch_lightning/callbacks/lr_logger.py   add test for naming   base implementation   docs + implementation   fix styling   add lr string   renaming   CHANGELOG.md   add tests   Apply suggestions from code review   Co-Authored-By: Adrian Wälchli aedu.waelchli@gmail.com   Apply suggestions from code review   Update pytorch_lightning/callbacks/lr_logger.py   Update pytorch_lightning/callbacks/lr_logger.py   add test for naming   Update pytorch_lightning/callbacks/lr_logger.py   Co-Authored-By: Adrian Wälchli aedu.waelchli@gmail.com   suggestions from code review   fix styling   rebase   fix tests   Co-authored-by: Nicki Skafte nugginea@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
6975,Fix setup-doc for pypi (#472),0.69475484,Enable PyTorch 1.7 compatibility (#3541),  Don't convert namedtuple to tuple   Test namedtuples sent to device correctly ,0
6976,update package info (#768),0.5616598,Key updates,  added warning to crash   formatting   Co-authored-by: J. Borovec jirka.borovec@seznam.cz,0
6977,Use workflow name for docs page redirector (#13193),0.50226676,refactored dataloader process hook (#3139),fix a typo,0
6978,Deprecate description and env parameters in LightningCLI.init (#15651),0.8393401,"Deprecated description, env_prefix and env_parse parameters in LightningCLI.__init__ in favour of giving them through parser_kwargs (#15651)",  add log output for slurm   change log levels   formatting   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
6979,update codeowners (#4881),0.576071,Deprecated @data_loader decorator  (#926),,0
6980,[Feat] Add TORCH_DISTRIBUTED_BACKEND env variable  (#5981),0.5766746,Setting the torch-distributed backend,  Fixed broken link in PR template.   Updated CHANGELOG.md ,0
6981,"change default save_top_k, save_last to None (#3680)",0.86826074,Changed defaults of save_top_k and save_last to None in ModelCheckpoint (#3680),Check if the optional filepath is None before checking if it exists Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,1
6982,Assume the fetcher input will be a CombinedLoader (#17129),0.667815,      for batch in dataloader:, edit doc  mentioned in #646   edit doc   underline   class reference   Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
6983,Fix metric attribute lookup (#8181),0.6894362,Remove MetricsHolder (#7909),,0
6984,ddp backend refactor (#3207),0.92888945,refactored DDP backend forward (#3119),,1
6985,template label typo (#2195),0.53151274,Use correct python version in lightning component template (#13790),Only call load_spawn_weights if COLAB_GPU or KAGGLE_URL_BASE,0
6986,drop duplicated func _module_available,0.5541427,  * Removed the deprecated `pl_module` argument from the distributed module wrappers,environment variables are set,0
6987,Fabric checkpointing 3/n: Implement missing get_module_state_dict for strategies (#16487),0.5855528,Deprecated checkpoint_callback from the Trainer constructor in favor of enable_checkpointing (#9754),,0
6988,Add favicon path (#4331),0.54009223,Changed Checkpoint path parameter from filepath to dirpath (#1016),Propper master...,0
6989,docstring changes in accelerators (#6327),0.664988,move specific accelerator code (#3457),,0
6990,Update introduction video (#17059),0.5938007,nvidia/apex deprecation (#16039),,0
6991,Update BECOMING_A_CORE_CONTRIBUTOR.md,0.656162,This release fixes that core issue,,0
6992,Handle errors due to uninitailized parameters (#7642),0.6302226,Improved error messages for invalid configure_optimizers returns (#3587),fix hparams issue cache cache cache cache ddp pickle ddp pickle ddp pickle ddp pickle ddp pickle ddp pickle ddp pickle ddp fix pep8,0
6993,fix noqa appearing in docs (#10116),0.5037096,    return None,ddp pickle ddp pickle ddp pickle ddp pickle ddp pickle ddp pickle ddp pickle,0
6994,Introduce checkpoint migration (#15237),0.70562446,Resuming from checkpoints (#16167),Update tensorboard.py Update CHANGELOG.md Update tensorboard.py Update test_tensorboard.py Update test_tensorboard.py tests pep8,1
6995,Fix iterable dataset docs (#1342),0.6618391,return iterabledataset,,0
6996,Fix comet logger to log after train (#892),0.69219446,Using .comet.config file for CometLogger (#1913),Remove warning Update CHANGELOG.md,0
6997,Remove deprecated Trainer.running_sanity_check (#9209),0.9005592,Removed deprecated property Trainer.running_sanity_check in favor of Trainer.sanity_checking (#9209),fix hparams issue fix hparams issue,1
6998,Update clip gradients signature for precision plugins (#6764),0.76036155,Precision Plugins (#5718),spec cache spec cache trigger extras revert refactor cache cache cache cache,1
6999,Fixes print issues and data_loader (#1080),0.8494035,Monir bug fix with print issues and data_loader (#1080),  changelog   warning   pull   typo   typo ,1
7000,Provide access to unwrapped model in Lite (#12597),0.64743966,        # Let Lite setup your dataloader(s),,0
7001,fix docs render (#5610),0.5581265,- Fixed a bug where the upload files endpoint would raise an error when running locally ([#14924](https://github.com/Lightning-AI/lightning/pull/14924)),,0
7002,ref: inner train loop (intermediate step) 16/n,0.72367394,"inner train loop (#3359, #3361, #3362, #3363, #3365, #3366, #3367, #3368, #3369, #3370, #3371, #3372, #3373, #3374, #3375, #3376, #3385, #3388, #3397)",,1
7003,docs: lightning-bolts (#6967),0.7739847,LightningLite:,test pickling,1
7004,Use valid pypi versions for install in assistant (#14750),0.70144117,Enable PyTorch 1.7 compatibility (#3541),,1
7005,Remove a bool type assignment in lr_finder.py (#13652),0.70360184,Removed non-finite values from loss in LRFinder (#1862),,1
7006,sooner CI testing (#2037),0.61682534,The point of this release is more bug fixes ahead of v 1.0.0. We now have CI tests on TPU thanks to @zcain117 from Google! :slightly_smiling_face:,,0
7007,drop deprecated docs (#5768),0.6359093,Removed deprecated: (#2760),,0
7008,Remove the HivemindStrategy (#16407),0.76471686,- Hivemind Strategy,,1
7009,Deprecate the default EarlyStopping callback monitor value (#7907),0.8449898,Deprecated default value of monitor argument in EarlyStopping callback to enforce monitor as a required argument (#7907),,1
7010,[TPU] Fix PjRT tests (#17408),0.6717726,Updated logic for checking TPUs availability (#6767),resume GH action,0
7011,release v0.2.5.2,0.7940867,0.4.0,Fixes CPU DDP breaking change and DDP change,1
7012,Update setuptools requirement from <65.7.0 to <67.7.0 in /requirements (#17050),0.5638265,Setup: added requirement freeze for the next major version (#14480),Allow metrics logged together with hparams,0
7013,Remove DP (#16748),0.7506546,Removed deprecated: (#2760),Fix ModelCheckpoint not being picklable.,1
7014,fix warning (#3800),0.6986714,Updated Multinode Warning (#16091),Remove warning,0
7015,Fix self.log(on_epoch=True) on_batch_start (#9780),0.6986331,"self.log(""loss"", loss, prog_bar=True, on_step=True)",,0
7016,Added advise for num_workers=0 in docs/speed (#10215),0.64928937,"    num_workers=10,",,0
7017,make bug_report_model minimal (#7191),0.5627121,Allow ModelCheckpoint monitor to be None (#3633),,0
7018,cleaned up progbar (#165),0.6240032,"Cleaning (#5948, #5949, #5950)",,0
7019,Replace _DataModuleWrapper with __new__ [1/2] (#7289),0.95762706,Replaced _DataModuleWrapper with __new__ (#7289),,1
7020,Document behaviour when setting both on_step=True and on_epoch=True in self.log (#4327),0.71626586,"self.log(""loss"", loss, prog_bar=True, on_step=True)",,1
7021,Bump Lightning-AI/utilities from 0.4.1 to 0.6.0 (#16812),0.7067193,- Deprecated `LightningDeepSpeedModule` ([#14000](https://github.com/Lightning-AI/lightning/pull/14000)),,1
7022,Upgrade to HPU release 1.7.1 (#15956),0.6115372,0.4.0,,0
7023,[App] Multiprocessing-safe work pickling (#15836),0.7995391,Utility for pickling work object safely even from a child process (#15836),,1
7024,metrics: add SSIM (#2671),0.866444,Updated SSIM metric (#4566)(#4656),,1
7025,Standalone Lite: DDP Spawn Strategy Family (#14675),0.6727897,"decoupled DDP, DDP spawn (#3733, #3817, #3819, #3927)",,0
7026,Tune Conda CI timeout and other minor improvements (#10769),0.566889,Resolve bug with Finetuning (#5744),,0
7027,move state extraction for CaptureMapDataset (#9484),0.63904524,"Add {,load_}state_dict to the progress tracking dataclasses (#8140)",,0
7028,added demo tfx images,0.5195101,    pil_image = T.ToPILImage()(image),,0
7029,Add support for logging the model checkpoints to MLFlowLogger (#15246),0.70810544,- Allow logging to an existing run ID in MLflow with `MLFlowLogger` ([#12290](https://github.com/PyTorchLightning/pytorch-lightning/pull/12290)),,1
7030,Update outdated deadlock detection test (#15594),0.6872857,- Removed deadlock detection / process reconciliation (`PL_RECONCILE_PROCESS=1`) ([#16204](https://github.com/Lightning-AI/lightning/pull/16204)),,0
7031,Remove fn check for ipu output (#7915),0.62760586,Stopped optimizer_zero_grad from being called after IPU execution (#12913),,0
7032,CI: fix pypi pkg (#15630),0.7288364,"Removed PyTorch 1.6 support (#10367, #10738)",,1
7033,[App] Add display name property to the work (#16095),0.6005453,Add support for printing application logs using CLI lightning show logs <app_name> [components] (#13634),,0
7034,Remove psf/black references (#5739),0.60631526,remove _evaluate fx (#3197),,0
7035,fix https://github.com/PyTorchLightning/pytorch-lightning/issues/2635 (#2738),0.80263567,- Fixed an issue that caused the Tuner to affect the random state ([#11870](https://github.com/PyTorchLightning/pytorch-lightning/pull/11870)),,1
7036,[TPU] Add support for PJRT from PyTorch/XLA 2.0 (#17352),0.7908523,PyTorch 1.5  support,,1
7037,Bump Python version for mypy check (#15126),0.5999694,Skipped testing with PyTorch 1.7 and Python 3.9 on Ubuntu (#11217),Hparams,0
7038,rename callback FineTune arg round (#9711),0.5300895,Resolve bug with Finetuning (#5744),,0
7039,ref: added data connector (#3285),0.7375125,move prepare_data to data connector (#3307),,1
7040,Call optimizer.zero_grad() before backward inside closure in AutoOpt (#6147),0.7595916,    optimizer1.zero_grad(),,1
7041,Improves the PanelFrontend docs (#14493),0.62364984,Adds PanelFrontend to easily create complex UI in Python (#13531),,0
7042,Code cleaning in preparation for #7258 [3/n] (#7262),0.75061387,"Cleaning (#5948, #5949, #5950)",  spec cache   spec cache   trigger   extras   checkout ,1
7043,Remove old test artifacts (#14574),0.6703836,Deprecated the TestTubeLogger (#9065),  changelog   warning   pull   typo   typo ,0
7044,Remove redundant install info in CLI (#17577),0.5574584,Introducing CLI commands for apps (#13602)!,,0
7045,[IPU] Add hooks for IPU lifecycle 4/5 (#7864),0.69209886,Graphcore IPU devices,,0
7046,Fix TPU circleci tests (#13432),0.71474934,Updated logic for checking TPUs availability (#6767),,1
7047,Attempt to query device count via NVML (#14631),0.5976243,"Accessing dataloaders (#16726, #16800)",,0
7048,add docs (#5902),0.61926,Docs,,0
7049,Add a required job checker as an action (1/2) (#14363),0.543239,    # 2. Add the outputs to the list,,0
7050,[App] Resolve run installation (#15974),0.69588244,Improved support for running apps when dependencies aren't installed (#15711),,0
7051,consistent behavior for reduce method across all Plugins (#6011),0.8775003,Made the Plugin.reduce method more consistent across all Plugins to reflect a mean-reduction by default (#6011),,1
7052,add pre-commit hooks (#7906),0.612418,Updated hooks arguments - breaking for setup and teardown (#2850),,0
7053,Update trainer.rst,0.6885847,"Refactor RunningStage and TrainerState usage (#4945, #7173)",,0
7054,feat: add LargeFileManager configuration to Jupyter in Dockerfile (#17553),0.5568161,Reset val_dataloader in tuner/batch_size_scaling for binsearch (#9975),,0
7055,[DOC] Clarify tpu_cores training. (#4475),0.8231616,TPU training (#2708),,1
7056,Parity test (#7832),0.5938727,test_percent_check in favour of limit_test_batches,,0
7057,Avoid wrapping LightningModule in DDP plugins when not fitting (#9096),0.7710296,"- The LightningModule no longer gets wrapped with data-parallel modules when not fitting in DDPPlugin, DDPSpawnPlugin, DDPShardedPlugin, DDPSpawnShardedPlugin.",,1
7058,Test that connector defaults match the ones in Trainer/Fabric (#16463),0.6324402,Automatically set sync_batchnorm for training_type_plugin (#6536),,0
7059,Fix logging of nan parameters (#9364),0.63102984,all logging related calls in a connector (#3395),,0
7060,Update train_model_basic.rst (#15352),0.71700203,  model.train(),  fixed hparams section   docs clean up ,1
7061,Removed redundant computations in clip_gradients that slowed down the gradient clipping. (#1523),0.7156708,Ensure that clip gradients is only called if the value is greater than 0 (#6330),  fixed distutil parsing   fixed distutil parsing   Apply suggestions from code review   log   fixed distutil parsing   fixed distutil parsing   fixed distutil parsing   fixed distutil parsing   doctest   fixed hparams section   fixed hparams section   fixed hparams section   formatting   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: J. Borovec jirka.borovec@seznam.cz,1
7062,Incorporate pytorch's fixes in device_count_nvml (#16795),0.63426423,from pytorch_lightning.callbacks import DeviceStatsMonitor,  Create docker_builds.yml   Update docker_builds.yml   Update docker_builds.yml   Update docker_builds.yml   Update docker_builds.yml ,0
7063,distributed docs,0.6827122,Distributed Backend,,0
7064,Bump playwright from 1.27.1 to 1.28.0 in /requirements (#15903),0.50383264,[1.4.0] - 2021-07-27,  diable val and test shuffling   diable val and test shuffling   diable val and test shuffling   diable val and test shuffling   log   condition   shuffle   refactor   Co-authored-by: J. Borovec jirka.borovec@seznam.cz,0
7065,Fabric PPO example share_data flag (#17397),0.5321176,@ptl.data_loader,,0
7066,Fix selection of standalone tests (#10857),0.6554965,- fixed all the .test() calls,  Create Dockerfile   add readme   Update MANIFEST.in   Update Dockerfile   Co-authored-by: J. Borovec jirka.borovec@seznam.cz,0
7067,Update README.md (#5018),0.5254956,Updated mlflow with using resolve_tags (#6746),  spacing   slurm docs ,0
7068,Annotate Fabric.log_dict with mapping input (#16325),0.5721296,Minor improvements to apply_to_collection and type signature of log_dict (#7851),,0
7069,Merge branch 'nccl' of https://github.com/williamFalcon/pytorch-lightning into nccl,0.53903586,- Include the `pytorch_lightning` version as a header in the CLI config files ([#12532](https://github.com/Lightning-AI/lightning/pull/12532)),  multi processing warnings   multi processing warnings   multi processing warnings   multi processing warnings   multi processing warnings   multi processing warnings ,0
7070,Comet logger documentation improvements (#13847),0.79875463,Using .comet.config file for CometLogger (#1913),,1
7071,Fix bagua strategy raising AttributeError during manual optimization (#13137),0.61739874,Improved error messages for invalid configure_optimizers returns (#3587),  try delete in async or DDP us0-ecase   changelog   add model chekpoint rank   simple delete   flake8   use global rank   chnagelog   fix review   fix import   proposal   proposal   proposal   improve proposal (fix problems with method call self)   cleaning   Co-authored-by: Adrian Wälchli adrian.waelchli@students.unibe.ch Co-authored-by: William Falcon waf2107@columbia.edu,0
7072,Fix Fabric CHANGELOG (#16247),0.640323,Fabric, Fixed dataset docs and disabled auto-sampler for iterable dataset,0
7073,Add support for devices flag to Trainer (#8440),0.6720729,Trainer Device Attributes,,0
7074,Enable logger connector re-design (#7891),0.8089262,Dramatically simplify the LoggerConnector (#7882),  fix parity   update deprecated call ,1
7075,[FEAT] Add pytest section to Contribution how to ?  (#4633),0.5651418,- Added support for dataclasses in `apply_to_collections` ([#11889](https://github.com/PyTorchLightning/pytorch-lightning/pull/11889)),,0
7076,Remove unused docstring parameter device (#13448),0.62215865,Replaced _DataModuleWrapper with __new__ (#7289),,0
7077,"changed hard coded paramater, and moved it to parent_parser (#238)",0.58196604,Override some of the params with new values, fix(wandb): allow use of sweeps  overwrite run config parameters due to precision error fix #1290   docs(wandb): update changelog   test(wandb): update config test   Co-authored-by: William Falcon waf2107@columbia.edu,0
7078,fixed bad warn for result obj (#3072),0.6461344,Improved error messages for invalid configure_optimizers returns (#3587),,0
7079,Fix testing for mac OS (#399),0.64810073,Updated app testing (#16000),,0
7080,Fix the examples/app_dag App (#14359),0.5694116,Updated app testing (#16000),  fixes gpu parsing   fixes gpu parsing ,0
7081,Add files via upload,0.7643831,Add support to upload files to the Drive through an asynchronous upload_file endpoint (#14703), squash and rebase  sanity check hooks sanity check callback hook finish moved core progress bar functionality into callback wip remove duplicate merge clean up imports docs sanity check progress bar main sanity move callback calls init progrss bar callback configuration and docs changelog rate decorator pass process_position disable on rank > 0 position index is_enabled remove decorator refactor init tqdm bars callback method ordering  cannot reset when disabled sequence -> list default values fix has no attr _time()  move on_val_end to proper place fix the pickle issue update warning properties check for None remove old comment switch order pull out non-tqdm functionality into base class documentation for the base class docs fix refresh rate issue in validation restrict type hint of trainer arg more docs update trainer docs rst docs fix lines too long fix test add missing type hints fix typo move docstring to init solves doctest failures remove doctest :(( can't fix the pickle error fix example simplify by saving trainer reference fix docs errors move docstring initial value multiple val checks per epoch simpler handling of inf dataset sizes update inf docs renamed training_tqdm_dict rename get_tqdm_dict rename occurences of tqdm  update changelog fix doctest fix formatting errors added callback tests progress bar on off test more tests for progress bar weird test fix? add ignored property disable default progress bar in LR finder change enable/disable behavior trying doctest in CI again undo doctest pickle error undo doctest pickle error :(( remove progress_bar_callback Trainer arg and fix tests restore progress bar after auto lr find update docs fix rebase fix wrong negation   fix fast dev run total   more thorough testing   remove old args   fix merge   fix merge   separate tests   type hint total batches   reduce if   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  is_disabled  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  is_enabled  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   rename enabled/disabled   move deprecated api   remove duplicated test from merge   fix rename is_disabled   newline   test also testprogress for fast dev run   Co-authored-by: J. Borovec jirka.borovec@seznam.cz Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
7082,Fix weights path (#1445),0.6666375,- Removed the deprecated `weights_save_path` Trainer argumnent and `Trainer.weights_save_path` property ([#14424](https://github.com/Lightning-AI/lightning/pull/14424)),"  The epoch was being logged to metrics, which isn't read, rather than to current_metrics.   Updated the tests to account for the epoch arriving at the logger. ",0
7083,"fix: nb is set total number of devices, when nb is -1. (#4209)",0.68178856,    devices=1,  check deprecation warnings   extend warning test   try   unimport modules   update ,0
7084,fixed warning,0.7896205,warning_utils >> warnings,  now func merge_dicts works with nested dictionaries   CHANGELOG.md upd ,1
7085,[WIP] Fix wrong example paths in README.md (#444),0.5790196,- all the file path errors with loggers (txs @awaelchli),,0
7086,[2/N] Define dataclasses for progress tracking (#7574),0.77849746,"Connect the progress tracking dataclasses to the loops (#8244, #8362)",,1
7087,Remove should_rank_save_checkpoint property from Trainer (#9433),0.9896661,Removed should_rank_save_checkpoint property from Trainer (#9433),  fixed new amp bugs   fixed new amp bugs ,1
7088,Fault Tolerant Manual: Add is_obj_stateful utility (#10646),0.64288604,- Fault Tolerant Manual,,0
7089,[docs] distributed_backend -> accelerator (#4429),0.6459541,Refactored accelerator backends:,  adding native amp suppport   adding native amp suppport   adding native amp suppport   adding native amp suppport   autocast   autocast   autocast   autocast   autocast   autocast   removed comments   removed comments   added state saving   added state saving   try install amp again   added state saving   drop Apex reinstall   Co-authored-by: J. Borovec jirka.borovec@seznam.cz Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
7090,Add max_size mode to CombinedLoader (#16939),0.7599668,"combined_loader = CombinedLoader(iterables, mode=""min_size"")",  fix   whitespace   Co-authored-by: Josh Karlin karlinjf@gmail.com,1
7091,releasing 1.8.0.post1 (#15476),0.71482635,"    version=""0.0.1"",",  devel image   try parallel   new image ,1
7092,Add support for iterable datasets when val_check_interval=1.0 (#1283),0.67348945,Support for arbitrary iterables (#16726),  allow determine of filepath at runtime   typing   Co-authored-by: Nicki Skafte nugginea@gmail.com,0
7093,Add a direct link to built docs in CI (#11514),0.58188224,"For a full tutorial and running example, visit our docs. TODO: add to docs",  fix boolean argparse #1570   update change log ,0
7094,releasing v1.8 RC (#15205),0.6499079,"    version=""0.0.1"",",,0
7095,scheduled removal of auto_move_data decorator (#9231),0.9473444,Removed deprecated auto_move_data decorator (#9231),  check for kaggle env variable   added changelog ,1
7096,Add artifcact_location arg to MLFlow logger (#6677),0.5917195,- Allow logging to an existing run ID in MLflow with `MLFlowLogger` ([#12290](https://github.com/PyTorchLightning/pytorch-lightning/pull/12290)),,0
7097,Set total number of batches in progress bar while testing (#425),0.7635522,Note how the total batches (128) is the sum of the training batches (32) and the three validation runs (3 x 32). And this is how the progress bar looks like now:,  Initial commit of Horovod distributed backend implementation   Update distrib_data_parallel.py   Update distrib_data_parallel.py   Update tests/models/test_horovod.py   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update tests/models/test_horovod.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   Fixed tests   Added six   tests   Install tox for GitHub CI   Retry tests   Catch all exceptions   Skip cache   Remove tox   Restore pip cache   Remove the cache   Restore pip cache   Remove AMP   Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: J. Borovec jirka.borovec@seznam.cz,1
7098,[App] Resolve bi-directional queue bug (#15642),0.55272627,Resolved a bug about a race condition when sending the work state through the caller_queue (#14074),  tests for pytorch 1.5   up Win   win   win   win   win   win   win ,0
7099,Update allowlist for leaked env variables set by XLA (#9753),0.52487373,Only check versions / env when not in the cloud (#15504),  default test logger   fix tests   spawn   try   simplify tests   simplify tests   formatting   loggers   loggers   revert to TestTube   default   default   wraps   world size   optim imports ,0
7100,Remove dead code in accelerator connector (#10100),0.72707576,Remove hardcoding of local rank in accelerator connector (#6878),,1
7101,CI pass (#671),0.4894926,"Epoch 8:  53%|█████    | 17/32 [5.13/s, v_num=2, loss=0.5643]",  Revert this   typos   version++   Co-authored-by: J. Borovec jirka.borovec@seznam.cz,0
7102,added saving tests to cpu,0.62652546,CPU stats monitoring,  Update init.py   Update setup.py ,0
7103,Fix restoring lr scheduler states with deepspeed strategy (#11322),0.7833259,- Fixed the lr-scheduler state not being dumped to checkpoint when using the deepspeed strategy ([#11307](https://github.com/PyTorchLightning/pytorch-lightning/pull/11307)),  fix changelog   formatting   add ddp_cpu   docs   add another ,1
7104,CI: fix valid schema (#11744),0.60881495,Resolve bug with Finetuning (#5744),  change lr scheduler step interval to update every backwards pass instead of every forwards pass   update CHANGELOG   fix spacing   Add TODO to lr schedule update   remove trailing whitespace   Co-authored-by: William Falcon waf2107@columbia.edu,0
7105,Avoid inference_mode with torch.compile (#17215),0.77021956,Set Torch inference mode for prediction (#15719),  fix horror bug   update changelog   fix doctest   liine too long ,1
7106,HPU & TPU doesn't support torch.inference_mode (#13014),0.7247871,- Fixed an issue with unsupported torch.inference_mode() on hpu backends by making it use no_grad ([#13014](https://github.com/Lightning-AI/lightning/pull/13014)),,1
7107,Introduce Stateful DataModule (#11637),0.6586448,Support shorthand notation to instantiate datamodules (#10011),,0
7108,Enable logging hparams only if there are any (#11105),0.70490193,Allow logging of metrics together with hparams (#1630),  slurm check in ddp_train and init_ddp_connection   Remove code example in init_ddp_connection   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  remove blank line  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  improve for test coverage  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   update changelog   Default values and warnings for DDP env variables   fix merge artifacts   update localhost value   change to NODE_RANK   Co-authored-by: Alexander Reshytko areshytko@Alexanders-MacBook-Pro.local Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: William Falcon waf2107@columbia.edu,1
7109,Update installation instructions for FairScale (#5099),0.57754123,Avoided requiring the FairScale package to use precision with the fsdp native strategy (#14092),  store html artifacts in circle ci   add note to contributing.md ,0
7110,Fix logging's step values when multiple dataloaders are used during evaluation (#12184),0.6888914,Logging with multiple loggers,  Add explicit flag for ddp sampler replacement   Add flag for sampler replacement in ddp   Update data_loading.py   Update CHANGELOG.md   pep8 fixes   pep8 ,0
7111,Allow exporting to onnx when input is tuple (#8800),0.665677,Avoid the deprecated onnx.export(example_outputs=...) in torch 1.10 (#11116),  fixed memory leak from opt return   fixed memory leak from opt return   fixed memory leak from opt return   fixed memory leak from opt return   fixed memory leak from opt return   fixed memory leak from opt return   fixed memory leak from opt return   fixed memory leak from opt return   fixed memory leak from opt return   fixed memory leak from opt return   fixed memory leak from opt return   fixed memory leak from opt return   fixed memory leak from opt return   fixed memory leak from opt return   fixed memory leak from opt return   fixed memory leak from opt return   fixed memory leak from opt return   fixed memory leak from opt return   fixed memory leak from opt return ,0
7112,[App] Improve pdb for multiprocessing (#15950),0.65957165,DDP Debugging Improvements,"  Attempt to fix #1468   Remove the if statement, it doesn't actually make any difference   Update docs   Correct warnings I caused in the last commit   Add to changelog   Actually add to changelog   Clarify documentation and examples   Update CHANGELOG.md   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",0
7113,Fix global step increment on training_epoch_end (#3673),0.6945674,The current_epoch and global_step attributes now get restored irrespective of the Trainer task (#9413),Fixes #1522,0
7114,ci: adjust version in all requirements (#16100),0.59698725,Implemented ready for components (#16129), Fixed Trainer gpus arg type issue  Fixes #1388  Disallow boolean gpus parameter  Co-Authored-By: Adrian Wälchli aedu.waelchli@gmail.com  Fixed missing paranthesis  Co-authored-by: Adrian Wälchli aedu.waelchli@gmail.com,0
7115,Improve support for custom DataLoaders when instantiated in *_dataloader hook (#12981),0.8533546,- Improved support for custom `DataLoader`s when instantiated in `*_dataloader` hook ([#12981](https://github.com/Lightning-AI/lightning/pull/12981)),  add QA to docs   not about doc updates by @awaelchli   Apply suggestions from code review   Co-Authored-By: Adrian Wälchli adrian.waelchli@students.unibe.ch  help  Co-authored-by: Adrian Wälchli adrian.waelchli@students.unibe.ch,1
7116,Enable validation during overfitting (#12527),0.86865973,Validation now runs in overfitting mode,,1
7117,[metrics] IoU Metric (#2062),0.69564104,"docs for all Metrics (#2184, #2209)",  feat: save checkpoint before deleting old ones   fix: make sure that the new model is not deleted   changelog   Co-authored-by: J. Borovec jirka.borovec@seznam.cz Co-authored-by: William Falcon waf2107@columbia.edu,0
7118,fix loading past checpoints (#2405),0.54443944,Fix for load_from_checkpoint() not working with absolute path on Windows (#2294), improved docs for core  update links add references to hooks lifecycle wip continue with init.py improve docs for memory.py improve docs for saving.py simpler links fix formatting   move hooks lifecycle to top of file   fix doctest import problem   add missing hook in lifecycle ,0
7119,Fix frozen dataclass instance error in apply_to_collection (#10927),0.64083755,- Show a better error message when frozen dataclass is used as a batch ([#10927](https://github.com/PyTorchLightning/pytorch-lightning/pull/10927)),"  improve init   improve logger base   improve comet logger docs   improved docs for mlflow   improved nepune logger docs   fix matplotlib import issue   improve tensorboard docs   improve docs for test tube   improved trains logger docs   improve wandb logger docs   improved docs in experiment_logging.rst   added MLflow to the list of loggers   fix too long lines   fix trains doctest   fix neptune doctest   fix mlflow doctest   Apply suggestions from code review   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   Apply suggestions from code review   fix whitespace   try bypass mode for neptune (fix doctest api key error)   try ""test"" as api key   Revert ""try ""test"" as api key""   This reverts commit fd77db26d551f08b4b4a12bb93cbd8f7a0814f29.   try test as api key   update neptune docs   bump neptune minimal version   revert unnecessary bypass code   test if CI runs doctests in .rst files   Revert ""test if CI runs doctests in .rst files""   This reverts commit a45aeb460a8c4b7445a35dd7b49265f48d11c485.   add doctest directive   neptune demo links   added tutorial link for W&B   fix line too long   fix merge error   fix merge error   add instructions how to install loggers   add instructions how to install the loggers   hide _abc_impl property from docs   review Borda, 4 spaces   indentation in example sections   blank   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",0
7120,Fix global_step when gradient accumulation > 1 (#832),0.647825,+ global_step += 1,  call on_before_zero_grad   update changelog   add note about overriding both hooks   added test   move test_hooks.py to models folder ,0
7121,docs: default_root_path -> default_root_dir (#4942),0.6208862,Allowed root path to run the app on /path (#14972),  feat(semantic_segmentation): allow customization of unet   feat(semseg): allow model customization   style(semseg): format to PEP8   fix(semseg): rename logger   docs(changelog): updated semantic segmentation example   suggestions   suggestions   flake8   Co-authored-by: J. Borovec jirka.borovec@seznam.cz,0
7122,[App] Add rm one level below project level (#16740),0.5440486,Enabled cp (upload) at project level (#16631),"  Add tests for distributed backend config   Refactor set_distributed_mode   Use gloo backend on cpu   Use 127.0.0.1 instead of 127.0.0.2   Not totally clear on why this is necessary, but it seemt to work   Update LightningDDP so that it works with CPU   Add ddp_cpu backend and num_processes Trainer arg   PEP8   Fix test skipping. Inequalities are hard :/   Skip ddp_cpu test on Windows   Make a few more cases fall back to ddp_cpu   New function name   Flake8   Don't test distributed on MacOS with torch < 1.3   Support for distributed in MacOS was added in Torch 1.3.0   Add ddp_cpu and num_processes to docs   Parametrize trainer config tests   Tweak warning   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   Remove redundant test   Replace pass branches with comments   Add missing warnings import   save_path -> root_dir   Use new rank_zero_warn   Whitespace   Apply suggestions from code review   formatting   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: J. Borovec jirka.borovec@seznam.cz",0
7123,Bugfix: horovod optimizer missing 2 required positional arguments (#7840),0.58400834,"refactored Horovod backend (#3121, #3122)",  remove error when test dataloader used in test   remove error when test dataloader used in test   remove error when test dataloader used in test   remove error when test dataloader used in test   remove error when test dataloader used in test   remove error when test dataloader used in test   fix lost model reference   remove error when test dataloader used in test   fix lost model reference   moved optimizer types   moved optimizer types   moved optimizer types   moved optimizer types   moved optimizer types   moved optimizer types   moved optimizer types   moved optimizer types   added tests for warning   fix lost model reference   fix lost model reference   added tests for warning   added tests for warning   refactoring   refactoring   fix imports   refactoring   fix imports   refactoring   fix tests   fix mnist   flake8   review   Co-authored-by: J. Borovec jirka.borovec@seznam.cz,0
7124,Rework of Sklearn Metrics (#1327),0.8915798,Sklearn metrics classes (#1327),,1
7125,Mark AMP test as flaky (#15055),0.6465766,Apex mixed precision gets replaced with AMP (#16149),  flushing loggers   flushing loggers   flushing loggers   flushing loggers   changelog   typo   fix trains   optimize imports   add logger test all   add logger test pickle   flake8   fix benchmark   hanging loggers   try   del   all   cleaning ,0
7126,Update tests in strategies directory in preparation for #11040 (#12467),0.55104953,Updated app testing (#16000),  removed some .items   added speed tests   added speed tests   Update benchmarks/test_rnn_parity.py   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update benchmarks/test_trainer_parity.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   fix lost model reference   added speed tests   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
7127,Fix DDPStrategy import in app framework after #14952  (#16029),0.6601449,Merged the implementation of DDPSpawnStrategy into DDPStrategy and removed DDPSpawnStrategy (#14952),docs typo,0
7128,made early stop checkpoint optional,0.76675117,Checkpoint and early stopping now work without val. step (#1041),  Replace automatic nan check with optional flag   Update CHANGELOG.md ,1
7129,testing slurm ddp,0.65867114,separate SLURM from DDP (#3809),it should be val_batch here.,0
7130,Configure native deepspeed schedulers with interval='step' (#11788),0.80292606,"- Configure native Deepspeed schedulers with interval='step' ([#11788](https://github.com/PyTorchLightning/pytorch-lightning/pull/11788)), ([#12031](https://github.com/PyTorchLightning/pytorch-lightning/pull/12031))",Co-authored-by: Nicki Skafte nugginea@gmail.com,1
7131,Fix support for logging within callbacks returned from LightningModule (#10991),0.7234567,"LightningModule now raises an error when calling log(on_step=False, on_epoch=False) (#10227)","  initial structure   rebase   incorporate suggestions   update CHANGELOG.md   initial docs   fixes based on reviews   added trainer arg   update docs   added saving/restore of model state   initial tests   fix styling   added more tests   fix docs, backward compatility and progressbar   fix styling   docs update   updates based on review   changed saving to standard functions   consistent naming   fix formatting   improve docs, added support for nested fields, improve codecov   update CHANGELOG.md   Update lr_finder.rst   Update pytorch_lightning/trainer/trainer.py   Update trainer.py   Update CHANGELOG.md   Update path   restoring   test   attribs   docs   doc typo   Co-authored-by: Nicki Skafte nugginea@gmail.com Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: J. Borovec jirka.borovec@seznam.cz",1
7132,reduce loop structure leakage into the TrainingEpochLoop (#9490),0.7112539,Refactored training loop (#2336),,1
7133,Make HorovodPlugin.all_gather return a tensor (#9696),0.7895978,Changed HorovodPlugin.all_gather to return a torch.Tensor instead of a list (#9696),  continues develop   changelog   typo ,1
7134,support launching Lightning ddp with traditional command (#7480),0.7007297,Add support for Lightning App Commands through the configure_commands hook on LightningFlow and ClientCommand  (#13602),  renamed default path to actual root_dir   added default weights path   added default weights path   added default weights path ,1
7135,Fix TPU testing and collect all tests (#11098),0.7292391,Updated logic for checking TPUs availability (#6767)," Add automatic GPU choice to trainer  This commit adds the gpu_choice parameter to Trainer. By default, this parameter is set to 'manual' which causes no observable difference in behavior. When gpu_choice is set to ""auto"" and gpus is an int, then the trainer will automatically allocate the first available GPU. This is especially useful when GPUs are configured to be in ""exclusive mode"", which means that only one process at a time can use them.  Rename gpu_choice -> auto_select_gpus",1
7136,early stopping checks on_validation_end (#1458),0.990135,Early stopping checks on_validation_end (#1458),  Add test_dataloaders to test method   Remove test_dataloaders from .fit()   Fix code comment   Fix tests   Add test_dataloaders to test method (#1393)   Fix failing tests   Update docs (#1393) ,1
7137,Fix CVE-2020-1747 and CVE-2020-14343 (#11099),0.62409747,This release fixes that core issue,"  configure_optimizer from dict with only ""optimizer"" key. bug fixed   autopep8   pep8speaks suggested fixes   CHANGELOG.md upd ",0
7138,"Move to percentage diff, increase diff",0.57801753,diff,,0
7139,Remove AcceleratorConnector.num_processes and deprecate Trainer.num_processes (#12388),0.7485664,- Removed `AcceleratorConnector.num_processes` property ([#12388](https://github.com/PyTorchLightning/pytorch-lightning/pull/12388)),,1
7140,update apply_to_collections to support dataclass inputs (#11889),0.70968175,- Added support for dataclasses in `apply_to_collections` ([#11889](https://github.com/PyTorchLightning/pytorch-lightning/pull/11889)),,1
7141,release LAI docs as stable (#14250),0.9984387,Release LAI docs as stable (#14250),  grid sample   grid sample   grid sample   grid sample   grid sample   changelog   version   Co-authored-by: J. Borovec jirka.borovec@seznam.cz,1
7142,Enable distributed training with CombinedDataLoader and max_size_cycle (#10374),0.6548319,Trainer(distributed_backend='ddp2')  ,  Fix gradient clipping   Relax accuracy constraint ,0
7143,ref: accelerator connector methods x/n (#3469),0.9650877,"accelerator connector methods x/n (#3469, #3470, #3474)",  returns   changelog ,1
7144,Fixes,0.73725617,"At last, lots of bug fixes (see below).",,1
7145,Fix parameters and docs in metrics (#2473),0.73096025,"docs for all Metrics (#2184, #2209)",  add rank warning   changelog   use rank_zero_warn   user trainer_init   replace warnings   fix test   flake8   docs   changelog   bug lol ,1
7146,added safeguards for callbacks in loading saving,0.69522536,Callback hooks for loading and saving checkpoints,,0
7147,Move deepspeed summary test to correct folder (#13478),0.6773658,DeepSpeed single file saving (#6900),,0
7148,Prune deprecated trainer attributes 2 (#7502),0.679942,Removed the deprecated TrainerTrainingTricksMixin class (#8679),,0
7149,Update README.md,0.48788628,Full Changelog,  added slurm doc   added slurm doc ,0
7150,Fabric checkpointing 2/n: DeepSpeed implementation (#16452),0.7008792,Ensure we check deepspeed/sharded in multinode DDP (#6297),,1
7151,[docs] Update checkpointing.rst and callbacks.rst for Stateful support (#12351),0.6064905,Forced ModelCheckpoint callbacks to run after all others to guarantee all states are saved to the checkpoint (#5731),  Fix TrainsLogger doctest failing (switch to bypass mode in GitHub CI)   fix   test ci   debug   debug CI   Fix CircleCI   Fix Any CI environment switch to bypass mode   Removed debug prints   Improve code coverage   Improve code coverage   Reverted   Improve code coverage   Test CI   test codecov   Codecov fix   remove pragma   Co-authored-by: bmartinn <>,0
7152,Update auto_scale_batch_size error message and docstring with LightningDataModule (#15351),0.70473444,Update the Lightning App docs (#13537),  Print test results only if prog_bar_metrics is not empty   Update evaluation_loop.py   Co-authored-by: vitor-guizilini vitor.guizilini@tri.global Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
7153,Rename ClusterEnvironment.creates_processes (#10106),0.76977897,Deprecated ClusterEnvironment.creates_children() in favor of ClusterEnvironment.creates_processes_externally (property) (#10106),  added multiple loader docs   added multiple loader docs   added multiple loader docs   added multiple loader docs   added multiple loader docs   Apply suggestions from code review   added multiple loader docs   added build docs script   typo   added build docs script   added build docs script   added build docs script   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: J. Borovec jirka.borovec@seznam.cz,1
7154,reduce verbosity level in drone ci (#5190),0.510829,Increased DeepDiff's verbose level to properly handle dict changes (#13960),  remove incorrect comment in training_step   added comment for on_batch_start in hooks.py   update early stopping docs   typo fix   whitespace fix   Apply suggestions from code review   Update docs/source/early_stopping.rst   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
7155,Clearer disable validation logic (#650),0.78437144,"Simplified ""should run validation"" logic (#7682)","  add_argparse_args method fixed (argument types added)   autopep8 fixes   --gpus=0 removed from test (for ci tests)   Update pytorch_lightning/trainer/trainer.py   Co-Authored-By: Joe Davison joe@huggingface.co   test_with_accumulate_grad_batches added   agg_and_log_metrics logic added to the base logger class   small format fix   agg metrics strategies removed (not to complicate stuff)   agg metrics: handle zero step   autopep8   changelog upd   flake fix   metrics aggregators factored out, metrics_agg.py added + tests   metrics agg default value added   Update pytorch_lightning/loggers/metrics_agg.py   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   metrics aggregators factored out, metrics_agg.py added + tests   metrics agg default value added   Update pytorch_lightning/loggers/metrics_agg.py   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   remove .item which causes sync issues (#1254)   remove .item which causes sync issues   fixed gradient acc sched   fixed gradient acc sched   test_metrics_agg.py removed (all tested in doctrings), agg metrics refactored   test_metrics_agg.py removed (all tested in doctrings), agg metrics refactored   autopep8   loggers base.py types fixed   test   test   metrics aggregation for loggers: each key now has a specific function (or default one)   metrics aggregation for loggers: each key now has a specific function (or default one)   docstrings upd   manual typehints removed from docstrings   batch_size decreased for test test_with_accumulate_grad_batches   extend running accum   refactor   fix tests   fix tests   allowed_types generator scoped   trainer.py distutils was imported twice, fixed   TensorRunningAccum refactored   TensorRunningAccum added to change log (Changed)   change log pull link added   Co-authored-by: Joe Davison joe@huggingface.co Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: J. Borovec jirka.borovec@seznam.cz",1
7156,Remove on_tpu argument from optimizer_step module hook (#16537),0.7346818,- Removed the `on_tpu` argument from `LightningModule.optimizer_step` hook ([#16537](https://github.com/Lightning-AI/lightning/pull/16537)),  Update requirements.txt   fix SG typo   Co-authored-by: J. Borovec jirka.borovec@seznam.cz,1
7157,Remove DDPSpawnStrategy.get_mp_spawn_kwargs in favor of launchers (#11966),0.82086396,- Removed `get_mp_spawn_kwargs` from `DDPSpawnStrategy` and `TPUSpawnStrategy` in favor of configuration in the `_SpawnLauncher` ([#11966](https://github.com/PyTorchLightning/pytorch-lightning/pull/11966)),,1
7158,Add carmocca to core (#5038),0.5460427,This conversation was marked as resolved by carmocca,  formatting   fix missing image on pypi   fix pypi push ,0
7159,Rename min_gpus to min_cuda_gpus (#13133),0.5790914,Renamed TPUHalfPrecisionPlugin to TPUBf16PrecisionPlugin (#10026),,0
7160,tests: add default_root_dir=tmpdir (#2392),0.58610183,    root_node = '127.0.0.2',  exclude tests   compress image   compress image   update Manifest   update action ,0
7161,Extend support for logging a collection (#7771),0.71993124,Un-balanced logging properly supported (#5119),  instructions for changelog   instructions for changelog   on ,1
7162,fixed doc of timer (#13393),0.6356591,Enforce an epoch scheduler interval when using SWA (#6588),,0
7163,MLFlow now uses env variable as default tracking uri (#7457),0.8568711,MLflowLogger now uses the env variable MLFLOW_TRACKING_URI as default tracking URI (#7457),,1
7164,[Bugfix] Fixed epoch level schedulers not being called when val_check_interval < 1.0 (#6075),0.6584814,Enforce an epoch scheduler interval when using SWA (#6588),  add pypi user   changelog   changelog ,0
7165,Create .drone.jsonnet (#4968),0.46271122,Making threadpool non-default from LightningCloud client  (#14757),,0
7166,readme,0.453971,        ],  update docs/changelog   fix   Co-authored-by: William Falcon waf2107@columbia.edu,0
7167,enable Dockers for PT 1.9 (#7363),0.52268046,support for latest test-tube logger optimized for PT 1.2.0.   ,,0
7168,check trainerfn == FITTING before configuring sync_batchnorm (#11919),0.75154495,Automatically set sync_batchnorm for training_type_plugin (#6536),  tensorboard logger version if root_dir not exist   update changelog   resolve comments   Co-authored-by: Alexander Reshytko areshytko@Alexanders-MacBook-Pro.local Co-authored-by: William Falcon waf2107@columbia.edu,1
7169,Fix typo in contributing docs (#2076),0.5168214,Docs,  Fix unimplemented type() on TPU   Add changelog entry   Add quotation marks ,0
7170,Fix CLI snippet in the docs (#12275),0.6265638,Introducing CLI commands for apps (#13602)!,Co-authored-by: Alexander Reshytko areshytko@Alexanders-MacBook-Pro.local,0
7171,fix import and typo in AMP (#4871),0.6384974,enable_pl_optimizer=False by default to temporarily fix AMP issues (#5163),  Set precision=16 when use_amp is passed as True   Update CHANGELOG.md   add use_amp to deprecated API   Update trainer.py   Update trainer.py   move the use_amp attribute to deprecated API   move use_amp deprecation back to Trainer's init   drop unsed   drop deprecated   reorder imports   typing   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: J. Borovec jirka.borovec@seznam.cz,0
7172,Eval epoch can now log independently (#3843),0.91913754,epoch can now log independently (#3843),  improve docs for LightingModule   fix typos   revert a doctest   more fixes ,1
7173,CI: Azure (#5882),0.5050371,Resolved a bug where the wrong client was passed to collect cloud logs (#14684),  Update requirements.txt   Update docs/requirements.txt   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   freeze sphinx version   Update docs/source/conf.py   Co-Authored-By: Adrian Wälchli aedu.waelchli@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: J. Borovec jirka.borovec@seznam.cz,0
7174,Early stopping (#332),0.6917319,EarlyStopping now runs at the end of the training epoch by default (#8286),  add trainer attribute to denote if interrupted   bugfix and formatting ,0
7175,Remove type error handling in _configure_checkpoint_callbacks (#9823),0.72538185,Replace MisconfigurationException with warning in ModelCheckpoint Callback (#4560),  added intersphinx links for base packages   remove comment ,1
7176,remove tpu barrier (#2260),0.64021635,moved TPU xxx_step to backend (#3118),,0
7177,Tpu features (#932),0.7109493,TPU core selection,  Add warning for few workers   Fix style issue   Update CHANGELOG.md   Update test   formatting   formatting   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
7178,Document running dev lightning on the cloud (#15962),0.67547154,- Fixed a bug with a default CloudCompute for Lightning flows ([#15371](https://github.com/Lightning-AI/lightning/pull/15371)),  forgot change logs   more missing   more missing ,0
7179,Update community section (#11510),0.51220703,Updated governance docs,  model_checkpoint to save all models   changelog   rise if   Co-authored-by: jamesjjcondon jamesjjcondon@gmail.com Co-authored-by: J. Borovec jirka.borovec@seznam.cz,0
7180,add Sphinx Check (#844),0.58241737,Pinned sphinx-autodoc-typehints with <v1.15 (#11400),  improved docs for callbacks   class references   make doctest pass   doctests   fix lines too long   fix line too long   fix permission error in doctest   Apply suggestions from code review   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   fix doctest   fix default   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
7181,scaled batch size,0.8332751,Tune batch size,  simplify examples structure   update changelog   fix imports   rename example   rename scripts   changelog ,1
7182,Fix pre-commit isort failure on tests/loggers/*.py (#5425),0.6674839,Dropped official support/testing for PyTorch <1.6 (#8288),  SA: for #958: set torch cuda device when finding root   SA: for #958: removing root gpu hack in trainer/evaluation_loop   SA: setting torch cuda device   comment line too long   check if root gpu exists or available   Incorporating suggestions on #1094   since root gpu returns none instead of -1 for cpu   undo changes   fixed dp memory thing   Co-authored-by: Shubham Agarwal shubhamagarwal92@gmail.com,0
7183,Update bug_report.md,0.6006475,Fixing critical bugs in newly added hooks and hparams assignment.,  generalize reinstantiation of dataloader   fix condition   add test   update changelog   fix changelog   Co-authored-by: J. Borovec jirka.borovec@seznam.cz,0
7184,cleaned up demos,0.63344544,some minor cleaning,  added warnings and removed default optimizer   opt   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
7185,App: Remove the unsupported params for CloudCompute (#14852),0.6073117,Resolved a bug where the wrong client was passed to collect cloud logs (#14684),"  Allow reinits in sub procs   Dont create an experiment on pickle, name, or project   Comments consistency   Fix test   Apply suggestions from code review   Co-authored-by: Chris Van Pelt vanpelt@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",0
7186,CI: Avoid using fixed container name (#15397),0.5729105,Re-enabled naming metrics in ckpt name (#3060),  fix(wandb): fix watch method   rebased   Apply suggestions from code review   Co-authored-by: Boris Dayma boris.dayma@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
7187,new log section [skip ci] (#4121),0.65268743,Removed LoggerStages (#5673),  Doc fixes from #1357 (awaelchli's comments) + changelog.   Fix indentation.   Add blank line to fix doc build? ,0
7188,fix mypy typing errors in pytorch_lightning/strategies/single_tpu.py (#13534),0.70077294,+ # pytorch_lightning==1.7.0,  update basic examples   update domain examples   reinforse -> reinforce   update full examples   update multi node examples   update examples readme   fix copy paste   fix line too long ,1
7189,Add _gpus_arg_default in argparse_utils for backward compatibility (#7402),0.71573853,argparse_utils >> argparse,,1
7190,updated auto ddp for > 1 node,0.65686685,Deprecated num_nodes and sync_batchnorm arguments in DDPPlugin and DDPSpawnPlugin (#7026),,0
7191,Updates links to components in the Gallery (#14807),0.56179243,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",,0
7192,Update DDPStrategy to use optimizers property from within class (#11777),0.71739155,Refactored optimizer (#4658),,1
7193,Add change log for Accelerator (#11591),0.6863004,Registering Custom Accelerators,,0
7194,update tests to not rely on patched dataloaders (#9905),0.67535484,"Removed automatic patching of {train,val,test,predict}_dataloader() on the LightningModule (#9764)",  Add parity test for simple RNN   Update test_rnn_parity.py   Co-authored-by: William Falcon waf2107@columbia.edu,0
7195,Minor edit in README (#3959),0.5285814,Truncated long version numbers in progress bar (#2594),  reorder if clauses   fix wrong method overload in test   fix formatting   update change_log   fix line too long ,0
7196,Fix docs typo (#2803),0.5151395,Fixing critical bugs in newly added hooks and hparams assignment.,  Make training_epoch_end behave like validation_epoch_end + minor fixes in docstrings.   Minor fixes (Borda's comments).   Detach tensors in batch_output (to avoid possible memory leak) + doc fix.   Co-authored-by: Jean-Baptiste SCHIRATTI jean-baptisteschiratti@MacBook-Pro-de-Jean-Baptiste.local,0
7197,Match the number of outputs of backward with forward for AllGatherGrad (#6625),0.63649935,    # 4. Do something with all outputs,  quick patch   testing fix   testing fix   testing fix   testing fix   testing fix   testing fix   testing fix   testing fix   testing fix   testing fix   testing fix   testing fix   testing fix ,0
7198,fix self.device access in DataParallel (#6414),0.60895866,"Accessing dataloaders (#16726, #16800)",  Fix typo   Fix typo ,0
7199,Update CHANGELOG following patch release (#9255),0.6684428,Full Changelog: https://github.com/Lightning-AI/lightning/compare/1.8.0...1.8.0.post1,"  show progress bar dependent on refresh_rate   test progress_bar_refresh control show bar   remove show_progress_bar from other tests   borda fixes   flake8 fix   changelog update prog bar refresh rate   move show_progress_bar to deprecated 0.9 api   rm show_progress_bar references, test deprecated   Update pytorch_lightning/trainer/init.py   fix test   changelog   minor CHANGELOG.md format   Update pytorch_lightning/trainer/init.py   Update pytorch_lightning/trainer/trainer.py   Co-authored-by: Gerard Bentley gbkh2015@mymail.pomona.edu Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: J. Borovec jirka.borovec@seznam.cz",0
7200,Changelog (#869),0.8279759,Complete changelog,  MNIST digits   increase test acc   smaller parity   drone builds   increase GH action timeout   drone format   fix paths   drone cache   circle cache   fix test   lower nb epochs   circleCI   user orb   fix test   fix test   circle cache   circle cache   circle cache   comment caches   benchmark batch size   cache dataset   smaller dataset   smaller dataset   fix nb samples   batch size   fix test ,1
7201,[fix] Enable manual optimization DeepSpeed (#7970),0.9641441,Support for manual optimization with DeepSpeed (#7970),  add python 3.8 test   update info   py38 >> torch 1.2   skip py38 minimal   changelog   Co-authored-by: William Falcon waf2107@columbia.edu,1
7202,Update endpoint access examples: added info about accessing auth-protected apps (#16145),0.552601,Updated app URLs to the latest format (#16568),  add check_model_configuration method   trying to fix errors   trying to fix tests   added test_epoch_end to lightning template   fix tests   fix new test after rebase   fix spelling   added more checks   updated formating   added tests   fixed CHANGELOG   Apply suggestions from code review   move test to new module   change check on configure_optimizers   Co-authored-by: Nicki Skafte nugginea@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
7203,Update docs (#1656),0.654456,Updated governance docs,,0
7204,fix tests (#938),0.69558024,- fixed all the .test() calls,"  Add warning when using default optimizer   Refactor optimizer tests to test_optimizers   Remove default optimizer, add option to use no optimizer   Update CHANGELOG.md   Update pytorch_lightning/trainer/optimizers.py   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Fix style  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",0
7205,Seed all workers when using DDP (#7942),0.5943657,Default seed_everything(workers=True) in the LightningCLI (#7504),  Add Workflow for Automated pypi releases   Rename pypi_release to pypi_release.yml   Update .github/workflows/pypi_release.yml   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update .github/workflows/pypi_release.yml  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Add releases to test.pypi  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
7206,Fix double precision during evaluation (#12983),0.71751297,Mixed precision overhaul (#16783),  removes need to unsqueeze from dp   removes need to unsqueeze from dp   fixed examples   added auto unsqueeze   added auto unsqueeze   added auto unsqueeze   added auto unsqueeze   Update pytorch_lightning/overrides/data_parallel.py   Co-Authored-By: Adrian Wälchli adrian.waelchli@students.unibe.ch   fixed dp parse   fixed dp parse   Co-authored-by: Adrian Wälchli adrian.waelchli@students.unibe.ch,1
7207,Remove AcceleratorConnector.tpu_cores and Deprecate Trainer.tpu_cores (#12437),0.79664373,- Removed `AcceleratorConnector.tpu_cores` property ([#12437](https://github.com/PyTorchLightning/pytorch-lightning/pull/12437)),  feat(wandb): save models on wandb   docs(changelog): allow to upload models on W&B ,1
7208,deprecate flush_logs_every_n_steps on Trainer (#9366),0.81415844,"Deprecated passing flush_logs_every_n_steps as a Trainer argument, instead pass it to the logger init if supported (#9366)",  fixed extra dataloader bug   Update pytorch_lightning/trainer/training_loop.py   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   updated CHANGELOG   Small non-repetition change   self.get_model() => model as it was already defined   Update CHANGELOG.md   changed argument name to reload_train_dataloader_every_epoch   fixed doc underline too short   reverted to reload_dataloaders_every_epoch   fixed val and test reloading   fixed val and test reloading   Co-authored-by: TevenLeScao teven.lescao@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
7209,Remove the unused utilities.metrics.metrics_to_scalars (#16681),0.7836727,Removed deprecated metrics (#8586),  sampler   check for dataloader type   check for dataloader type   fixed sampler cases ,1
7210,checkpoint only on rank=0 now,0.6723858,Checkpoint and early stopping now work without val. step (#1041),  sampler   sampler   sampler   check for dataloader type   check for dataloader type ,0
7211,Remove silent behavior when num_slurm_tasks does not correspond to number of processes in Trainer (#14300),0.7108426,"Setting Trainer(accelerator=""ddp_cpu"") now does not spawn a subprocess if num_processes is kept 1 along with num_nodes > 1 (#9603)","  init_optimizers accepts Dict, Sequence[Dict] and returns optimizer_frequencies. optimizer_frequencies was added as a member of Trainer.   Optimizer frequencies logic implemented in training_loop. Description added to configure_optimizers in LightningModule   optimizer frequencies tests added to test_gpu   Fixed formatting for merging PR #1269   Apply suggestions from code review   Apply suggestions from code review   Co-Authored-By: Asaf Manor 32155911+asafmanor@users.noreply.github.com   Update trainer.py   Moving get_optimizers_iterable() outside.   Update note   Apply suggestions from code review   formatting   formatting   Update CHANGELOG.md   formatting   Update CHANGELOG.md   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",1
7212,removing this troubling test that has random behavior (#3941),0.60799694,Removed no return warning from val/test step (#6139),,0
7213,Bugfix/_has_len (#2307),0.6032628,tons of bug fixes :wink:,  move some tests to trainer file   fix imports ,0
7214,Minor LightningLite clean up (#15780),0.7658031,LightningLite,"  refactor and add types   add Prorfiler summary   fix imports   Revert ""refactor and add types""   This reverts commit b4c552fa   changelog   revert rename   fix test   mute verbose ",1
7215,[1/n] Add support for iterable datasets to Rich ProgressBar (#9734),0.68872666,New Rich Progress Bar,Co-authored-by: mergify[bot] 37929162+mergify[bot]@users.noreply.github.com,0
7216,update PR guidelines (#993),0.5508088,[1.3.0] - 2021-05-06,  early stop fallback to train epoch   added test   fix imports   update docs   update changelog   fix typo ,0
7217,Disable recurrent events on forks (#8668),0.58976316,Avoid redundant callback restore warning while tuning (#13026),  Replace Wandb callback's finalize with no-op   Update pytorch_lightning/loggers/wandb.py   Update wandb.py   remove wandb logger's finalize and update tests   update changelog   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: William Falcon waf2107@columbia.edu,0
7218,"Deprecate num_processes,gpus, tpu_cores, and ipus from the Trainer constructor (#11040)",0.7392533,"- Deprecated `num_processes`, `gpus`, `tpu_cores,` and `ipus` from the `Trainer` constructor in favor of using the `accelerator` and `devices` arguments ([#11040](https://github.com/Lightning-AI/lightning/pull/11040))",  error_on_zero_length   update CHANGELOG.md   added test   Update pytorch_lightning/trainer/data_loading.py   Co-authored-by: Nicki Skafte nugginea@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
7219,Replace range with RandomDataset in example (#17325),0.5638029,Sampler replacement in distributed strategies (#16829),  formatting   formatting   fix interval   fix train loop   fix test   parametrize test   Apply suggestions from code review   Co-Authored-By: Adrian Wälchli adrian.waelchli@students.unibe.ch   fix calling   flake8   add types   Co-authored-by: Adrian Wälchli adrian.waelchli@students.unibe.ch Co-authored-by: William Falcon waf2107@columbia.edu,0
7220,meta pkg: wrap imports for traceability (#13924),0.75938296,Wrapped imports for traceability (#13924),  update mergify nb checks   lover merge action   draft auto review   fix rule ,1
7221,fix tests (#10138),0.68408465,- fixed all the .test() calls,  clear skipping tests   fix simple/multi GPU   review: simplify ,0
7222,PKG: distribute single semver (#15374),0.49819955,separate SLURM from DDP (#3809),  made load from checkpoint flexible   made load from checkpoint flexible   made load from checkpoint flexible ,0
7223,Update CHANGELOG after the 1.6.4 release (#13201),0.7048303,Full Changelog: https://github.com/Lightning-AI/lightning/compare/1.8.0...1.8.0.post1,"  fix RunningMean   changelog   fix none   Update supporters.py   just needed to multiply by zero for init  Revert ""Update supporters.py""  This reverts commit 7e0da6c6   fix NaN   formatting   Co-authored-by: William Falcon waf2107@columbia.edu",1
7224,use fsspec instead of gfile for all IO (#3320),0.9903204,Used fsspec instead of gfile for all IO (#3320),"  added custom mnist without torchvision dep   move files so it does not conflict with mnist gitignore   mock torchvision for tests   fix line too long   fix line too long   fix ""module level import not at top of file"" warning   move mock imports to init.py   simplify MNIST a lot and download directly the .pt files   further simplify and clean up mnist   revert import overrides   make as before   drop  PIL requirement   move mnist.py to datasets subfolder   use logging instead of print   choose same name as in torchvision   remove torchvision and pillow also from yml file   refactor if train   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   capitalized class attr   moved mnist to models   re-added datsets ignore   better name for file variable   Update mnist.py   move dataset classes to datasets.py   new line   update   update   fix automerge   move to base folder   adapt testingmnist to new mnist base class   remove temporal fix   fix datatype   remove old testingmnist   readable   fix import   fix whitespace   docstring   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update tests/base/datasets.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   changelog   added types   Update CHANGELOG.md   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  exist->isfile  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   index -> idx   temporary fix for trains error   better changelog message   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",1
7225,Fix typo in doc (#5270),0.5522722,Changed overwrite to True (#16009),  adding test   adding test   added base parity model   added base parity model   added parity test   added parity test   added parity test   added parity test   added parity test   added parity test   added parity test   added parity test   added parity test   added parity test   added parity test   added parity test   added parity test   added parity test   added parity test   move parity to benchmark   formatting   fixed gradient acc sched   move parity to benchmark   formatting   fixed gradient acc sched   skip for CPU   call last   Co-authored-by: J. Borovec jirka.borovec@seznam.cz,0
7226,Mark certain Trainer APIs as protected (#7420),0.7208384,Changed Trainer connectors to be protected attributes:,  make evaluate private   changelog ,1
7227,fix 1.9 test (#7441),0.63935137,- fixed all the .test() calls,  auto_add_sampler() fix   auto_add_sampler() fix   Co-authored-by: seth seth@duckpapa.com,0
7228,Add tutorial extension to the docs (#9167),0.70617104,"For a full tutorial and running example, visit our docs. TODO: add to docs",,1
7229,Add the on_before_optimizer_step hook (#8048),0.7855064,"The on_before_optimizer_step hook previously ran before the entire optimization closure, including backward. This was unintended behavior and if you rely on this, move your code to the new on_before_backward` hook.",  remove .item which causes sync issues   fixed gradient acc sched   fixed gradient acc sched ,1
7230,Fix CheckpointIO doc annotations (#8931),0.67720413,CheckpointIO Plugins,,0
7231,Refactor notebooks (#7752),0.62043536,Refactored EpochResultStore (#5522),  Updated Sequencial Data docs   Sequntial Data section now contains info on using IterableDatasets    Undid reformatting of bullet points     added information about val_check_interval    Co-authored-by: Donal Byrne Donal.Byrne@xperi.com,0
7232,True half-precision support in Fabric (#17287),0.6207685,fabric.backward(loss),  Add support for iterable datasets when val_check_interval=1.0   Update CHANGELOG.md ,0
7233,Update Fabric docs navigation (#16957),0.56559336,Learn more about Fabric and what it can do in the new docs!,,0
7234,Fix unpickle redirection (#15382),0.5996779,Avoid relpath bug on Windows (#16164),"  Example: Simple RL example using DQN/Lightning   DQN RL Agent using Lightning   Uses Iterable Dataset for Replay Buffer   Buffer is populated by agent as training is carried out, updating the dataset   Applied autopep8 fixes    Updated line length from 120 to 110    Update pl_examples/domain_templates/dqn.py   simplify get_device method Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update pl_examples/domain_templates/dqn.py  Re-ordered imports Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   CI: split tests-examples (#990)   CI: split tests-examples   tests without template   comment depends   CircleCI typo   add doctest   update test req.   CI tests   setup macOS   longer train   lover pred acc   fix model   rename default model   lower tests acc   typo   imports   fix test optimizer   update calls   fix Win   lower Drone image   fix call   pytorch image   fix test   add dev image   add dev image   update image   drone volume   lint   update test notes   rename tests/models >> tests/base   group models   conftest   optim imports   typos   fix import   fix tests   install AMP   tests   fix import   Clean up   added module docstring   renamed variables to be more descriptive   Added missing docstrings and type annotations   Added gym to example requirements   Added note to changelog   updated example image   update types   rename script   Update CHANGELOG.md   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   another rename   Disable validation when val_percent_check=0 (#1251)   fix disable validation   add test   update changelog   update docs for val_percent_check   make ""fast training"" docs consistent   calling self.forward() -> self() (#1211)   self.forward() -> self()   update changelog   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   Fix requirements-extra.txt Trains package to release version (#1229)   Fix requirement-extra use released Trains package   Update README.md add Trains and links to the external Visualization section   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com  Remove unnecessary parameters to super() in documentation and source code (#1240)  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   update deprecation warning (#1258)   update docs for progress bat values (#1253)   lower timeouts for inactive issues (#1250)   update contrib list (#1241)   Co-authored-by: William Falcon waf2107@columbia.edu   Fix outdated docs (#1227)   Fix typo (#1224)   drop unused Tox (#1242)   system info (#1234)   system info   update big info   test script   update config   rename script   import path   Changed smoothing in tqdm to decrease variability of time remaining between training / eval (#1194)   Example: Simple RL example using DQN/Lightning   DQN RL Agent using Lightning   Uses Iterable Dataset for Replay Buffer   Buffer is populated by agent as training is carried out, updating the dataset   Applied autopep8 fixes    Updated line length from 120 to 110    Update pl_examples/domain_templates/dqn.py   simplify get_device method Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update pl_examples/domain_templates/dqn.py  Re-ordered imports Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   Clean up   added module docstring   renamed variables to be more descriptive   Added missing docstrings and type annotations   Added gym to example requirements   Added note to changelog   update types   rename script   Update CHANGELOG.md   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  another rename  Co-authored-by: Donal Byrne Donal.Byrne@xperi.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: Adrian Wälchli adrian.waelchli@students.unibe.ch Co-authored-by: Jeremy Jordan 13970565+jeremyjordan@users.noreply.github.com Co-authored-by: Martin.B 51887611+bmartinn@users.noreply.github.com Co-authored-by: Tyler Yep tyep@stanford.edu Co-authored-by: Shunta Komatsu 59395084+skmatz@users.noreply.github.com Co-authored-by: Jack Pertschuk jackpertschuk@gmail.com",0
7235,Tpu logging (#2230),0.66940725,Logging,  Update Contributors in Readme   Update Governance ,0
7236,Update fit_loop.py (#12450),0.61986136,- Move `Strategy.process_dataloader` function call from `fit/evaluation/predict_loop.py` to `data_connector.py` ([#12251](https://github.com/PyTorchLightning/pytorch-lightning/pull/12251)),  save_top_k   num_gpus   print_nan_grads fix leftovers   undo doctest removal ,0
7237,Update tests/utilities/*.py to use devices instead of gpus or ipus (#11458),0.6463887,Dropped official support/testing for PyTorch <1.6 (#8288),,0
7238,MlflowLogger limit parameter value length to 250 char (#5893),1.0,MlflowLogger limit parameter value length to 250 char (#5893),  system info   update big info   test script   update config   rename script   import path ,1
7239,Remove the deprecated Trainer.reset_train_val_dataloaders (#16131),0.9634168,"Removed the deprecated Trainer.reset_train_val_dataloaders() in favor of Trainer.reset_{train,val}_dataloader (#16131)",,1
7240,Fabric docs feedback 2/n (#16480),0.5547994,Fabric,,0
7241,LightningCLI support for optimizers and schedulers via dependency injection (#15869),0.7275169,"    def configure_optimizers(lightning_module, optimizer, lr_scheduler=None):",,1
7242,Docs: fix link to title (#14730),0.5359068,Deprecated tags_csv in favor of hparams_file (#1271),Co-authored-by: William Falcon waf2107@columbia.edu,0
7243,Add non-existing resume_from_checkpoint acceptance for auto-resubmit (#4402),0.725248,Skip restore from resume_from_checkpoint while testing (#5161),,1
7244,docs (#4101),0.7535108,"docs for all Metrics (#2184, #2209)",,1
7245,update batch size in DataModule when auto scaling batch size (#3266),0.752891,Tune batch size,,1
7246,[App] Fix environment check with command redirection (#16883),0.5630203,Auto-upgrade / detect environment mis-match from the CLI (#15434),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
7247,"Mocking Loggers (part 3b, comet) (#3853)",0.6486729,Using .comet.config file for CometLogger (#1913),  Fix requirement-extra use released Trains package   Update README.md add Trains and links to the external Visualization section   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
7248,Remove unneeded filename print (#540),0.62314934,Deprecated Profiler(output_filename) in favor of dirpath and filename (#6621),  self.forward() -> self()   update changelog   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
7249,Fix typing in pl.overrides.fairscale (#10799),0.5990716,Changed overwrite to True (#16009),"  fix disable validation   add test   update changelog   update docs for val_percent_check   make ""fast training"" docs consistent ",0
7250,Added multi-optimizer tests with hpu (#13217),0.71045125,Working with multiple optimizers (#16539),,1
7251,Fix type checker issue with explicit cast of ref_model object (#4457),0.94383705,Changed type checker with explicit cast of ref_model object (#4457),  CI: split tests-examples   tests without template   comment depends   CircleCI typo   add doctest   update test req.   CI tests   setup macOS   longer train   lover pred acc   fix model   rename default model   lower tests acc   typo   imports   fix test optimizer   update calls   fix Win   lower Drone image   fix call   pytorch image   fix test   add dev image   add dev image   update image   drone volume   lint   update test notes   rename tests/models >> tests/base   group models   conftest   optim imports   typos   fix import   fix tests   install AMP   tests   fix import ,1
7252,Renamed DDPShardPlugin to DDPShardStrategy (#11187),0.79839885,Merged the implementation of DDPSpawnStrategy into DDPStrategy and removed DDPSpawnStrategy (#14952),  add_argparse_args method fixed (argument types added)   CHANGELOG.md upd   autopep8 fixes   --gpus=0 removed from test (for ci tests)   typo fixed   reduce on plateau scheduler fixed   Trainer cli related tests moved to test_trainer_cli.py   refactored: get_init_arguments_and_types is a public classmethod of the Trainer now   test_get_init_arguments_and_types added   autopep8 fixes   Trainer cli related tests moved to test_trainer_cli.py   refactored: get_init_arguments_and_types is a public classmethod of the Trainer now   test_get_init_arguments_and_types added   autopep8 fixes   Trainer cli related tests moved to test_trainer_cli.py   refactored: get_init_arguments_and_types is a public classmethod of the Trainer now   test_get_init_arguments_and_types added   autopep8 fixes   Trainer cli related tests moved to test_trainer_cli.py   test_get_init_arguments_and_types added   autopep8 fixes   Apply suggestions from code review   cosmetics   cosmetics   Update pytorch_lightning/trainer/trainer.py   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   Trainer.get_init_arguments_and_types now returns arg types wrapped in tuples (not in sets)   deprecated args are now ignored in argparser   get_deprecated_arg_names small refactor   get_deprecated_arg_names bug fixed   Trainer cli related tests moved to test_trainer_cli.py   refactored: get_init_arguments_and_types is a public classmethod of the Trainer now   test_get_init_arguments_and_types added   autopep8 fixes   Trainer cli related tests moved to test_trainer_cli.py   autopep8 fixes   Trainer cli related tests moved to test_trainer_cli.py   Trainer cli related tests moved to test_trainer_cli.py   test_get_init_arguments_and_types added   autopep8 fixes   autopep8 fixes   Apply suggestions from code review   cosmetics   cosmetics   Update pytorch_lightning/trainer/trainer.py   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   Trainer.get_init_arguments_and_types now returns arg types wrapped in tuples (not in sets)   deprecated args are now ignored in argparser   get_deprecated_arg_names small refactor   get_deprecated_arg_names bug fixed   Update pytorch_lightning/trainer/trainer.py   Co-Authored-By: Joe Davison joe@huggingface.co  Update pytorch_lightning/trainer/trainer.py  Co-Authored-By: Joe Davison joe@huggingface.co Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Joe Davison joe@huggingface.co Co-authored-by: William Falcon waf2107@columbia.edu,1
7253,Add FSDP docs (#7791),0.6410465,Read more about FSDP and how layer wrapping works in our docs.,  update chnagelog   formatting ,0
7254,Add bagua installation in dockerfile (#11283),0.51588506,Add --app_args support from the CLI (#13625),,0
7255,required tensorflow for tensorboardx install,0.8312762,Switch from tensorboard to tensorboardx in TensorBoardLogger (#15728),,1
7256,Fix typo on tpu.rst (#2759),0.5868485,Renamed TPUHalfPrecisionPlugin to TPUBf16PrecisionPlugin (#10026),  update links in docs   fix ,0
7257,"Update numpy requirement from <1.24.2,>=1.17.2 to >=1.17.2,<1.24.4 in /requirements (#17538)",0.61771446,"- num_devices = max(1, trainer.num_gpus, trainer.num_processes)",  Mergify: configuration update   Update .mergify.yml   Update .mergify.yml   Update .mergify.yml ,0
7258,Make manual optimization mandatory for multiple optimizers (#16539),0.87559056,Working with multiple optimizers (#16539),  move docs images   click GH badges   fixed docs build error   Co-Authored-By: Adrian Wälchli adrian.waelchli@inf.unibe.ch Co-authored-by: Adrian Wälchli adrian.waelchli@inf.unibe.ch,1
7259,use tox,0.41995984,Allow any input in to_onnx and to_torchscript (#4378),  add missing overfit_pct docs   move arg to old position   move arg docs ,0
7260,5/n: Extract reference model call to plugins/accelerators (#4773),0.74984354,Refactored Accelerators and Plugins (#5743),,1
7261,Deprecate process_position from the Trainer constructor (#9222),0.7964995,Deprecated passing process_position to the Trainer constructor in favor of adding the ProgressBar callback with process_position directly to the list of callbacks (#9222),  increase profiler test coverage   fix line length   tests for valueerror assertions ,1
7262,Fix Trainer arg name in docs (#2879),0.67915916,"trainer/separate argparse (#3421, #3428, #3432)",  pylint   model API   update test   formatting   disable logger   fix checking overwrite   fix test   typo   deprecated model   fix for DDP   drop Flake8 in GH actions   Update pytorch_lightning/trainer/evaluation_loop.py   fix imports   Co-authored-by: Nic Eggert nic@eggert.io,0
7263,docs: fix fomatting,0.5973022,Docs improvements,"  add argument to force warn   fix automodule error   fix permalink error   fix indentation warning   fix warning   fix import warnings   fix duplicate label warning   fix bullet point indentation warning   fix duplicate label warning   fix ""import not top level"" warning   line too long   fix indentation   fix bullet points indentation warning   fix hooks warnings   fix reference problem with excluded test_tube   fix indentation in print   change imports for trains logger   remove pandas type annotation   Update pytorch_lightning/core/lightning.py   include bullet points inside note   remove old quick start guide (unused)   fix unused warning   fix formatting   fix duplicate label issue   fix duplicate label warning (replaced by class ref)   fix tick   fix indentation warnings   docstring ticks   remove obsolete docstring typing   Revert ""remove old quick start guide (unused)""   This reverts commit d51bb40695442c8fa11bc9df74f6db56264f7509.   added old quick start guide to navigation   remove unused  tutorials file   ignore some modules that got deprecated and are not used anymore   fix duplicate label warning   move examples doc and exclude pl_examples from autodoc   fix formatting for configure_optimizer   fix no blank line warnings   fix ""see also"" labels and add paramref extension   fix more reference problems   fix multi-gpu reference   fix weird warning   fix indentation and unrecognized characters in code block   fix warning ""... not included in toctree""   fix PIL import error   fix duplicate target ""here"" warning   fix broken link   revert accidentally moved pl_examples   changelog   stdout   note some things to know   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: J. Borovec jirka.borovec@seznam.cz Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",0
7264,need to fix readme for pypi,0.6426143,"PyTorch 1.10 and Python 3.7 no longer supported (#16492, #16579)","  check for nan values   test nan detection on loss   sys.exit   whitespace   detect nan and inf values in loss and params   update   added documentation   moved detect nan to training loop, remove flag for print   blank line   test   rename   deprecate print_nan_grads   deprecated print_nan_grads   remove unused imports   update changelog   fix line too long   correct deprecated version   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  raise exception instead of sysexit  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  raise exception instead of sysexit  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/trainer/training_tricks.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/trainer/training_tricks.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  fix test  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",0
7265,[TPU] Fix workflow condition (#17379),0.63890916,Updated logic for checking TPUs availability (#6767),,0
7266,[CI] Comment flaky tests (#10084),0.5613334,test_percent_check in favour of limit_test_batches,  Update docs so users know the desired manner of configuring learning rate schedulers.   update list   as note   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
7267,Add doc strings to CSV logger (#9112),0.5777248,- Added support for writing logs remote file systems on `CSVLoggers`. ([#16880](https://github.com/Lightning-AI/lightning/pull/16880)),  Add support for hierarchical dict   Support nested Namespace   Add docstring   Migrate hparam flattening to each logger   Modify URLs in CHANGELOG   typo   Simplify the conditional branch about Namespace   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update CHANGELOG.md  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   added examples section to docstring   renamed _dict -> input_dict   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
7268,Remove Result(minimize) parameter [4/n] (#7628),0.62900025,remove _evaluate fx (#3197),  ignore in setup   show report   abs imports   abstract pass   cover loggers   doctest trains   locals   pass   revert tensorboard   use tensorboardX   revert tensorboardX   fix trains   Add TrainsLogger.set_credentials (#1179)   Add TrainsLogger.set_credentials to control trains server configuration and authentication from code. Sync trains package version. Fix CI Trains tests   Add global TrainsLogger set_bypass_mode (#1187)   Add global TrainsLogger set_bypass_mode skips all external communication   Co-authored-by: bmartinn <>  rm some no-cov  Co-authored-by: Martin.B 51887611+bmartinn@users.noreply.github.com,0
7269,Fix environment variable order for global rank determination (#11406),0.7653053,- Fixed environment variable priority for global rank determination ([#11406](https://github.com/PyTorchLightning/pytorch-lightning/pull/11406)),  check if hparams_type exists in checkpoint dictionary for backward compatibility   concisely maintain backward compatibility for hparams type   Bug fix in checkpoint loading (#1132) ,1
7270,Fix num_classes arg in F1 metric (#5663),0.6110802,Changed resolve_training_type_plugins to allow setting num_nodes and sync_batchnorm from Trainer setting (#7026),Fix test Fix format Update pytorch_lightning/init.py Separate imports,0
7271,Don't raise a warning when nn.Modules are not saved under hparams (#12669),0.9884987,Don't raise a warning when nn.Module is not saved under hparams (#12669),  fixed docs   Docs (#1164)   fixed docs   fixed docs   fixed docs   fixing Win failed import (#1163)   version   try fix distrib   update try import   fixed docs   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
7272,[App] Add Missing Copyright (#16625),0.4822567,@property,  version   try fix distrib   update try import ,0
7273,Label new issues as needs triage by default (#12403),0.61390066,Fixing critical bugs in newly added hooks and hparams assignment.,  fixed docs   fixed docs   fixed docs ,0
7274,[App] PoC: Add support for Request (#16047),0.5610353,Add --app_args support from the CLI (#13625),,0
7275,Update tests/plugins/test_sharded_plugin.py,0.70482796,Skipped testing with PyTorch 1.7 and Python 3.9 on Ubuntu (#11217),  bug fix and test   update CHANGELOG.md   Co-authored-by: Nicki Skafte nugginea@gmail.com,1
7276,Added black formater for the code with code-checker on pull (#1610),0.51324856,"From now on, the backend has to be set in the code (#14693):",  drop pandas   formatting ,0
7277,Warn when http URLs are configured (#14233),0.6254438,Do not override PYTHONWARNINGS (#4700),"  removed project and experiment from getstate   added tests for closing experiment, updated token in example to user neptuner   updated teoken   Update neptune.py   added a link to example experiment   added exmaple experiment link   dropped duplication   flake fixes   merged with master, added changes information to CHANGELOG ",0
7278,Cascade SIGTERM to subprocesses (#16525),0.6030797,Another feature is automatic SIGTERM handling:,,0
7279,[FIX] Native FSDP precision + tests (#12985),0.72152895,Generalized Optimizer validation to accommodate both FSDP 1.x and 2.x (#16733),"  Add allegro.ai TRAINS experiment manager support   improve docstring and type hinting, fix the bug in log_metrics, add support torch.Tensor to input into log_image   complete missing docstring of constructor's arguments   fix docs   pep8   pep8   remove redundant typing use logging fix typing and pep8   remove deprecated interface   add TrainsLogger test   add TrainsLogger PR in CHANGELOG   add id/name property documentation   change logging as log   Co-authored-by: bmartinn <> Co-authored-by: Sou Uchida s.aiueo32@gmail.com",1
7280,Expose extract_batch_size method and add corresponding tests. (#8357),0.6999267,Run batch size finder for validate/test/predict.,  Added support for non-primitive types to tensorboard logger   added EOF newline   PEP8   Updated CHANGELOG for PR #1130. Moved _sanitize_params to base logger. Cleaned up _sanitize_params   Updated CHANGELOG for PR #1130. Moved _sanitize_params to base logger. Cleaned up _sanitize_params   changed convert_params to static method   PEP8   Cleanup Doctest for _sanitize_params   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   Removed OrderedDict import   Updated import order to conventions   Co-authored-by: Manbir Gulati manbirgulati@Manbirs-MBP.hsd1.md.comcast.net Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
7281,Introduce lightning connect (#14452),1.0000002,Introduce lightning connect (#14452),  update config   try Drone cache   drop Drone cache   move import   remove token ,1
7282,Add custom logic to each OutputResult subclass [2/2] (#9424),0.5946022,"adding compute environments (#3837, [#3842)","  first pass for LightningModule typehints   fix return types   add missing types   add type annotations to grads.py   add type annotations to hooks.py   add type annotation to memory.py   proper docstring quotation marks   add type annotations to saving.py   fix cyclic import problem   fix cyclic import problem   add missing whitespace   finish type hints for load_from_ methods   docs: prepare_data does not return anything   fix auto types in docs   revert typehint for trainer in hook   remove unnecessary return docs   some fixes for memory docs   revert typing for args kwargs   added all missing None return types   remove unused import   add more details to dict/list return types   fix line too long   optimize imports   linted   Revert ""linted""   This reverts commit 85559611e84e312bce64f4e73b638d4999a8439e.   remove whitespace   update   update   update   update   update   changelog   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: William Falcon waf2107@columbia.edu",0
7283,fix mypy typing errors in pytorch_lightning.setup.py (#13472),0.7192251,+ # pytorch_lightning==1.7.0,  fix tmpdir   just str path ,1
7284,Bump Lightning-AI/utilities from 0.6.0 to 0.7.1 (#16879),0.6984195,- Removed the `LightningModule.precision` attribute ([#16203](https://github.com/Lightning-AI/lightning/pull/16203)),"  Add support for IterableDatasets everywhere   Added type hints, simplified code and improved coverage in data_loading.py   Update CHANGELOG.md ",0
7285,Remove deprecated checkpoint_callback flag in Trainer (#13027),0.79721034,Deprecated checkpoint_callback from the Trainer constructor in favor of enable_checkpointing (#9754),tested only for the CPU version,1
7286,try to fix imports for parsing (#6256),0.61923397,import argparse,,0
7287,Prelease 1.1.2rc (#5171),0.65886104,"    version=""0.0.1"",",,0
7288,fic CI parsing Horovod version (#3804),0.6597647,"refactored Horovod backend (#3121, #3122)",,0
7289,Also update progress_bar in training_epoch_end (#1724),0.92609394,The progress bar metrics now also get updated in training_epoch_end (#1724),,1
7290,"Remove start_{training,evaluating,predicting} from HorovodPlugin  (#10989)",0.63565683,Deprecated the HorovodStrategy class,Co-authored-by: xingzhaolee xingzhaolee@users.noreply.github.com,0
7291,Fault Tolerant Manual: Add stateful dataloader iter (#10674),0.6866943,Make the _LiteDataLoader an iterator and add supports for custom dataloader (#10279),,0
7292,Docs section for SLURM troubleshooting (#14873),0.63892156,"Unify sLURM/TorchElastic under backend plugin (#4578, #4580, #4581, #4582, #4583)",,0
7293,Simple reproducibility with minimum boilerplate CLI training with LightningCLI (#4492),0.72231364,LightningCLI improvements,,1
7294,Fixes #2942 (#2969),0.61441875,Resolve bug with Finetuning (#5744),  add Drone config   update Drone config   add Drone config   list GPUs   add type   native torch   native torch   fix image   update   SLURM_LOCALID   add badge   simple test ,0
7295,ref: clean config [1/n] add intermediate setters (#4990),0.5914161,setup(,  update chnagelog   update chnagelog ,0
7296,Refactor log_dir usage in the CLI (#8419),0.7028775,Refactored logging,,1
7297,Flash predict step (#6577),0.5550764,"- Added support for `predict_step(dataloader_iter, batch_index)` ([#16726](https://github.com/Lightning-AI/lightning/pull/16726))",,0
7298,"[Hot Fix] Give priority to plugins to set distributed mode, and then accelerator (#6089)",0.69252205,Refactored Accelerators and Plugins (#5743),,0
7299,docs cleaning - testcode (#5595),0.7409955,"Cleaning (#5948, #5949, #5950)",,1
7300,added cpu 16 bit,0.5916074,16-bit can now be used with a single GPU (no DP or DDP in this case). bypasses issue with NVIDIA apex and PT compatibility for DP+16-bit training. ,  print issue   print issue   print issue   print issue   print issue ,0
7301,initial commit,0.41345024,setup(,,0
7302,Merge pull request #1645 from danpcampbell/bugfix/1637_slurm-fix,0.58635706,refactored dataloader process hook (#3139),,0
7303,Add Test for memory consumption (#7733),0.72967434,Resolve memory leak for evaluation (#6326),,1
7304,Update Grid links to Lightning AI (#14081),0.72803295,Add support for listing Lightning AI apps (#13987),  change version in CHangelog   warning   remove der data_loader   Co-authored-by: William Falcon waf2107@columbia.edu,1
7305,tests drop macOS py38 (#2054),0.65048563,Dropped official support/testing for PyTorch <1.6 (#8288),,0
7306,[CLI] Shorthand notation to instantiate models (#9588),0.89194393,Support shorthand notation to instantiate models (#9588),,1
7307,Fix dataloaders in TPU example (#1174),0.7379766,Ensured process_dataloader is called when tpu_cores > 1 to use Parallel DataLoader (#6015),,1
7308,enable returning only opt list (#114),0.5590422,Changed calling of untoggle_optimizer(opt_idx) out of the closure function (#7563),,0
7309,Refine style guide (#11394),0.5425941,Refactoring,  Update README.md   Update README.md   Update README.md   Update README.md   Update README.md   Update README.md   Update README.md   Test deprecated API for 0.8.0 and 0.9.0 (#1071)   till 0.8   refactor   fix tests   fix tests   deprx till 0.9   Update trainer.py   Apply suggestions from code review   Co-authored-by: William Falcon waf2107@columbia.edu   updated test   updated test   updated test   updated test   updated test   updated test   updated test   updated test   updated test   updated test   updated test   updated test   updated test   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
7310,passing batch outputs to on_train_batch_end (#4369),0.93835753,Pass batch outputs to on_train_batch_end instead of epoch_end outputs (#4369),,1
7311,Mem crash (#299),0.49434572,"refactor DDP backend (#3204, #3207, #3208, #3209, #3210)",  till 0.8   refactor   fix tests   fix tests   deprx till 0.9   Update trainer.py   Apply suggestions from code review   Co-authored-by: William Falcon waf2107@columbia.edu,0
7312,Fix LearningRateMonitor logging with multiple param groups optimizer with no scheduler (#10044),0.7484087,Allowed log_momentum for adaptive optimizers in LearningRateMonitor (#5333),,1
7313,Moved common functions into utilities,0.57075894,"Add deprecated metric utility functions back to functional (#5067, #5068)",  fix copy-paste errors after renaming *_end methods   line too long   update   Update lightning.py   Update lightning.py   Update lightning.py   Co-authored-by: William Falcon waf2107@columbia.edu,0
7314,Prune deprecated utils modules (#7503),0.6981505,"Removed deprecated utils modules model_utils, warning_utils, xla_device_utils and partially argparse_utils (#7503)",,0
7315,Refactor load in checkpoint connector (#4593),0.99999994,Refactor load in checkpoint connector (#4593),  wip   notes   WIP   example   notes ,1
7316,CI name for TPU (#15258),0.67824745,TPU core selection,,0
7317,Remove deprecated on_init_start_end (#14867),0.6647334,Removed deprecated early_stop_callback (#3982),,0
7318,Remove deprecated pytorch_lightning.core.decorators.parameter_validation (#13514),0.92759705,- Removed deprecated `pytorch_lightning.core.decorators.parameter_validation` from `decorators` ([#13514](https://github.com/Lightning-AI/lightning/pull/13514)),,1
7319,remove EvalModelTemplate from tests (#10970),0.74176276,Removed deprecated EvalResult (#5633),,1
7320,Fix test failing on master due to bad auto-merge (#16118),0.5782956,"Removed experimental fault-tolerance support (#16516, #16533)",  fix deprecated warnings   deprecated ,0
7321,CI: Reuse clear cache (#14593),0.56985676,clean up data reset (#3161),,0
7322,fix typos (#11937),0.5799968,Changed overwrite to True (#16009),  new image   new image ,0
7323,log named parameters (#660),0.67765915,Changed gradient logging to use parameter names instead of indexes (#660),  prepare_data assignment   prepare_data assignment   prepare_data assignment   prepare_data assignment   prepare_data assignment   new image ,0
7324,enable Codecov (#1133),0.589394,Introducing CLI commands for apps (#13602)!,  prepare_data assignment   prepare_data assignment   prepare_data assignment   prepare_data assignment   prepare_data assignment   prepare_data assignment ,0
7325,Remove dead ModelCheckpoint code (#15534),0.8082936,Removed the deprecated model argument from ModelCheckpoint.save_checkpoint (#8688),fix import in documentation sample code,1
7326,Fix typing in pl.plugins.environments (#10943),0.6303965,Updated error message for interactive incompatible plugins (#9896),  fix example indent and warnings   appease the linter ,0
7327,Mapkeys (#2900),0.54747826,Key updates,,0
7328,add coding styleguide (#460),0.57540894,Allowing decorate model init with saving hparams inside (#4662),,0
7329,Deprecate LightningDistributed and keep logic in ddp/ddpSpawn directly (#9691),0.8324986,Deprecated LightningDistributed and moved the broadcast logic to DDPPlugin and DDPSpawnPlugin directly (#9691),,1
7330,Fix argparse default value bug (#2526),0.72432685,argparse_utils >> argparse,,1
7331,Fix domain_template scripts (#2014),0.59761775,- Removed deprecated ClusterEnvironment properties `master_address` and `master_port` in favor of `main_address` and `main_port` ([#13458](https://github.com/Lightning-AI/lightning/pull/13458)),,0
7332,Address code review,0.51423806,- Code coverage (99%),,0
7333,[App] Change app root / config path to be the app.py parent directory (#15654),0.7637071,"Changed the root directory of the app (which gets uploaded) to be the folder containing the app file, rather than any parent folder containing a .lightning file (#15654)",,1
7334,Run on_train_epoch_end after the LM for callbacks that monitor (#16567),0.67354584,Changed EarlyStopping callback from by default running EarlyStopping.on_validation_end if only training is run. Set check_on_train_epoch_end to run the callback at the end of the train epoch instead of at the end of the validation epoch (#7069),,0
7335,Docs 2/n (#15521),0.680211,Docs,,0
7336,use badges only with push (#3914),0.5337964,Changed the Trainer's checkpoint_callback argument to allow only boolean values (#7539),  docs   docs   docs   docs ,0
7337,Bugfix/torchtext include lengths (#2689),0.5595418,Resolve bug with Finetuning (#5744),  using new API   typo ,0
7338,et al. (#6050),0.6493052,[1.4.5] - 2021-08-31,  docs   docs ,0
7339,Update EarlyStopping docs (#7121),0.61218184,Docs improvements,  format Trainer   format LModule   format LModule   linted   Update lightning.py   Co-authored-by: William Falcon waf2107@columbia.edu,0
7340,Allow a CombinedLoader as the training data in DDP (#11648),0.7582506,- Added support for DDP when using a `CombinedLoader` for the training data ([#11648](https://github.com/PyTorchLightning/pytorch-lightning/pull/11648)),  training_end to training_step_end   training_end to training_step_end   training_end to training_step_end   training_end to training_step_end   training_end to training_step_end ,1
7341,CI: partial move from CircleCI (#2378),0.5643748,moved eval loop (#3412[#3408),  training_end renamed to training_step_end   training_end renamed to training_step_end   training_end renamed to training_step_end   training_end renamed to training_step_end   training_end to training_step_end   training_end to training_step_end   training_end to training_step_end   training_end to training_step_end   fix lost model reference   training_end to training_step_end   training_end to training_step_end   training_end to training_step_end   training_end to training_step_end   training_end to training_step_end   training_end to training_step_end   training_end to training_step_end   training_end to training_step_end   training_end to training_step_end   training_end to training_step_end   training_end to training_step_end   training_end to training_step_end   training_end to training_step_end   training_end to training_step_end   training_end to training_step_end   training_end to training_step_end   training_end to training_step_end   training_end to training_step_end   training_end to training_step_end   training_end to training_step_end   training_end to training_step_end   training_end to training_step_end   training_end to training_step_end   training_end to training_step_end   training_end to training_step_end   training_end to training_step_end   training_end to training_step_end   training_end to training_step_end ,0
7342,Always use the local rank zero imports (#16178),0.59048784,"Deprecated importing rank_zero_{warn,deprecation} directly from pytorch_lightning.utilities.distributed (#8085)",  remove deprecated args to learning rate step function   step based scheduler   mixing models for testing   fix styling   tests   update documentation   smaller fix   update to dict structure   updated test   update documentation   update CHANGELOG.md   fix styling   fix problems with trainer io   fix tests   simplification of code   fix styling   change from batch to step   update to tests   fix styling   fixed some logic   Update pytorch_lightning/core/lightning.py   duplicated test   fix test on amp   small update to tests   added monitor key for ReduceLROnPlateau   Update trainer.py   Update training_loop.py   fix test after introducing monitor keyword   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: William Falcon waf2107@columbia.edu,0
7343,Add more pytree tests (#16825),0.71843016,Skipped testing with PyTorch 1.7 and Python 3.9 on Ubuntu (#11217),  enabled early stopping/checkpooiunt even  without val step   enabled early stopping/checkpooiunt even  without val step   enabled early stopping/checkpooiunt even  without val step   enabled early stopping/checkpooiunt even  without val step   enabled early stopping/checkpooiunt even  without val step   enabled early stopping/checkpooiunt even  without val step   enabled early stopping/checkpooiunt even  without val step   enabled early stopping/checkpooiunt even  without val step   enabled early stopping/checkpooiunt even  without val step   enabled early stopping/checkpooiunt even  without val step   enabled early stopping/checkpooiunt even  without val step   enabled early stopping/checkpooiunt even  without val step   enabled early stopping/checkpooiunt even  without val step   enabled early stopping/checkpooiunt even  without val step   enabled early stopping/checkpooiunt even  without val step   enabled early stopping/checkpooiunt even  without val step   enabled early stopping/checkpooiunt even  without val step   enabled early stopping/checkpooiunt even  without val step   enabled early stopping/checkpooiunt even  without val step   enabled early stopping/checkpooiunt even  without val step   enabled early stopping/checkpooiunt even  without val step   enabled early stopping/checkpooiunt even  without val step   enabled early stopping/checkpooiunt even  without val step   enabled early stopping/checkpooiunt even  without val step   enabled early stopping/checkpooiunt even  without val step   enabled early stopping/checkpooiunt even  without val step   enabled early stopping/checkpooiunt even  without val step   enabled early stopping/checkpooiunt even  without val step   name formatting   version   testing   add test   fix test   Update model_checkpoint.py   doctests   pylint   tests   debug   debug   enabled early stopping/checkpooiunt even  without val step   fix MNIST download (#1044)   fix MNIST download   simple   name formatting   version   testing   add test   fix test   doctests   tests   debug   debug   rebased 1041   rebased 1041   tests   rebased 1041   rebased 1041   rebased 1041   rebased 1041   rebased 1041   rebased 1041   rebased 1041   rebased 1041   rebased 1041   rebased 1041   rebased 1041   rebased 1041   rebased 1041   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
7344,cleaning imports (#1032),0.69939506,"Cleaning (#5948, #5949, #5950)",  fix MNIST download   simple ,0
7345,release v0.122,0.75558126,This release includes:,  hparams as dict   hparams as dict   fixing   fixing   fixing   fixing   typing   typing   chnagelog   update set hparams   use setter   simplify   chnagelog   imports   pylint   typing   Update training_io.py   Update training_io.py   Update lightning.py   Update test_trainer.py   Update init.py   Update base.py   Update utils.py   Update test_trainer.py   Update training_io.py   Update test_trainer.py   Update test_trainer.py   Update test_trainer.py   Update test_trainer.py   Update callback_config.py   Update callback_config.py   Update test_trainer.py   Co-authored-by: William Falcon waf2107@columbia.edu,1
7346,Fabric checkpointing 1/n: base implementation (#16434),0.6494218,A base Checkpoint class for extra customization,,0
7347,Remove manual tracking of optimizer steps (#9957),0.74819124,"Removed {Accelerator,TrainingTypePlugin,PrecisionPlugin}.post_optimizer_step (#9746)",  changed path   changed path   changed path   changed path   changed path   changed path   changed path   changed path   changed path   changed path   changed path   added cv   added cv   added cv   added cv   added cv   added cv   added cv   added cv   added cv   added cv   added cv   added cv   added cv   added cv   added cv   added cv   added cv   added cv ,1
7348,progressive restoring of trainer state (#7652),0.72744066,"Refactor RunningStage and TrainerState usage (#4945, #7173)",  changed path   changed path   changed path   changed path   changed path   changed path   changed path   changed path   changed path   changed path   changed path   added cv   added cv   added cv   added cv   added cv   added cv   added cv   added cv   added cv   added cv   added cv   added cv   added cv   added cv   added cv   added cv   added cv ,1
7349,Modernize the ImageNet example (#11691),0.5713632,Changed computer_vision_fine_tunning example to use BackboneLambdaFinetuningCallback (#5377),,0
7350,fix comparing versions (#6434),0.57954913,Parsed local package versions (#13933),  update checkpoint docs   fix tests   fix tests   formatting   typing   filename   fix tests   fixing tests   fixing tests   fixing tests   unique name   fixing   fixing   Update model_checkpoint.py   Co-authored-by: William Falcon waf2107@columbia.edu,0
7351,Disable saving checkpoints if not trained (#4372),1.0000001,Disable saving checkpoints if not trained (#4372),,1
7352,Rename ProgressBarBase to ProgressBar (#17058),0.83489776,progress bar,,1
7353,DDp interpreter (#2482),0.68545616,DDP(2) backend (#2796),,0
7354,Fixes a bug that causes CSVLogger to overwrite version_0 when root_dir is a relative path. (#17139),0.56904024,Changed Checkpoint path parameter from filepath to dirpath (#1016),  added community examples   added community examples ,0
7355,Configure the check-group app (#14165),0.5821524,group prepare data hook (#3212),Co-authored-by: peterAsapp 31011756+peterAsapp@users.noreply.github.com,0
7356,[CI] Drop brew update (#6985),0.52210224,Drop PyTorch 1.9 support (#15347),  added checkpoint defaults   added checkpoint defaults   added checkpoint defaults   added checkpoint defaults   added checkpoint defaults   added checkpoint defaults   added checkpoint defaults   added checkpoint defaults   added checkpoint defaults   added checkpoint defaults   added checkpoint defaults   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   added community examples   added community examples ,0
7357,update PR template (#4523),0.6030426,Use correct python version in lightning component template (#13790),"  Added default parser for trainer and class method to construct trainer from default args   Removed print statement   Added test for constructing Trainer from command line args   Removed extra line   Removed redundant imports, removed whitespace from empty lines   Fixed typo   Updated default parser creation to get class attributes automatically   Updated default parser creation to get class attributes automatically   Added method to get default args for trainer   Trimmed trainer get default args method   Updated from argparse method to not return trainer with static arguments   Update trainer get default args to classmethod   adjustment   fix   Fixed variable name   Update trainer.py   Update test_trainer.py   Update trainer.py   Update tests/trainer/test_trainer.py   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   Update trainer.py   Update test_trainer.py   Update trainer.py   Update test_trainer.py   Update tests/trainer/test_trainer.py   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/trainer/trainer.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   Update trainer.py   Update test_trainer.py   Co-authored-by: Mudit Tanwani mudittanwani@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",0
7358,added auto restore,0.6201761,Removed auto val reduce (#2462),,0
7359,Do not pass non_blocking=True if it does not support this argument (#2910),0.50556904,"Simplified ""should run validation"" logic (#7682)",  tpu 16 bit   tpu 16 bit   tpu 16 bit ,0
7360,chlog for 1.3.2 + legacy test (#7676),0.6071658,Deprecated the TestTubeLogger (#9065),  consolidate callbacks and hooks   ensure callbacks recieve proper arg types   remove model from init callback events   clean up early stopping event   update changelog   remove on_fit_start and on_fit_end   fix args for on_init_start and on_init_end   handle case where early stopping is not used   show all callback methods   wrap checkpoint callback logic into proper class   fix check for main process in checkpoint callback   move callbacks test to separate file   refactor arg checks   get model and call hook on same line   define trainer_options dict in one call   add more asserts to callback test ,0
7361,Update definition of optimizer in introduction_guide.rst (#10822),0.79549015,Refactored optimizer (#4658),  updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs ,1
7362,Remove truncated_bptt_steps from Trainer constructor (#8825),0.81819755,Removed the deprecated Trainer.truncated_bptt_steps in favor of LightningModule.truncated_bptt_steps (#8826),,1
7363,Bump playwright from 1.29.1 to 1.30.0 in /requirements (#16735),0.48931932,[1.4.0] - 2021-07-27,  updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs   updated checkpoint docs ,0
7364,Enable running slow tests on Windows in CI (#12761),0.5501478,Run ddp_spawn dataloader checks on Windows (#6930),"  Update README.md   Update README.md   Use callable object for patching dataloaders (#971)   Use callable object for patching dataloaders   Add test for ddp with dataloaders passed to fit()   Update pytorch_lightning/trainer/trainer.py   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/trainer/trainer.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   merge load functions   update tests   fix documentation warnings   fix line too long   fix line too long   print deprecation warning   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   move tags_csv argument to end of signature   fix typo, update version numbers   fix line too long   add typing as requested   update changelog   Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: Sho Arora sho854@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com",0
7365,Make pytest not run .github/* (#10012),0.704237,Remove pytest as a requirement to run app by @manskx in https://github.com/Lightning-AI/lightning/pull/15449,  Refactor logger tests   Update and add tests for wandb logger   Update and add tests for logger bases   Update and add tests for mlflow logger   Improve coverage   Updates   Update CHANGELOG   Updates   Fix style   Fix style   Updates ,1
7366,Fix RMSLE metric (#3188),0.68516654,Deprecated reorder parameter of the auc metric (#4237),,0
7367,[App] Add automatic conversion to structures (#15961),0.5864379,Support shorthand notation to instantiate datamodules (#10011),  remove autoload   remove autoload   added weights loading docs   checkpoint loading saving docs   checkpoint loading saving docs   checkpoint loading saving docs   docs (#1010)   remove autoload   remove autoload   added weights loading docs   checkpoint loading saving docs   checkpoint loading saving docs   checkpoint loading saving docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs   docs ,0
7368,Move benchmarks into the test directory (#10614),0.59490025,Deprecated the TestTubeLogger (#9065),,0
7369,Fix typo in docstring (#5835),0.56243396,Docs,  Use callable object for patching dataloaders   Add test for ddp with dataloaders passed to fit()   Update pytorch_lightning/trainer/trainer.py   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/trainer/trainer.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
7370,missing chlogs (#2806),0.5396114,The *_epoch_end hooks were removed (#16520),,0
7371,Update device attribute in Lite's module wrapper (#14822),0.6543834,"- In Lightning Lite, state-dict access to the module wrapper now gets passed through to the original module reference ([#14629](https://github.com/Lightning-AI/lightning/pull/14629))",,0
7372,Update callback_connector.py (#8805),0.6690917,Renamed and moved core/step_result.py to trainer/connectors/logger_connector/result.py (#7736),,0
7373,Remove deprecated automatic logging of gpu metrics (#12657),0.80052733,- Removed the deprecated automatic logging of GPU stats by the logger connector ([#12657](https://github.com/Lightning-AI/lightning/pull/12657)),  docs   docs   docs   docs ,1
7374,"Update aiohttp requirement from <=3.8.3,>=3.8.0 to >=3.8.0,<=3.8.4 in /requirements (#17315)",0.512631,- Fixed default `amp_level` for `DeepSpeedPrecisionPlugin` to `O2` ([#13897](https://github.com/Lightning-AI/lightning/pull/13897)),  removed abstract requirement so LightningModule == nn.Module   default adam   default adam   default adam   default adam   default adam   default adam   default adam   default adam   default adam   default adam   default adam   default adam   default adam   docs   docs   docs ,0
7375,Fix reference to basic level doc (#12848),0.58560735,"Base classes (#1326, #1877)",  update PR guidelines   update tests   add CircleCI   Update CONTRIBUTING.md   Co-authored-by: William Falcon waf2107@columbia.edu,0
7376,Add a warning to deepspeed when inferring batch size (#9221),0.78568655,Do not fail if batch size could not be inferred for logging when using DeepSpeed (#10438),We don't (yet) support storing hparams a a dict. It must be an argparse.Namespace for checkpoint saving and loading to work.,1
7377,Add specifics around DeepSpeed docs (#6142),0.7310983,Updated precision attributes in DeepSpeedPlugin (#10164),,1
7378,readme: update logo (#17093),0.5265927,Set version as today (#13906),,0
7379,Update TPU Accelerator docs (#12850),0.8146454,Update Gradient Clipping for the TPU Accelerator (#6576),,1
7380,prevent memory test failing due to earlier test leaking memory (#8029),0.81455874,Resolve memory leak for evaluation (#6326),  clean docs   clean docs   clean docs ,1
7381,Add a custom PossibleUserWarning category (#10675),0.57482255,Avoid optional Tracker attributes (#9320),  Trainer cleanup   update abstract   remove ...   remove init   update mixin types   update callbacks   fix   lower test acc ,0
7382,Avoid using the same port number for autoscaler works (#15966),0.6859638,Porting fixes to autoscaler component (#16249),  add more underline   fix LightningMudule import error   remove unneeded blank line   escape asterisk to fix inline emphasis warning   add PULL_REQUEST_TEMPLATE.md   add init.py and import imagenet_example   fix duplicate label   add noindex option to fix duplicate object warnings   remove unexpected indent   refer explicit LightningModule   fix minor bug   refer EarlyStopping explicitly   restore exclude patterns   change the way how to refer class   remove unused import   update badges & drop Travis/Appveyor (#826)   drop Travis   drop Appveyor   update badges   fix missing PyPI images & CI badges (#853)   docs - anchor links (#848)   docs - add links   add desc.   add Greeting action (#843)   add Greeting action   Update greetings.yml   Co-authored-by: William Falcon waf2107@columbia.edu   add pep8speaks (#842)   advanced profiler describe + cleaned up tests (#837)   add py36 compatibility   add test case to capture previous bug   clean up tests   clean up tests   Update lightning_module_template.py   Update lightning.py   respond lint issues   break long line   break more lines   checkout conflicting files from master   shorten url   checkout from upstream/master   remove trailing whitespaces   remove unused import LightningModule   fix sphinx bot warnings   Apply suggestions from code review   just to trigger CI  Update .github/workflows/greetings.yml  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: William Falcon waf2107@columbia.edu Co-authored-by: Jeremy Jordan 13970565+jeremyjordan@users.noreply.github.com,0
7383,Future 5/n: Move requirements (#13306),0.60404146,move specific accelerator code (#3457),  Fix loggers and update docs   Update trainer.py ,0
7384,"Mocking Loggers (part 4b, mlflow) (#3885)",0.7480551,"Mocking loggers (#3596, #3617, #3851, #3859, #3884, #3853, #3910, #3889, #3926)",  clip   Update pytorch_lightning/trainer/training_tricks.py   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/trainer/training_tricks.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   pull out epsilon   add fp16 case   Update pytorch_lightning/trainer/training_tricks.py   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
7385,User-friendly exception if root flow does not override the run() method (#14760),0.61643887,Allowed root path to run the app on /path (#14972),,0
7386,added option to change default tensor,0.6774862,Move tensorboardX to extra dependencies. Use the CSVLogger by default (#16349),  split trainer tests   Apply suggestions from code review   format string   add CI timeout ,0
7387,Move Conversational AI (Nemo) to Examples section (#11344),0.48848158,- Improved Trainer CLI arguments handling (generalization),  fixes tpu data loader bug   fixes tpu data loader bug ,0
7388,"feature(ui): Lightning AI doc theme update, integrates global header and footer with docs (#14053)",0.6037255,Add support for listing Lightning AI apps (#13987),  Refactor dataloading   Refactor dataloading   Refactor dataloading   Add shuffle to test ,0
7389,add make cmd - clean (#5204),0.488232,Allowing decorate model init with saving hparams inside (#4662),  Add callback system + associated test   Add trainer and pl_module args to callback methods   typing   typo in docstring   Switch to on_.*_start()   fix on_test_start   fix the mess after rebasing ,0
7390,Fixes #292 (#303),0.6084033,"refactor DDP backend (#3204, #3207, #3208, #3209, #3210)",,0
7391,docs/fix_typo (#3847),0.5241538,Renamed TPUHalfPrecisionPlugin to TPUBf16PrecisionPlugin (#10026)," feat(trainer): add enable_benchmarking option  closes #370   Update trainer.py   Update trainer.py   Update trainer.py   Update trainer.py   Update trainer.py   Update trainer.py   Update trainer.py   Update trainer.py   add test   try to make the lint work   fix typo   add test, verify torch.backends.cudnn.benchmark   make lint happy   make lint happy   Co-authored-by: William Falcon waf2107@columbia.edu",0
7392,Reset metrics before each task starts (#9410),1.0,Reset metrics before each task starts (#9410),"  Add support for multiple loggers   Fix PEP   Cleanup   Cleanup   Add typing to loggers   Update base.py   Replace duck typing with isinstance check   Update CHANGELOG.md   Update comet experiment type, Switch to abstractmethod in logging.py   Fix test   Add passes to LightningLoggerBase   Update experiment_logging.rst ",1
7393,scheduled removal of DeepSpeedPlugin.cpu_offload* parameters (#9244),0.86877877,"Removed deprecated properties DeepSpeedPlugin.cpu_offload* in favor of offload_optimizer, offload_parameters and pin_memory (#9244)",,1
7394,doc: Add hint towards using ArgumentParser.add_argument_group (#5911),0.6478139,1. Add the arguments you need,  abs import   rename test model   update trainer   revert test_step check   move tags   fix test_step   clean tests   fix template   update dataset path   fix parent order ,0
7395,Update gradient clipping docs in Fabric (#17470),0.72965205,Gradient Clipping Customization,relax model loading hparams test wip wip fix warning finish test remove unused import,1
7396,Update CI workflow docs (#16578),0.60219777,Updated governance docs,,0
7397,moved sampler,0.6275883,Automatic distributed samplers, Update tests README  Point to tests/requirements.txt as part of instructions  Update requirements to dependencies,0
7398,remove outdated info (#6032),0.7009245,Removed deprecated: (#2760),  Caching MNIST dataset for testing   Added MNIST datset to the tests directory   Caches dataset based off hash of the test.pt file   Cleaned Up yml file   Cleaned Up yml file   Removed MNIST Data from framework   Set cache key for dataset to 'mnist'   Apply suggestions from code review   Apply suggestions from code review   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
7399,use ModuleNotFoundError instead of ImportError (#9867),0.9354968,Module imports are now catching ModuleNotFoundError instead of ImportError (#9867),  fix tests   fix tests ,1
7400,Loop flattening: remove .replace() (#16361),0.7087068,Refactored Loops,  Fix test requiring both test_step and test_end   Add test   Co-authored-by: William Falcon waf2107@columbia.edu,1
7401,Plugin Docs (#6952),0.7895246,Enabled plugins (#4041),  added guide   added self.print()   added self.print() ,1
7402,Fix call to accelerator,0.70061266,Selects the accelerator,  added get dataloaders directly using a getter   deleted decorator   added prepare_data hook   refactored dataloader init   refactored dataloader init   added dataloader reset flag and main loop   added dataloader reset flag and main loop   added dataloader reset flag and main loop   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   made changes   fixed bad loaders   fixed bad loaders   fixed bad loaders   fixed bad loaders   fixed bad loaders   fixed bad loaders   fixed bad loaders   fixed bad loaders   fixed bad loaders   fixed error in .fit with loaders   fixed error in .fit with loaders   fixed error in .fit with loaders   fixed error in .fit with loaders   fixed error in .fit with loaders   fixed error in .fit with loaders   fixed error in .fit with loaders   fixed error in .fit with loaders   fixed error in .fit with loaders   fixed error in .fit with loaders   fixed error in .fit with loaders   fixed error in .fit with loaders   fixed error in .fit with loaders   fixes #909   fixes #909   bug fix   Fixes #902 ,1
7403,Use cmake installed with apt (#12907),0.5525913,Enabled cp (upload) at project level (#16631),  update docs for map location   update return description ,0
7404,release v0.1.dev21,0.7084513,"    version=""0.0.1"",",  Update data_loading.py   Update training_io.py   Update trainer.py ,1
7405,warn about dp + manual optimization in docs (#7230),0.6574614,     # 1. Switch to manual optimization,  add .vscode in .gitignore   Split callbacks in individual files + add a  property to Callback for easy trainer instance access   formatting   Add a conda env file for quick and easy env setup to develop on PL   Adress comments   add fix to kth_best_model   add some typing to callbacks   fix typo   add autopep8 config to pyproject.toml   format again   format   fix toml   fix toml again   consistent max line length in all config files   remove conda env file   Update pytorch_lightning/callbacks/early_stopping.py   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/callbacks/model_checkpoint.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   docstring   Update pytorch_lightning/callbacks/model_checkpoint.py   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update pytorch_lightning/callbacks/model_checkpoint.py  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com   fix logic error   format   simplify if/else   format   fix linting issue in changelog   edit changelog about new callback mechanism   fix remaining formating issue on CHANGELOG   remove lambda function because it's compatible with pickle (used during ddp)   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
7406,release v0.4.8,0.7837684,0.4.0, typehints for trainer   fix type links in docs fix types in docs type hints for trainer methods fix fit docs switch to comments readability added sphinx typehints extension wip remove typehints from docstring more type annotations fix spaces  Update trainer.py  Co-authored-by: William Falcon waf2107@columbia.edu,1
7407,Standalone Lite: Cluster Environments (#14509),0.77111113,Enabled custom clusters (#4048),Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,1
7408,update Docs [links & formatting] (#769),0.6121861,"We have released New documentation, please bear with us as we fix broken links and patch in missing pieces. ",,0
7409,Fix Adagrad optimizer not working with DDP/GPU (#7277),0.69347966,Disabled optimizers setup during testing (#3059),  Fix comet logger to log after train   Add clarifying comment to COmetLogger code   Explains the need to use CometExistingExperiment in the CometLogger class after CometLogger.finalize.,0
7410,Validate the combination of CloudCompute and BuildConfig (#14929),0.5933177,enabled multiple dataloaders for validation.    ,,0
7411,[App] Hot Fix: Missing root flow in app.flows (#15531),0.6956998,Improved the error message when the root LightningFlow passed to LightningApp is missing the run method (#14760),  Properly restore current epoch and global step on resume   Add test   Move increment to saving rather than loading   Fix other tests that refer to current epoch   Formatting   Add warning for mid-epoch resuming   Formatting   Fix warning check for accumulated batches   Add variable to init   Formatting   Add check for 0 training steps   Make check more readable ,0
7412,add memory parity for PL vs Vanilla (#5170),0.60213625,Speed/memory optimizations.,  add sphinx bot   source   typo   Make a change to the docs (#2)   Make a change to the docs  Introduce an error Install git before building docs Apply suggestions from code review  Co-authored-by: Jirka Borovec Borda@users.noreply.github.com   Update docs/source/apex.rst   Update docs/source/apex.rst   Co-authored-by: Ammar Askar ammar_askar@hotmail.com,0
7413,added test for no dist sampler,0.58735484,Did not interfere with a default sampler (#1318),  add a conda env file for easy PL conda env setup   Update environment.yml   Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update environment.yml  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update environment.yml  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update environment.yml  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update environment.yml  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com  Update environment.yml  Co-Authored-By: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
7414,Update logging docs (#12245),0.684679,Refactored logging,  add warnings   fix link ,0
7415,Add ModelCheckpoint.to_yaml method (#3048),0.7219653,Updated model_checkpoint's to_yaml to use fsspec open (#3801),"  added file that contains information on the minimal versions needed for the supported loggers   copied minimal version, combined files, deleted duplicates   sorted functions in tests/test_loggers.py to be consistent   expanded wandb logging test; added minimal versions for requirements-extra.txt; increased the amount of training data that is used for tests   formatting   added requirements-extra.txt to MANIFEST.in   reverted wandb test; ensured minimal version for dependencies in requirements-extra.txt in ci-testing.yml ",1
7416,"Update tqdm requirement from <=4.63.0,>=4.57.0 to >=4.57.0,<4.65.0 in /requirements (#13875)",0.6122145,Updated logic for checking TPUs availability (#6767),,0
7417,add pypi user (#1401),0.61302924,Enable PyTorch 1.7 compatibility (#3541),,0
7418,new feature for profiling training runs (#782),0.71095896, - Profiling: `Trainer(profiler=...)`,add info about TPU and segmentation,1
7419,Fix Google Tag Manager for the Lightning App docs (#14731),0.7120408,Update the Lightning App docs (#13537),typo in my name lol,1
7420,Remove unnecessary parameters to super() in documentation and source code (#1240),0.5787142,Properly pass some Logger's parent's arguments to super().__init__() (#12609),  updated governance docs   added maintainers to readme   added governance docs   added governance docs ,0
7421,[App] Fix resolution of latest version in CLI (#17351),0.61078346,Introducing CLI commands for apps (#13602)!,"  cleanup docstrings, _get_total_cprofile_duration in module   relax profiler overhead tolerance ",0
7422,Avoid torch amp cuda warning with bf16 on cpu (#11161),0.69851565,"device = ""cuda"" if torch.cuda.is_available() else ""cpu""",,0
7423,Fix import error when running doctests for RL examples (#12010),0.6075497,Module imports are now catching ModuleNotFoundError instead of ImportError (#9867),  updated governance docs   added maintainers to readme   added governance docs ,0
7424,bug fix for #157 (#158),0.59180796,Improved the error message when the LightningWork is missing the run method (#14759),  new way of passing dataloaders   fixed docs   fixed codestyle to follow flake8   allow val/test be list of dataloaders and smarter checking   added test   fix flake error   fix linking to new test model   split into multiple test   fix naming and typo   minor documentation changes   remove random file   Update trainer.py   Update trainer.py   Update trainer.py   Update trainer.py   Update trainer.py   Update trainer.py   better error/warning message   final adjustments   update CHANGELOG.md   Co-authored-by: William Falcon waf2107@columbia.edu,0
7425,Fixes #380,0.65579504,Mixed precision overhaul (#16783),"I am updating the project's Sphinx documentation to fix (#819). The issue is related to a library the Sphinx extension nbsphinx (to load Jupyter Notebooks) loads into the docs context (RequireJS). That library conflicts with other theme libraries, causing the latter to be not loaded. This would result in several crashes, the most obvious of them the lack of anchors. The fix above solves all errors -- and now anchors work.",0
7426,Remove deprecated profiler import (#10443),0.7119844,- Removed deprecated `import pytorch_lightning.profiler.profilers` in favor of `import pytorch_lightning.profiler` ([#10443](https://github.com/PyTorchLightning/pytorch-lightning/pull/10443)),  updated docs   updated docs   upd ,1
7427,Add CITATION.cff (#9139),0.52989745,Configuration Validator (#9779),  Added max number of steps in Trainer   Added docstring   Fix flake8 errors   Clarified docstrings   Fixed flake8 error   Added min_steps to Trainer   Added steps and epochs test   flake8   minor fix   fix steps test in test_trainer   Split steps test into 2 tests   Refactor steps test   Update test_trainer.py   Minor in test_trainer.py   Update test_trainer.py   Address PR comments   Minor   Co-authored-by: William Falcon waf2107@columbia.edu,0
7428,Add Strategy.on_exception (#16646),0.7052169,"on_exception = OnExceptionCheckpoint(""."")",,1
7429,Small improvements to _init_debugging_flags (#10620),0.61234844,Deprecated flags: (#2213),,0
7430,[Refactor] Simplify data loading logic around replacing sampler to prevent confusion (#9721),0.6849381,Disabled sampler replacement when using IterableDataset (#11507),,0
7431,add apex test (#2921),0.5956407,"nb_test_batches to num_test_batches,",  added tpu docs   added tpu flags   add tpu docs + init training call   amp   amp   amp   amp   optimizer step   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   fix test pkg create (#873)   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added test return and print   added test return and print   added test return and print   added test return and print   added test return and print   Update pytorch_lightning/trainer/trainer.py   Co-Authored-By: Luis Capelo luiscape@gmail.com   Fix segmentation example (#876)   removed torchvision model and added custom model   minor fix   Fixed relative imports issue   Fix/typo (#880)   Update greetings.yml   Update greetings.yml   Changelog (#869)   Create CHANGELOG.md   Update CHANGELOG.md   Update CHANGELOG.md   Update PULL_REQUEST_TEMPLATE.md   Update PULL_REQUEST_TEMPLATE.md   Add PR links to Version 0.6.0 in CHANGELOG.md   Add PR links for Unreleased in CHANGELOG.md   Update PULL_REQUEST_TEMPLATE.md   Fixing Function Signatures (#871)   added tpu docs   added tpu flags   add tpu docs + init training call   amp   amp   amp   amp   optimizer step   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added auto data transfer to TPU   added test return and print   added test return and print   added test return and print   added test return and print   added test return and print   added test return and print   added test return and print   added test return and print   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Luis Capelo luiscape@gmail.com Co-authored-by: Akshay Kulkarni akshayk.vnit@gmail.com Co-authored-by: Ethan Harris ewah1g13@soton.ac.uk Co-authored-by: Shikhar Chauhan xssChauhan@users.noreply.github.com,0
7432,[IPU] Support manually instantiating the poptorch.DataLoader (#12116),0.7108383,- No longer set a `DistributedSampler` to the `poptorch.DataLoader` when IPUs are used ([#12114](https://github.com/PyTorchLightning/pytorch-lightning/pull/12114)),,1
7433,"Update gym[classic_control] requirement from <=0.23.1,>=0.17.0 to >=0.17.0,<0.24.2 in /requirements (#13307)",0.632985,"Deprecated setting Trainer(max_steps=None); To turn off the limit, set Trainer(max_steps=-1) (default) (#9460)",,0
7434,Update DataModule docs following property deprecations (#8864),0.6538507,Replaced _DataModuleWrapper with __new__ (#7289),,0
7435,Document speed comparison (#2072),0.63996494,"many speed improvements (how we move data, adjusted some flags & PL now adds 300ms overhead per epoch only!)",  Create CHANGELOG.md   Update CHANGELOG.md   Update CHANGELOG.md   Update PULL_REQUEST_TEMPLATE.md   Update PULL_REQUEST_TEMPLATE.md   Add PR links to Version 0.6.0 in CHANGELOG.md   Add PR links for Unreleased in CHANGELOG.md   Update PULL_REQUEST_TEMPLATE.md ,0
7436,Add typing to LightningModule.trainer (#12345),0.7383741,reference to the Trainer on the LightningDataModule (#3684),  Update greetings.yml   Update greetings.yml ,1
7437,Support limit_mode_batches (int) for infinite dataloader (#2787),0.6946455,"Refactor dataloading, supports infinite dataloader (#955)",  removed torchvision model and added custom model   minor fix   Fixed relative imports issue ,0
7438,[pre-commit.ci] pre-commit autoupdate (#7577),0.5273535,Full Changelog: https://github.com/Lightning-AI/lightning/compare/1.8.0...1.8.0.post1,,0
7439,add note in LightningCLI docs that --optimizer must be given for --lr_scheduler to work (#17552),0.78046864,LightningModule.lr_scheduler_step,  added initial semantic segmentation example   removed unnecessary lines.   changed according to reviews   minor changes   Added some documentation for Dataset class   Fixed some long lines   added docstring for LightningModule ,1
7440,Prefetch if it's not a sized iterable (#16776),0.6912161,Support for arbitrary iterables (#16726),  drop duplicated guides   prevent copy to git ,0
7441,Pin setup-gcloud to v0 instead of master (#12375),0.51893926,Initial plugin server (#16523),"  Refactor callbacks   flake8   Update docstrings   Simplified callback, protected trainer   .set_trainer() check   update docs   missed super().ini()   Updated tests   Use uppercase   refine checkpoint callback tests   Added test_begin() and test_end() ",0
7442,Fixed val interval (#405),0.66161895,val_percent_check in favour of limit_val_batches,,0
7443,"Update matplotlib requirement from <=3.5.1,>3.1 to >3.1,<3.5.3 in /requirements (#13053)",0.49722555,Dropped torchvision dependency in tests and added own MNIST dataset class instead (#986),  add py36 compatibility   add test case to capture previous bug   clean up tests   clean up tests ,0
7444,[App] Add work.delete (#16103),0.52650785,Resolved a bug where the work statuses will grow quickly and be duplicated (#13970),  allow to specify 'step' key   add test   docs to log_metrics   fix test   rename   also rename ,0
7445,Minor imports cleaning (#402),0.7083243,"Cleaning (#5948, #5949, #5950)",,1
7446,[Feat] DeepSpeed single file saving (#6900),0.9894938,DeepSpeed single file saving (#6900),  add Greeting action   Update greetings.yml   Co-authored-by: William Falcon waf2107@columbia.edu,1
7447,Update Changelog after 1.5.5 release (#10977),0.73200476,Full Changelog,,1
7448,Fix Namespace loading in PyYAML 5.4.x (#6673),0.55478805,"Removed PyTorch 1.6 support (#10367, #10738)",  docs - add links   add desc. ,0
7449,Move HPU broadcast override to the HPU strategy file (#17011),0.9863278,Moveed HPU broadcast override to the HPU strategy file (#17011),,1
7450,adding dataparallel,0.6860664,Implemented DataParallelPlugin._setup_model (#10010),  add autopep8 to Contrib.   simplify cmd   update GH templates   add pytest-flake8   update GH template ,0
7451,better simple profiler,0.6124313,Simpler interface,  drop Travis   drop Appveyor   update badges ,0
7452,temporary suspend master update (#6017),0.58068365,- Fixed reloading of the last checkpoint on run restart ([#14907](https://github.com/Lightning-AI/lightning/pull/14907)),  init GH building   try run   circleci: install   add cache   add artifacts   fix cache   update ,0
7453,"Update gym[classic_control] requirement from <0.24.2,>=0.17.0 to >=0.17.0,<0.25.2 in /requirements (#14005)",0.6405885,"Deprecated setting Trainer(max_steps=None); To turn off the limit, set Trainer(max_steps=-1) (default) (#9460)",  upgrade sphinx   badge   Update README.md   Co-authored-by: William Falcon waf2107@columbia.edu,0
7454,Fixing word tense (#6974),0.53801256,"    """"""",https://github.com/tox-dev/tox/issues/1516,0
7455,Specify packaging version to be more than 17.0 (#8030),0.5014337,Set version as today (#13906), COMET_KEY -> COMET_API_KEY COMET_REST_KEY -> COMET_REST_API_KEY,0
7456,added support for logging in different trainer stages (#16002),0.7490164,"The loggers have also been extended to support for multiple concurrent loggers to be passed to Trainer as an iterable, docs and added support for step-based learning rate scheduling.",ref: https://www.sphinx-doc.org/en/master/usage/configuration.html#confval-html_add_permalinks,1
7457,remove deprecated sync step argument from WandbLogger (#8763),0.9813722,Removed the deprecated sync_step argument from WandbLogger (#8763),,1
7458,CI: TPU drop install horovod (#4622),0.6751704,"refactored Horovod backend (#3121, #3122)",  update deprecated messages   formatting   fix docs tags ,0
7459,added min accuracy to models test,0.59619194,Renaming of precision recall metric (#3308),,0
7460,Gradient accumulation callback (#150),0.61916786,Do not reset the progress tracking dataclasses total counters (#8475),,0
7461,docs(trainer): fix gradient clipping entry (#85),0.66270757,gradient_clip -> gradient_clip_val    ,  added outline of all features   updated common use cases doc   updated common use cases doc   updated common use cases doc   updated common use cases doc   updated common use cases doc   updated common use cases doc   updated common use cases doc   updated common use cases doc   updated common use cases doc   updated common use cases doc   updated common use cases doc   updated common use cases doc   updated common use cases doc   updated docs ,0
7462,Support Injecting Secrets into Apps Running in the Cloud (#14612),0.7984472,Add --secret option to CLI to allow binding Secrets to app environment variables when running in the cloud (#14612),"  drop torchvision, tests only   manifest   move test utils ",1
7463,[docs] Add copy and toggle buttons to sphinx (#3054),0.6540128,Pinned sphinx-autodoc-typehints with <v1.15 (#11400),"  Allow experiment versions to be overridden by passing a string value. Allow experiment names to be empty, in which case no per-experiment subdirectory will be created and checkpoints will be saved in the directory given by the save_dir parameter.   Document tensorboard api changes   Review comment fixes plus fixed test failure for minimum requirements build   More format fixes from review ",0
7464,Updated ModelCheckpoint documentation (#6873),0.7383471,Removed the deprecated model argument from ModelCheckpoint.save_checkpoint (#8688),  fix test for profiler   use allclose   user relative tol ,1
7465,Update test_pruning.py to use devices instead of gpus or ipus (#11339),0.6586765,Skipped testing with PyTorch 1.7 and Python 3.9 on Ubuntu (#11217),  wip   wip   debug imports docs formatting   WIP   formatting   fix setup ,0
7466,Remove accelerator hooks from being called in call_hook (#12237),0.7691033,Removed call_configure_sharded_model_hook property from Accelerator and TrainingTypePlugin (#9612),  update license   Update LICENSE   Co-authored-by: William Falcon waf2107@columbia.edu,1
7467,Spelling and grammar updates in documentation (#11861),0.60065985,Syntax changes are: ,"  initial implementation   formatting, pass through profiler, docstring   call profiler during training   add initial tests   report stats when training is done   fix formatting   error handling, bugfix in passthroughprofiler   finish documenting profiler arg in Trainer   relax required precision for profiling tests   option to dump cProfiler results to text file   use logging, format with black   include profiler in docs   improved logging and better docs   appease the linter   better summaries, wrapper for iterables   fix typo   allow profiler=True creation   more documentation   add tests for advanced profiler   Update trainer.py   make profilers accessible in pl.utilities   reorg profiler files   change import for profiler tests   Co-authored-by: William Falcon waf2107@columbia.edu",0
7468,shard doc improvements (#4993),0.7706409,Docs improvements,,1
7469,Add SSH command to CLI (#15310),0.6854328,- Added support for managing SSH-keys via CLI ([#15291](https://github.com/Lightning-AI/lightning/pull/15291)),  overridable tqdm_dict   Slim down default tqdm_metrics   gpu fix ,0
7470,disable warnings summary for pytest (#9743),0.73469806,Do not override PYTHONWARNINGS (#4700),  [update] : #675 : set warnings   [fix] : #675 : remove white space ,1
7471,Update torch_xla wheels installation link (#9436),0.5985533,- Fixed an issue with comparing torch versions when using a version of torch built from source ([#17030](https://github.com/Lightning-AI/lightning/pull/17030)),"""Research seed""  seed still missing as the repo seems to be not existing any longer",0
7472,"CI: e2e adding queue_type: ""http"" (#16175)",0.542586,- Added an HTTPQueue as an optional replacement for the default redis queue ([#14978](https://github.com/Lightning-AI/lightning/pull/14978),,0
7473,Remove mentions of @awaelchli from source code (#14425),0.5915338,Removed deprecated EvalResult (#5633),  write to single file   fix import ,0
7474,Fix tests failing on a single GPU (#11753),0.68694687,GPU training (#2704),"  remove unnecessary pass statements   use isinstance for type checks   remove unnecessary else/elif after return   remove unnecessary return statements   move doc string to top   merge isinstance calls   remove unnecessary else/elif after raise   use list comprehension   do not use len without comparison   add missing shebang   revert isinstance check back to type   broke tests, because bool is actually subclass of int   add missing period to doc string   remove unnecessary pass statements   use isinstance for type checks   remove unnecessary else/elif after return   remove unnecessary return statements   move doc string to top   merge isinstance calls   remove unnecessary else/elif after raise   use list comprehension   do not use len without comparison   add missing shebang   revert isinstance check back to type   broke tests, because bool is actually subclass of int   add missing period to doc string   Fix default ckpt path when logger exists (#771)   rename logging -> loggers (#767)   move logging >> loggers   add warning   fix tests   logging alias   formatting   formatting   use isinstance for type checks   revert isinstance check back to type   broke tests, because bool is actually subclass of int   add more detail to tbptt example (#755)   add more detail to tbptt example   warn user about new arg in training_step   Co-authored-by: Vadim Bereznyuk kuynzereb@gmail.com Co-authored-by: Jirka Borovec Borda@users.noreply.github.com Co-authored-by: Jeremy Jordan 13970565+jeremyjordan@users.noreply.github.com",0
7475,Fix imports for lightning cli examples (#16871),0.78823376,from lightning.pytorch.cli import LightningCLI,  update info   fix duplicate #733   update Appveyor budge #626   budge to master ,1
7476,update license (#809),0.5362723,Refactored EpochResultStore (#5522),  add more detail to tbptt example   warn user about new arg in training_step ,0
7477,Add bfloat16 support to DeepSpeedStrategy (#12508),0.74586403,"Implemented DeepSpeedPlugin._setup_model_and_optimizers (#10009, #10064)",  move logging >> loggers   add warning   fix tests   logging alias   formatting   formatting ,1
7478,Removed the deprecated Trainer.data_parallel_device_ids function in favour of Trainer.device_ids (#14422),0.82576156,"Removed the deprecated Trainer.reset_train_val_dataloaders() in favor of Trainer.reset_{train,val}_dataloader (#16131)",,1
7479,Updated distributed Demos (#215),0.55908585,"    version=""0.0.1"",","  removed dependency on pandas, instead use generic csv   remove mnist files, pushed by accident   added docstring and small fixes   Update memory.py   fixed path   Co-authored-by: William Falcon waf2107@columbia.edu",0
7480,New speed documentation (#7665),0.69391143,"many speed improvements (how we move data, adjusted some flags & PL now adds 300ms overhead per epoch only!)"," use tqdm.auto in trainer  This will import the ipywidgets version of tqdm if available. This works nicely in notebooks by not filling up the log. In the terminal it will use the same old tqdm. We might also want to consider passing in the tqdm we want as an argument since there may be some edge cases where ipywidgets is available but the interface doesn't support it (e.g. vscode?) or isn't working. In which case people will get a warning message, but may want to configure it themselves.   use from tqdm.auto in eval loop   indents ",0
7481,Update MANIFEST.in,0.61747617,Key updates,,0
7482,add weighted average to results obj (#2930),0.9542922,weighted average in results obj (#2930),  update log   fix links   formatting   fixing docs path   formatting ,1
7483,Move type annotation into init (#14943),0.51632184,moves init apex from LM to apex connector (#3923),,0
7484,Rename master_params to main_params (#10105),0.76774335,Deprecated PrecisionPlugin.master_params() in favor of PrecisionPlugin.main_params() (#10105),,1
7485,Fix error in pre-optim logic (#5995),0.6829599,Improved error messages for invalid configure_optimizers returns (#3587),  update logger init   formatting ,0
7486,Automatically find and run special tests (#6669),0.5864086,"nb_test_batches to num_test_batches,",  Early stopping fix   Update trainer.py   Don't force validation sanity check   fix tests   update   Added early_stopping check_metrics   Updated docs   Update docs   Do not call early stopping when validation is disabled   Co-authored-by: William Falcon waf2107@columbia.edu,0
7487,add azure timeout (#5907),0.58908266,- Fixed `TQDMProgressBar` reset and update to show correct time estimation (2/2) ([#13962](https://github.com/Lightning-AI/lightning/pull/13962)),,0
7488,Update Distributed training.md,0.6322049,"State-of-the-art distributed training strategies (DDP, FSDP, DeepSpeed) and mixed precision out of the box",,0
7489,enable passing in custom accelerators (#4050),0.988101,Enabled passing in custom accelerators (#4050),,1
7490,release v0.11,0.8159968,"    version=""0.0.1"",",,1
7491,Include app templates to the lightning and app packages (#13731),0.9870944,Included app templates to the lightning and app packages (#13731),,1
7492,"Update fsspec[http] requirement from <2022.8.0,>2021.06.0 to >2021.06.0,<2023.2.0 in /requirements (#16544)",0.5763668,Updated model_checkpoint's to_yaml to use fsspec open (#3801),,0
7493,fixing Win failed import (#1163),0.60224104,Module imports are now catching ModuleNotFoundError instead of ImportError (#9867),,0
7494,add logger to all (#6854),0.7403437,Logging with multiple loggers,,1
7495,[Metrics] class based embedding similarity + tests (#3358),0.52505934,"Standardized the dataloaders arguments of trainer.{fit,valdiate,test,tune} (#7431)",,0
7496,feat: LAI2-10296 check if user has sufficient credits to run an app from the cli (#14285),0.60873175,Introducing CLI commands for apps (#13602)!,,0
7497,Update gan.py,0.5991576,PyTorch 1.5  support,,0
7498,Fix conversion in on_before_forward,0.6716727,"The on_before_optimizer_step hook previously ran before the entire optimization closure, including backward. This was unintended behavior and if you rely on this, move your code to the new on_before_backward` hook."," implement forward and update args (#709)  Fixes the following issues as discussed in issue #709 1) Implement forward method wrapped. 2) Set default value for seed. ""None"" breaks tensorboard. 3) Update redundant hparams.data to new hparams.data_path. 4) Update 'use-16bit' to 'use_16bit' to maintain consistency.   Fix failing GPU tests (#722)   Fix distributed_backend=None test   We now throw a warning instead of an exception. Update test to reflect this.   Fix test_tube logger close when debug=True   Clean docs (#725)   updated gitignore   updated gitignore   updated links in ninja file   updated docs   finished callbacks   finished callbacks   finished callbacks   fixed left menu   added callbacks to menu   added direct links to docs   added direct links to docs   added direct links to docs   added direct links to docs   added direct links to docs   added direct links to docs   updated gitignore   updated links in ninja file   updated docs   finished callbacks   finished callbacks   finished callbacks   fixed left menu   added callbacks to menu   added direct links to docs   added direct links to docs   added direct links to docs   added direct links to docs   added direct links to docs   added direct links to docs   finished rebase   making private  members   making private  members   making private  members   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   set auto dp if no backend   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   fixed lightning import   cleared  spaces   cleared  spaces   cleared  spaces   cleared  spaces   cleared  spaces   cleared  spaces   cleared  spaces   cleared  spaces   cleared  spaces   cleared  spaces   finished lightning module   finished lightning module   finished lightning module   finished lightning module   added callbacks   added loggers   added loggers   added loggers   added loggers   added loggers   added loggers   added loggers   added loggers   set auto dp if no backend   added loggers   added loggers   added loggers   added loggers   added loggers   added loggers   flake 8   flake 8   fix docs path   updated gitignore   updated gitignore   updated links in ninja file   updated docs   finished callbacks   finished callbacks   finished callbacks   fixed left menu   added callbacks to menu   added direct links to docs   added direct links to docs   added direct links to docs   added direct links to docs   added direct links to docs   added direct links to docs   updated gitignore   updated docs   finished callbacks   finished callbacks   finished callbacks   fixed left menu   added callbacks to menu   added direct links to docs   added direct links to docs   added direct links to docs   added direct links to docs   added direct links to docs   added direct links to docs   finished rebase   making private  members   making private  members   making private  members   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   set auto dp if no backend   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   fixed lightning import   cleared  spaces   cleared  spaces   cleared  spaces   cleared  spaces   cleared  spaces   finished lightning module   finished lightning module   finished lightning module   finished lightning module   added callbacks   added loggers   added loggers   added loggers   added loggers   added loggers   added loggers   added loggers   added loggers   added loggers   added loggers   added loggers   flake 8   flake 8   fix docs path   flake 8   Update theme_variables.jinja   implement forward and update args (#709)   Fixes the following issues as discussed in issue #709 1) Implement forward method wrapped. 2) Set default value for seed. ""None"" breaks tensorboard. 3) Update redundant hparams.data to new hparams.data_path. 4) Update 'use-16bit' to 'use_16bit' to maintain consistency.  use self.forward for val step (#709)  Co-authored-by: Nic Eggert nic@eggert.io Co-authored-by: William Falcon waf2107@columbia.edu",0
7499,train = False in test_dataloader (#162),0.6832614,train_dataset = trainer.train_dataloader.loaders[0].dataset,  updated gitignore   updated gitignore   updated links in ninja file   updated docs   finished callbacks   finished callbacks   finished callbacks   fixed left menu   added callbacks to menu   added direct links to docs   added direct links to docs   added direct links to docs   added direct links to docs   added direct links to docs   added direct links to docs   updated gitignore   updated links in ninja file   updated docs   finished callbacks   finished callbacks   finished callbacks   fixed left menu   added callbacks to menu   added direct links to docs   added direct links to docs   added direct links to docs   added direct links to docs   added direct links to docs   added direct links to docs   finished rebase   making private  members   making private  members   making private  members   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   set auto dp if no backend   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   fixed lightning import   cleared  spaces   cleared  spaces   cleared  spaces   cleared  spaces   cleared  spaces   cleared  spaces   cleared  spaces   cleared  spaces   cleared  spaces   cleared  spaces   finished lightning module   finished lightning module   finished lightning module   finished lightning module   added callbacks   added loggers   added loggers   added loggers   added loggers   added loggers   added loggers   added loggers   added loggers   set auto dp if no backend   added loggers   added loggers   added loggers   added loggers   added loggers   added loggers   flake 8   flake 8   fix docs path   updated gitignore   updated gitignore   updated links in ninja file   updated docs   finished callbacks   finished callbacks   finished callbacks   fixed left menu   added callbacks to menu   added direct links to docs   added direct links to docs   added direct links to docs   added direct links to docs   added direct links to docs   added direct links to docs   updated gitignore   updated docs   finished callbacks   finished callbacks   finished callbacks   fixed left menu   added callbacks to menu   added direct links to docs   added direct links to docs   added direct links to docs   added direct links to docs   added direct links to docs   added direct links to docs   finished rebase   making private  members   making private  members   making private  members   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   set auto dp if no backend   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   fixed lightning import   cleared  spaces   cleared  spaces   cleared  spaces   cleared  spaces   cleared  spaces   finished lightning module   finished lightning module   finished lightning module   finished lightning module   added callbacks   added loggers   added loggers   added loggers   added loggers   added loggers   added loggers   added loggers   added loggers   added loggers   added loggers   added loggers   flake 8   flake 8   fix docs path   flake 8   Update theme_variables.jinja ,0
7500,allow e2e test image to be changed via env variable (#15200),0.48786685,Only check versions / env when not in the cloud (#15504), Fix distributed_backend=None test  We now throw a warning instead of an exception. Update test to reflect this.  Fix test_tube logger close when debug=True,0
7501,prune check on Trainer fit result (#5453),0.6863203,trainer.fit(model),,0
7502,Support ddp_fork strategy with native AMP by attempting NVML-based CUDA availability assessment (#14984),0.7178893,- Added native AMP support for `ddp_fork` (and associated alias strategies) with CUDA GPUs ([#14983](https://github.com/Lightning-AI/lightning/pull/14983)),,1
7503,refactor imports of optional dependencies (#4859),0.646153,import argparse,,0
7504,Stop loading a few properties if checkpoint's dirpath has changed (#12045),0.74882925,Changed Checkpoint path parameter from filepath to dirpath (#1016),,1
7505,Replace readme DQN link with bolts implementation (#4841),0.5151658,Refactor load in checkpoint connector (#4593),,0
7506,publishing workflow (#15239),0.5556467,Enabled cp (upload) at project level (#16631),  Added atomic checkpoint creation   Added documentation for _atomic_checkpoint ,0
7507,Fix default value for enable_progress_bar in docs (#13584),0.6250391,Changed the default progress bar to print to stdout instead of stderr (#531),,0
7508,"Update pandas requirement from <1.5.1,>1.0 to >1.0,<1.5.2 in /requirements (#15263)",0.72290593,Removed dependency on pandas (#736),  fix typos   update org paths   update links from READMe to docs   add svg logo   add svg logo-text   update logos   testing temp paths   prune links from readme   optimize imports   update logo   update paths in README   missing imports ,1
7509,updated test docs,0.71120876,Docs improvements,  add version_ prefix to log_dir   add version_ prefix ,1
7510,Use PL is_overridden in BYOT example (#17070),0.49013245,Deprecated is_overridden(model=...) in favor of is_overridden(instance=...) (#7918),,0
7511,Enable DataLoader state restoration for the evaluation loop  (#9563),0.7538992,"Refactored evaluation loop interface; added new classes DataLoaderLoop, EvaluationLoop, EvaluationEpochLoop (#7990, #8077)",,1
7512,Update tests/trainer/*.py to use devices instead of gpus or ipus (#11697),0.6479565,Dropped official support/testing for PyTorch <1.6 (#8288),  updated gitignore   Update README.md   updated gitignore   updated links in ninja file   updated docs   Update README.md   Update README.md   finished callbacks   finished callbacks   finished callbacks   fixed left menu   added callbacks to menu   added direct links to docs   added direct links to docs   added direct links to docs   added direct links to docs   added direct links to docs   fixing TensorBoard (#687)   flake8   fix typo   fix tensorboardlogger drop test_tube dependence   formatting   fix tensorboard & tests   upgrade Tensorboard   test formatting separately   try to fix JIT issue   add tests for 1.4   added direct links to docs   updated gitignore   updated links in ninja file   updated docs   finished callbacks   finished callbacks   finished callbacks   fixed left menu   added callbacks to menu   added direct links to docs   added direct links to docs   added direct links to docs   added direct links to docs   added direct links to docs   added direct links to docs   finished rebase   making private  members   making private  members   making private  members   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   set auto dp if no backend   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   working on trainer docs   fixed lightning import   cleared  spaces   cleared  spaces   cleared  spaces   cleared  spaces   cleared  spaces   cleared  spaces   cleared  spaces   cleared  spaces   cleared  spaces   cleared  spaces   finished lightning module   finished lightning module   finished lightning module   finished lightning module   added callbacks   added loggers   added loggers   added loggers   added loggers   added loggers   added loggers   added loggers   added loggers   set auto dp if no backend   added loggers   added loggers   added loggers   added loggers   added loggers   added loggers   flake 8   flake 8   Co-authored-by: Jirka Borovec Borda@users.noreply.github.com,0
7513,Rename DeepSpeedPlugin to DeepSpeedStrategy (#11194),0.8126441,    * Renamed the `DeepSpeedPlugin` to `DeepSpeedStrategy` ([#11194](https://github.com/PyTorchLightning/pytorch-lightning/pull/11194)),,1
7514,Precise description of reload_dataloaders_every_n_epochs (#14245),0.76600385,Deprecated reload_dataloaders_every_epoch argument of Trainer in favor of reload_dataloaders_every_n_epochs (#5043),,1
7515,added single node example,0.65070283,Slightly safer multi node (#15538),  flake8   fix typo   fix tensorboardlogger drop test_tube dependence   formatting   fix tensorboard & tests   upgrade Tensorboard   test formatting separately   try to fix JIT issue   add tests for 1.4 ,0
7516,MetricsHolder clean-up + typing (#6645),0.74597913,Remove MetricsHolder (#7909),,1
7517,Update setup logic in training type plugins (data-parallel) [3 / n] (#10010),0.7436852,Automatically set sync_batchnorm for training_type_plugin (#6536),,1
7518,Add docs for GpuUsageLogger (#2945),0.63543427,"docs for all Metrics (#2184, #2209)",,0
7519,Allow Trainer's gpus arg type to be subclass of currently accepted types (#1423),0.7443744,Trainer.add_argparse_args classmethod fixed. Now it adds a type for the arguments (#1147),,1
7520,"Update cloudpickle requirement from <=2.1.0,>=1.3 to >=1.3,<2.3.0 in /requirements (#15840)",0.6682821,Refactor cloud dispatch and update to new API (#16456),  refactor   refactor   refactor   made tensorboard the default not test-tube ,0
7521,[CLI] Fix status message on cluster creation (#14477),0.77208424,Cluster creation and deletion now waits by default [#15458,,1
7522,remove deprecated in v0.9 (#2760),0.90426856,Removed deprecated: (#2760),,1
7523,Separate the Gradient Accumulation Scheduler from Trainer (#16729),0.9999999,Separate the Gradient Accumulation Scheduler from Trainer (#16729),,1
7524,[app] Add CloudCompute ID serializable within the flow and works state  (#14819),0.62711084,Refactor cloud dispatch and update to new API (#16456),  Clearer disable validation logic   fix for fast_dev_run   flake8 fix   Test check fix   update error message ,0
7525,update notebooks (#14340),0.55196863,Set version as today (#13906),"When one follows the Readme, the example will fail once we call trainer.test() because the methods are not overridden. Fixes https://github.com/williamFalcon/pytorch-lightning/issues/428",0
7526,"Docs and Tests for ""gpus"" Trainer Argument (#593)",0.7571103,"Standardized the dataloaders arguments of trainer.{fit,valdiate,test,tune} (#7431)",  Basic wandb support   refactor(wandb): remove unused variables and document logger   docs(wandb): explain how to use WandbLogger   test(wandb): add tests for WandbLogger   feat(wandb): add save_dir   fix(wandb): allow pickle of logger   fix(wandb): save logs in custom directory   test(wandb): test import   docs(wandb): simplify docstring and use doctest   test: increase number of epochs for satisfactory accuracy   test(test_load_model_from_checkpoint): ensure we load last checkpoint   Co-authored-by: Chris Van Pelt vanpelt@wandb.com Co-authored-by: William Falcon waf2107@columbia.edu,1
7527,Fix docs - missing Trainer (#1159),0.6695939,Removed the deprecated TrainerTrainingTricksMixin class (#8679),,0
7528,Fix imports,0.78098655,import argparse,"  added neptune integration   added tests for NeptuneLogger, added neptune to docs   updated link to neptune support   fixed docstrings, fixed try/except in tests, changed append_tags input   fixed docstrings line lenght   bumped epoch nr in model restore tests   added tags support for single strings   fixed passing neptune token to backend   fixed project name in offline mode   added save_top_k=-1 to checkpoint callback   reformated initialization of neptune in online mode   bumped epoch nr to 4 in test_load_model_from_checkpoint   bumped epoch nr to 5   Co-authored-by: William Falcon waf2107@columbia.edu",1
7529,ci: fix env vars (#16323),0.5493719,Renamed utils modules (#5199), fix dangling gradients  make sure only the gradients of the current optimizer's paramaters are calculated in the training step.   add note about multiple optimizer gradient update   Update training_loop.py ,0
7530,CI: Added isort import check for the code on pull-request (#4242),0.59600675,import argparse,"Makes tracking experiment names confusion, especially when using uuids.",0
7531,Prune deprecated trainer attributes (#7501),0.6864074,Removed the deprecated TrainerTrainingTricksMixin class (#8679),  fix pillow in test   test acc   update version in deprecated msg ,0
7532,fix max_epochs setup in basic example (#1105),0.7738745,"max_nb_epochs to max_epochs,",  Fix the number of processed training batches   Fix tests   fix tests   fix tests   One more attempt   Fix another test ,1
7533,Merge pull request #55 from williamFalcon/continue,0.489371,Removed teardown from ParallelPlugin (#8943),  fix percent_checks   Added _percent_range_check   remove max ,0
7534,calling self.forward() -> self() (#1211),0.775934,            self.backward(loss)  # instead of loss.backward(),Fix typo 'buildins' -> 'builtins',1
7535,Fix for multiple callbacks (#6197),0.7335096,  callbacks:," Run AMP tests in their own process  With opt_level=""O1"" (the default), AMP patches many torch functions, which breaks any tests that run afterwards. This patch introduces a pytest extension that lets tests be marked with @pytest.mark.spawn so that they are run in their own process using torch.multiprocessing.spawn so that the main python interpreter stays un-patched. Note that tests using DDP already run AMP in its own process, so they don't need this annotation.  Fix AMP tests  Since AMP defaults to O1 now, DP tests no longer throw exceptions. Since AMP patches torch functions, CPU inference no longer works. Skip prediction step for AMP tests.  typo",1
7536,tests: switch imports for fabric (#16592),0.7149242,from lightning.fabric import Fabric,  upgrade python 3.7   upgrade python 3.7 ,1
7537,Pass current_epoch/global_step as monitor candidates [1/2] (#7344),0.5788301,"Moved attributes global_step, current_epoch, max/min_steps, max/min_epochs, batch_idx, and total_batch_idx to TrainLoop (#7437)",,0
7538,fix mypy typing errors in pytorch_lightning/strategies/strategy.py (#13519),0.70273197,+ # pytorch_lightning==1.7.0,,1
7539,Change probot workflow source (#15492),0.63085645,refactored dataloader process hook (#3139),,0
7540,ref: organize args 1/n (#3435),0.87303865,"organize args (##3435, #3442, #3447, #3448, #3449, #3456)",,1
7541,Update index.rst (#2870),0.58776045,Updated SSIM metric (#4566)(#4656),,0
7542,ddp fix,0.7551514,DDP Debugging Improvements,,1
7543,Mark trainer.data_connector as protected (#10031),0.7966676,Changed Trainer connectors to be protected attributes:,,1
7544,ref: slurm connector 1/n (#3476),0.6785437,Used checkpoint_connector.hpc_save in SLURM (#4217),,0
7545,Refactor grad_norm function (#9742),0.63417166,Gradient norm tracking (#16745),,0
7546,Fix docstring in saving.py (#9738),0.5715694,Fix saved filename in ModelCheckpoint if it already exists (#4861),,0
7547,[refactor] Add setup to profilers + _run_stage_setup to trainer 2/5 (#6633),0.7211036,"Refactor RunningStage and TrainerState usage (#4945, #7173)",,1
7548,Tests: refactor trainer (#1728),0.8235711,trainer.test(),  Change papi to api   Added try catch for old/new api reference ,1
7549,Loop flattening: remove .connect() (#16384),0.6256111,Connect and Disconnect node (#16700),  fix early stopping off by 2   add min_epochs example in docs ,0
7550,Delete test_on_before_accelerator_backend_setup (#11803),0.6971698,Refactored setup_training and remove test_mode (#5388),,0
7551,Remove add_to_queue and remove_from_queue from LightningModule (#13600),0.83407336,- Removed the deprecated `LightningModule.add_to_queue` and `LightningModule.get_from_queue` method ([#13600](https://github.com/Lightning-AI/lightning/pull/13600)),"  Change nb to num in ABCs, comments, and tqdm logging   Fix warnings text   Make warnings one line   Change num to number in comments ",1
7552,Merge pull request #13250 from PyTorchLightning/ci/rm-base,0.66451186,- Removed `configure_sync_batchnorm` from `ParallelStrategy` and all other strategies that inherit from it ([#11754](https://github.com/PyTorchLightning/pytorch-lightning/pull/11754)),,0
7553,Fix distrib_type not being set when Plugin instances being passed to Trainer (#10251),0.7079834,trainer = pl.Trainer(plugins=SLURMEnvironment(auto_requeue=False)),  fix logger tests   fix missing flush   fix tensorboard   fix namespace   fix flush   fix add_hparams ,1
7554,[App] Add lightning open command (#16482),0.8070648,Add support for Lightning App Commands through the configure_commands hook on LightningFlow and ClientCommand  (#13602),,1
7555,Remove deprecated NeptuneLogger code (#14727),0.73411024,Changed to the NeptuneLogger (#16761):,,1
7556,fixes #5,0.67802763,"At last, lots of bug fixes (see below).",  Implement TensorboardLogger   Pass default_save_path to trainers   Update tensorboard.py ,0
7557,unify pep8 speaks (#5607),0.49866045,[1.1.8] - 2021-02-08,,0
7558,Cleanup Lite apply_funcs utilitites (#14560),0.6124041,    * Cleanup some fault tolerant utilities ([#10703](https://github.com/PyTorchLightning/pytorch-lightning/pull/10703)),  fix mlflow test   fix mlflow test   update logger / mlflow   flake8   fix appveyor ,0
7559,Refactor simplify tests (#5861),0.6468327,Refactoring,,0
7560,Update Accelerator Connector for Registry (#7214),0.7418678,Removed deprecated connect_precision_plugin and connect_training_type_plugin from Accelerator (#9019),  Renamed on_sanity_check_start to on_train_start and added on_train_end to ModelHooks   changed tests to use on_train_start instead of on_sanity_check_start ,1
7561,ci: unfreeze for master (#17116),0.5508946,Renamed utils modules (#5199),,0
7562,Lite: Flatten XLAStrategy (#15838),0.5721977,Flattening Wandb Hyperparameters (#2459),  change CI install   change CI install   change CI install ,0
7563,[App] Mock missing package imports when launching in the cloud (#15711),0.5962736,Module imports are now catching ModuleNotFoundError instead of ImportError (#9867),"  add table for gpus argument   fix typo in error message   tests for supported values   tests for unsupported values   fix typo   add table for gpus argument   fix typo in error message   tests for supported values   tests for unsupported values   fix typo   fix typo list->str   fix travis warning ""line too long"" ",0
7564,Bring back access app state (#14258),0.58892405,Updated app URLs to the latest format (#16568), type: debug  Calculate the adequate number of steps to run during sanity_check. This fixes the bug when there are two or more validation dataloaders.  Before: total=self.num_sanity_val_steps  After: total=self.num_sanity_val_steps*len(self.get_val_dataloaders())   type: refactor   Put total=... in the next line  type: refactor  run flake8,0
7565,Add new CHANGELOG section (#5580),0.7346636,Full Changelog,  monkeypatch atexit.register to fix problem with cometml logging   Use experiment id for version in cometml ,1
7566,updated fast training docs with latest usage (#884),0.66594577,"Refactor RunningStage and TrainerState usage (#4945, #7173)",  extend documentation   update index   fix list ,0
7567,updated doc indexes,0.6638745,Docs improvements,,0
7568,Increment the total batch idx before the accumulation early exit (#7692),0.5932745,Reset epoch progress with batch size scaler (#13846),  add slack badge   Update README.md ,0
7569,Clarify what gpus=0 means in docs (#2876),0.6287245,"docs for all Metrics (#2184, #2209)","  rename trainer modules, drop _mixin   fix imports ",0
7570,Refactor progress bar initialization to avoid extra attribute set on Trainer (#10553),0.81179035,Changed progress_bar_refresh_rate trainer flag to disable progress bar when setting to 0. (#1108),  make partial Trainer classes as abstract   add empty attributes/methods   flake8   fix mixin order   update abstact   reorder ,1
7571,Remove the deprecated trainer.call_hook (#14869),0.77291757,Removed the deprecated TrainerTrainingTricksMixin class (#8679),  fixed gan template   Update gan.py ,1
7572,Fix isort failures in utilities (#5530),0.60023487,Improved the error message when the LightningWork is missing the run method (#14759),,0
7573,Remove logger_connector legacy code (#6733),0.98465264,Removed logger_connector legacy code (#6733),  use print for INFO and lower levels summarize()   use logging.INFO instead of magic number   bring logging.info back for other cases   move logging config to init.py   prepend the model summary with a newline ,1
7574,ref: moving train loop to own object 2/n (intermediate steps) (#3314),0.837021,"train loop refactor - moving train loop to own object (#3310, #3312, #3313, #3314)",,1
7575,Prune deprecated Trainer(checkpoint_callback=ModelCheckpoint()) (#6166),0.7643117,Removed passing a ModelCheckpoint instance to Trainer(checkpoint_callback) (#6166),  fix logging error   no need for the '+' sign   move space to beginning of next line ,1
7576,simple tests restructure (#5452),0.6306319,"Refactored training_batch + tests to verify correctness (#2327, #2328)",  fix defecation warnings   flake8   update deprecations ,0
7577,"reverted ""temporary drop metrics tests while speeding them up"" and SKIP (#4115)",0.6606673,Drop duplicate metrics (#5014),"  rename nb -> num   flake8   batch_nb, epoch_nb, gpu_nb, split_nb   add _num deprecations ",0
7578,Merge pull request #13 from cinjon/on_tng_metrics,0.50295365,Integrated metrics API with self.log (#3961),  format docstring in tests   prune unused vars   optimize imports   drop duplicated var ,0
7579,Integrate with lightning_utilities.core.enums (#14558),0.7669566,- Integrate the `lightning_utilities` package (,  Use pytest tmpdir   Switch to tmpdir fixtures   Switch to tmpdir fixture   tmpdir fixture   Fix more conflicts ,1
7580,release v0.2.4.1,0.85355866,0.4.0,  feat: add reducelronplateau callback   feat: use reducelronplateau callback in trainer   feat: only on unsupported lr schedulers   feat: last but not the least merge of master   feat: merge master   feat: support only on scheduler in reduceLrOnPlateauScheduler   refactor: code style   Update pt_callbacks.py   Update trainer.py   Update train_loop_mixin.py   Update trainer.py   Update train_loop_mixin.py ,1
7581,Fix App Docs for lightning ssh-keys command (#15773),0.7670181,lightning add ssh-key CLI command has been transitioned to lightning create ssh-key,  min pyTorch 1.1   try fixed test-tube   try fixed test-tube   try fixed test-tube   cleaning   Update requirements.txt ,1
7582,Make unimplemented dataloader hooks raise NotImplementedError (#9161),0.69627696,Raised a MisconfigurationException if batch transfer hooks are overriden with IPUAccelerator (13961),  Add resume_from_checkpoint   Fix variable name   515 Remove did_restore   515 Simplify code   515 Update doc for resume_from_checkpoint   515 Add on_gpu ,0
7583,cd: releasing packages (#13489),0.7029536,This release includes:,,1
7584,Describe the behavior with limit_*_batches=1|1.0 (#11950),0.60279965,Disabled training when limit_train_batches=0 (#4371),,0
7585,Rename opt_idx to optimizer_idx in docs for complex training loops (#5712),0.72795856,"    optimizer_idx,",,1
7586,drop deprecated checkpoint filepath (#5321),0.88448906,Removed deprecated checkpoint argument filepath (#5321),  set auto dp if no backend   fix imagenet example   run flake8 first to fail build on syntax first ,1
7587,Add Support for multiple train loaders (#1959),0.5851063,train_dataset = trainer.train_dataloader.loaders.dataset,"  upgrade req.   move MkDocs   create Sphinx   init Sphinx   move md from MkDocs to Sphinx   CI: build docs   build Sphinx   formatting move docs from MD to docstring in particular package/modules formatting add Sphinx ext. rename root_module to core drop implicit name ""_logger"" drop duplicate name ""overwrite"" fix imports use pytorch theme add sample link mapping try fix RTD build use forked template fix some docs warnings fix paths add deprecation warnings fix flake8 fix paths revert refactor revert MLFlowLogger   revert example import   update link   Update lightning_module_template.py ",0
7588,Reuse assistant code to create the mirror package (#15365),0.5061754,Support **DictConfig for hparam serialization (#2519),"  extend CI timeout   add short MNIST   lower dataset and stop thr   refactor imports   formatting   early stop   play params   play params   minor refactoring   Conflicts: pytorch_lightning/testing/init.py pytorch_lightning/testing/lm_test_module.py pytorch_lightning/testing/lm_test_module_base.py pytorch_lightning/testing/lm_test_module_mixins.py pytorch_lightning/testing/model.py pytorch_lightning/testing/model_base.py pytorch_lightning/testing/model_mixins.py pytorch_lightning/testing/test_module.py pytorch_lightning/testing/test_module_base.py pytorch_lightning/testing/test_module_mixins.py  typo  Co-Authored-By: Ir1dXD sirius.caffrey@gmail.com  Revert ""refactor imports""  This reverts commit b86aee92  update imports",0
7589,Fix a deprecation warning (#2746),0.8484378,Deprecation warning (#3844),  refactor: rename some modules   add deprecation warnings   fix paths ,1
7590,Fix logger bug and prepare data bug (#1933),0.73020875,Cleaning up stale logger tests (#3490),,1
7591,governance updates in documentation (#9025),0.82867813,Updated governance docs,  min pytorch 1.2   fix IterableDataset   upgrade torchvision   fix msg ,1
7592,Clarify when and why LearningRateScheduler is called (#13267),0.72280127,Changed LearningRateLogger to LearningRateMonitor (#3251),,1
7593,fix val accuracy printing in lite example (#12632),0.578373,print(trainer.num_devices),  install nim req.   update requirements   drop Cython ,0
7594,Update docs for seed_everything in LightningCLI (#15308),0.7980422,Removed support for LightningCLI(seed_everything_default=None) (#16131),,1
7595,Fix reset TensorRunningAccum (#5106),1.0,Fix reset TensorRunningAccum (#5106),,1
7596,ref: moved accelerator router (#3309),0.98072934,moved accelerator router (#3309),,1
7597,make save fx part of model checkpoint cb (#4284),0.72831285,    # put all logic related to saving a checkpoint here," Avoid race condition in creating checkpoint directories  In multi-GPU training, several processes run the code that creates checkpoint dirs. This fix avoids a probably rare situation (but it happened to me) where another process created a dir between the exists check and the makedirs call.  Remove the now unneeded check for dir existence",1
7598,formatting (#4898),0.55354226,Allowing decorate model init with saving hparams inside (#4662),  Default write progress bar to stdout   Change validation progress too ,0
7599,fix generate checkpoint (#5489),0.696837,Refactor load in checkpoint connector (#4593),"  docs: enable syntax highlight   feat: change Checkpoint callback's save_best_only to save_top_k   fix #70   docs: update docs for save_top_k   revert other files   style: lint for travis-ci   fix typo   make flake8 happy   update according to review   add tests   rename func to private   add doc on save_top_k == 0   make flake8 happy   update according to PR comments   change some f-strings   Update pt_callbacks.py   Update test_models.py   update options   create folders   Update test_models.py   change epoch num   support calling multiple times, add docs and tests   update docs   roll back changes in earlystopping   clean test files   make flake8 happy   fix epoch number   update tests about epoch numbers   clean debugging code   fix testing utils codes   fix testing utils codes   fix testing utils codes   fix testing utils codes   change save_dir to tests/tests according to previous lines   remove unused overwrite option   make flake8 happy   change var name as per review   make flake8 happy   update property name to work on master   elaborate in the docs   update docs as per review   revert previous commit   accidentally pressed wrong button when solving conflicts",0
7600,fix tb version (#2985),0.61705834,This release fixes that core issue," Fix returning only 2 values on an early exit.   This fixes a bug  ValueError: not enough values to unpack (expected 3, got 2)   Update train_loop_mixin.py   Change to return dict   The return value was actually a dict even though that variable is initialized as a list.",0
7601,Added fix to ensure that custom logged metrics within test_epoch_end are appended to the result object even without step reduced metrics (#4251),0.76344204,Changed the behaviour when logging evaluation step metrics to no longer append /epoch_* to the metric name (#7351),,1
7602,dev2 release,0.5608199,0.4.0,,0
7603,codecov can't parse GPU/TPU coverage - temporary skip until that is fixed on their end (#2743),0.62401295,GPU training (#2704),,0
7604,Fixed typos in README (#1219),0.54590845,Changed overwrite to True (#16009),,0
7605,Update Gradient Clipping for TPU Accelerator (#6576),0.99746966,Update Gradient Clipping for the TPU Accelerator (#6576),,1
7606,set minimal req. PT 1.4 (#5418),0.59677744,Deprecated max_nb_epochs and min_nb_epochs (#567),,0
7607,Add Google Colab badges (#5111),0.49925596,Changed the default value for the progress_bar_refresh_rate Trainer argument in Google COLAB notebooks to 20 (#5516),  add CircleCI config   fix CircleCI   fix CircleCI ,0
7608,Fix tests_pytorch import error in legacy checkpoint CI (#15566),0.7183284,from pytorch_lightning.plugins import CheckpointIO,"  Fixing comet ml bug and adding functionality   Updating documents   Fixing code style issues in comet_logger   Changing comet_logger experiment to execute lazily   Adding tests for comet_logger and addressing comments from @Borda   Setting step_num to optional keyword argument in log_metrics() to comply to other loggers   Adding offline logging mode for comet_ml, updating tests and docs   Switching to MisconfigurationException ",1
7609,Fix ddp accelerator choice for cpu (#8645),0.7617505,Fix hanging in DDP HPC accelerators (#5157),,1
7610,"Support DeepSpeed >=0.6.0, <0.6.5 (#13863)",0.7474532,Support for manual optimization with DeepSpeed (#7970),  remove O2 crash   remove O2 crash   bananas ,1
7611,[Feat] Cleanup ModelCheckpoint / EarlyStopping by moving logic to LoggerConnector (#5218),0.75959694,Dramatically simplify the LoggerConnector (#7882),,1
7612,"Update omegaconf requirement from <2.3.0,>=2.0.5 to >=2.0.5,<2.4.0 in /requirements (#16545)",0.49830288,0.4.0,,0
7613,Fixed wandb logger documentation (#11540),0.800807,Removed wandb logger's finalize method (#1193),,1
7614,Improve building times of IPU docker image (#14934),0.63735425,"- When training with `precision=16` on IPU, the cast has been moved off the IPU onto the host, making the copies from host to IPU cheaper ([#13880](https://github.com/Lightning-AI/lightning/pull/13880))",  ImageNet example   cleanup   cleanup   Minor changes from feedback   More cleanup ,0
7615,Move save_hyperparameters to its own function (#7119),0.9868771,Moved save_hyperparameters to its own function (#7119),  add Twine to CI   freeze Twine   freeze Twine   minor refactoring   try another   fix req.   update README   fix doc   fix multiple req. test-tube ,1
7616,Typo in printing app.py (#16643),0.4977727,package pytorch_lightning.logging,,0
7617,added lbfgs support,0.74251866,BFloat16 Support,,1
7618,spec cache,0.51982147,Enabled custom clusters (#4048),,0
7619,Update CI in README.md (#12495),0.47736236,refactored dataloader process hook (#3139),,0
7620,limit auto scaling batch size to the size of the training dataset (#3271),0.6858146,Tune batch size,,0
7621,add sanity check on nb available GPUs (#6092),0.7373519,GPU training (#2704),,1
7622,[2/4] Add DeviceStatsMonitor callback (#9712),0.72814804,device_stats = DeviceStatsMonitor(),,1
7623,Add manual logging to training_step manual optimization (#4476),0.69009054,Consistently use step=trainer.global_step in LearningRateMonitor independently of logging_interval (#4376),,0
7624,Fix hparams loading for model that accepts *args (#2911),0.65526396,argparse_utils >> argparse,,0
7625,Adding a new section to the docs: Example Lightning Project Structures (#1851),0.73526114,LightningCLI additions:,,1
7626,Refactor: skipif for AMPs 3/n (#6293),0.719223,training AMP scaling refactor (#3135),  smurf ethics   smurf ethics   removed auto ddp fix   removed auto ddp fix   removed auto ddp fix   removed auto ddp fix   removed auto ddp fix   removed auto ddp fix ,1
7627,Sharded Plugin 2/n: Allow ddp plugin to modify optimizer state saving (#4675),0.7676368,- Added support for saving sharded optimizer state dict outside of `DDPShardedStrategy` ([#14208](https://github.com/Lightning-AI/lightning/pull/14208)),,1
7628,Allow training type plugin to delay optimizer creation (FSDP 2/n) (#6331),0.9326769,Allowed training type plugin to delay optimizer creation (#6331),,1
7629,auto port kill before starting ddp,0.5661191,Enabled traditional/manual launching of DDP processes through LOCAL_RANK and NODE_RANK environment variable assignments (#7480),,0
7630,add CI event published (#7353),0.4853334,# Add this if you need the loss to be displayed in the progress bar,,0
7631,"Remove the deprecated on_{train,val,test,predict}_dataloader hooks (#13033)",0.7835331,Removed on_reset_*_dataloader hooks in TrainingType Plugins and Accelerators (#8858),,1
7632,hotfix to unblock hparams and OmniConf - removes auto_register_init_args by default (#2025),0.55422485,"Removed deprecated ModelCheckpoint arguments prefix, mode=""auto"" (#6162)",  Make name and version properties required   Warn before deleting files in checkpoint directory   Get default checkpoint path from any logger   Fix typos   Uncomment logger tests   Whitespace   Update callback_config_mixin.py   checkpoints and version file names would just have a number. it's easy to tell what you're looking at with version_ prepended   Address comments   Fix broken tests ,0
7633,Rename old references to training type plugin in tests (#13421),0.69757986,- Deprecated `training_type_plugin` property in favor of `strategy` in `Trainer` and updated the references ([#11141](https://github.com/PyTorchLightning/pytorch-lightning/pull/11141)),  added training_end   added training_end   added training_end   added training_end   added training_end   added training_end   added training_end   added training_end   added training_end   added training_end   added training_end   added training_end   allow ddp and apex to be configured   allow ddp and apex to be configured   bananas   bananas   bananas   bananas   bananas   bananas   bananas   bananas   bananas   bananas   bananas   bananas   bananas   bananas   bananas   added eval and train for redundancy   added eval and train for redundancy   added training_end   added training_end   added training_end   added training_end   added training_end   added training_end   added training_end   added training_end   added training_end   added training_end   added training_end   added training_end   allow ddp and apex to be configured   allow ddp and apex to be configured   bananas   bananas   bananas   bananas   bananas   bananas   bananas   bananas   bananas   bananas   bananas   bananas   bananas   bananas   bananas   added eval and train for redundancy   added eval and train for redundancy ,0
7634,attach model parallelism video inside docs (#10274),0.59628177,Data Parallelism,,0
7635,Change min max gpu memory to be on their own plots (#1358),0.94401884,Changed min-max GPU memory to be on their own plots (#1358),  452 Fix ValueError   452 Use subprocess.run   452 Simplify code for gpu_memory_map   452 Simplify code for min max memory   452 Add test for get_memory_profile   452 Use os.sep   452 Use os.linesep ,1
7636,Fix typing in pl.callbacks.lr_monitor (#10802),0.61791706,Avoid redundant callback restore warning while tuning (#13026),  change print to logging   always use logging.info   use f-strings   update code style   set logging configs   remove unused code ,0
7637,Fix typo (#2646),0.58099985,Changed overwrite to True (#16009),,0
7638,check for kaggle env variable (#1568),0.4311663,Update lr_finder to check for attribute if not running fast_dev_run (#5990), Update CONTRIBUTING.md  add coding styleguide  Update CONTRIBUTING.md,0
7639,fix imagenet example,0.58636403,Fixing critical bugs in newly added hooks and hparams assignment.,,0
7640,chore: add model as recommended parameter for validate() (#15086),0.6640059,"Simplified ""should run validation"" logic (#7682)",,0
7641,Find last checkpoints on restart (#14907),0.7833098,"Automatically reload the ""last"" checkpoint",  Splitted progress bars   Iterable dataset total batches fix   Use dynamic ncols and use batch as units   Count epochs from 1 in progress bar   Fix for disabled progress bar   Code simplifications ,1
7642,req: formatting (#17335),0.54473555,Refactoring,  packed sequence clarification in train_dataloader   moved changes to training loop   removed changes from required interface   added index entry ,0
7643,fixed imports,0.7957127,import argparse,  Fix wrong example paths   correct dataloading wrong condition in Readme ,1
7644,Delete unused TPU CI files (#15611),0.56830955,rm: Delete files from your Cloud Platform Filesystem,,0
7645,Add check for bf16 in deepspeed inference (#16973),0.6734288,Update the logic to check for accumulation steps with deepspeed (#9826),"  Add truncated bptt   Fix rebase error   AutoPep8   Address comments, incl default bptt_split impl   Add tbptt test   Add default split for lists/tuples   Add tbptt docs   Fix trainer spacing   Update RequiredTrainerInterface.md ",0
7646,NumPy to Torch for lightning/fabric (#17291),0.7035662,Lightning Fabric,"Related issue #432 The old documentation suggested that the way to restore a training session is to use a test_tube Experiment. Trainer no longer takes an experiment as a parameter, so it seems the current way to restore a training session is to pass an experiment via a TestTubeLogger. Even if this is not the most elegant solution, updating the docs will at least point new users in the right direction.",1
7647,quick start docs (#2731),0.5993843,"docs for all Metrics (#2184, #2209)",,0
7648,Remove deprecated dataloader arguments in Trainer methods (#10325),0.8294581,Removed trainer.reset_*_dataloader() methods (#16726),  Fixed total number of batches   Fixed flake8 warning   Update train_loop_mixin.py   Update train_loop_mixin.py ,1
7649,Report leaking environment variables in tests (#5872),0.50761163,Resolve memory leak for evaluation (#6326),  mem clear   mem clear ,0
7650,Remove duplicate no_grad context managers (#16773),0.5502275,Avoid using the deprecated LooseVersion (#16162),,0
7651,[bug] [docs] Clearer optimizer_step override instructions (#4455),0.7219665,Executing the optimizer_closure is now required when overriding the optimizer_step hook (#9360),  add package info #358   Update init.py   wrap package info   update CI   fix package info   fix for #388   prune duplicated configs   fix install   use req from file   move info to sep. module drop comments from req   add setup req.   add setup req.   update get info   refactor init   update pip   fix failing on buildins   fix failing open   fix test imports   fix tests   fix pep8 ,1
7652,fix tpu transfer bug 2,0.6091378,Updated logic for checking TPUs availability (#6767),,0
7653,Deprecate DeviceType in favor of _AcceleratorType (#10503),0.6538006,Moved DeviceDtypeModuleMixin and HyperparametersMixin mixin to core (#8396),  Fixes #356   Fixes #356   Fixes #356   Fixes #356   Fixes #356   Fixes #356 ,0
7654,Add typing for ResultCollection [3/3] (#9271),0.61901474,    # 2. Add the outputs to the list,  refactor dataloading   refactor dataloading   refactor dataloading   refactor dataloading   refactor dataloading   refactor dataloading   refactor dataloading   refactor dataloading ,0
7655,Minor doc fixes (#5139),0.695606,Docs improvements,,0
7656,Fixing Function Signatures (#871),0.61545455,Resolve bug with Finetuning (#5744),,0
7657,Refactor dataloading (#955),0.8363046,"Refactor dataloading, supports infinite dataloader (#955)",,1
7658,Changed hook doctstring (#11345),0.6101485,"Removed output argument from *_batch_end hooks (#3965, #3966)",,0
7659,fix dirpath in log_dir for CSVLogger (#16401),0.6966287,Return the default_root_dir as the log_dir when the logger is a LoggerCollection (#8187),  hpc restore takes priority over non hpc weights   hpc restore takes priority over non hpc weights   hpc restore takes priority over non hpc weights   hpc restore takes priority over non hpc weights   hpc restore takes priority over non hpc weights   hpc restore takes priority over non hpc weights   hpc restore takes priority over non hpc weights ,0
7660,Make _get_nvidia_gpu_stats public (#10406),0.5721905,Log just the GPU stats,  clear memory cache before train starts   clear memory cache before train starts ,0
7661,Add batch size script argument for standalone tests (#13841),0.685581,Run batch size finder for validate/test/predict.,,0
7662,Deprecate DDPPlugin.task_idx (#8203),0.78384227,Deprecated DDPPlugin.task_idx in favor of DDPPlugin.local_rank (#8203),,1
7663,Make sure all kwargs come after args in _load_model_state() (#3063),0.62503326,"def on_fit_start(self, *args, **kwargs):","  Move global_step incrementing to the end of a batch loop, per https://github.com/williamFalcon/pytorch-lightning/issues/411   Move met_batch_limit condition to the end   cleanup whitespace   Update train_loop_mixin.py ",0
7664,fix shell injection vulnerability in subprocess call (#2786),0.5258643,Attempted SLURM auto resume call when non-shell call fails (#6002),  refactored tests   refactored tests   refactored tests   refactored tests   refactored tests   refactored tests   refactored tests   refactored tests   refactored tests ,0
7665,Fix hiddens type annotation (#9377),0.53540146,        hiddens = hiddens.detach(),,0
7666,Docs (#315),0.7910467,Docs,"  Unit tests for num_gpu property as proxy for __parse_gpu_ids.   Refactoring __parse_gpu_ids   Moved the function outside the class as it is an utility function and did not depend on class in any way.   Added unit tests for it.   Mocked torch.cuda.device_count function in tests.   This allows the tests to be run on machines that do not have gpus.  Fixed the parse_gpu_ids function to handle -1 case.  Function now handles -1 the same way as it does for '-1'.  Unit tests for root_gpu added.  Added backend as a parameter as currently depending on backend set or not, code fails with exception in certain circumstances, before giving a wrong answer.  Moved __set_root_gpu function out of the class.  This function does not depend on the class and can be tested more easily this way. Also added unit tests for this function. They simply reuse data for the root_gpu property.   determine_root_gpu_device passes unit tests.   num_gpus passes unit tests.   Also added a None test for this function.  parse_gpu_ids tests changed to reflect desired state after refactoring.  Planning to refactor parse_gpu_ids to always return list of ints. This will simplify code that use output of this function.    parse_gpu_ids always returns lists   parse_gpu_ids checks given ids against available ids parse_gpu_ids raises exception for non existant ids parse_gpu_ids returns None when no gpus are available cleaned up determine_root_gpu_device cleaned up num_gpus property  Updated unit tests to reflect changes in the functions   Flake8 fixes   Moved fixture code up before where it is used.   Updated documentation.   Changed tests to match the API:  gpus=-1 or gpus='-1' should use all available gpu devices gpus=N N=0: no gpus should be used. N>0: N gpus should be used    gpus=list of ints or a comma separated string of numbers:     Use the gpus indicated by the list or the string.   Fixed code to pass all the changed tests for parsing gpus param.   Refactoring parse_gpu_ids function.   flake8 fixes.   Updating documentation.   flake8 fixes.   flake8 fixes.   flake8 fixes   Update trainer.py   Update dp_mixin.py   Make reduce_distributed_output a stand alone function. Fix imports. Fix flake8.   Add comet_ml dependency to tests requirements.txt   Revert ""Make reduce_distributed_output a stand alone function. Fix imports. Fix flake8.""   This reverts commit eac0338  Merge with master.",1
7667,Update horovod.py (#11917),0.7463958,"refactored Horovod backend (#3121, #3122)",,1
7668,Minor fix with data_path flag to imagenet script (examples) (#12637),0.61698043,- all the file path errors with loggers (txs @awaelchli),  Save and load hparams from checkpoints   Update docs   Add warning when not saving hparams   Missing import   Update .run_local_tests.sh   Update lm_test_module_mixins.py   Update lightning_module_template.py ,0
7669,test selecting the correct backend. temp backends while slurm and TE are decoupled (#3848),0.892092,test selecting the correct backend. temp backends while slurm and TorchElastic are decoupled (#3848),,1
7670,[App] Porting fixes to autoscaler component (#16249),0.9578472,Porting fixes to autoscaler component (#16249),,1
7671,Merge pull request #9690 from PyTorchLightning/feature/codeowners-rohit,0.642502,- Removed `configure_sync_batchnorm` from `ParallelStrategy` and all other strategies that inherit from it ([#11754](https://github.com/PyTorchLightning/pytorch-lightning/pull/11754)),,0
7672,slurm job id (#1605),0.77177745,# use slurm job id for the port number,,1
7673,update changelog after App 0.6.2 (#14853),0.6773884,Complete changelog,,0
7674,Configure mypy to install dependencies in CI and update pyproject.toml (#10682),0.6217632,Enable PyTorch 1.7 compatibility (#3541),,0
7675,Fix deprecation test version for accelerator collective (#9892),0.6762321,reduced accelerator selection (#3211),,0
7676,fix Trainer docs example indent and warnings (#1065),0.64264333,Removed Warning from trainer loop (#1634),,0
7677,skip warning test (#1533),0.6909776,warning_utils >> warnings,,0
7678,Document exceptions in accelerators (#9558),0.6113242,Enabled passing in custom accelerators (#4050),,0
7679,Update single_gpu_node_ddp_template.py,0.6131041,Deprecated num_nodes and sync_batchnorm arguments in DDPPlugin and DDPSpawnPlugin (#7026),,0
7680,fix restoring finetune callbacks after accelerator setup on training resume (#8501),0.68689644,Skip restore from resume_from_checkpoint while testing (#5161),  moved COMET_DISABLE_AUTO_LOGGING out of modeule for flake8 compliance   Update init.py ,0
7681,CI: update HPU pool (#14733),0.6328622,Moveed HPU broadcast override to the HPU strategy file (#17011),,0
7682,prune data parallel (#7510),0.59586143,Data Parallelism,,0
7683,[IPU] Call accelerator hooks regardless if LM hook overridden 1/n (#7826),0.66777503,Accelerator hooks are called regardless if LightningModule overrides the same hooks (#7826),,0
7684,CI: sanity check for req. pkgs (#11819),0.601066, - Sanity checking: `Trainer(num_sanity_val_steps>0)`,,0
7685,Unify checkpoint load paths [redo #9693] (#10061),0.7580895,Refactor load in checkpoint connector (#4593),,1
7686,fix amp/apex misconfiguration error for cpu (#6107),0.7071111,enable_pl_optimizer=False by default to temporarily fix AMP issues (#5163),  code cleaning   drop unused imports   optimize imports ,1
7687,Fix governance references in docs (#8984),0.7595839,Updated governance docs,,1
7688,fixing some compatibility with PT 1.8 (#5864),0.59879386,Enable PyTorch 1.7 compatibility (#3541),,0
7689,Fix batch normalization statistics in StochasticWeightAveraging (#15113),0.76788557,- Fixed batch normalization statistics calculation in `StochasticWeightAveraging` callback ([#14866](https://github.com/Lightning-AI/lightning/pull/14866)),,1
7690,Convert error to warning for logging without a Trainer (#9733),0.7769554,self.log-ing without a Trainer reference now raises a warning instead of an exception (#9733),  added fixed frequency val batch check   added fixed frequency val batch check   Finished IterableDataset support   flake8   flake8   flake8 ,1
7691,don't pass tpu weights back on test (#2566),0.6733438,TPU training (#2708),"  added comet logger   bug fix in cases where comet was not imported before torch   fixed mlflow logger to be consistent with docs, updated cometLogger and cometLoggers docs + flake 8 compliance ",0
7692,Add __len__ method to IndexBatchSamplerWrapper (#7681),0.62955517,class EvalBatchSizeFinder(BatchSizeFinder):,"  moved dp, ddp outside of trainer   added main mixins   finished major mixin refactor   flake8   finished major mixin refactor   finished major mixin refactor   finished major mixin refactor   finished major mixin refactor   finished major mixin refactor   finished major mixin refactor   finished major mixin refactor ",0
7693,fix logger experiment version in multiple run DDP (#7077),0.72404736,DDP + loggers should be fixed,,1
7694,Remove the unused utilities.parsing.flatten_dict (#16744),0.6543386,- Removed the unused `lightning.pytorch.utilities.parsing.flatten_dict` function ([#16744](https://github.com/Lightning-AI/lightning/pull/16744)),  Provide backward compatibility for e681253   typo fix ,0
7695,add metrics to pl module (#2506),0.6416022,pl_module = ProductionReadyModel(),  fix test for MacOS   formatting   fix pkg names ,0
7696,Make Trainer Debuggable and understandable again (1/n) (#14861),0.7786442,Allow easy trainer re-instantiation (#7508),,1
7697,Delete legacy dataloader processing utility (#8439),0.639593,Removed InterBatchProcessor in favor of DataLoaderIterDataFetcher (#9052),,0
7698,docs (#4093),0.7524502,"docs for all Metrics (#2184, #2209)",,1
7699,Show tf32 info only on rank 0 (#16152),0.55796295,Rank-zero only EarlyStopping messages,  changes to test fx   changes to test fx   changes to test fx   changes to test fx   changes to test fx   changes to test fx   changes to test fx   changes to test fx   changes to test fx   changes to test fx ,0
7700,Fix pip install with no PACKAGE_NAME or editable mode (#15853),0.69642967,pip install rich,,0
7701,Flatten fetching abstract interface (#16664),0.63744116,Simpler interface,,0
7702,Make WandbLogger run initialization lazy for non-spawn distributed operation (#17573),0.6515644,Allow use of same WandbLogger instance for multiple training loops (#2055),  changes to seed for tests   changes to seed for tests   changes to seed for tests   changes to seed for tests   changes to seed for tests   changes to seed for tests   changes to seed for tests   changes to seed for tests   changes to seed for tests   changes to seed for tests   changes to seed for tests   changes to seed for tests   changes to seed for tests   changes to seed for tests   changes to seed for tests ,0
7703,Update ipython[all] requirement from <8.6.1 to <8.7.1 in /requirements (#16220),0.51093566,Refactor cloud dispatch and update to new API (#16456),,0
7704,fixed issue where callback_metrics was replaced instead of updated (#492),0.7484417,Metric compute() method will no longer automatically call reset() (#5409),,1
7705,Update Slack link (#12460),0.5987594,- Removed the deprecated `LightningDataModule.on_save/load_checkpoint` hooks ([#14909](https://github.com/Lightning-AI/lightning/pull/14909)),,0
7706,Docs5 (#1036),0.70330036,Docs,,1
7707,"Remove todo, ensure we only check rank 0 for deepspeed warning (#9311)",0.61832494,- Removed deprecated support for passing the `rank_zero_warn` warning category positionally ([#14470](https://github.com/Lightning-AI/lightning/pull/14470)),,0
7708,[TPU] update is_tpu_exists utils internal logic to rely on xmp.spawn  (#6719),0.7475413,Updated logic for checking TPUs availability (#6767),  any key in logs or progress bar is a candidate for callback metric   any key in logs or progress bar is a candidate for callback metric ,1
7709,Disable validation completely when overfit_batches>0 (#9709),0.8392052,Validation now runs in overfitting mode,,1
7710,[Fix] TPU Training Type Plugin (#6816),0.7988246,TPU training (#2708),This reverts commit a7f26a67ac1b28f2ddd47eaec96ca7e92befbfc8.,1
7711,move unnecessary dict trainer_options (#1469),0.6846173,Removed trainer.reset_*_dataloader() methods (#16726),  add package info #358   Update init.py   Update init.py ,0
7712,"Prevent artifactual ""running from outside your current environment"" error (#15647)",0.95653814,"Prevent artefactual ""running from outside your current environment"" error (#15647)","@williamFalcon needs to create own appveyor ""account"" and update the badge ",1
7713,Add SyncBatchNormPlugin (#11754),0.6819896,- Added `LayerSync` and `NativeSyncBatchNorm` plugins ([#11754](https://github.com/PyTorchLightning/pytorch-lightning/pull/11754)),,0
7714,Fix bug comparing max_steps to global step which inits at 0 (#4278),0.6481316,- global_step += 1,,0
7715,Support state restoration of logged results 2/2(#7966),0.6247426,Logging the loss to the progress bar (#16192),,0
7716,[App] Improve cluster creation / deletion experience (#15458),0.8029064,Made cluster creation/deletion async by default (#16185),,1
7717,fix dtype/device property not getting updated in submodules (#2657),0.6299261,Moved DeviceDtypeModuleMixin and HyperparametersMixin mixin to core (#8396),,0
7718,"Update docker requirement from <=5.0.3,>=5.0.0 to >=5.0.0,<6.0.2 in /requirements (#16007)",0.5419823,"    version=""0.0.1"",",,0
7719,Fix reference link to s3fs (#10737),0.5809281,Changed fsspec to tuner (#4458),,0
7720,Horovod & py3.8 (#2764),0.7174496,"refactored Horovod backend (#3121, #3122)",,1
7721,Pin Sphinx<4.0 (#7456),0.67035234,Pinned sphinx-autodoc-typehints with <v1.15 (#11400),,0
7722,ref: add eval loop object to streamline eval loop (#3138),0.97311306,add eval loop object to streamline eval loop (#3138),,1
7723,Remove AcceleratorConnector.tpu_id (#12387),0.85730696,- Removed `AcceleratorConnector.tpu_id` property ([#12387](https://github.com/PyTorchLightning/pytorch-lightning/pull/12387)),,1
7724,Rename training_type_plugin file to strategy (#11239),0.7492168,"All TrainingTypePlugins have been renamed to Strategy (#11120). Strategy is a more appropriate name because it encompasses more than simply training communcation. This change is now aligned with the changes we implemented in 1.5, which introduced the new strategy and devices flags to the Trainer.",  fixing tests   fixing tests   fixing tests   fixing tests   fixing tests   fixing tests   fixing tests   fixed tests   fixed tests   fixed tests   fixed tests   fixed tests   fixed tests   fixed tests   fixed tests   fixed tests ,1
7725,formatting tests1/n (#5843),0.6360794,"nb_test_batches to num_test_batches,",  fix test   fix test   fix test   fix test   fix test   fix test   fix test   fix test   fix test   fix test   fix test   fix test   fix test   no warnings always   no warnings always   no warnings always   no warnings always ,0
7726,decrease some training times (#2256),0.7048429,Disabled training when limit_train_batches=0 (#4371),,1
7727,Fix app e2e race condition. Add more logs verbosity (#16251),0.5693212,Re-Enable Logger's ImportErrors (#1938),"  Allow disabling logger, early stopping, and checkpoints   Typo   Get tests passing   Update trainer.py ",0
7728,typo (#2415),0.51250434,Changed overwrite to True (#16009),,0
7729,update versions (#7409),0.66672075,Set version as today (#13906),,0
7730,fix PT version in CUDA docker images (#3739),0.51990104,Renamed TPUHalfPrecisionPlugin to TPUBf16PrecisionPlugin (#10026),,0
7731,ci: limit keeping artifacts for PRs (#16976),0.5107368,Refactored EpochResultStore (#5522),,0
7732,Introduce primitives for input/output dtype conversion in Lite Precision (#14792),0.585655,Enable mixed precision in DDPFullyShardedStrategy when precision=16 (#12965),,0
7733,"Update numpy requirement from <1.24.1,>=1.17.2 to >=1.17.2,<1.24.2 in /requirements (#16473)",0.61674774,"- num_devices = max(1, trainer.num_gpus, trainer.num_processes)",,0
7734,Deprecate weights_save_path from the Trainer constructor  (#12084),0.80213594,"Deprecated Trainer attribute ckpt_path, which will now be set by weights_save_path (#2681)",  fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests   fixed ckpt tests ,1
7735,Docs (#813),0.78462076,Docs,,1
7736,Replace occurrences of on_before_accelerator_backend_setup_called with setup  (#11568),0.7851944,  * `Callback.on_before_accelerator_backend_setup` in favor of `Callback.setup`,,1
7737,inspect training_step for opt_idx (#573),0.68413526,# 3. Remove the `optimizer_idx` argument from `training_step`,  weights go into default logger folder   weights go into default logger folder   weights go into default logger folder   weights go into default logger folder   weights go into default logger folder   weights go into default logger folder   ckpt callback in pretrain routine so exp already has version   ckpt callback in pretrain routine so exp already has version   ckpt callback in pretrain routine so exp already has version ,0
7738,Combined setup of model and optimizer with FSDP (#17305),0.76903653,Generalized Optimizer validation to accommodate both FSDP 1.x and 2.x (#16733),  weights go into default logger folder   weights go into default logger folder   weights go into default logger folder   weights go into default logger folder   weights go into default logger folder   weights go into default logger folder ,1
7739,Fix typo in README.md,0.46967345,Monir bug fix with print issues and data_loader (#1080),,0
7740,Initialize ModelCheckpoint state as early as possible (#11108),0.7554213,ModelCheckpoint now runs at the end of the training epoch by default (#8389),,1
7741,:sparkles: Use store_true for bool args (#1822),0.52431047,WandbLogger does not force wandb reinit arg to True anymore and creates a run only when needed (#4648),  removed hparam calls   removed hparam calls   removed hparam calls   removed hparam calls   removed hparam calls   Update test_models.py ,0
7742,Fix load_from_checkpoint to return model on correct device (#17308),0.6610252,Refactor load in checkpoint connector (#4593),  fixes non python type callback metrics   fixed fast dev run   fixed fast dev run   fixed fast dev run   fixed fast dev run   fixed fast dev run   fixed fast dev run   fixed fast dev run ,0
7743,Remove the FairScale integration (#16400),0.6964155,Avoided requiring the FairScale package to use precision with the fsdp native strategy (#14092),  Ensure logger.finalize is called   Call logger.finalize   Update mlflow_logger.py   Update test_logging.py   Update trainer.py ,0
7744,fix demo,0.55498725,This release fixes that core issue,  remove os.exit from early stopping   remove os.exit from early stopping   fixed weight summary   fixed weight summary   fixed weight summary   fixed weight summary   fixed weight summary   fixed weight summary   fixed weight summary ,0
7745,Implement correct transfer to GPU for batches (#200),0.7330665,Changed min-max GPU memory to be on their own plots (#1358),  callbacks use all other keys in return dict   callbacks use all other keys in return dict   callbacks use all other keys in return dict   callbacks use all other keys in return dict   remove os.exit from early stopping ,1
7746,Use searchsorted over argmax (#9670),0.61668706,"organize args (##3435, #3442, #3447, #3448, #3449, #3456)","  print thousands as K, M, B, T, ...   add option to print top-level modules only   added doc string and added spacing   do not print summary if neither ""full"" nor ""top""   updated docs showing summary print options   fix line length for travis ",0
7747,move block_ddp_sync_behaviour to utilities (#9192),0.80455494,Moved block_ddp_sync_behaviour out of TrainingBatchLoop to loop utilities (#9192),,1
7748,Add SWA warning if not running every epoch (#6987),0.797089,Enforce an epoch scheduler interval when using SWA (#6588),,1
7749,Delete overview.jpg,0.5576093,- The base Plugin class has been removed. ,It now points to the current examples folder.,0
7750,Improve skipping step tests (#4109),0.6439854,"Deprecated model steps training_end, validation_end and test_end (#1051, #1056)",,0
7751,Add CLI Command to Delete Lightning App (#15783),0.8042395,CLI Commands for Lightning Apps,,1
7752,Use accelerator backend,0.8617827,Refactored accelerator backends:,,1
7753,updated test-tube dep number,0.73470914,Deprecated the TestTubeLogger (#9065),  changed rank 0   changed rank 0   changed rank 0   changed rank 0   changed rank 0   changed rank 0   changed rank 0   changed rank 0   changed rank 0   changed rank 0   changed rank 0   changed rank 0   changed rank 0   changed rank 0   changed rank 0   changed rank 0   changed rank 0   changed rank 0   changed rank 0   changed rank 0   changed rank 0   changed rank 0   changed rank 0   changed rank 0   models wait to restore weights   models wait to restore weights ,1
7754,Constrain IPU precision choices (#10030),0.65532184,Stopped optimizer_zero_grad from being called after IPU execution (#12913),,0
7755,refactor checkpoint loading for training type plugins (#7928),0.73702466,CheckpointIO Plugins,  cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up docs   cleaned up test_tube logger   cleaned up test_tube logger   cleaned up test_tube logger ,1
7756,moved badge,0.46334428,Changed,,0
7757,Skip ddp fork tests on windows (#14121),0.6943368,Run ddp_spawn dataloader checks on Windows (#6930),,0
7758,Fixes to import,0.77450377,import argparse,,1
7759,group fit data links,0.60245275,group prepare data hook (#3212),,0
7760,"[TPU] For XLA Strategy, added function arg to control broadcast_master_param() (#17522)",0.5911729,Enabled MultiNode Components to support state broadcasting (#15607),,0
7761,Update signature of LightningModule.training_step_end (#10094),0.782714,Simplified training phase as LightningEnum (#5419),,1
7762,"Support deterministic=""warn"" in Trainer for Pytorch 1.11+ (#12588)",0.8005873,"In Pytorch 1.11, operations that do not have a deterministic implementation can be set to throw a warning instead of an error when ran in deterministic mode. This is now supported by our Trainer:",,1
7763,Refactor model summary + generalize example input array (#1773),0.5673353, - Model summary: `Trainer(enable_model_summary=True)`,,0
7764,Fix early stopping off by 2 (min_epochs) (#617),0.66665685,EarlyStopping now runs at the end of the training epoch by default (#8286),  cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos   cleaning up demos ,0
7765,Merge pull request #1632 from quinor/patch-1,0.5836023,"merge backends (#3476, #3477, #3478, #3480, #3482)",  cleaning up demos   Update job_submit.sh   Update README.md ,0
7766,Raise an error when resuming training with Apex (#14341),0.75979364,"- When resuming training with Apex enabled, the `Trainer` will now raise an error ([#14341](https://github.com/Lightning-AI/lightning/pull/14341))",  Fixes #234   default logger version is now slurm job id   default logger version is now slurm job id ,1
7767,switch LAI deployment branch (#15194),0.55678594,Release LAI docs as stable (#14250),,0
7768,Use literal syntax instead of function calls to create data structure (#8406),0.587943,Support shorthand notation to instantiate datamodules (#10011),,0
7769,Unify names in Utils (#5199),0.7834984,Unified module names in Utils (#5199),,1
7770,Move parameter validation specific to TPU Training plugins (#7415),0.7192274,TPU training (#2708),,1
7771,CI: Correct test path to publish test results (#13862),0.50172776,Enabling val/test loop disabling (#2692),,0
7772,Fixed example implementation of AutoEncoder. (#3190),0.62876904,Porting fixes to autoscaler component (#16249),,0
7773,System customization syncing for jobs run (#16932),0.9999999,System customization syncing for jobs run (#16932),,1
7774,release v0.7.0,0.73200697,"    version=""0.0.1"",",,1
7775,Improvements to standalone scripts (#13840),0.5331186,Removed the SingleProcessRuntime (#15933),,0
7776,move torch.cuda.set_device() to enable collective calls earlier in setup (#8312),0.99000657,Moved torch.cuda.set_device() to enable collective calls earlier in setup (#8312),,1
7777,Bump actions/upload-artifact from 2 to 3 (#13622),0.505798,Changed warnings and recommendations for dataloaders in ddp_spawn (#6762),  decoupled training metrics from logging metrics   decoupled validation metrics from log metrics   updated docs   updated docs   updated docs   Fixed test   merged master   merged master   merged master   merged master   merged master   merged master   merged master   merged master   merged master   merged master   merged master   merged master   merged master   merged master   merged master   merged master   merged master   merged master   merged master   merged master   merged master   merged master   merged master   merged master   merged master ,0
7778,Deprecate trainer.disable_validation (#8291),0.8948522,Deprecated the Trainer.disable_validation property in favor of not Trainer.enable_validation (#8291),  Fixes #289   Fixes #289   added lbfgs support   Fixes #280 (#309)   added test seeds (#306)   added test seeds   added test seeds   updated docs   added lbfgs support (#310)   added lbfgs support   added lbfgs support   added lbfgs support   Fixes #280 (#309)   added test seeds (#306)   added test seeds   added test seeds   updated docs   added lbfgs support   added lbfgs support   added lbfgs support   added lbfgs support   added lbfgs support   added lbfgs support   added lbfgs support   added lbfgs support   Fixes #289   Fixes #289   merged master   merged master ,1
7779,added docs page,0.6754795,Docs improvements,  added lbfgs support   added lbfgs support   added lbfgs support   Fixes #280 (#309)   added test seeds (#306)   added test seeds   added test seeds   updated docs   added lbfgs support   added lbfgs support   added lbfgs support   added lbfgs support   added lbfgs support   added lbfgs support   added lbfgs support   added lbfgs support ,0
7780,workers warning not on windows (#1430),0.5417576,warning_utils >> warnings,  added test seeds   added test seeds   updated docs ,0
7781,prog bar option,0.5890059,progress bar,,0
7782,[fix] Fix multi-node DDP launch by using local rank instead of global rank for main process (#7061),0.7298453,Deprecated DDPPlugin.task_idx in favor of DDPPlugin.local_rank (#8203),,1
7783,[Feat] 2/n Add Fault Tolerant Training to LightningFetcher (#8891),0.7988715,reference to the Trainer on the LightningDataModule (#3684),  early stopping callback is not default   added a default logger   added default checkpoint callback   added default checkpoint/loggers   added default checkpoint/loggers   updated docs   cleaned demos   cleaned demos   cleaned demos   clean up docs around loggers   clean up docs around loggers   clean up docs around loggers   clean up docs around loggers   clean up docs around loggers   clean up docs around loggers   clean up docs around loggers   clean up docs around loggers   clean up docs around loggers   clean up docs around loggers   clean up docs around loggers   clean up docs around loggers   clean up docs around loggers ,1
7784,Add issue triage config (#11880),0.6004931,Configuration Validator (#9779),,0
7785,Fix Special Tests (#7841),0.68565506,Refactored setup_training and remove test_mode (#5388),,0
7786,ci: adjust torch version requirements in IPU pipeline (#7383),0.6513749,"Set PyTorch 1.4 as min requirements, also for testing and examples torchvision>=0.5 and torchtext>=0.5 (#5418)",,0
7787,"Disable checkpointing, earlystopping and logging with fast_dev_run (#5277)",0.9671555,"Disabled checkpointing, earlystopping and logging with fast_dev_run (#5277)",  Update root_module.py   Update root_module.py   Update root_module.py   tests fix   tests fix   tests fix ,1
7788,"Add conda version/downloads, discourse badges to readme (#3247)",0.4857171,adding Trainer.tune() (#3293),,0
7789,docs: move fabric on its own (#16742),0.64867103,Learn more about Fabric and what it can do in the new docs!,,0
7790,"Update mlflow requirement from <1.28.0,>=1.0.0 to >=1.0.0,<1.29.0 in /requirements (#14311)",0.68404293,Updated mlflow with using resolve_tags (#6746),  Update root_module.py   Update root_module.py   Update root_module.py   tests fix   tests fix ,0
7791,Update issue template to use discussions for questions (#6155),0.50830674,Allow passing hparams as a keyword argument to LightningModule when loading from checkpoint (#1639),,0
7792,Separate epoch validation from step validation (#5208),0.99999994,Separate epoch validation from step validation (#5208),  fixes memory crash   fixes memory crash ,1
7793,Update ipython[all] requirement from <8.4.1 to <8.5.1 in /requirements (#14671),0.5171852,Refactor cloud dispatch and update to new API (#16456),  Use getter instead of python property for the dataloaders   Fix lint   Update trainer.py ,0
7794,accept dist sampler classes,0.62634814,Automatic distributed samplers,  adds ddp2 option where on each node a single  process  uses all gpus   added ddp2  test   added ddp2 docs   Update Distributed training.md   delete ref to old update_training_log_metrics   delete ref to old update_training_log_metrics   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   banana pancakes   debug   debug   debug   debug   debug   debug   debug   debug   cheesecake ,0
7795,0.9.0 (#3081),0.70977116,"    version=""0.0.1"",",,1
7796,update header links in docs (#1184),0.62922347,Deprecated tags_csv in favor of hparams_file (#1271),,0
7797,changes to seed for tests,0.60048914,Check environ before selecting a seed to prevent warning message (#4743),,0
7798,docs: deploy all (#16951),0.6475462,"docs for all Metrics (#2184, #2209)",,0
7799,Clarification on omegaconf's interpolation support in LightningCLI (#12571),0.70336425,LightningCLI additions:,  Moved grad_norm tracking code to __run_tng_batch + added norms to tqdm_metrics   Update trainer.py   Update trainer.py   Update trainer.py   Update trainer.py ,1
7800,expanded apex install (#255),0.53587186,apex plugin (#3502), Create underlying loggers lazily  This avoids creating duplicate experiments or run in multi-node DDP.   Save hyperparameters automatically   Update docs for snapshotting hyperparams   Fix test tube   Fix test tube pickling ,0
7801,Add missing f prefix to f-strings (#12869),0.533027,Deprecated prefix argument in ModelCheckpoint (#4765)," Hacky fix for mlflow logger  It dies when ""created_at"" is logged  Log warning",0
7802,Clarify fast_dev_run docs (#12751),0.64852375,Tuner algorithms will be skipped if fast_dev_run=True (#3903),,0
7803,removed logging,0.8556251,Removed LoggerStages (#5673),,1
7804,fix to avoid common hook warning if no hook is overridden (#12131),0.79555523,- Fixed to avoid common hook warning if no hook is overridden ([#12131](https://github.com/PyTorchLightning/pytorch-lightning/pull/12131)),,1
7805,Fixed the load_from_checkpoint path detected as URL bug (#2244),0.7477219,Fix for load_from_checkpoint() not working with absolute path on Windows (#2294),,1
7806,Fix load_from_checkpoint docs (#978),0.69695675,Fix for load_from_checkpoint() not working with absolute path on Windows (#2294),,0
7807,Update Changelog for v1.5.1 (#10439),0.78793687,Complete changelog,,1
7808,Add LightningCLI(auto_registry) (#12108),0.84716916,LightningCLI(auto_registry=True),  Implement generic loggers for experiment tracking   Add tests for loggers   Get model tests passing   Test and fix logger pickling   Expand pickle test and fix bug   Missed exp -> logger conversion   Remove commented code   Add docstrings   Update logging docs   Add mlflow to test requirements   Make linter happy   Fix mlflow timestamp   Update Logging.md   Update test_models.py   Update test_models.py   Update test_models.py   Update properties.md   Fix tests   Line length ,1
7809,Handle KeyboardInterrupt during training (#2134),0.8399186,"Support graceful training cleanup after Keyboard Interrupt (#856, #1019)",,1
7810,Initialize loggers only once (#270),0.7493298,for logger in loggers:,,1
7811,Update hooks.md,0.66173476,Fixing critical bugs in newly added hooks and hparams assignment.,  always calls the lr scheduler  with epoch nb   added docs for cluster grid search   added docs for cluster grid search   undo test changes   undo test changes ,0
7812,CI: Reuse check schema (#14469),0.5801735,Refactored training loop (#2336),,0
7813,"docs: configure_sync_batchnorm, amp, readme python/conda badge (#3328)",0.54133356,Ensure the existence of DDPPlugin._sync_dir in reconciliate_processes (#8939),,0
7814,Add test for job_id (#10774),0.593391,    # use the last 4 numbers in the job id as the id,,0
7815,update mergify rules (#8443),0.60397935,Deprecation warning (#3844),,0
7816,Avoid changing the current cudnn.benchmark value (#13154),0.6730722,Prevent modification of torch.backends.cudnn.benchmark when Trainer(benchmark=...) is not set (#13154),,0
7817,general docs improvements (#12747),0.7978328,Docs improvements,  enables samplers which dont need set epoch   added docs for single gpu ddp   added docs for single gpu ddp   added docs for cluster grid search   added docs for cluster grid search   added docs for cluster grid search   added docs for cluster grid search   added docs for cluster grid search   added docs for cluster grid search   added docs for cluster grid search   added docs for cluster grid search   added docs for cluster grid search   added docs for cluster grid search   added docs for cluster grid search   added docs for cluster grid search   added docs for cluster grid search   added docs for cluster grid search   added docs for cluster grid search   added docs for cluster grid search   added docs for cluster grid search   added docs for cluster grid search   added docs for cluster grid search   added docs for cluster grid search ,1
7818,release v0.2,0.7449434,0.4.0,  added ignore warnings module   added ignore warnings module   Fixes #249   Update ignored_warnings.py ,1
7819,Optimize required checks when Lite tests are modified (#15232),0.66041017,Generalized Optimizer validation to accommodate both FSDP 1.x and 2.x (#16733),,0
7820,Fix sample code of LightningModule DataLoaders (#3360),0.7583219,Deprecated on_{train/val/test/predict}_dataloader() from LightningModule and LightningDataModule (#9098),,1
7821,enabled manual returns (#4089),0.98265755,Enabled manual returns (#4089),,1
7822,Merge pull request #1473 from aiyolo/patch-3,0.5799974,"merge backends (#3476, #3477, #3478, #3480, #3482)",,0
7823,move deprecation test to correct 1.6 test file (#8446),0.63276666,Deprecated the TestTubeLogger (#9065),,0
7824,make evaluate private (#1260),0.7801448,Made evaluate method private >> Trainer._evaluate(...). (#1260),,1
7825,Remove unused AcceleratorConnector argument (#12686),0.7337949,- Removed `AcceleratorConnector.num_ipus` property ([#12386](https://github.com/PyTorchLightning/pytorch-lightning/pull/12386)), data_batch → batch batch_i → batch_idx dataloader_i → dataloader_idx tng → training training_dataloader → train_dataloader add_log_row_interval → row_log_interval gradient_clip → gradient_clip_val prog → progress tqdm_dic → tqdm_dict,1
7826,Test metric_attribute for different children module structures (#8675),0.5376594,Module hook on_sanity_check_start and loading load_from_metrics,  Update Training Loop.md   Update index.md   Update README.md   Update Training Loop.md   Update Training Loop.md ,0
7827,Fix exception when creating a multiprocessing Pool after importing Lightning (#15292),0.8622383,- Fixed an exception that would occur when creating a `multiprocessing.Pool` after importing Lightning ([#15292](https://github.com/Lightning-AI/lightning/pull/15292)), Added missing parameters  added missing distributed_backend parameter and added the parameter to step 4 Init Trainer.  Update single_gpu_node_dp_template.py,1
7828,Remove metric tracking from dev debugger (#7759),0.7979783,Remove MetricsHolder (#7909)," changed hard coded paramater, and moved it to parent_parser  ```python # ------------------------ # 4 INIT TRAINER # ------------------------ trainer = Trainer(     experiment=exp,     checkpoint_callback=checkpoint,     early_stop_callback=early_stop,     gpus=hparams.gpus,     distributed_backend=hparams.dist_bak_end )   parent_parser.add_argument('--dist_bak_end', type=str, default='ddp',                             help='When using multiple GPUs set Trainer(distributed_backend=dp) (or ddp)')  ```  Update single_gpu_node_ddp_template.py",1
7829,fixing docs (#2227),0.61457616,Docs improvements,,0
7830,rename LICENSE,0.5274986,Renames model steps (#1051),,0
7831,Add amp_scaling_state (apex) migration (#16161),0.6065662,- Added migration logic to warn about checkpoints with apex AMP state ([#16161](https://github.com/Lightning-AI/lightning/pull/16161)),,0
7832,Fault Tolerant Manual: Add support for DDP (#10638),0.72847545,- Fault Tolerant Manual,,1
7833,remove executable bit on source files (#5929),0.53075385,Remove deprecated distributed_backend from Trainer (#10017),,0
7834,Integrate global step with progress tracking (#11805),0.82104856,Progress tracking,,1
7835,remove docs (#5287),0.62880284,Remove MetricsHolder (#7909),,0
7836,simple examples instead of skips (#17482),0.50079066,Support for arbitrary iterables (#16726),,0
7837,release v0.3.6.4,0.80686486,0.4.0,,1
7838,Fix the case where logger=None is passed to Trainer (#12249),0.7975563,- Fixed the case where `logger=None` is passed to the Trainer ([#12249](https://github.com/PyTorchLightning/pytorch-lightning/pull/12249)),  load from metrics defaults to CPU   load from metrics defaults to CPU   load from metrics defaults to CPU ,1
7839,Use default_root_dir as the log_dir with LoggerCollections (#8187),0.95438725,Return the default_root_dir as the log_dir when the logger is a LoggerCollection (#8187),,1
7840,Fix: skip importing DistributedOptimizer for Windows (#10071),0.58325845,from setuptools import setup,,0
7841,Reset datafetcher references in teardown (#9387),0.67001164,refactored dataloader process hook (#3139),,0
7842,changed path (#1031),0.7523092,Changed Checkpoint path parameter from filepath to dirpath (#1016),,1
7843,Update governance docs (#15479),0.9107124,Updated governance docs,,1
7844,"[feat] Add {,load_}state_dict to ResultCollection 1/n (#7948)",0.6901151,"Add {,load_}state_dict to the progress tracking dataclasses (#8140)",,0
7845,Update version to 1.7.0 for release (#13977),0.702415,"    version=""0.0.1"",",  added load on CPU first   added load on CPU first   added load on CPU first   added load on CPU first   added load on CPU first   added load on CPU first   added load on CPU first   added load on CPU first   added load on CPU first   added load on CPU first   added load on CPU first   added load on CPU first   added load on CPU first   added load on CPU first   added load on CPU first   added load on CPU first   added load on CPU first   added load on CPU first   added load on CPU first   added load on CPU first   added load on CPU first   added load on CPU first   added load on CPU first   added load on CPU first   added load on CPU first   added load on CPU first   added load on CPU first   added print logs   added print logs   changed close order   changed close order ,1
7846,Loop Refactor 2/N - Remove Old Training Loop (#7985),0.80656874,Refactored training loop (#2336),,1
7847,[req] Set min version for skimage for tests (#4598),0.57924294,Deprecated the TestTubeLogger (#9065),,0
7848,Fix imports & issues in lightning optimizer refactor merge,0.6948124,Check LightningOptimizer doesn't delete optimizer hooks (#6305), Fix incorrect warning for DistributedSampler.  Check whether dataloader.sampler is an instance of DistributedSampler instead of checking the dataloader.   Update trainer.py   merged ,0
7849,"Update hydra-core requirement from <=1.1.*,>=1.0.5 to >=1.0.5,<1.3.0 in /requirements (#13309)",0.5360842,Temporarily removed support for Hydra multi-run (#15737),,0
7850,PSNR metric (#2483),0.6808908,Updated SSIM metric (#4566)(#4656),,0
7851,Fix to print scaler value in progress bar (#4053),0.70126104,Reset epoch progress with batch size scaler (#13846),,1
7852,Remove the legacy get_deprecated_arg_names (#14415),0.74752307,- Removed the legacy and unused `Trainer.get_deprecated_arg_names()` ([#14415](https://github.com/Lightning-AI/lightning/pull/14415)),  enable single gpu per node   enable single gpu per node   enable single gpu per node   enable single gpu per node   enable single gpu per node   enable single gpu per node ,1
7853,added UserWarnings if max_epochs not set in the Trainer class (#10700),0.7198419,trainer = pl.Trainer(max_epochs=2),,1
7854,clean AMP logic (#5994),0.59115887,training AMP scaling refactor (#3135),,0
7855,New fabric parity tests (#16899),0.61703587,Fabric,,0
7856,update chlog after 1.8.6 (#16174),0.6607846,0.4.0,  added simple cluster template   added simple cluster template   added simple cluster template   added simple cluster template   added simple cluster template   added simple cluster template   added simple cluster template   added simple cluster template   added simple cluster template   added simple cluster template   sets correct backend for possible combinations of gpu inputs   sets correct backend for possible combinations of gpu inputs   simple slurm example   simple slurm example   simple slurm example ,0
7857,Deprecate prepare_data_per_node flag on Trainer and set it as a property for DataHooks (#8958),0.86597127,"Deprecated prepare_data_per_node flag on Trainer and set it as a property of DataHooks, accessible in the LightningModule and LightningDataModule (#8958)",,1
7858,CI: signal lai build (#15871),0.55118287,Graphcore IPU devices,  added nvidia flag set   added nvidia flag set   added nvidia flag set   added nvidia flag set   added nvidia flag set   added nvidia flag set   added nvidia flag set   added nvidia flag set   added simple cluster template   sets correct backend for possible combinations of gpu inputs   sets correct backend for possible combinations of gpu inputs   sets correct backend for possible combinations of gpu inputs   sets correct backend for possible combinations of gpu inputs   sets correct backend for possible combinations of gpu inputs   sets correct backend for possible combinations of gpu inputs   sets correct backend for possible combinations of gpu inputs   sets correct backend for possible combinations of gpu inputs   sets correct backend for possible combinations of gpu inputs   sets correct backend for possible combinations of gpu inputs   sets correct backend for possible combinations of gpu inputs   sets correct backend for possible combinations of gpu inputs   sets correct backend for possible combinations of gpu inputs   sets correct backend for possible combinations of gpu inputs   sets correct backend for possible combinations of gpu inputs   sets correct backend for possible combinations of gpu inputs ,0
7859,fix typo in early stopping (#260),0.5412115,Avoid redundant callback restore warning while tuning (#13026),,0
7860,added grad hook,0.6681527,New Hooks,This seems more useful for debugging.,0
7861,[App] Fix idle timeout e2e (#16786),0.52877545,Removed deprecated early_stop_callback (#3982),,0
7862,callback docs (#2794),0.68764997,Read more about callback entry points in our docs.,  added docs. removed options. added weights_save option   removed old restore   cleaned up save path   cleaned up save path   flake8 ,0
7863,Update changelog after 1.7.2. release (#14251),0.72110254,Full Changelog,,1
7864,Update changelog after v1.7.0 release (#13982),0.7095354,Full Changelog,  split trainer mixins   Update multi_node_cluster_template.py   Update single_cpu_template.py   Update single_gpu_node_16bit_template.py   Update single_gpu_node_ddp_template.py   Update single_gpu_node_dp_template.py   Update trainer_cpu_template.py   Update trainer_io.py   split trainer mixins   Update multi_node_cluster_template.py   deconflicted   deconflicted   deconflicted ,1
7865,remake nvidia docker (#6686),0.68378294,nvidia/apex deprecation (#16039),  added slurm signal handler   added restore weight functions   set slurm signal handling inside process   added resubmit docs   added resubmit docs   fixed missing param   Update trainer.py   fixed missing param   fixed missing param   debugging tests   debugging tests   debugging tests   debugging tests   debugging tests   debugging tests   debugging tests ,0
7866,Add deprecation path for renamed training type plugins (#11227),0.6708471,Removed Plugin in base_plugin.py in favor of accessing TrainingTypePlugin and PrecisionPlugin directly instead (#9066),  add PR template   Update PULL_REQUEST_TEMPLATE.md ,0
7867,ci: upload only with release (#16194),0.685531,Enabled cp (upload) at project level (#16631),  Pass outputs from all dataloaders to test_end and validation_end   Update tests   Update docs   Update trainer.py   Update test_models.py ,0
7868,Fixes CPU and hanging GPU crash (#2118),0.6912011,Fix hanging in DDP HPC accelerators (#5157),  extend pip install info   Update README.md   Update README.md ,0
7869,Fix the condition for calling update_learning_rates (#7032),0.63681704,- Fixed error handling in learning rate finder when not enough data points are available to give a good suggestion ([#13845](https://github.com/Lightning-AI/lightning/pull/13845)),,0
7870,Update nvidia gpg key to fix nightly docker builds (#12930),0.5397384,Key updates,,0
7871,update codecov badge (#13852),0.5328968,Changed progress_bar_refresh_rate trainer flag to disable progress bar when setting to 0. (#1108),  add CI macOS   add CI Windows   update CI   drop Win   update CI   update CI ,0
7872,Fix LightningDataModule hparams parsing (#12806),0.77095145,Removed deprecated LightningModule hparams setter (#6207),,1
7873,support for native amp (#1561),0.6883092,Trainer now raises an exception when requesting amp_level with native amp_backend (#9755),,0
7874,Future 3/n: docs adjustment (#13299),0.65727305,[1.3.0] - 2021-05-06, Allow to deactivate GPU memory logging in Trainer  Adds the flag log_gpu_memory to Trainer to deactivate logging of GPU memory utilization. On some servers logging the GPU memory usage can significantly slow down training.   Update Logging.md   Update trainer.py ,0
7875,freeze DALI (#4922),0.62875247,Setup: added requirement freeze for the next major version (#14480),,0
7876,ref: inner train loop (intermediate step) 12/n (#3371),0.7883334,"inner train loop (#3359, #3361, #3362, #3363, #3365, #3366, #3367, #3368, #3369, #3370, #3371, #3372, #3373, #3374, #3375, #3376, #3385, #3388, #3397)",,1
7877,Remove metrics references from docs (#10567),0.7300526,Removed deprecated metrics (#8586),"  Expectopatronum implement #89 (#182)   rename validate -> evaluate; implement test logic; allow multiple test_loaders   add test_step and test_end to LightningModule   add in_test_mode to pretraining to implement case 2 (test pretrained model)   fix code style issues   LightningTestModel: add optional second test set, implement test_step and test_end   implemented test for multiple test_dataloaders; fixed typo   add two test cases for #89   add documentation for test_step, test_end; fix computation of loss in validation_step example   Update trainer.py   Update trainer.py   Update trainer.py   Update trainer.py   Update trainer.py   Update trainer.py   Added proper dp ddp routing calls for test mode   Update trainer.py   Update test_models.py   Update trainer.py   Update trainer.py   Update override_data_parallel.py   Update test_models.py   Update test_models.py   Update trainer.py   Update trainer.py   Update trainer.py   Update test_models.py   Update test_models.py   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   Update trainer.py   Update override_data_parallel.py   Update debug.py   Update lm_test_module.py   Update test_models.py   release v0.4.8   Update README.md   add training loop docs   testing loop docs   testing loop docs   Convert __dataloader to _dataloader   This will let inherited classes use it   Factor common test model setup into base class   Specialized test modules inherit from LightningTestModelBase   Fix __is_overriden so that it works with more complicated inheritance   Use mixins to add functionality to test models   Fix test with no val_dataloader   Remove unused imports   Get rid of wild card import   Update trainer.py   Update lm_test_module.py ",1
7878,Implement log_graph for CometLogger. (#5295),0.78325105,Using .comet.config file for CometLogger (#1913),,1
7879,[bugfix] Resolve Kineto Profiler for Conda (#7376),0.606671,Precision Plugins (#5718),,0
7880,Do not override the logged epoch in logged_metrics (#7982),0.93103576,Do not override the existing epoch value in logged_metrics when already logged by the user (#7982),,1
7881,Typing for accelerators and plugins (#7022),0.75206155,Refactored Accelerators and Plugins (#5743),,1
7882,Suppress Warning in PredictionEpochLoop (#11189),0.70986915,"Marked several methods in PredictionLoop as protected: on_predict_start, on_predict_epoch_end, on_predict_end, on_predict_model_eval (#9516)",,1
7883,save apex scaler states (#2828),0.6587565,Reset epoch progress with batch size scaler (#13846),"  rename validate -> evaluate; implement test logic; allow multiple test_loaders   add test_step and test_end to LightningModule   add in_test_mode to pretraining to implement case 2 (test pretrained model)   fix code style issues   LightningTestModel: add optional second test set, implement test_step and test_end   implemented test for multiple test_dataloaders; fixed typo   add two test cases for #89   add documentation for test_step, test_end; fix computation of loss in validation_step example   Update trainer.py   Update trainer.py   Update trainer.py   Update trainer.py   Update trainer.py   Update trainer.py   Added proper dp ddp routing calls for test mode   Update trainer.py   Update test_models.py   Update trainer.py   Update trainer.py   Update override_data_parallel.py   Update test_models.py   Update test_models.py   Update trainer.py   Update trainer.py   Update trainer.py   Update test_models.py   Update test_models.py   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   debug   Update trainer.py   Update override_data_parallel.py   Update debug.py   Update lm_test_module.py   Update test_models.py ",0
7884,Enable DDP Plugin to pass through args to LightningDistributedDataParallel (#4382),0.7370018,"Deprecated add_to_queue, get_from_queue from LightningModule in favor of corresponding methods in the DDPSpawnPlugin (#9118)","  Gradient accumulation callback   little test case   typo   import fix   method name fix   fix epochs indexing from 1   better code style   code style fix v2 :/   change interface   fix Trainre new api in tests   trainer api bug fix   new raising error, new update method   extentions tests   a little better tests   typo fix   flack8 better   using scheduler for int and dict   typo   firs epoch bug fix   test update   empty dict exception   floats check   codestyle fix   grad counting test   someday, i will install normal linter   add more checks   Update test_models.py   Update test_models.py   Update test_models.py   Update test_models.py   Update test_models.py   Update test_models.py   Update test_models.py ",1
7885,Use Set operations in Environment.detect (#10673),0.5605929,setup(,  feat(val_sanity): enable skipping validation sanity when self.nb_sanity_val_steps is 0   docs: elaborate on skipping ,0
7886,"Update torch requirement from <=1.12.0,>=1.9.* to >=1.9.0.a,<1.13.0 in /requirements (#14088)",0.7413566,"Set PyTorch 1.4 as min requirements, also for testing and examples torchvision>=0.5 and torchtext>=0.5 (#5418)",,1
7887,deprecate hpc_load() and integrate it with restore() (#7955),0.6931807,Deprecated the use of CheckpointConnector.hpc_load() in favor of CheckpointConnector.restore() (#7652),,0
7888,Speedup of metric tests (#4122),0.5833797,"many speed improvements (how we move data, adjusted some flags & PL now adds 300ms overhead per epoch only!)",,0
7889,feat(wandb): offset logging step when resuming (#5050),0.6765288,Un-balanced logging properly supported (#5119),,0
7890,argparse: Add use_argument_group=True (#6088),0.682497,Trainer.add_argparse_args classmethod fixed. Now it adds a type for the arguments (#1147),,0
7891,update chnagelog (#1091),0.5282993,Update the Lightning App docs (#13537),"A couple code blocks used ""{.python}"" instead of just ""python"" for the syntax highlighting, which doesn't render properly in GitHub markdown.",0
7892,Update methods.md (#507),0.5980965,Updated mlflow with using resolve_tags (#6746),  cleaned up progbar   cleaned up progbar   cleaned up progbar   cleaned up progbar   cleaned up progbar   cleaned up progbar   cleaned up progbar   updated base files   updated base files   updated base files   updated base files   updated base files   updated base files   updated base files   updated base files   updated base files   updated base files   updated base files   updated base files   updated base files   updated base files   updated base files   updated base files   updated base files   updated base files   updated base files   updated base files   updated base files   flake 8 ,0
7893,Mark internal Lite APIs as protected (#15307),0.65397966,"- Marked the `{Accelerator,Signal,Callback,Checkpoint,Data,Logger}Connector` classes as protected ([#17008](https://github.com/Lightning-AI/lightning/pull/17008))",,0
7894,[App] Remove outdated warning from cloud requirements (#16140),0.65257925,Refactor cloud dispatch and update to new API (#16456),,0
7895,created minor doc fixes [ci skip] (#3958),0.6324561,Docs improvements,"A solution for https://github.com/williamFalcon/pytorch-lightning/issues/142. Since hasattr ""calls getattr(object, name) and to see whether it raises an AttributeError or not"", I replaced it with a single call to getattr. See also https://stackoverflow.com/questions/24971061/python-hasattr-vs-getattr",0
7896,early epoch stopping,0.6660054,    for epoch in range(5):,Set all to be 0 instead of  None.  Cleaned up val batch,0
7897,"Remove on_train_batch_{start,end}(dataloader_idx=...) (#12977)",0.74201703,  * Removed the `LoggerConnector.on_train_split_start` method,,1
7898,added image-gradients (#4763)  [1/2] (#5056),0.559407,"    deserializers = {""x"": Image(224, 224).deserialize}",,0
7899,Bugfix/update trainer properties (#3975),0.6465367,| Attribute Trainer.tuning                                    | 1.10             | No longer supported         |,,0
7900,Fix notebook links (#8089),0.5723726,- Fixed a bug where the upload files endpoint would raise an error when running locally ([#14924](https://github.com/Lightning-AI/lightning/pull/14924)),,0
7901,fix init nan for checkpointing (#3863),0.6280543,Save a checkpoint to restore the state on exception (opt-in) (#8362),A small change to the CoolModel example. Now test_dataloader returns the MNIST test dataset.,0
7902,Do not trigger PyTorch GPU tests on Lite test changes (#15348),0.6651067,Dropped official support/testing for PyTorch <1.6 (#8288),  Separate condition list/tuple case into separated cases   Add test for tuple of tensor list and list of tensor dict   Update test_models.py ,0
7903,Install project specific dependencies (#17376),0.68631893,Improved support for running apps when dependencies aren't installed (#15711),  fixes #154   Update trainer.py   Update trainer.py ,0
7904,update links in callback examples pointing to bolts (#10117),0.569536,- Removed deprecated callback hooks ([#14834](https://github.com/Lightning-AI/lightning/pull/14834)),,0
7905,[App] Enable Python Server and Gradio Serve to run on accelerated device such as GPU CUDA / MPS (#15813),0.659932,"- `accelerator=""gpu""` now automatically selects an available GPU backend (CUDA and MPS currently) ([#13642](https://github.com/Lightning-AI/lightning/pull/13642))",  bug fix for #138   split if for readability ,0
7906,Update recommendation on dataloader_idx (#10318),0.6929966,Changed warnings and recommendations for dataloaders in ddp_spawn (#6762),See discussion in https://github.com/williamFalcon/pytorch-lightning/issues/139.,0
7907,Simplify predict attribute (#13635),0.6090653,Simplify optimization Logic (#4984),,0
7908,Change default value of the max_steps Trainer argument from None to -1 (#9460),0.9881259,Changed default value of the max_steps Trainer argument from None to -1 (#9460),,1
7909,Update Changelog with weekly releases (#9056),0.66180587,Complete changelog,,0
7910,Add support for functions (#15098),0.59479547,Implemented ready for components (#16129),,0
7911,Refactor: drop LoggerStages (#5673),0.8799391,Removed LoggerStages (#5673),,1
7912,Standalone Lite: Single Device TPU Strategy (#14663),0.6721859,Enabled manual optimization for TPUs (#8458),,0
7913,moved port name,0.5938726,"# if user gave a port number, use that one instead",,0
7914,Replace iteration_count and other index attributes in the loops with progress dataclasses (#8477),0.9605678,iteration_count and other index attributes in the loops has been replaced with progress dataclasses (#8477),,1
7915,fix missing call to untoggle_optimizer when accumulating gradients  (#8284),0.6767237,Moved the gradient unscaling in NativeMixedPrecisionPlugin from pre_optimizer_step to post_backward (#9606),,0
7916,Add deprecation path for the old Callback module (#13031),0.6279842,Updated references to self.forward() to instead use the __call__ interface. (#1211),,0
7917,fix error when logging to progress bar with reserved name (#5620),0.7168505,Logging the loss to the progress bar (#16192),,1
7918,prevent Travis caching (#590),0.53418803,Prevent modification of torch.backends.cudnn.benchmark when Trainer(benchmark=...) is not set (#13154),This seems to be a typo. Throws TypeError: 'Tensor' object is not callable.,0
7919,moves configure ddp to each backend (#3924),1.0000001,moves configure ddp to each backend (#3924),  tensorboarX to tensorboardX   Update properties.md ,1
7920,"Skip strategy=ddp_spawn, accelerator=cpu, python>=3.9 tests (#10550)",0.620934,Skipped testing with PyTorch 1.7 and Python 3.9 on Ubuntu (#11217),,0
7921,Run CI helpers' doctests in a workflow (#14498),0.61894584,"For a full tutorial and running example, visit our docs. TODO: add to docs",Fix for the bug mentioned in https://github.com/williamFalcon/pytorch-lightning/issues/139,0
7922,Fix SignalConnector._has_already_handler check for callable type (#10483),0.6032731,Replace MisconfigurationException with warning in ModelCheckpoint Callback (#4560),,0
7923,Add trainer.predict(ckpt_path) (#7430),0.7936412,trainer.ckpt_path = None,,1
7924,Fix TPU test CI (#14926),0.73312795,Updated logic for checking TPUs availability (#6767),  Update Training Loop.md   update docs and elaborate on the correlation ,1
7925,clean imports (#2867),0.7088029,import argparse,,1
7926,Introduce new precision layout in fabric (#16767),0.5849997,Mixed precision overhaul (#16783),  fix typo   fix typo   fix typo   fix list ,0
7927,Check if optimizer supports closure (#4981),1.0,Check if optimizer supports closure (#4981),,1
7928,Added experiment_id to NeptuneLogger (#3462),0.7244357,Changed to the NeptuneLogger (#16761):,,1
7929,Example of K-fold Cross Validation with Fabric (#16909),0.53366613,fabric.backward(loss),,0
7930,Move implementation of LightningModule.add_to_queue/get_from_queue (#10936),0.7989696,"Deprecated add_to_queue, get_from_queue from LightningModule in favor of corresponding methods in the DDPSpawnPlugin (#9118)",,1
7931,update docs (#4739),0.6714041,Update the Lightning App docs (#13537),,0
7932,Skip horovod 0.24.0 only (#12248),0.71965843,"refactored Horovod backend (#3121, #3122)",,1
7933,[Docs revamp 2/N] New doc for managing data (#8034),0.7044791,Docs improvements,,1
7934,Add check for windows to plugin,0.5910278,CheckpointIO Plugins,,0
7935,New section for changelog [ci skip] (#4279),0.6961466,Full Changelog,,0
7936,[FSDP] Adding Native FSDP Strategy (#12447),0.8625368,Native FSDP implementation,,1
7937,added option and flag,0.61526823,"In the previous release, we added shorthand notation support for registered components. In this release, we added a flag to automatically register all available components:",,0
7938,Update to_disk to use fsspec for remote file support (#3930),0.6310873,Used fsspec instead of gfile for all IO (#3320),,0
7939,added global rank var name,0.584983,Enabled prepare_data from correct processes - clarify local vs global rank (#2166),,0
7940,Add a migration for the dataloader loops (#17125),0.67618084,"Connect the progress tracking dataclasses to the loops (#8244, #8362)",  added smarter optimizer options   added smarter optimizer options   added smarter optimizer options tests   added smarter optimizer options tests   added smarter optimizer options tests   added smarter optimizer options tests   added smarter optimizer options tests   added smarter optimizer options tests   added smarter optimizer options tests   added single gpu data transfer recursive   added single gpu data transfer recursive   added single gpu data transfer recursive   added single gpu data transfer recursive   added single gpu data transfer recursive ,0
7941,remove source-lit docs (#15525),0.5904628,Removed the deprecated LightningDeepSpeedModule (#16041),  fix appveyor   fix appveyor ,0
7942,Rename conflicting test directories (#4451),0.5802796,Deprecated Profiler(output_filename) in favor of dirpath and filename (#6621),  added tests   added single gpu data transfer recursive   added single gpu data transfer recursive   added single gpu data transfer recursive   added single gpu data transfer recursive   added single gpu data transfer recursive   added single gpu data transfer recursive ,0
